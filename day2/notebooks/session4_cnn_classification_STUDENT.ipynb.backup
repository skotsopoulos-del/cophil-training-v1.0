{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf617c0",
   "metadata": {},
   "source": [
    "# Session 4: CNN Hands-On Lab - EuroSAT Classification\n",
    "\n",
    "## Building CNNs with TensorFlow/Keras\n",
    "\n",
    "**Duration:** 90 minutes | **Difficulty:** Intermediate  \n",
    "**Dataset:** EuroSAT (27,000 Sentinel-2 images, 10 classes)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "By the end of this lab, you will:\n",
    "\n",
    "1. ‚úÖ Build a CNN from scratch using TensorFlow/Keras\n",
    "2. ‚úÖ Train on real satellite imagery (EuroSAT dataset)\n",
    "3. ‚úÖ Achieve >90% accuracy on land use classification\n",
    "4. ‚úÖ Evaluate model performance comprehensively\n",
    "5. ‚úÖ Understand training dynamics and hyperparameters\n",
    "6. ‚úÖ Apply data augmentation for better generalization\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Lab Structure\n",
    "\n",
    "| Step | Activity | Duration |\n",
    "|------|----------|----------|\n",
    "| **1** | Environment Setup & GPU Check | 5 min |\n",
    "| **2** | Dataset Download & Exploration | 15 min |\n",
    "| **3** | Data Preprocessing & Augmentation | 15 min |\n",
    "| **4** | Build CNN Architecture | 20 min |\n",
    "| **5** | Training & Monitoring | 20 min |\n",
    "| **6** | Evaluation & Analysis | 15 min |\n",
    "\n",
    "---\n",
    "\n",
    "## üåç About EuroSAT Dataset\n",
    "\n",
    "**What:** Benchmark dataset for satellite image classification  \n",
    "**Source:** Sentinel-2 RGB and multi-spectral  \n",
    "**Images:** 27,000 labeled patches (64√ó64 pixels)  \n",
    "**Classes:** 10 land use/land cover types  \n",
    "**Purpose:** Standardized comparison of classification methods\n",
    "\n",
    "**Classes:**\n",
    "1. Annual Crop\n",
    "2. Forest\n",
    "3. Herbaceous Vegetation\n",
    "4. Highway\n",
    "5. Industrial\n",
    "6. Pasture\n",
    "7. Permanent Crop\n",
    "8. Residential\n",
    "9. River\n",
    "10. SeaLake\n",
    "\n",
    "---\n",
    "\n",
    "Let's build your first CNN! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ed9f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 1: Environment Setup (5 minutes)\n",
    "\n",
    "First, let's import libraries and check if GPU is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# scikit-learn for metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(f\"‚úì TensorFlow version: {tf.__version__}\")\n",
    "print(f\"‚úì Keras version: {keras.__version__}\")\n",
    "print(f\"‚úì NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22ccd8",
   "metadata": {},
   "source": [
    "### Check GPU Availability\n",
    "\n",
    "GPUs dramatically speed up training. Let's check if one is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103ca3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(f\"\\n‚úì GPU(s) Available: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu.name}\")\n",
    "    \n",
    "    # Enable memory growth (prevents TensorFlow from allocating all GPU memory)\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"\\n‚úì GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No GPU found - training will use CPU (slower)\")\n",
    "    print(\"   Consider using Google Colab with GPU runtime\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"\\n‚úì Environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f8ec4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 2: Dataset Download & Exploration (15 minutes)\n",
    "\n",
    "We'll download the EuroSAT RGB dataset from TensorFlow Datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Using TensorFlow Datasets (easiest)\n",
    "# If this doesn't work, we'll provide manual download instructions\n",
    "\n",
    "try:\n",
    "    import tensorflow_datasets as tfds\n",
    "    print(\"‚úì TensorFlow Datasets available\")\n",
    "    USE_TFDS = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  TensorFlow Datasets not installed\")\n",
    "    print(\"   Installing: pip install tensorflow-datasets\")\n",
    "    USE_TFDS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b497b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download EuroSAT dataset\n",
    "if USE_TFDS:\n",
    "    print(\"Downloading EuroSAT RGB dataset...\")\n",
    "    print(\"(This may take a few minutes on first run - ~90MB)\")\n",
    "    \n",
    "    # Load dataset\n",
    "    (ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "        'eurosat/rgb',\n",
    "        split=['train[:70%]', 'train[70%:85%]', 'train[85%:]'],\n",
    "        as_supervised=True,\n",
    "        with_info=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Dataset loaded successfully!\")\n",
    "    print(f\"  Total images: {ds_info.splits['train'].num_examples}\")\n",
    "    print(f\"  Train split: 70% ({ds_train.cardinality().numpy()} images)\")\n",
    "    print(f\"  Val split: 15% ({ds_val.cardinality().numpy()} images)\")\n",
    "    print(f\"  Test split: 15% ({ds_test.cardinality().numpy()} images)\")\n",
    "    \n",
    "    # Class names\n",
    "    class_names = ds_info.features['label'].names\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    print(f\"\\n  Classes ({num_classes}):\")\n",
    "    for i, name in enumerate(class_names):\n",
    "        print(f\"    {i}: {name}\")\n",
    "else:\n",
    "    print(\"Manual dataset download required\")\n",
    "    print(\"See: https://github.com/phelber/EuroSAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323ee76",
   "metadata": {},
   "source": [
    "### Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Take 20 samples\n",
    "sample_images = list(ds_train.take(20))\n",
    "\n",
    "for idx, (image, label) in enumerate(sample_images):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Display image\n",
    "    ax.imshow(image.numpy())\n",
    "    ax.set_title(f\"{class_names[label.numpy()]}\\n(Class {label.numpy()})\",\n",
    "                 fontsize=10, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('EuroSAT Sample Images', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Dataset looks good!\")\n",
    "print(\"  Notice the variety of land use patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e5261b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 3: Data Preprocessing & Augmentation (15 minutes)\n",
    "\n",
    "We need to:\n",
    "1. Normalize pixel values (0-255 ‚Üí 0-1)\n",
    "2. Batch the data for efficient training\n",
    "3. Apply data augmentation to prevent overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3734ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess(image, label):\n",
    "    \"\"\"\n",
    "    Normalize image to [0, 1] range\n",
    "    \"\"\"\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing\n",
    "ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_val = ds_val.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"‚úì Images normalized to [0, 1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe2f49",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Augmentation creates variations of training images to improve generalization.\n",
    "\n",
    "**Techniques we'll use:**\n",
    "- Random horizontal flips\n",
    "- Random vertical flips\n",
    "- Random rotations (90¬∞, 180¬∞, 270¬∞)\n",
    "- Random brightness adjustment\n",
    "\n",
    "**Why safe for satellite imagery:**\n",
    "- Land use patterns have no preferred orientation\n",
    "- Small brightness variations are realistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e52c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation layer\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.25),  # Up to 90 degrees\n",
    "    layers.RandomBrightness(0.1),  # ¬±10% brightness\n",
    "    layers.RandomContrast(0.1),\n",
    "], name='data_augmentation')\n",
    "\n",
    "# Augmentation function\n",
    "def augment(image, label):\n",
    "    image = data_augmentation(image, training=True)\n",
    "    return image, label\n",
    "\n",
    "# Apply augmentation to training data only\n",
    "ds_train_augmented = ds_train.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"‚úì Data augmentation configured\")\n",
    "print(\"  Augmentation applied to training data only\")\n",
    "print(\"  Validation and test data unchanged (for fair evaluation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a0f561",
   "metadata": {},
   "source": [
    "### Visualize Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original vs augmented\n",
    "sample_image, sample_label = next(iter(ds_train))\n",
    "\n",
    "# Convert label to integer for indexing\n",
    "label_idx = int(sample_label.numpy())\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(sample_image.numpy())\n",
    "axes[0, 0].set_title('Original', fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Augmented versions\n",
    "for idx in range(1, 8):\n",
    "    row = idx // 4\n",
    "    col = idx % 4\n",
    "    \n",
    "    augmented = data_augmentation(tf.expand_dims(sample_image, 0), training=True)[0]\n",
    "    axes[row, col].imshow(augmented.numpy())\n",
    "    axes[row, col].set_title(f'Augmented {idx}', fontweight='bold')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "# Fixed: Use label_idx instead of sample_label.numpy()\n",
    "plt.suptitle(f'Data Augmentation Examples\\nClass: {class_names[label_idx]}',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Augmentation creates realistic variations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9acb9",
   "metadata": {},
   "source": [
    "### Create Batched Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b0f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "# Batch and prefetch for performance\n",
    "ds_train_final = ds_train_augmented.shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "ds_val_final = ds_val.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "ds_test_final = ds_test.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"‚úì Datasets configured\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(list(ds_train_final))}\")\n",
    "print(f\"  Val batches: {len(list(ds_val_final))}\")\n",
    "print(f\"  Test batches: {len(list(ds_test_final))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d0d01",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 4: Build CNN Architecture (20 minutes)\n",
    "\n",
    "Now we'll design and build a CNN from scratch!\n",
    "\n",
    "## Architecture Design\n",
    "\n",
    "We'll create a **3-block CNN**:\n",
    "\n",
    "```\n",
    "Input (64√ó64√ó3)\n",
    "    ‚Üì\n",
    "Block 1: Conv(32) ‚Üí Conv(32) ‚Üí MaxPool ‚Üí Dropout\n",
    "    ‚Üì\n",
    "Block 2: Conv(64) ‚Üí Conv(64) ‚Üí MaxPool ‚Üí Dropout\n",
    "    ‚Üì\n",
    "Block 3: Conv(128) ‚Üí MaxPool ‚Üí Dropout\n",
    "    ‚Üì\n",
    "Flatten\n",
    "    ‚Üì\n",
    "Dense(256) ‚Üí Dropout\n",
    "    ‚Üì\n",
    "Output (10 classes)\n",
    "```\n",
    "\n",
    "**Design Principles:**\n",
    "- Start with 32 filters, double each block (32‚Üí64‚Üí128)\n",
    "- Use 3√ó3 convolutions (standard)\n",
    "- MaxPool after each block (reduce dimensions)\n",
    "- Dropout for regularization (prevent overfitting)\n",
    "- ReLU activation (all hidden layers)\n",
    "- Softmax activation (output layer)\n",
    "\n",
    "---\n",
    "\n",
    "## 4.1: Define the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "def build_cnn_model(input_shape=(64, 64, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    Build a 3-block CNN for EuroSAT classification\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_shape : tuple\n",
    "        Input image dimensions\n",
    "    num_classes : int\n",
    "        Number of output classes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Model\n",
    "        Compiled CNN model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # Input\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1_1'),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1_2'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "        layers.Dropout(0.25, name='dropout1'),\n",
    "        \n",
    "        # Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2_1'),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2_2'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "        layers.Dropout(0.25, name='dropout2'),\n",
    "        \n",
    "        # Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3_1'),\n",
    "        layers.MaxPooling2D((2, 2), name='pool3'),\n",
    "        layers.Dropout(0.25, name='dropout3'),\n",
    "        \n",
    "        # Classifier\n",
    "        layers.Flatten(name='flatten'),\n",
    "        layers.Dense(256, activation='relu', name='fc1'),\n",
    "        layers.Dropout(0.5, name='dropout4'),\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name='EuroSAT_CNN')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = build_cnn_model(input_shape=(64, 64, 3), num_classes=num_classes)\n",
    "\n",
    "print(\"‚úì CNN model created\")\n",
    "print(f\"  Architecture: 3-block CNN\")\n",
    "print(f\"  Input shape: (64, 64, 3)\")\n",
    "print(f\"  Output classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cbabbe",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model architecture\n",
    "model.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nüìä Total Parameters: {total_params:,}\")\n",
    "print(f\"   Trainable: {total_params:,}\")\n",
    "\n",
    "# Breakdown by layer type\n",
    "conv_params = sum([layer.count_params() for layer in model.layers if 'conv' in layer.name])\n",
    "dense_params = sum([layer.count_params() for layer in model.layers if 'dense' in layer.name or 'fc' in layer.name])\n",
    "\n",
    "print(f\"\\n   Convolutional layers: {conv_params:,}\")\n",
    "print(f\"   Dense layers: {dense_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc40fb",
   "metadata": {},
   "source": [
    "### Visualize Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd5a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model architecture\n",
    "keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='cnn_architecture.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',  # Top to bottom\n",
    "    dpi=96\n",
    ")\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image\n",
    "Image('cnn_architecture.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ce8c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.2: Compile the Model\n",
    "\n",
    "We need to configure:\n",
    "- **Loss function:** Sparse categorical crossentropy (for integer labels)\n",
    "- **Optimizer:** Adam (adaptive learning rate)\n",
    "- **Metrics:** Accuracy (percentage of correct predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc95006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"‚úì Model compiled\")\n",
    "print(f\"  Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"  Loss: Sparse Categorical Crossentropy\")\n",
    "print(f\"  Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c3aac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 5: Training & Monitoring (20 minutes)\n",
    "\n",
    "Time to train! We'll use callbacks to:\n",
    "- **Early Stopping:** Stop if validation loss doesn't improve\n",
    "- **Model Checkpoint:** Save best model weights\n",
    "- **Reduce LR:** Lower learning rate when plateauing\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1: Configure Callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    # Early stopping: stop if val_loss doesn't improve for 10 epochs\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Model checkpoint: save best model\n",
    "    ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate: divide by 2 if plateau\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úì Callbacks configured\")\n",
    "print(\"  - Early stopping (patience=10)\")\n",
    "print(\"  - Model checkpoint (save best)\")\n",
    "print(\"  - Reduce LR on plateau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21d689",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5.2: Train the Model\n",
    "\n",
    "‚è±Ô∏è **Training Time:** ~15-20 minutes on GPU, 1-2 hours on CPU\n",
    "\n",
    "We'll train for up to 50 epochs, but early stopping will likely halt around 20-30 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc249015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "history = model.fit(\n",
    "    ds_train_final,\n",
    "    validation_data=ds_val_final,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9224f7d3",
   "metadata": {},
   "source": [
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d24b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "print(f\"\\nüìä Final Training Metrics:\")\n",
    "print(f\"   Train Accuracy: {final_train_acc*100:.2f}%\")\n",
    "print(f\"   Train Loss: {final_train_loss:.4f}\")\n",
    "print(f\"   Val Accuracy: {final_val_acc*100:.2f}%\")\n",
    "print(f\"   Val Loss: {final_val_loss:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "gap = final_train_acc - final_val_acc\n",
    "if gap > 0.05:\n",
    "    print(f\"\\n‚ö†Ô∏è  Possible overfitting: {gap*100:.1f}% gap between train/val accuracy\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Good generalization: only {gap*100:.1f}% gap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c5a84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 6: Evaluation & Analysis (15 minutes)\n",
    "\n",
    "Now let's evaluate on the test set and analyze results.\n",
    "\n",
    "---\n",
    "\n",
    "## 6.1: Test Set Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(ds_test_final, verbose=0)\n",
    "\n",
    "print(f\"\\nüéØ Test Set Results:\")\n",
    "print(f\"   Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"   Loss: {test_loss:.4f}\")\n",
    "\n",
    "if test_accuracy > 0.90:\n",
    "    print(\"\\nüéâ Excellent! You've achieved >90% accuracy!\")\n",
    "elif test_accuracy > 0.85:\n",
    "    print(\"\\n‚úì Good! Above 85% is solid for first attempt\")\n",
    "else:\n",
    "    print(\"\\nüí° Room for improvement - try tuning hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e99fa4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.2: Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "print(\"Generating predictions for confusion matrix...\")\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in ds_test_final:\n",
    "    predictions = model.predict(images, verbose=0)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "print(f\"‚úì Predictions generated for {len(y_true)} test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f986614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'}, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Confusion Matrix - EuroSAT Test Set', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Confusion matrix generated\")\n",
    "print(\"  Diagonal = correct predictions\")\n",
    "print(\"  Off-diagonal = misclassifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b0df3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.3: Per-Class Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "\n",
    "# Convert to DataFrame for nice display\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nüìä Per-Class Performance:\")\n",
    "print(\"=\" * 80)\n",
    "print(report_df.round(3))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Highlight best and worst classes\n",
    "metrics_df = report_df[:-3]  # Exclude accuracy, macro avg, weighted avg\n",
    "best_class = metrics_df['f1-score'].idxmax()\n",
    "worst_class = metrics_df['f1-score'].idxmin()\n",
    "\n",
    "print(f\"\\n‚ú® Best performing class: {best_class} (F1={metrics_df.loc[best_class, 'f1-score']:.3f})\")\n",
    "print(f\"‚ö†Ô∏è  Worst performing class: {worst_class} (F1={metrics_df.loc[worst_class, 'f1-score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25168259",
   "metadata": {},
   "source": [
    "### Visualize Per-Class Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f29eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot per-class F1 scores\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "classes = metrics_df.index.tolist()\n",
    "f1_scores = metrics_df['f1-score'].values\n",
    "\n",
    "bars = ax.barh(classes, f1_scores, color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Color best and worst\n",
    "bars[classes.index(best_class)].set_color('green')\n",
    "bars[classes.index(worst_class)].set_color('orange')\n",
    "\n",
    "ax.set_xlabel('F1 Score', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Class', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Class F1 Scores', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 1.0)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, score) in enumerate(zip(bars, f1_scores)):\n",
    "    ax.text(score + 0.01, i, f'{score:.3f}', \n",
    "            va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2903c0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.4: Analyze Misclassifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75757b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "misclassified_indices = np.where(y_true != y_pred)[0]\n",
    "print(f\"\\nTotal misclassifications: {len(misclassified_indices)} / {len(y_true)}\")\n",
    "print(f\"Error rate: {len(misclassified_indices)/len(y_true)*100:.2f}%\")\n",
    "\n",
    "# Show some misclassified examples\n",
    "if len(misclassified_indices) > 0:\n",
    "    # Get first 12 misclassifications\n",
    "    sample_errors = misclassified_indices[:12]\n",
    "    \n",
    "    # Get corresponding images\n",
    "    test_images = []\n",
    "    for images, labels in ds_test_final:\n",
    "        test_images.extend([img.numpy() for img in images])\n",
    "    test_images = np.array(test_images)\n",
    "    \n",
    "    # Plot misclassifications\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, error_idx in enumerate(sample_errors):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Display image\n",
    "        ax.imshow(test_images[error_idx])\n",
    "        \n",
    "        true_label = class_names[y_true[error_idx]]\n",
    "        pred_label = class_names[y_pred[error_idx]]\n",
    "        \n",
    "        ax.set_title(f'True: {true_label}\\nPred: {pred_label}',\n",
    "                     fontsize=9, fontweight='bold',\n",
    "                     color='red')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Misclassifications', fontsize=14, fontweight='bold', color='red')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Misclassification Analysis:\")\n",
    "    print(\"  Look for patterns:\")\n",
    "    print(\"  - Similar-looking classes (e.g., forest types)\")\n",
    "    print(\"  - Ambiguous examples\")\n",
    "    print(\"  - Data augmentation might help\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9afef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéâ Lab Complete!\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've successfully:\n",
    "\n",
    "‚úÖ **Built** a CNN from scratch (3 blocks, ~300K parameters)  \n",
    "‚úÖ **Trained** on 27,000 Sentinel-2 images  \n",
    "‚úÖ **Achieved** 90%+ accuracy on EuroSAT dataset  \n",
    "‚úÖ **Evaluated** with confusion matrix and per-class metrics  \n",
    "‚úÖ **Analyzed** misclassifications  \n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### What Worked Well\n",
    "- **Architecture:** 3-block design with progressive filters (32‚Üí64‚Üí128)\n",
    "- **Regularization:** Dropout prevented overfitting\n",
    "- **Data Augmentation:** Improved generalization\n",
    "- **Callbacks:** Early stopping saved training time\n",
    "\n",
    "### Compared to Random Forest (Session 1-2)\n",
    "- **CNN:** 92-95% accuracy (automatic features)\n",
    "- **Random Forest:** 85-90% accuracy (manual features)\n",
    "- **Improvement:** +5-10% for critical applications\n",
    "\n",
    "### What's Next?\n",
    "- **Transfer Learning:** Use pre-trained ResNet (Session 4B)\n",
    "- **U-Net:** Pixel-level segmentation (Session 4C)\n",
    "- **Palawan:** Apply to real Philippine data\n",
    "- **Production:** Deploy model for monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises to Try\n",
    "\n",
    "### Easy\n",
    "1. Change batch size (16, 64) and observe effects\n",
    "2. Modify dropout rates (0.1, 0.5)\n",
    "3. Try different learning rates\n",
    "\n",
    "### Medium\n",
    "4. Add another convolutional block\n",
    "5. Use different augmentation techniques\n",
    "6. Experiment with optimizer (SGD vs Adam)\n",
    "\n",
    "### Advanced\n",
    "7. Implement learning rate scheduling\n",
    "8. Add batch normalization layers\n",
    "9. Try different architectures (VGG-style, ResNet-style)\n",
    "10. Fine-tune on Palawan-specific classes\n",
    "\n",
    "---\n",
    "\n",
    "## Save Your Work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a3f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "model.save('eurosat_cnn_final.h5')\n",
    "print(\"‚úì Model saved: eurosat_cnn_final.h5\")\n",
    "\n",
    "# Save training history\n",
    "import pickle\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(\"‚úì Training history saved: training_history.pkl\")\n",
    "\n",
    "# Export predictions\n",
    "results = pd.DataFrame({\n",
    "    'true_label': [class_names[i] for i in y_true],\n",
    "    'predicted_label': [class_names[i] for i in y_pred],\n",
    "    'correct': y_true == y_pred\n",
    "})\n",
    "results.to_csv('test_predictions.csv', index=False)\n",
    "print(\"‚úì Predictions saved: test_predictions.csv\")\n",
    "\n",
    "print(\"\\nüéä All done! Continue to transfer learning notebook...\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}