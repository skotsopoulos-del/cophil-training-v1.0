{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1, Session 4: Google Earth Engine Python API\n",
    "\n",
    "## CoPhil 4-Day Advanced Online Training on AI/ML for Earth Observation\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. Authenticate and initialize Google Earth Engine (GEE) in Python\n",
    "2. Understand GEE core concepts (Image, ImageCollection, Feature, FeatureCollection)\n",
    "3. Access and filter Sentinel-1 and Sentinel-2 data collections\n",
    "4. Apply cloud masking and create temporal composites\n",
    "5. Calculate spectral indices (NDVI) at scale\n",
    "6. Export processed data for use in AI/ML workflows\n",
    "7. Apply GEE best practices for computational efficiency\n",
    "\n",
    "---\n",
    "\n",
    "## Why Google Earth Engine?\n",
    "\n",
    "Google Earth Engine provides:\n",
    "\n",
    "- **Petabyte-scale data catalog**: Sentinel-1, Sentinel-2, Landsat, MODIS, and more\n",
    "- **Cloud computing**: Process data without downloading\n",
    "- **Planetary-scale analysis**: Analyze entire countries or continents\n",
    "- **Free access**: For research and education\n",
    "\n",
    "Perfect for **preparing training data** for AI/ML models!\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of Session 3 (Python geospatial basics)\n",
    "- Google Earth Engine account (sign up at https://earthengine.google.com/)\n",
    "- Basic understanding of Sentinel-1 and Sentinel-2 missions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Authentication\n",
    "\n",
    "### 1.1 Install Earth Engine API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install Earth Engine Python API\n!pip install earthengine-api -q\n\nprint(\"Earth Engine API installed successfully!\")\n\n# Import libraries\nimport ee\nimport geemap.core as geemap\n\n# Authenticate and Initialize Earth Engine\nee.Authenticate()\nee.Initialize(project='YOUR-PROJECT-ID')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.2 Authentication\n\n**Important:** You need to authenticate once per environment. Follow the instructions that appear:\n\n1. Click the authentication link\n2. Select your Google account\n3. Grant permissions\n4. Copy the authorization code\n5. Paste it back into the notebook"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 1.3 Initialize Earth Engine\n\n**Important:** Earth Engine requires a Google Cloud Project for initialization.\n\n**If you encounter issues:**\n\n1. You need to enable the **Google Earth Engine API** for your project\n2. You need to request **non-commercial access** to Earth Engine\n\n**Follow these steps if you get an error:**\n\n1. Go to the [Earth Engine registration page](https://code.earthengine.google.com/register)\n2. Sign in with your Google account\n3. Register for non-commercial use\n4. Wait for approval (usually immediate for academic/research use)\n5. Once approved, come back and run the initialization below"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Import Additional Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Troubleshooting Authentication\n\nIf you encounter issues:\n\n**Common Error: \"ee.Initialize: no project found\"**\n\nThis means you need to:\n\n1. **Register for Earth Engine Access**\n   - Visit: https://code.earthengine.google.com/register\n   - Sign in with your Google account\n   - Register for non-commercial use (academic/research/education)\n   - Approval is usually instant\n\n2. **Create/Select a Google Cloud Project**\n   - Go to: https://console.cloud.google.com/\n   - Create a new project or select existing one\n   - Note your project ID (shown in the project selector)\n\n3. **Enable Earth Engine API**\n   - Go to: https://console.cloud.google.com/apis/library/earthengine.googleapis.com\n   - Select your project\n   - Click \"Enable\"\n\n4. **Request Project Access**\n   - Go to: https://code.earthengine.google.com/register\n   - Select \"Register a Noncommercial or Commercial Cloud project\"\n   - Choose your project from dropdown\n   - Submit for approval (usually instant for non-commercial use)\n\n5. **Update Your Code**\n   - In cell 1.3 above, replace `'your-project-id'` with your actual project ID\n   - Example: `ee.Initialize(project='YOUR-PROJECT-ID')`\n\n**Other Issues:**\n\n- **Not signed up?** Complete registration at https://code.earthengine.google.com/register\n- **Permission errors?** Ensure you're using the correct Google account\n- **Already authenticated?** Skip authentication and go directly to initialization\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GEE Core Concepts\n",
    "\n",
    "### 2.1 Geometry Objects\n",
    "\n",
    "Earth Engine uses geometries to define areas of interest (AOI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Point geometry (Metro Manila)\n",
    "manila_point = ee.Geometry.Point([121.0, 14.6])\n",
    "print(\"Manila Point:\", manila_point.getInfo())\n",
    "\n",
    "# Create a Rectangle (Palawan)\n",
    "palawan_bbox = ee.Geometry.Rectangle([117.5, 8.5, 119.5, 11.5])\n",
    "print(\"\\nPalawan Bounding Box:\", palawan_bbox.getInfo())\n",
    "\n",
    "# Create a Polygon (custom AOI)\n",
    "custom_polygon = ee.Geometry.Polygon([\n",
    "    [[120.8, 14.4], [121.2, 14.4], [121.2, 14.8], [120.8, 14.8], [120.8, 14.4]]\n",
    "])\n",
    "print(\"\\nCustom Polygon:\", custom_polygon.getInfo())\n",
    "\n",
    "# Buffer around point (10 km)\n",
    "manila_buffer = manila_point.buffer(10000)  # meters\n",
    "print(\"\\nManila 10km Buffer area (kmÂ²):\", manila_buffer.area().divide(1e6).getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Troubleshooting Authentication\n\n**If you get an error \"ee.Initialize: no project found\":**\n\nYou need to set up Earth Engine access. Follow these steps:\n\n**Step 1: Register for Earth Engine**\n- Go to: https://code.earthengine.google.com/register\n- Sign in with your Google account\n- Select \"Register a Noncommercial or Commercial Cloud project\"\n- Follow the registration process for non-commercial use (academic/research/education)\n\n**Step 2: Enable Google Earth Engine API**\n- Go to: https://console.cloud.google.com/apis/library/earthengine.googleapis.com\n- Select or create a Google Cloud Project\n- Click \"Enable\" to activate the Earth Engine API\n- Note your project ID (e.g., 'YOUR-PROJECT-ID')\n\n**Step 3: Request Non-Commercial Access**\n- Return to: https://code.earthengine.google.com/register\n- Choose your project from the dropdown menu\n- Submit your request for non-commercial access\n- Approval is usually instant for academic/research use\n\n**Step 4: Update the Code**\n- In cell 1.1 above, replace `'YOUR-PROJECT-ID'` with your actual project ID\n- Example: `ee.Initialize(project='your-project-id')`\n- Re-run cell 1.1\n\n**Other common issues:**\n\n- **Permission errors?** Make sure you're using the same Google account for authentication and project access\n- **Not signed up?** Complete the registration at https://code.earthengine.google.com/register\n- **Already authenticated?** You can skip the `ee.Authenticate()` step and just run `ee.Initialize(project='your-project-id')`\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access Sentinel-2 Surface Reflectance collection\n",
    "s2_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "\n",
    "# Get collection size (total images available - this will be very large!)\n",
    "# Note: This queries the entire global archive, so we'll filter first\n",
    "\n",
    "# Filter to Manila area for 2024\n",
    "manila_s2 = s2_collection.filterBounds(manila_point) \\\n",
    "                         .filterDate('2024-01-01', '2024-12-31')\n",
    "\n",
    "print(f\"Sentinel-2 images over Manila in 2024: {manila_s2.size().getInfo()}\")\n",
    "\n",
    "# Get the first image\n",
    "first_image = manila_s2.first()\n",
    "print(f\"\\nFirst image ID: {first_image.get('system:id').getInfo()}\")\n",
    "print(f\"Acquisition date: {ee.Date(first_image.get('system:time_start')).format('YYYY-MM-dd').getInfo()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature and FeatureCollection\n",
    "\n",
    "Features are vector data (points, lines, polygons with attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Feature (point with properties)\n",
    "manila_feature = ee.Feature(\n",
    "    manila_point,\n",
    "    {'name': 'Metro Manila', 'population': 13000000, 'type': 'capital'}\n",
    ")\n",
    "\n",
    "print(\"Manila Feature properties:\", manila_feature.getInfo()['properties'])\n",
    "\n",
    "# Create a FeatureCollection\n",
    "cities = ee.FeatureCollection([\n",
    "    ee.Feature(ee.Geometry.Point([121.0, 14.6]), {'name': 'Manila', 'pop': 13000000}),\n",
    "    ee.Feature(ee.Geometry.Point([125.6, 7.1]), {'name': 'Davao', 'pop': 1800000}),\n",
    "    ee.Feature(ee.Geometry.Point([123.9, 10.3]), {'name': 'Cebu', 'pop': 3000000})\n",
    "])\n",
    "\n",
    "print(f\"\\nNumber of cities: {cities.size().getInfo()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Filters and Reducers\n",
    "\n",
    "**Filters** select subsets of collections.\n",
    "**Reducers** aggregate or summarize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering examples\n",
    "filtered = s2_collection \\\n",
    "    .filterBounds(palawan_bbox) \\\n",
    "    .filterDate('2024-06-01', '2024-08-31') \\\n",
    "    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 20)\n",
    "\n",
    "print(f\"Filtered images (Palawan, Jun-Aug 2024, <20% clouds): {filtered.size().getInfo()}\")\n",
    "\n",
    "# Reducer examples\n",
    "# Create a median composite (reduces time dimension)\n",
    "median_composite = filtered.median()\n",
    "\n",
    "# Calculate mean NDVI over region (reduces spatial dimension)\n",
    "# We'll do this in detail later\n",
    "print(\"\\nReducers allow you to:\")\n",
    "print(\"  - Temporal: mean(), median(), max(), min() across time\")\n",
    "print(\"  - Spatial: reduceRegion() for statistics over an area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Working with Sentinel-2\n",
    "\n",
    "### 3.1 Define Area of Interest (AOI)\n",
    "\n",
    "We'll focus on **Palawan Province** - important for Natural Resource Management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Palawan AOI\n",
    "aoi = ee.Geometry.Rectangle([117.8, 9.0, 119.2, 10.8])\n",
    "\n",
    "# Calculate AOI area\n",
    "aoi_area_km2 = aoi.area().divide(1e6).getInfo()\n",
    "print(f\"AOI Area: {aoi_area_km2:.2f} kmÂ²\")\n",
    "\n",
    "# Visualize AOI bounds\n",
    "bounds = aoi.bounds().getInfo()['coordinates'][0]\n",
    "print(f\"AOI Bounds: {bounds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Access Sentinel-2 Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range (2024 dry season - less clouds)\n",
    "start_date = '2024-01-01'\n",
    "end_date = '2024-03-31'\n",
    "\n",
    "# Access Sentinel-2 Surface Reflectance\n",
    "s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "    .filterBounds(aoi) \\\n",
    "    .filterDate(start_date, end_date) \\\n",
    "    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 30)\n",
    "\n",
    "print(f\"Sentinel-2 images found: {s2.size().getInfo()}\")\n",
    "\n",
    "# List image dates and cloud cover\n",
    "def get_image_info(image):\n",
    "    date = ee.Date(image.get('system:time_start')).format('YYYY-MM-dd')\n",
    "    clouds = image.get('CLOUDY_PIXEL_PERCENTAGE')\n",
    "    return ee.Feature(None, {'date': date, 'clouds': clouds})\n",
    "\n",
    "image_info = s2.map(get_image_info).getInfo()['features']\n",
    "\n",
    "print(\"\\nAvailable images:\")\n",
    "print(f\"{'Date':<15} {'Cloud %':>10}\")\n",
    "print(\"-\" * 25)\n",
    "for info in image_info[:10]:  # Show first 10\n",
    "    props = info['properties']\n",
    "    print(f\"{props['date']:<15} {props['clouds']:>10.1f}\")\n",
    "\n",
    "if len(image_info) > 10:\n",
    "    print(f\"... and {len(image_info) - 10} more images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Cloud Masking Function\n",
    "\n",
    "Sentinel-2 Level-2A includes quality bands for cloud masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskS2clouds(image):\n",
    "    \"\"\"\n",
    "    Mask clouds and cirrus in Sentinel-2 imagery using QA60 band.\n",
    "    \n",
    "    QA60 is a bitmask band:\n",
    "    - Bit 10: Opaque clouds\n",
    "    - Bit 11: Cirrus clouds\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    image : ee.Image\n",
    "        Sentinel-2 Level-2A image\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ee.Image : Cloud-masked image\n",
    "    \"\"\"\n",
    "    qa = image.select('QA60')\n",
    "    \n",
    "    # Bits 10 and 11 are clouds and cirrus, respectively\n",
    "    cloudBitMask = 1 << 10\n",
    "    cirrusBitMask = 1 << 11\n",
    "    \n",
    "    # Both flags should be set to zero, indicating clear conditions\n",
    "    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(\n",
    "           qa.bitwiseAnd(cirrusBitMask).eq(0))\n",
    "    \n",
    "    return image.updateMask(mask).copyProperties(image, ['system:time_start'])\n",
    "\n",
    "print(\"Cloud masking function defined!\")\n",
    "print(\"This function will:\")\n",
    "print(\"  1. Read the QA60 quality band\")\n",
    "print(\"  2. Check bits 10 (clouds) and 11 (cirrus)\")\n",
    "print(\"  3. Mask pixels where either bit is set\")\n",
    "print(\"  4. Preserve image metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Apply Cloud Masking and Create Composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cloud mask to all images\n",
    "s2_masked = s2.map(maskS2clouds)\n",
    "\n",
    "print(f\"Cloud masking applied to {s2_masked.size().getInfo()} images\")\n",
    "\n",
    "# Create median composite\n",
    "composite = s2_masked.median().clip(aoi)\n",
    "\n",
    "print(\"\\nMedian composite created!\")\n",
    "print(\"Why median?\")\n",
    "print(\"  - Robust to outliers (remaining clouds, shadows)\")\n",
    "print(\"  - Better than mean for temporal composites\")\n",
    "print(\"  - Produces clean, cloud-free images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Visualize with Thumbnail\n",
    "\n",
    "Earth Engine can generate quick preview images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define visualization parameters for True Color (RGB)\n",
    "vis_params_rgb = {\n",
    "    'bands': ['B4', 'B3', 'B2'],  # Red, Green, Blue\n",
    "    'min': 0,\n",
    "    'max': 3000,\n",
    "    'gamma': 1.4  # Enhance contrast\n",
    "}\n",
    "\n",
    "# Get thumbnail URL\n",
    "thumbnail_url = composite.getThumbURL({\n",
    "    'region': aoi,\n",
    "    'dimensions': 512,\n",
    "    **vis_params_rgb\n",
    "})\n",
    "\n",
    "print(\"True Color Composite (Sentinel-2 RGB):\")\n",
    "display(Image(url=thumbnail_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Color Composite (NIR, Red, Green) - highlights vegetation\n",
    "vis_params_false = {\n",
    "    'bands': ['B8', 'B4', 'B3'],  # NIR, Red, Green\n",
    "    'min': 0,\n",
    "    'max': 4000,\n",
    "    'gamma': 1.4\n",
    "}\n",
    "\n",
    "thumbnail_url_false = composite.getThumbURL({\n",
    "    'region': aoi,\n",
    "    'dimensions': 512,\n",
    "    **vis_params_false\n",
    "})\n",
    "\n",
    "print(\"False Color Composite (NIR-R-G) - Vegetation appears RED:\")\n",
    "display(Image(url=thumbnail_url_false))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Calculate NDVI\n",
    "\n",
    "NDVI = (NIR - Red) / (NIR + Red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NDVI using normalized difference\n",
    "ndvi = composite.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "\n",
    "print(\"NDVI calculated!\")\n",
    "\n",
    "# Get NDVI statistics over AOI\n",
    "ndvi_stats = ndvi.reduceRegion(\n",
    "    reducer=ee.Reducer.mean().combine(\n",
    "        reducer2=ee.Reducer.minMax(),\n",
    "        sharedInputs=True\n",
    "    ),\n",
    "    geometry=aoi,\n",
    "    scale=10,  # 10m resolution\n",
    "    maxPixels=1e9\n",
    ").getInfo()\n",
    "\n",
    "print(\"\\nNDVI Statistics:\")\n",
    "print(f\"  Mean: {ndvi_stats['NDVI_mean']:.3f}\")\n",
    "print(f\"  Min:  {ndvi_stats['NDVI_min']:.3f}\")\n",
    "print(f\"  Max:  {ndvi_stats['NDVI_max']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize NDVI\n",
    "vis_params_ndvi = {\n",
    "    'bands': ['NDVI'],\n",
    "    'min': -0.2,\n",
    "    'max': 0.8,\n",
    "    'palette': ['blue', 'white', 'yellow', 'green', 'darkgreen']\n",
    "}\n",
    "\n",
    "thumbnail_url_ndvi = ndvi.getThumbURL({\n",
    "    'region': aoi,\n",
    "    'dimensions': 512,\n",
    "    **vis_params_ndvi\n",
    "})\n",
    "\n",
    "print(\"NDVI (Normalized Difference Vegetation Index):\")\n",
    "print(\"Blue/White: Water/Bare soil\")\n",
    "print(\"Yellow: Sparse vegetation\")\n",
    "print(\"Green: Moderate vegetation\")\n",
    "print(\"Dark Green: Dense vegetation\\n\")\n",
    "display(Image(url=thumbnail_url_ndvi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Change Location and Dates\n",
    "\n",
    "**Task:** Modify the code to analyze a different Philippine location and time period.\n",
    "\n",
    "**Suggestions:**\n",
    "- **Metro Manila**: `[120.9, 14.4, 121.1, 14.7]`\n",
    "- **Mindanao (Davao)**: `[125.3, 6.9, 125.7, 7.3]`\n",
    "- **Cebu**: `[123.7, 10.2, 124.0, 10.5]`\n",
    "\n",
    "Try different seasons:\n",
    "- **Dry season**: January-May\n",
    "- **Wet season**: June-November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Example: Metro Manila during wet season\n",
    "\n",
    "# Define new AOI\n",
    "manila_aoi = ee.Geometry.Rectangle([120.9, 14.4, 121.1, 14.7])\n",
    "\n",
    "# New date range (wet season)\n",
    "new_start = '2024-07-01'\n",
    "new_end = '2024-09-30'\n",
    "\n",
    "# Query Sentinel-2\n",
    "manila_s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "    .filterBounds(manila_aoi) \\\n",
    "    .filterDate(new_start, new_end) \\\n",
    "    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 30) \\\n",
    "    .map(maskS2clouds)\n",
    "\n",
    "print(f\"Images found: {manila_s2.size().getInfo()}\")\n",
    "\n",
    "# Create composite\n",
    "manila_composite = manila_s2.median().clip(manila_aoi)\n",
    "\n",
    "# Visualize\n",
    "manila_thumb = manila_composite.getThumbURL({\n",
    "    'region': manila_aoi,\n",
    "    'dimensions': 512,\n",
    "    **vis_params_rgb\n",
    "})\n",
    "\n",
    "print(\"\\nMetro Manila True Color Composite:\")\n",
    "display(Image(url=manila_thumb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Working with Sentinel-1 SAR\n",
    "\n",
    "Sentinel-1 provides **all-weather, day-night** radar imagery - essential for the Philippines' cloudy tropical climate!\n",
    "\n",
    "### 4.1 Access Sentinel-1 Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "sar_aoi = palawan_bbox\n",
    "sar_start = '2024-01-01'\n",
    "sar_end = '2024-03-31'\n",
    "\n",
    "# Access Sentinel-1 GRD (Ground Range Detected)\n",
    "s1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
    "    .filterBounds(sar_aoi) \\\n",
    "    .filterDate(sar_start, sar_end) \\\n",
    "    .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
    "    .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING')) \\\n",
    "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
    "    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n",
    "\n",
    "print(f\"Sentinel-1 images found: {s1.size().getInfo()}\")\n",
    "\n",
    "print(\"\\nFilters applied:\")\n",
    "print(\"  - Instrument Mode: IW (Interferometric Wide swath)\")\n",
    "print(\"  - Orbit: Descending (evening pass)\")\n",
    "print(\"  - Polarization: VV and VH (dual-pol)\")\n",
    "print(\"\\nWhy these filters?\")\n",
    "print(\"  - IW mode: Standard for land monitoring (250km swath)\")\n",
    "print(\"  - Descending: Consistent geometry\")\n",
    "print(\"  - VV/VH: Sensitive to different surface properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create SAR Composite\n",
    "\n",
    "For SAR, we use **mean** to reduce speckle noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select VV and VH bands\n",
    "s1_composite = s1.select(['VV', 'VH']).mean().clip(sar_aoi)\n",
    "\n",
    "print(\"SAR composite created using mean (reduces speckle)\")\n",
    "\n",
    "# Calculate VV/VH ratio (useful for land cover)\n",
    "vv_vh_ratio = s1_composite.select('VV').divide(s1_composite.select('VH')).rename('VV_VH_ratio')\n",
    "\n",
    "print(\"VV/VH ratio calculated (useful for classification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualize SAR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VV polarization visualization\n",
    "vis_params_vv = {\n",
    "    'bands': ['VV'],\n",
    "    'min': -25,\n",
    "    'max': 0\n",
    "}\n",
    "\n",
    "sar_thumb_vv = s1_composite.getThumbURL({\n",
    "    'region': sar_aoi,\n",
    "    'dimensions': 512,\n",
    "    **vis_params_vv\n",
    "})\n",
    "\n",
    "print(\"Sentinel-1 VV Polarization (dB):\")\n",
    "print(\"Dark areas: Water, calm surfaces (low backscatter)\")\n",
    "print(\"Bright areas: Urban, rough surfaces (high backscatter)\\n\")\n",
    "display(Image(url=sar_thumb_vv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False color SAR (VV, VH, VV/VH ratio)\n",
    "sar_false_color = ee.Image.cat([\n",
    "    s1_composite.select('VV'),\n",
    "    s1_composite.select('VH'),\n",
    "    vv_vh_ratio\n",
    "])\n",
    "\n",
    "vis_params_sar_false = {\n",
    "    'min': [-25, -25, 0],\n",
    "    'max': [0, 0, 2]\n",
    "}\n",
    "\n",
    "sar_thumb_false = sar_false_color.getThumbURL({\n",
    "    'region': sar_aoi,\n",
    "    'dimensions': 512,\n",
    "    **vis_params_sar_false\n",
    "})\n",
    "\n",
    "print(\"Sentinel-1 False Color (VV-VH-Ratio):\")\n",
    "display(Image(url=sar_thumb_false))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare VV and VH Polarizations\n",
    "\n",
    "**Task:** Create side-by-side visualizations of VV and VH polarizations.\n",
    "\n",
    "**Hint:** VH is often more sensitive to vegetation volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Solution:\n",
    "\n",
    "# VH polarization\n",
    "vis_params_vh = {\n",
    "    'bands': ['VH'],\n",
    "    'min': -25,\n",
    "    'max': 0\n",
    "}\n",
    "\n",
    "sar_thumb_vh = s1_composite.getThumbURL({\n",
    "    'region': sar_aoi,\n",
    "    'dimensions': 512,\n",
    "    **vis_params_vh\n",
    "})\n",
    "\n",
    "print(\"VV Polarization:\")\n",
    "display(Image(url=sar_thumb_vv))\n",
    "\n",
    "print(\"\\nVH Polarization (more sensitive to vegetation structure):\")\n",
    "display(Image(url=sar_thumb_vh))\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"  - VV: Better for water detection, urban areas\")\n",
    "print(\"  - VH: Better for vegetation, forest structure\")\n",
    "print(\"  - Using both improves classification accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Data Export for AI/ML Workflows\n",
    "\n",
    "To train custom ML models, we need to export data from Earth Engine.\n",
    "\n",
    "### 5.1 Export Image to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Sentinel-2 composite\n",
    "export_task_s2 = ee.batch.Export.image.toDrive(\n",
    "    image=composite.select(['B2', 'B3', 'B4', 'B8']),  # Select bands to export\n",
    "    description='Palawan_S2_Composite_Q1_2024',\n",
    "    folder='EarthEngine_Exports',\n",
    "    fileNamePrefix='palawan_s2_composite',\n",
    "    region=aoi,\n",
    "    scale=10,  # 10m resolution\n",
    "    crs='EPSG:4326',\n",
    "    maxPixels=1e9\n",
    ")\n",
    "\n",
    "# Start the export task\n",
    "export_task_s2.start()\n",
    "\n",
    "print(\"Export task started!\")\n",
    "print(f\"Task ID: {export_task_s2.id}\")\n",
    "print(\"\\nExport details:\")\n",
    "print(f\"  - Destination: Google Drive/EarthEngine_Exports/\")\n",
    "print(f\"  - Filename: palawan_s2_composite.tif\")\n",
    "print(f\"  - Bands: B2, B3, B4, B8 (Blue, Green, Red, NIR)\")\n",
    "print(f\"  - Resolution: 10m\")\n",
    "print(f\"  - Format: GeoTIFF\")\n",
    "print(\"\\nMonitor status at: https://code.earthengine.google.com/tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Check Export Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check task status\n",
    "task_status = export_task_s2.status()\n",
    "print(f\"Task Status: {task_status['state']}\")\n",
    "\n",
    "if task_status['state'] == 'RUNNING':\n",
    "    print(\"Task is running... Check back in a few minutes.\")\n",
    "elif task_status['state'] == 'COMPLETED':\n",
    "    print(\"Task completed! Check your Google Drive.\")\n",
    "elif task_status['state'] == 'FAILED':\n",
    "    print(f\"Task failed: {task_status.get('error_message', 'Unknown error')}\")\n",
    "else:\n",
    "    print(f\"Task state: {task_status['state']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Export NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export NDVI layer\n",
    "export_task_ndvi = ee.batch.Export.image.toDrive(\n",
    "    image=ndvi,\n",
    "    description='Palawan_NDVI_Q1_2024',\n",
    "    folder='EarthEngine_Exports',\n",
    "    fileNamePrefix='palawan_ndvi',\n",
    "    region=aoi,\n",
    "    scale=10,\n",
    "    crs='EPSG:4326',\n",
    "    maxPixels=1e9\n",
    ")\n",
    "\n",
    "export_task_ndvi.start()\n",
    "\n",
    "print(f\"NDVI export started!\")\n",
    "print(f\"Task ID: {export_task_ndvi.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Export Training Samples (for ML)\n",
    "\n",
    "For ML model training, we often need to export **training samples** as vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample points for different land cover types\n",
    "# In practice, you would digitize these in GEE Code Editor or use existing data\n",
    "\n",
    "# Example: Random sample points\n",
    "sample_points = composite.sample(\n",
    "    region=aoi,\n",
    "    scale=30,  # Sample every 30m\n",
    "    numPixels=1000,  # Number of samples\n",
    "    seed=42  # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Generated {sample_points.size().getInfo()} sample points\")\n",
    "\n",
    "# Export samples to Drive as CSV\n",
    "export_task_samples = ee.batch.Export.table.toDrive(\n",
    "    collection=sample_points,\n",
    "    description='Palawan_Training_Samples',\n",
    "    folder='EarthEngine_Exports',\n",
    "    fileNamePrefix='palawan_samples',\n",
    "    fileFormat='CSV'\n",
    ")\n",
    "\n",
    "export_task_samples.start()\n",
    "\n",
    "print(f\"\\nSample points export started!\")\n",
    "print(f\"Task ID: {export_task_samples.id}\")\n",
    "print(\"\\nThese samples can be used for:\")\n",
    "print(\"  - Training ML classifiers\")\n",
    "print(\"  - Validating model predictions\")\n",
    "print(\"  - Feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Export Custom AOI Composite\n",
    "\n",
    "**Task:** Export a composite for your chosen location from Exercise 1.\n",
    "\n",
    "**Requirements:**\n",
    "- Use your custom AOI\n",
    "- Export RGB bands (B2, B3, B4)\n",
    "- 10m resolution\n",
    "- Give it a meaningful filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Solution template:\n",
    "\n",
    "my_export_task = ee.batch.Export.image.toDrive(\n",
    "    image=manila_composite.select(['B2', 'B3', 'B4']),\n",
    "    description='My_Custom_Export',\n",
    "    folder='EarthEngine_Exports',\n",
    "    fileNamePrefix='my_custom_composite',\n",
    "    region=manila_aoi,\n",
    "    scale=10,\n",
    "    crs='EPSG:4326',\n",
    "    maxPixels=1e9\n",
    ")\n",
    "\n",
    "my_export_task.start()\n",
    "print(f\"Custom export started! Task ID: {my_export_task.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Integration with AI/ML Workflows\n",
    "\n",
    "### 6.1 Preparing Training Data\n",
    "\n",
    "Earth Engine excels at preparing **analysis-ready data** for ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-band image stack for ML\n",
    "ml_stack = composite.select(['B2', 'B3', 'B4', 'B8', 'B11', 'B12']) \\\n",
    "                    .addBands(ndvi) \\\n",
    "                    .addBands(s1_composite.select(['VV', 'VH']))\n",
    "\n",
    "print(\"ML-ready image stack created!\")\n",
    "print(\"\\nBands included:\")\n",
    "band_names = ml_stack.bandNames().getInfo()\n",
    "for i, band in enumerate(band_names, 1):\n",
    "    print(f\"  {i}. {band}\")\n",
    "\n",
    "print(\"\\nWhy this combination?\")\n",
    "print(\"  - Optical bands (B2-B12): Spectral information\")\n",
    "print(\"  - NDVI: Vegetation index\")\n",
    "print(\"  - SAR (VV, VH): All-weather information\")\n",
    "print(\"  - Multi-sensor fusion improves classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Sampling for Training\n",
    "\n",
    "Extract feature vectors for ML model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training regions (in practice, digitize or load from shapefile)\n",
    "# For demonstration, we'll create simple point collections\n",
    "\n",
    "# Forest training points\n",
    "forest_points = ee.FeatureCollection([\n",
    "    ee.Feature(ee.Geometry.Point([118.5, 10.2]), {'landcover': 0, 'class_name': 'Forest'}),\n",
    "    ee.Feature(ee.Geometry.Point([118.6, 10.3]), {'landcover': 0, 'class_name': 'Forest'}),\n",
    "    ee.Feature(ee.Geometry.Point([118.4, 10.1]), {'landcover': 0, 'class_name': 'Forest'})\n",
    "])\n",
    "\n",
    "# Water training points\n",
    "water_points = ee.FeatureCollection([\n",
    "    ee.Feature(ee.Geometry.Point([118.2, 9.5]), {'landcover': 1, 'class_name': 'Water'}),\n",
    "    ee.Feature(ee.Geometry.Point([118.3, 9.6]), {'landcover': 1, 'class_name': 'Water'})\n",
    "])\n",
    "\n",
    "# Merge training points\n",
    "training_points = forest_points.merge(water_points)\n",
    "\n",
    "# Sample image at training points\n",
    "training_data = ml_stack.sampleRegions(\n",
    "    collection=training_points,\n",
    "    properties=['landcover', 'class_name'],\n",
    "    scale=10\n",
    ")\n",
    "\n",
    "print(f\"Training samples created: {training_data.size().getInfo()}\")\n",
    "print(\"\\nSample features:\")\n",
    "sample = training_data.first().getInfo()\n",
    "print(f\"Properties: {list(sample['properties'].keys())}\")\n",
    "\n",
    "print(\"\\nNext steps (Day 2):\")\n",
    "print(\"  1. Export training data\")\n",
    "print(\"  2. Train Random Forest classifier\")\n",
    "print(\"  3. Apply classifier to image\")\n",
    "print(\"  4. Validate results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Earth Engine Built-in ML (Preview)\n",
    "\n",
    "GEE has built-in classifiers for quick prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple classifier (Random Forest)\n",
    "# Note: This is a preview - we'll cover this in detail on Day 2\n",
    "\n",
    "classifier = ee.Classifier.smileRandomForest(\n",
    "    numberOfTrees=10\n",
    ").train(\n",
    "    features=training_data,\n",
    "    classProperty='landcover',\n",
    "    inputProperties=ml_stack.bandNames()\n",
    ")\n",
    "\n",
    "# Classify the image\n",
    "classified = ml_stack.classify(classifier)\n",
    "\n",
    "print(\"Simple classification performed!\")\n",
    "print(\"\\nNote: This is a minimal example.\")\n",
    "print(\"On Day 2, we'll learn:\")\n",
    "print(\"  - Proper training data collection\")\n",
    "print(\"  - Feature selection\")\n",
    "print(\"  - Model validation\")\n",
    "print(\"  - Accuracy assessment\")\n",
    "\n",
    "# Visualize classification\n",
    "vis_params_class = {\n",
    "    'min': 0,\n",
    "    'max': 1,\n",
    "    'palette': ['green', 'blue']  # Forest, Water\n",
    "}\n",
    "\n",
    "class_thumb = classified.getThumbURL({\n",
    "    'region': aoi,\n",
    "    'dimensions': 512,\n",
    "    **vis_params_class\n",
    "})\n",
    "\n",
    "print(\"\\nSimple Classification Result:\")\n",
    "display(Image(url=class_thumb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Best Practices and Tips\n",
    "\n",
    "### 7.1 Memory Management"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "**Earth Engine Best Practices:**\n\n**1. MEMORY MANAGEMENT:**\n- Avoid `.getInfo()` on large objects (use for small metadata only)\n- Use `.limit()` to restrict collection size during testing\n- Export large results instead of downloading\n\n**Example:** Limit collection size for testing\n```python\nlimited_collection = s2.limit(5)\nprint(f\"Limited collection size: {limited_collection.size().getInfo()}\")\n```\n\n**2. COMPUTATIONAL QUOTAS:**\n- Free tier: 250GB Cloud Storage, 10k+ compute hours/month\n- Set maxPixels appropriately (default: 1e8)\n- Use appropriate scale (don't oversample)\n\n**3. EFFICIENCY:**\n- Filter early: bounds â date â metadata\n- Select only needed bands\n- Clip to AOI before intensive operations"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 When to Use GEE vs Local Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "**USE GOOGLE EARTH ENGINE FOR:**\n- â Data access and pre-processing\n- â Large-scale spatial analysis\n- â Time series analysis\n- â Cloud masking and compositing\n- â Simple ML (Random Forest, CART)\n- â Zonal statistics\n- â Rapid prototyping\n\n**USE LOCAL PROCESSING (Python/Colab) FOR:**\n- â Deep learning (CNN, U-Net, LSTM)\n- â Custom model architectures\n- â Fine-grained control over training\n- â Integration with TensorFlow/PyTorch\n- â Advanced data augmentation\n- â Transfer learning\n\n**BEST WORKFLOW:**\n1. Use GEE for data preparation\n2. Export training data\n3. Train models locally (Colab GPU)\n4. Deploy models on new data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Troubleshooting Common Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "**COMMON ERRORS AND SOLUTIONS:**\n\n**1. 'User memory limit exceeded'**\n- â Reduce AOI size or increase scale\n- â Use `.limit()` on collections\n- â Export instead of `.getInfo()`\n\n**2. 'Computation timed out'**\n- â Simplify operations\n- â Filter collections more aggressively\n- â Break into smaller exports\n\n**3. 'EEException: Collection.first: No matching elements'**\n- â Check date range (no images available)\n- â Verify AOI (outside coverage?)\n- â Relax filters (clouds, etc.)\n\n**4. Export task fails**\n- â Check maxPixels limit\n- â Verify Google Drive space\n- â Check region coordinates\n\n**5. 'Image.select: Pattern X did not match any bands'**\n- â Check band names: `.bandNames().getInfo()`\n- â Verify dataset (S2 vs S2_SR bands differ)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "**What You've Learned:**\n",
    "\n",
    "1. **GEE Authentication & Setup**\n",
    "   - One-time authentication process\n",
    "   - Initialize for each session\n",
    "   - Python API basics\n",
    "\n",
    "2. **Core GEE Concepts**\n",
    "   - Geometry: Points, Rectangles, Polygons\n",
    "   - Image & ImageCollection: Raster data\n",
    "   - Feature & FeatureCollection: Vector data\n",
    "   - Filters: Subset data by space, time, metadata\n",
    "   - Reducers: Aggregate/summarize data\n",
    "\n",
    "3. **Sentinel-2 Workflows**\n",
    "   - Access surface reflectance data\n",
    "   - Cloud masking with QA60\n",
    "   - Create median composites\n",
    "   - Calculate NDVI\n",
    "   - Visualize with thumbnails\n",
    "\n",
    "4. **Sentinel-1 SAR**\n",
    "   - All-weather imaging capability\n",
    "   - VV and VH polarizations\n",
    "   - Speckle reduction with mean\n",
    "   - Complementary to optical data\n",
    "\n",
    "5. **Data Export**\n",
    "   - Export to Google Drive\n",
    "   - Images (GeoTIFF) and tables (CSV)\n",
    "   - Monitor tasks\n",
    "   - Prepare data for ML\n",
    "\n",
    "6. **ML Integration**\n",
    "   - Prepare multi-band stacks\n",
    "   - Sample training data\n",
    "   - Built-in classifiers (preview)\n",
    "   - GEE â Local workflow\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Next Steps\n",
    "\n",
    "**Day 2:** We'll apply these skills to build **Machine Learning classification models**:\n",
    "\n",
    "- Random Forest for land cover classification\n",
    "- Feature engineering and selection\n",
    "- Training data collection strategies\n",
    "- Model validation and accuracy assessment\n",
    "- Philippine case study: **Palawan land cover mapping**\n",
    "\n",
    "**Day 3-4:** Advanced deep learning:\n",
    "- CNNs for image classification\n",
    "- U-Net for semantic segmentation\n",
    "- Object detection\n",
    "- Time series analysis with LSTMs\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Additional Resources\n",
    "\n",
    "### Official Documentation\n",
    "- **Earth Engine Guide:** https://developers.google.com/earth-engine/\n",
    "- **Python API Intro:** https://developers.google.com/earth-engine/tutorials/community/intro-to-python-api\n",
    "- **Data Catalog:** https://developers.google.com/earth-engine/datasets/\n",
    "\n",
    "### Tutorials\n",
    "- **End-to-End GEE Course:** https://courses.spatialthoughts.com/end-to-end-gee.html\n",
    "- **GEE Community Tutorials:** https://github.com/google/earthengine-community\n",
    "- **Awesome Earth Engine:** https://github.com/giswqs/Awesome-GEE\n",
    "\n",
    "### Philippine Context\n",
    "- **PhilSA:** https://philsa.gov.ph/\n",
    "- **CoPhil Mirror Site:** (Coming 2025)\n",
    "- **DOST-ASTI:** https://asti.dost.gov.ph/\n",
    "\n",
    "### Books\n",
    "- *Cloud-Based Remote Sensing with Google Earth Engine* (Cardille et al.)\n",
    "- *Earth Observation Using Python* (Parente & Pepe)\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Practice Exercises\n",
    "\n",
    "To reinforce your learning, try these exercises:\n",
    "\n",
    "### Exercise A: Multi-temporal Analysis\n",
    "Create composites for different seasons (dry vs wet) and compare NDVI changes.\n",
    "\n",
    "### Exercise B: Multi-location Comparison\n",
    "Compare NDVI between different Philippine regions (urban vs forest vs agriculture).\n",
    "\n",
    "### Exercise C: Sentinel-1 Flood Detection\n",
    "Use SAR data to identify potential flood areas (low VV backscatter).\n",
    "\n",
    "### Exercise D: Data Fusion\n",
    "Combine Sentinel-1 and Sentinel-2 to create a comprehensive dataset for classification.\n",
    "\n",
    "### Exercise E: Time Series\n",
    "Plot NDVI time series for a specific location over an entire year.\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed Day 1 of the CoPhil AI/ML Training!\n",
    "\n",
    "You now have:\n",
    "- â Python geospatial skills (GeoPandas, Rasterio)\n",
    "- â Google Earth Engine proficiency\n",
    "- â Access to petabytes of satellite data\n",
    "- â Ability to prepare data for AI/ML\n",
    "\n",
    "**Tomorrow:** We build our first machine learning models!\n",
    "\n",
    "---\n",
    "\n",
    "*Generated with Claude Code for CoPhil Digital Space Campus*\n",
    "\n",
    "*EU-Philippines Copernicus Capacity Support Programme*\n",
    "\n",
    "*Data-Centric AI for Earth Observation*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}