<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="CoPhil Advanced Training Program">
<meta name="dcterms.date" content="2025-10-17">

<title>Session 1: Semantic Segmentation with U-Net for Earth Observation – CoPhil EO AI/ML Training</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../day3/sessions/session2.html" rel="next">
<link href="../../day3/index.html" rel="prev">
<link href="../../images/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c9822816d3895e59fda95a6fa7545fef.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-3775014fae9fc394bbda1d6ff89dd45e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-509191933"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-509191933', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<meta name="mermaid-theme" content="default">
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles/custom.css">
<link rel="stylesheet" href="../../styles/phase2-enhancements.css">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CoPhil EO AI/ML Training</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-training-days" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Training Days</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-training-days">    
        <li>
    <a class="dropdown-item" href="../../day1/index.html">
 <span class="dropdown-text">Day 1: EO Data &amp; Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day2/index.html">
 <span class="dropdown-text">Day 2: Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day3/index.html">
 <span class="dropdown-text">Day 3: Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day4/index.html">
 <span class="dropdown-text">Day 4: Advanced Topics</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="../../resources/setup.html">
 <span class="dropdown-text">Setup Guide</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/philippine-eo.html">
 <span class="dropdown-text">Philippine EO Links</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/cheatsheets.html">
 <span class="dropdown-text">Cheat Sheets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/faq.html">
 <span class="dropdown-text">FAQ</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/glossary.html">
 <span class="dropdown-text">Glossary</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../resources/downloads.html"> <i class="bi bi-download" role="img">
</i> 
<span class="menu-text">Materials</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day3/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day3/sessions/session1.html">Session 1: Semantic Segmentation with U-Net for Earth Observation</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Day 3: Deep Learning for Earth Observation</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Sessions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/sessions/session1.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Session 1: Semantic Segmentation with U-Net for Earth Observation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/sessions/session2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 2: Hands-on Flood Mapping with U-Net and Sentinel-1 SAR</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/sessions/session3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 3: Object Detection Techniques for Earth Observation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/sessions/session4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: Hands-on Object Detection from Sentinel Imagery</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Notebooks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/notebooks/Day3_Session2_Flood_Mapping_UNet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 2: Hands-on Flood Mapping with U-Net and Sentinel-1 SAR</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/notebooks/Day3_Session4_Object_Detection_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: Object Detection from Sentinel Imagery</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#session-1-semantic-segmentation-with-u-net" id="toc-session-1-semantic-segmentation-with-u-net" class="nav-link active" data-scroll-target="#session-1-semantic-segmentation-with-u-net">Session 1: Semantic Segmentation with U-Net</a>
  <ul class="collapse">
  <li><a href="#advanced-deep-learning-for-pixel-level-analysis" id="toc-advanced-deep-learning-for-pixel-level-analysis" class="nav-link" data-scroll-target="#advanced-deep-learning-for-pixel-level-analysis">Advanced Deep Learning for Pixel-Level Analysis</a></li>
  </ul></li>
  <li><a href="#session-overview" id="toc-session-overview" class="nav-link" data-scroll-target="#session-overview">Session Overview</a>
  <ul class="collapse">
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives">Learning Objectives</a></li>
  </ul></li>
  <li><a href="#presentation-slides" id="toc-presentation-slides" class="nav-link" data-scroll-target="#presentation-slides">Presentation Slides</a></li>
  <li><a href="#part-1-concept-of-semantic-segmentation" id="toc-part-1-concept-of-semantic-segmentation" class="nav-link" data-scroll-target="#part-1-concept-of-semantic-segmentation">Part 1: Concept of Semantic Segmentation</a>
  <ul class="collapse">
  <li><a href="#what-is-semantic-segmentation" id="toc-what-is-semantic-segmentation" class="nav-link" data-scroll-target="#what-is-semantic-segmentation">What is Semantic Segmentation?</a></li>
  <li><a href="#understanding-the-difference" id="toc-understanding-the-difference" class="nav-link" data-scroll-target="#understanding-the-difference">Understanding the Difference</a></li>
  <li><a href="#three-task-comparison" id="toc-three-task-comparison" class="nav-link" data-scroll-target="#three-task-comparison">Three Task Comparison</a></li>
  </ul></li>
  <li><a href="#part-2-u-net-architecture" id="toc-part-2-u-net-architecture" class="nav-link" data-scroll-target="#part-2-u-net-architecture">Part 2: U-Net Architecture</a>
  <ul class="collapse">
  <li><a href="#introduction-to-u-net" id="toc-introduction-to-u-net" class="nav-link" data-scroll-target="#introduction-to-u-net">Introduction to U-Net</a></li>
  <li><a href="#encoder-contracting-path" id="toc-encoder-contracting-path" class="nav-link" data-scroll-target="#encoder-contracting-path">Encoder (Contracting Path)</a></li>
  <li><a href="#bottleneck-layer" id="toc-bottleneck-layer" class="nav-link" data-scroll-target="#bottleneck-layer">Bottleneck Layer</a></li>
  <li><a href="#decoder-expansive-path" id="toc-decoder-expansive-path" class="nav-link" data-scroll-target="#decoder-expansive-path">Decoder (Expansive Path)</a></li>
  <li><a href="#skip-connections---the-key-innovation" id="toc-skip-connections---the-key-innovation" class="nav-link" data-scroll-target="#skip-connections---the-key-innovation">Skip Connections - The Key Innovation</a></li>
  <li><a href="#how-skip-connections-work" id="toc-how-skip-connections-work" class="nav-link" data-scroll-target="#how-skip-connections-work">How Skip Connections Work</a></li>
  <li><a href="#u-net-complete-architecture-summary" id="toc-u-net-complete-architecture-summary" class="nav-link" data-scroll-target="#u-net-complete-architecture-summary">U-Net Complete Architecture Summary</a></li>
  </ul></li>
  <li><a href="#part-3-applications-in-earth-observation" id="toc-part-3-applications-in-earth-observation" class="nav-link" data-scroll-target="#part-3-applications-in-earth-observation">Part 3: Applications in Earth Observation</a>
  <ul class="collapse">
  <li><a href="#why-u-net-is-popular-in-eo" id="toc-why-u-net-is-popular-in-eo" class="nav-link" data-scroll-target="#why-u-net-is-popular-in-eo">Why U-Net is Popular in EO</a></li>
  <li><a href="#application-1-flood-mapping" id="toc-application-1-flood-mapping" class="nav-link" data-scroll-target="#application-1-flood-mapping">Application 1: Flood Mapping</a></li>
  <li><a href="#application-2-land-cover-mapping" id="toc-application-2-land-cover-mapping" class="nav-link" data-scroll-target="#application-2-land-cover-mapping">Application 2: Land Cover Mapping</a></li>
  <li><a href="#application-3-road-network-extraction" id="toc-application-3-road-network-extraction" class="nav-link" data-scroll-target="#application-3-road-network-extraction">Application 3: Road Network Extraction</a></li>
  <li><a href="#application-4-building-footprint-delineation" id="toc-application-4-building-footprint-delineation" class="nav-link" data-scroll-target="#application-4-building-footprint-delineation">Application 4: Building Footprint Delineation</a></li>
  <li><a href="#application-5-vegetation-and-crop-monitoring" id="toc-application-5-vegetation-and-crop-monitoring" class="nav-link" data-scroll-target="#application-5-vegetation-and-crop-monitoring">Application 5: Vegetation and Crop Monitoring</a></li>
  </ul></li>
  <li><a href="#part-4-loss-functions-for-segmentation" id="toc-part-4-loss-functions-for-segmentation" class="nav-link" data-scroll-target="#part-4-loss-functions-for-segmentation">Part 4: Loss Functions for Segmentation</a>
  <ul class="collapse">
  <li><a href="#why-loss-functions-matter" id="toc-why-loss-functions-matter" class="nav-link" data-scroll-target="#why-loss-functions-matter">Why Loss Functions Matter</a></li>
  <li><a href="#challenge-class-imbalance-in-eo" id="toc-challenge-class-imbalance-in-eo" class="nav-link" data-scroll-target="#challenge-class-imbalance-in-eo">Challenge: Class Imbalance in EO</a></li>
  <li><a href="#loss-function-1-pixel-wise-cross-entropy" id="toc-loss-function-1-pixel-wise-cross-entropy" class="nav-link" data-scroll-target="#loss-function-1-pixel-wise-cross-entropy">Loss Function 1: Pixel-wise Cross-Entropy</a></li>
  <li><a href="#weighted-cross-entropy" id="toc-weighted-cross-entropy" class="nav-link" data-scroll-target="#weighted-cross-entropy">Weighted Cross-Entropy</a></li>
  <li><a href="#loss-function-2-dice-loss" id="toc-loss-function-2-dice-loss" class="nav-link" data-scroll-target="#loss-function-2-dice-loss">Loss Function 2: Dice Loss</a></li>
  <li><a href="#loss-function-3-iou-loss-jaccard-index" id="toc-loss-function-3-iou-loss-jaccard-index" class="nav-link" data-scroll-target="#loss-function-3-iou-loss-jaccard-index">Loss Function 3: IoU Loss (Jaccard Index)</a></li>
  <li><a href="#dice-vs-iou---when-to-choose" id="toc-dice-vs-iou---when-to-choose" class="nav-link" data-scroll-target="#dice-vs-iou---when-to-choose">Dice vs IoU - When to Choose?</a></li>
  <li><a href="#loss-function-4-combined-losses" id="toc-loss-function-4-combined-losses" class="nav-link" data-scroll-target="#loss-function-4-combined-losses">Loss Function 4: Combined Losses</a></li>
  <li><a href="#loss-function-selection-guide" id="toc-loss-function-selection-guide" class="nav-link" data-scroll-target="#loss-function-selection-guide">Loss Function Selection Guide</a></li>
  <li><a href="#practical-example-flood-mapping-loss-selection" id="toc-practical-example-flood-mapping-loss-selection" class="nav-link" data-scroll-target="#practical-example-flood-mapping-loss-selection">Practical Example: Flood Mapping Loss Selection</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a>
  <ul class="collapse">
  <li><a href="#core-references" id="toc-core-references" class="nav-link" data-scroll-target="#core-references">Core References</a></li>
  <li><a href="#datasets-for-practice" id="toc-datasets-for-practice" class="nav-link" data-scroll-target="#datasets-for-practice">Datasets for Practice</a></li>
  <li><a href="#tutorials" id="toc-tutorials" class="nav-link" data-scroll-target="#tutorials">Tutorials</a></li>
  <li><a href="#philippine-eo-context" id="toc-philippine-eo-context" class="nav-link" data-scroll-target="#philippine-eo-context">Philippine EO Context</a></li>
  </ul></li>
  <li><a href="#preparation-for-session-2" id="toc-preparation-for-session-2" class="nav-link" data-scroll-target="#preparation-for-session-2">Preparation for Session 2</a></li>
  <li><a href="#discussion-questions" id="toc-discussion-questions" class="nav-link" data-scroll-target="#discussion-questions">Discussion Questions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day3/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day3/sessions/session1.html">Session 1: Semantic Segmentation with U-Net for Earth Observation</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Session 1: Semantic Segmentation with U-Net for Earth Observation</h1>
<p class="subtitle lead">Advanced Deep Learning for Pixel-Level Analysis</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Instructor</div>
    <div class="quarto-title-meta-contents">
             <p>CoPhil Advanced Training Program </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Date</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 17, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<nav class="breadcrumb" aria-label="Breadcrumb">
<a href="../../index.html">Home</a> <span class="breadcrumb-separator" aria-hidden="true">›</span> <a href="../index.html">Day 3</a> <span class="breadcrumb-separator" aria-hidden="true">›</span> <span class="breadcrumb-current">Session 1</span>
</nav>
<section id="session-1-semantic-segmentation-with-u-net" class="level1 hero">
<h1>Session 1: Semantic Segmentation with U-Net</h1>
<section id="advanced-deep-learning-for-pixel-level-analysis" class="level3">
<h3 class="anchored" data-anchor-id="advanced-deep-learning-for-pixel-level-analysis">Advanced Deep Learning for Pixel-Level Analysis</h3>
<p>Master the U-Net architecture for precise Earth Observation segmentation tasks</p>
</section>
</section>
<section id="session-overview" class="level2">
<h2 class="anchored" data-anchor-id="session-overview">Session Overview</h2>
<div class="session-info">
<p><strong>Duration:</strong> 1.5 hours | <strong>Format:</strong> Lecture + Discussion | <strong>Platform:</strong> Presentation &amp; Slides</p>
</div>
<hr>
<p>This session introduces semantic segmentation as a pixel-wise classification task and explores the U-Net architecture—one of the most successful deep learning models for Earth Observation applications. You’ll understand how U-Net’s encoder-decoder structure with skip connections enables precise boundary delineation for tasks like flood mapping, land cover classification, and infrastructure extraction.</p>
<section id="learning-objectives" class="level3 learning-objectives">
<h3 class="anchored" data-anchor-id="learning-objectives">Learning Objectives</h3>
<p>By the end of this session, you will be able to:</p>
<ul>
<li><strong>Define</strong> semantic segmentation and distinguish it from classification and object detection</li>
<li><strong>Explain</strong> the U-Net architecture including encoder, decoder, and skip connections</li>
<li><strong>Describe</strong> how loss functions (Cross-Entropy, Dice, IoU) handle class imbalance</li>
<li><strong>Identify</strong> Earth Observation applications suited for semantic segmentation</li>
<li><strong>Evaluate</strong> when to use different loss functions for segmentation tasks</li>
</ul>
</section>
<hr>
</section>
<section id="presentation-slides" class="level2">
<h2 class="anchored" data-anchor-id="presentation-slides">Presentation Slides</h2>
<iframe src="../presentations/session1_unet_segmentation.html" width="100%" height="600" style="border: 1px solid #ccc; border-radius: 4px;">
</iframe>
<hr>
</section>
<section id="part-1-concept-of-semantic-segmentation" class="level2">
<h2 class="anchored" data-anchor-id="part-1-concept-of-semantic-segmentation">Part 1: Concept of Semantic Segmentation</h2>
<section id="what-is-semantic-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="what-is-semantic-segmentation">What is Semantic Segmentation?</h3>
<p><strong>Semantic segmentation</strong> is the task of classifying every pixel in an image into a category, producing a detailed, pixel-wise map of the image content. Unlike <strong>image classification</strong> (which assigns one label per image) or <strong>object detection</strong> (which locates objects with bounding boxes), segmentation provides a fine-grained understanding of the scene.</p>
<p>In segmentation, the output is an image where every pixel is labeled (e.g., water, building, forest, road), thus outlining the exact shapes and areas of these features. This level of detail is especially useful in geospatial contexts for creating thematic maps and analyzing spatial patterns.</p>
<p><strong>Example Comparison:</strong> - <strong>Classification:</strong> “Is this satellite patch urban or agricultural?” → Single label for entire image - <strong>Detection:</strong> “Where are the buildings?” → Bounding boxes around structures<br>
- <strong>Segmentation:</strong> “Label every pixel as building, road, vegetation, or water” → Complete pixel-level map</p>
<p>This pixel-level approach is crucial for Earth Observation tasks like mapping floods, land cover, or burn scars, where knowing the exact extent and shape of features matters. Unlike classification that might tell us a patch is “urban,” segmentation highlights exactly which pixels are buildings, roads, vegetation, water, etc., giving a much richer understanding of the scene.</p>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    A[Computer Vision Tasks] --&gt; B[Classification]
    A --&gt; C[Object Detection]
    A --&gt; D[Semantic Segmentation]
    
    B --&gt; B1["What's in this image?&lt;br/&gt;Output: Single label&lt;br/&gt;Granularity: Image-level"]
    C --&gt; C1["Where are the objects?&lt;br/&gt;Output: Bounding boxes&lt;br/&gt;Granularity: Object-level"]
    D --&gt; D1["Which pixels belong to what?&lt;br/&gt;Output: Pixel masks&lt;br/&gt;Granularity: Pixel-level"]
    
    style A fill:#0066cc,stroke:#003d7a,stroke-width:3px,color:#fff
    style D fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff
    style D1 fill:#00aa44,stroke:#006622,stroke-width:1px,color:#fff
</pre>
</div>
<p></p><figcaption> Computer Vision Task Hierarchy</figcaption> </figure><p></p>
</div>
</div>
</div>
</section>
<section id="understanding-the-difference" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-difference">Understanding the Difference</h3>
<p>In semantic segmentation, the output is an image where every pixel is labeled (e.g., water, building, forest), allowing <strong>precise delineation</strong> of different land cover types. This level of detail is especially useful in geospatial contexts for creating thematic maps and analyzing spatial patterns.</p>
<p>For example, in a satellite image we might label each pixel as water, building, forest, road, etc., thus outlining the exact shapes and areas of these features. Whereas an image classification might tell us an entire satellite patch is “urban” or “agriculture,” semantic segmentation can highlight <strong>exactly which pixels</strong> are buildings, roads, vegetation, water, etc., giving a much richer understanding of the scene.</p>
<p>We contrast these tasks visually and conceptually so the distinction is clear:</p>
<ul>
<li><strong>Image Classification</strong> answers: “Is this satellite patch urban or agricultural?” with a single label for the entire image</li>
<li><strong>Object Detection</strong> answers: “Where are the buildings?” by drawing bounding boxes around each structure</li>
<li><strong>Semantic Segmentation</strong> answers: “Label every pixel as water, forest, urban, or agriculture” producing a complete pixel-level classification map</li>
</ul>
<p>This pixel-level approach is crucial for Earth Observation tasks like mapping floods, land cover, or burn scars, where knowing the <strong>exact extent and shape</strong> of features matters for decision-making, planning, and response.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Why Semantic Segmentation for EO?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Semantic segmentation offers several critical advantages for Earth Observation applications that make it indispensable for many geospatial analysis tasks:</p>
<p><strong>Precise Delineation:</strong> Segmentation provides exact boundaries of features—the precise edge of flood extent, the exact boundary where forest stops and urban area begins, the specific outline of agricultural fields. This pixel-level precision is far superior to bounding boxes or image-level labels.</p>
<p><strong>Quantitative Analysis:</strong> With pixel-wise classification, we can calculate accurate areas down to the precision of individual pixels. For flood mapping, this means knowing exactly how many square kilometers are inundated. For forest monitoring, it means precise measurements of deforestation extent.</p>
<p><strong>Change Detection:</strong> Pixel-level comparison over time enables detailed change detection. We can identify exactly which pixels changed from forest to urban, or from dry land to water, enabling fine-grained temporal analysis.</p>
<p><strong>Thematic Mapping:</strong> Segmentation produces detailed land cover and land use maps where every location has a meaningful class label, creating rich thematic datasets for analysis, planning, and decision-making.</p>
<p><strong>Decision Support:</strong> The fine-grained information from segmentation directly supports disaster response and planning. For typhoon flood assessment, segmentation provides exact flood boundaries for targeted relief operations—identifying which specific buildings or roads are affected—not just a general “flooded” vs “not flooded” assessment for an entire region.</p>
</div>
</div>
</section>
<section id="three-task-comparison" class="level3">
<h3 class="anchored" data-anchor-id="three-task-comparison">Three Task Comparison</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 23%">
<col style="width: 28%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Classification</th>
<th>Object Detection</th>
<th>Semantic Segmentation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Question</strong></td>
<td>What’s in this image?</td>
<td>Where are objects?</td>
<td>Which pixels are what?</td>
</tr>
<tr class="even">
<td><strong>Output</strong></td>
<td>Single label</td>
<td>Bounding boxes + labels</td>
<td>Pixel-wise mask</td>
</tr>
<tr class="odd">
<td><strong>Granularity</strong></td>
<td>Image-level</td>
<td>Object-level</td>
<td>Pixel-level</td>
</tr>
<tr class="even">
<td><strong>Spatial Info</strong></td>
<td>None</td>
<td>Approximate (boxes)</td>
<td>Precise (pixels)</td>
</tr>
<tr class="odd">
<td><strong>Computation</strong></td>
<td>Fast</td>
<td>Moderate</td>
<td>Intensive</td>
</tr>
<tr class="even">
<td><strong>Use Case Example</strong></td>
<td>“Contains buildings”</td>
<td>“10 buildings detected”</td>
<td>“Building footprints mapped”</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="part-2-u-net-architecture" class="level2">
<h2 class="anchored" data-anchor-id="part-2-u-net-architecture">Part 2: U-Net Architecture</h2>
<section id="introduction-to-u-net" class="level3">
<h3 class="anchored" data-anchor-id="introduction-to-u-net">Introduction to U-Net</h3>
<p><strong>U-Net</strong> was developed by Ronneberger et al.&nbsp;(2015) for biomedical image segmentation and has since become one of the most popular architectures for Earth Observation applications.</p>
<p><strong>Why “U-Net”?</strong> - Architecture shape resembles the letter “U” - Symmetric encoder-decoder structure - <strong>Key Innovation:</strong> Skip connections that preserve spatial information</p>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A[Input Image&lt;br/&gt;H × W × C] --&gt; B[Encoder&lt;br/&gt;Contracting Path]
    B --&gt; C[Bottleneck&lt;br/&gt;Most Compressed]
    C --&gt; D[Decoder&lt;br/&gt;Expansive Path]
    D --&gt; E[Output Mask&lt;br/&gt;H × W × Classes]
    
    B -.-&gt;|Skip Connection 1| D
    B -.-&gt;|Skip Connection 2| D
    B -.-&gt;|Skip Connection 3| D
    B -.-&gt;|Skip Connection 4| D
    
    B --&gt; B1["Feature Extraction&lt;br/&gt;Spatial dimension ↓&lt;br/&gt;Feature depth ↑"]
    C --&gt; C1["Global Context&lt;br/&gt;What is in image"]
    D --&gt; D1["Spatial Reconstruction&lt;br/&gt;Spatial dimension ↑&lt;br/&gt;Feature depth ↓"]
    
    style A fill:#0066cc,stroke:#003d7a,stroke-width:2px,color:#fff
    style E fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff
    style C fill:#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff
</pre>
</div>
<p></p><figcaption> U-Net Architecture Overview</figcaption> </figure><p></p>
</div>
</div>
</div>
</section>
<section id="encoder-contracting-path" class="level3">
<h3 class="anchored" data-anchor-id="encoder-contracting-path">Encoder (Contracting Path)</h3>
<p><strong>Purpose:</strong> Extract hierarchical features at multiple scales while progressively compressing spatial information</p>
<p>The encoder is a series of convolutional and pooling layers that progressively downsample the image, extracting higher-level features while reducing spatial resolution (just as we learned with CNNs on Day 2).</p>
<p><strong>Operations:</strong> 1. <strong>Convolution blocks:</strong> - Two 3×3 convolutional layers (with ReLU activations) - (Optional) Batch normalization - <strong>Note:</strong> Often uses “same” padding to preserve spatial dimensions through conv layers</p>
<ol start="2" type="1">
<li><strong>Downsampling:</strong>
<ul>
<li>2×2 max pooling (reduces spatial resolution)</li>
<li>Spatial dimensions halve</li>
<li>Feature channels double</li>
<li>Creates hierarchical representation</li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Connection to Day 2 Concepts
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Recall from Day 2:</strong> The encoder uses the same CNN building blocks you learned: - <strong>Convolution layers</strong> apply learnable filters to extract features - <strong>Padding</strong> (“same” padding) helps preserve spatial dimensions so feature maps align for skip connections - <strong>Pooling</strong> reduces dimensionality—recall from Day 2 that pooling without padding reduces image size, losing some detail - <strong>ReLU activation</strong> introduces non-linearity</p>
<p>For instance, using 3×3 convolutions (with ReLU activations) and 2×2 max-pooling, the encoder learns rich features but shrinks the image size at each step. As we move down the encoder, image details are compressed and abstracted, capturing the <strong>context</strong> of what is in the image.</p>
</div>
</div>
<p><strong>Example Progression:</strong></p>
<pre><code>Input:     256×256×3   (RGB satellite image)
Block 1:   256×256×64  (after convolutions)
Pool 1:    128×128×64  (after max pooling)
Block 2:   128×128×128 (after convolutions)
Pool 2:    64×64×128   (after max pooling)
Block 3:   64×64×256   (after convolutions)
Pool 3:    32×32×256   (after max pooling)
Block 4:   32×32×512   (after convolutions)
Pool 4:    16×16×512   (after max pooling)</code></pre>
<p><strong>Multi-Scale Learning:</strong> - <strong>Early layers:</strong> Capture fine details (edges, textures, small features) - <strong>Deep layers:</strong> Capture semantic meaning (water bodies, urban areas, forests)</p>
</section>
<section id="bottleneck-layer" class="level3">
<h3 class="anchored" data-anchor-id="bottleneck-layer">Bottleneck Layer</h3>
<p>The central part of U-Net is the <strong>bottleneck layer</strong> (the bottom of the “U”), where the feature representation is most compressed. This is where the network holds a condensed encoding of the image’s content—<strong>maximum context, minimum spatial detail</strong>—before the decoder begins expanding it.</p>
<p><strong>Characteristics of the Bottleneck:</strong></p>
<p>At this point, we have the smallest spatial dimensions (e.g., 16×16 pixels) but the largest number of feature channels (e.g., 1024). This creates a highly compressed representation of the entire image.</p>
<p><strong>What It Captures:</strong></p>
<p>The bottleneck captures <strong>global context</strong>—it understands <strong>what’s in the image</strong> at a semantic level. It contains information like “there is water, buildings, vegetation” but has lost the precise spatial information about <strong>where exactly</strong> these features are located. This trade-off is intentional: by compressing spatial dimensions while expanding feature depth, the encoder creates an abstract, semantic understanding of the scene that the decoder can then use to reconstruct precise pixel-wise predictions.</p>
</section>
<section id="decoder-expansive-path" class="level3">
<h3 class="anchored" data-anchor-id="decoder-expansive-path">Decoder (Expansive Path)</h3>
<p><strong>Purpose:</strong> Reconstruct spatial resolution using encoded features to construct precise pixel-wise predictions</p>
<p>The decoder performs the reverse of the encoder: it uses upsampling (e.g., transpose convolutions) to increase the spatial resolution, gradually building the output segmentation map.</p>
<p><strong>Operations:</strong> 1. <strong>Upsampling:</strong> - <strong>Transpose convolution</strong> (learnable filters for upsampling, sometimes called “deconvolution”) OR - <strong>Bilinear/nearest upsampling</strong> (simpler interpolations) + regular convolution - Doubles spatial dimensions - Halves feature channels - Mirrors encoder downsampling in reverse</p>
<ol start="2" type="1">
<li><strong>Skip Connection Concatenation:</strong>
<ul>
<li>Copy high-resolution feature maps from corresponding encoder layer</li>
<li>The feature maps from the encoder are copied and <strong>concatenated</strong> onto the decoder’s feature maps at corresponding levels</li>
<li>Fuse high-resolution spatial details with semantic understanding</li>
<li><strong>Critical:</strong> Feature map sizes must align (achieved through proper padding in encoder)</li>
</ul></li>
<li><strong>Convolution blocks:</strong>
<ul>
<li>Two 3×3 convolutional layers<br>
</li>
<li>ReLU activation</li>
<li>Refine combined features into sharper predictions</li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Upsampling and Implementation Details
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Recall from Day 2:</strong> Upsampling is essentially the <strong>inverse of pooling</strong>—it increases spatial dimensions to expand the image back to full size.</p>
<p><strong>Two common approaches:</strong> - <strong>Transpose convolution:</strong> Learned transposed conv layers (sometimes called “deconvolution”) with trainable filters - <strong>Interpolation + Conv:</strong> Simpler interpolations (bilinear or nearest neighbor) followed by regular conv to refine</p>
<p><strong>Implementation Note:</strong> To make concatenation in skip connections seamless, we often use padding in convolutions to maintain equal sizes between encoder and decoder feature maps (Day 2 covered how “same” padding keeps dimensions). The <strong>original U-Net paper cropped feature maps</strong> instead, but modern frameworks simply pad zeros so that encoder outputs and decoder inputs align.</p>
<p>By the end of the decoder, a <strong>1×1 convolution</strong> produces the final segmentation map, with as many channels as target classes, so that each pixel receives a class label.</p>
</div>
</div>
<p><strong>Example Progression:</strong></p>
<pre><code>Bottleneck:  16×16×1024
Upsample 1:  32×32×512
Concat:      32×32×1024  (512 from decoder + 512 from encoder skip)
Conv Block:  32×32×512
Upsample 2:  64×64×256
Concat:      64×64×512   (256 from decoder + 256 from encoder skip)
Conv Block:  64×64×256
...
Final:       256×256×num_classes</code></pre>
</section>
<section id="skip-connections---the-key-innovation" class="level3">
<h3 class="anchored" data-anchor-id="skip-connections---the-key-innovation">Skip Connections - The Key Innovation</h3>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Why Skip Connections Matter
</div>
</div>
<div class="callout-body-container callout-body">
<p>A key innovation of U-Net is the <strong>skip connections</strong> linking matching encoder and decoder layers. The feature maps from the encoder (which contain fine-grained spatial details from earlier layers) are <strong>concatenated</strong> with the upsampled features in the decoder. This allows the model to “skip over” the bottleneck and directly inject high-resolution context into the decoding process.</p>
<p><strong>The Problem They Solve:</strong></p>
<p>Without skip connections, information is inevitably lost during the downsampling process (pooling operations). The decoder would have to reconstruct precise boundaries solely from the coarse, compressed features at the bottleneck. This results in blurry boundaries and loss of fine spatial detail—exactly what we want to avoid in Earth Observation applications.</p>
<p><strong>How Skip Connections Help:</strong></p>
<p>The skip connections preserve edges and small structures (e.g., the exact boundary of a flooded area or building outline) that might otherwise be lost during downsampling. The result is <strong>improved detail and accuracy</strong> in segmentation outputs, since the decoder doesn’t have to rely solely on the coarse feature maps after upsampling—it can leverage the original fine details as well.</p>
<p><strong>Best of Both Worlds:</strong></p>
<p>Crucially, U-Net’s decoder is fed by skip connections from the encoder: these skip connections provide high-resolution context to the decoder, ensuring that fine details (like precise boundaries) are preserved even after the image was compressed by the encoder.</p>
<p>In essence, the <strong>encoder captures what is in the image</strong> (context), and the <strong>decoder, aided by skips, ensures we know where those things are</strong> in the image (precise localization). By combining encoder and decoder features, U-Net captures both the <strong>what</strong> (context from the semantic understanding in the bottleneck) and the <strong>where</strong> (location from the high-resolution encoder features) for each class in the image.</p>
<p>Notably, U-Net implementations must handle the alignment of feature map sizes for concatenation—often using appropriate padding (“same” padding) on convolutions so that each encoder output matches the size of the corresponding decoder feature map.</p>
</div>
</div>
</section>
<section id="how-skip-connections-work" class="level3">
<h3 class="anchored" data-anchor-id="how-skip-connections-work">How Skip Connections Work</h3>
<p>Let’s walk through the skip connection mechanism step-by-step to understand how it preserves spatial information:</p>
<p><strong>The Process:</strong></p>
<ol type="1">
<li>As the encoder processes the input, it produces a feature map at a specific resolution, say 128×128×64 (128×128 spatial dimensions, 64 feature channels)</li>
<li>This feature map is <strong>copied and temporarily saved</strong> before any further processing</li>
<li>The encoder continues its downsampling path, applying pooling to reduce spatial dimensions further</li>
<li>The process continues through the bottleneck, where the representation is most compressed</li>
<li>The decoder begins upsampling, bringing the spatial dimensions back up. It produces, for example, a 128×128×32 feature map</li>
<li><strong>Concatenation happens:</strong> The decoder’s upsampled features (128×128×32) are combined channel-wise with the saved encoder features (128×128×64)</li>
<li>The <strong>result</strong> is a 128×128×96 combined feature map containing:
<ul>
<li><strong>High-level semantic context</strong> from the decoder path (understanding of what objects are present)</li>
<li><strong>Fine spatial details</strong> from the encoder path (precise localization of boundaries)</li>
</ul></li>
</ol>
<p><strong>Real-World Impact in Earth Observation:</strong></p>
<p>The difference is dramatic. In flood mapping applications, for instance: - <strong>Without skip connections:</strong> Flood boundary accuracy might be ±10 pixels (100-200 meters at 10m resolution) - <strong>With skip connections:</strong> Flood boundary accuracy improves to ±1-2 pixels (10-20 meters)</p>
<p>This precision is critical for applications requiring legal boundaries, property lines, or hazard zone delineation, where even small errors can have significant consequences for decision-making and resource allocation.</p>
</section>
<section id="u-net-complete-architecture-summary" class="level3">
<h3 class="anchored" data-anchor-id="u-net-complete-architecture-summary">U-Net Complete Architecture Summary</h3>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A[Input: 256×256×3] --&gt; B1[Conv + ReLU]
    B1 --&gt; B2[Conv + ReLU]
    B2 --&gt; C1[MaxPool ↓]
    B2 -.-&gt;|Skip 1| G1
    
    C1 --&gt; D1[Conv + ReLU]
    D1 --&gt; D2[Conv + ReLU]
    D2 --&gt; E1[MaxPool ↓]
    D2 -.-&gt;|Skip 2| F1
    
    E1 --&gt; F0[Bottleneck&lt;br/&gt;Conv Layers]
    
    F0 --&gt; F1[Upsample ↑]
    F1 --&gt; G1[Concatenate]
    G1 --&gt; H1[Conv + ReLU]
    
    H1 --&gt; I1[Upsample ↑]
    I1 --&gt; J1[Concatenate]
    J1 --&gt; K1[Conv + ReLU]
    
    K1 --&gt; L[Output: 256×256×Classes]
    
    style A fill:#0066cc,stroke:#003d7a,stroke-width:2px,color:#fff
    style F0 fill:#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff
    style L fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff
</pre>
</div>
<p></p><figcaption> U-Net Information Flow</figcaption> </figure><p></p>
</div>
</div>
</div>
<hr>
</section>
</section>
<section id="part-3-applications-in-earth-observation" class="level2">
<h2 class="anchored" data-anchor-id="part-3-applications-in-earth-observation">Part 3: Applications in Earth Observation</h2>
<section id="why-u-net-is-popular-in-eo" class="level3">
<h3 class="anchored" data-anchor-id="why-u-net-is-popular-in-eo">Why U-Net is Popular in EO</h3>
<p>U-Net has become a go-to architecture for many Earth Observation segmentation tasks due to its accuracy and efficiency in learning from limited data. Several key factors contribute to its widespread adoption:</p>
<p><strong>Data Efficiency:</strong></p>
<p>U-Net performs well with relatively modest amounts of training data—typically hundreds to thousands of training samples rather than the millions required by some other deep learning approaches. Data augmentation techniques (rotations, flips, which are particularly relevant for satellite nadir views) help further. This is <strong>critical</strong> when labeled EO data is expensive and time-consuming to acquire, requiring expert annotators and field validation.</p>
<p><strong>Spatial Precision:</strong></p>
<p>The skip connections preserve fine boundaries with remarkable accuracy, which is important for applications requiring legal boundaries, property lines, or precise hazard zone delineation. This enables accuracy from millimeter to meter level, depending on the input imagery resolution—essential for cadastral mapping, flood extent determination, and infrastructure monitoring.</p>
<p><strong>Multi-Scale Learning:</strong></p>
<p>The encoder’s hierarchical structure captures both local textures (in early layers) and global context (in deeper layers). This is essential for the varied scales of EO features, from small boats (a few pixels) to large water bodies (thousands of pixels). U-Net handles objects at multiple scales simultaneously within a single architecture.</p>
<p><strong>Transfer Learning Capability:</strong></p>
<p>U-Net encoders can leverage pre-trained weights from ImageNet or other large-scale datasets, enabling domain adaptation from natural images to satellite imagery. This significantly improves performance when labeled EO data is limited, allowing the model to start with general feature extraction capabilities and fine-tune to specific remote sensing tasks.</p>
</section>
<section id="application-1-flood-mapping" class="level3">
<h3 class="anchored" data-anchor-id="application-1-flood-mapping">Application 1: Flood Mapping</h3>
<p><strong>Use Case:</strong> Disaster response and damage assessment</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Philippine Context: Typhoon Flood Mapping
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Data Sources:</strong> - <strong>Sentinel-1 SAR</strong> (cloud-penetrating, all-weather capability) - <strong>Sentinel-2 optical</strong> (high resolution when clouds permit)</p>
<p><strong>Task:</strong> - Binary segmentation: Flooded vs Non-flooded pixels - Input: SAR backscatter (VV, VH polarizations) or optical RGB+NIR - Output: Precise flood extent mask</p>
<p><strong>Why U-Net Excels:</strong> - U-Net has been used to segment flooded areas in Sentinel-1 SAR and Sentinel-2 optical images with <strong>high accuracy</strong> - Studies have shown U-Net is effective in capturing flood patterns in SAR imagery, achieving high accuracy in delineating water from land - During floods, produces a <strong>binary map of floodwater vs.&nbsp;non-flood</strong> for each pixel, identifying flooded pixels vs.&nbsp;dry pixels across an entire region - Enables <strong>rapid assessment</strong> of flood extent for emergency response - Research shows U-Net achieves robust results even with relatively <strong>small training datasets</strong></p>
<p><strong>Benefits:</strong> - Rapid mapping within hours of satellite acquisition - Precise area calculations for damage assessment<br>
- Time-series monitoring of flood evolution and recession - Integration with GIS for evacuation planning and relief distribution</p>
<p><strong>Real Example:</strong> Typhoon Ulysses (2020) - Central Luzon floods mapped using U-Net on Sentinel-1 data, providing precise inundation extent for affected municipalities in the Pampanga River Basin.</p>
</div>
</div>
</section>
<section id="application-2-land-cover-mapping" class="level3">
<h3 class="anchored" data-anchor-id="application-2-land-cover-mapping">Application 2: Land Cover Mapping</h3>
<p><strong>Use Case:</strong> Environmental monitoring, urban planning, biodiversity assessment</p>
<p><strong>Data Sources:</strong> - Sentinel-2 multispectral (10m resolution) - Landsat 8/9 (30m, long time series) - High-resolution commercial imagery</p>
<p><strong>Task:</strong> - Multi-class segmentation: Water, Forest, Urban, Agriculture, Barren, Mangrove - Input: Multi-spectral bands (RGB, NIR, SWIR, Red Edge) - Output: Detailed land cover classification map</p>
<p><strong>U-Net’s Strength:</strong> - Combines <strong>broad context</strong> (distinguishing urban area from forest in general) with <strong>precise boundaries</strong> (exactly where forest stops and urban begins) - U-Net’s ability to preserve fine details helps <strong>delineate boundaries</strong> between different land cover types - Can outline exact shapes of urban districts, small water bodies, or forest edges - Research shows U-Net often <strong>outperforms older pixel-based or patch-based methods</strong> in remote sensing</p>
<p><strong>Benefits:</strong> - Pixel-accurate thematic maps - Change detection over time (deforestation, urbanization) - Biodiversity habitat assessments<br>
- Carbon stock estimation for climate reporting</p>
</section>
<section id="application-3-road-network-extraction" class="level3">
<h3 class="anchored" data-anchor-id="application-3-road-network-extraction">Application 3: Road Network Extraction</h3>
<p><strong>Use Case:</strong> Map updating, transportation planning, accessibility analysis</p>
<p><strong>Challenges:</strong> - Thin linear features difficult to detect - Occlusion by trees and shadows - Complex urban backgrounds - Need to maintain continuous structure</p>
<p><strong>U-Net Advantages:</strong> - Skip connections preserve <strong>road continuity</strong> (prevents gaps) - Learns to follow <strong>linear patterns</strong> across the image - Handles varying road widths (from highways to small paths) - Can trace continuous structures like roads and railways</p>
<p><strong>Task:</strong> - Binary segmentation: Road vs Background - Input: High-resolution aerial/satellite RGB or SAR - Output: Road network mask for vectorization</p>
<p><strong>Applications:</strong> - Automated map updating for rural areas - Transportation network planning - Accessibility analysis for disaster response</p>
</section>
<section id="application-4-building-footprint-delineation" class="level3">
<h3 class="anchored" data-anchor-id="application-4-building-footprint-delineation">Application 4: Building Footprint Delineation</h3>
<p><strong>Use Case:</strong> Urban mapping, population estimation, disaster risk assessment</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Philippine Application: Informal Settlement Detection
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Relevance:</strong> - Monitor unplanned urban growth in Metro Manila - Identify disaster-vulnerable communities<br>
- Support urban planning and housing programs</p>
<p><strong>Task:</strong> - Binary or multi-class: Building vs Background (or building types) - Input: Very high-resolution imagery (&lt;1m) or Sentinel-2 for large structures - Output: Building footprint polygons</p>
<p><strong>U-Net Performance:</strong> - With appropriate high-resolution data, U-Net can <strong>outline individual buildings</strong> or dense informal settlements, even with complex backgrounds - U-Net-based models have been used to extract buildings in urban areas and to map roads winding through forests or cities - Variants like <strong>Residual U-Net</strong> or <strong>Attention U-Net</strong> also popular for building segmentation - Core idea remains: encoder-decoder with skip connections for precise boundaries - Instead of just saying “there are buildings in this image,” we get a map of where each building is</p>
<p><strong>Benefits:</strong> - Automated mapping at scale - Pre/post disaster damage assessment - 3D city model generation (with height data) - Infrastructure planning - Aids urban planning and risk assessment</p>
</div>
</div>
</section>
<section id="application-5-vegetation-and-crop-monitoring" class="level3">
<h3 class="anchored" data-anchor-id="application-5-vegetation-and-crop-monitoring">Application 5: Vegetation and Crop Monitoring</h3>
<p><strong>Use Case:</strong> Precision agriculture, forestry, ecosystem health</p>
<p><strong>Data Sources:</strong> - Sentinel-2 multispectral (5-day revisit) - PlanetScope (3m daily coverage) - UAV imagery for field-scale monitoring</p>
<p><strong>Task:</strong> - Multi-class segmentation: Crop types (rice, corn, sugarcane, coconut) - Or binary: Vegetation vs Non-vegetation - Input: Multi-temporal + multi-spectral imagery - Output: Crop type map or vegetation mask</p>
<p><strong>U-Net Applications:</strong> - Identifying <strong>crop fields</strong> and <strong>forest cover</strong> at pixel level - Monitoring agricultural areas for food security - Tracking <strong>tree cover</strong> for forestry management - Detecting vegetation changes and health patterns</p>
<p><strong>Benefits:</strong> - Yield prediction and harvest planning - Irrigation requirement monitoring - Early disease detection - Deforestation and illegal logging tracking</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Research Evidence
</div>
</div>
<div class="callout-body-container callout-body">
<p>Across these examples, research and practice have shown <strong>U-Net achieves high segmentation accuracy</strong> in remote sensing. It has been demonstrated that U-Net can achieve <strong>robust results with relatively small training datasets</strong>, thanks to the efficiency of the architecture—one reason it was originally successful in medical imaging with limited training images.</p>
</div>
</div>
<hr>
</section>
</section>
<section id="part-4-loss-functions-for-segmentation" class="level2">
<h2 class="anchored" data-anchor-id="part-4-loss-functions-for-segmentation">Part 4: Loss Functions for Segmentation</h2>
<section id="why-loss-functions-matter" class="level3">
<h3 class="anchored" data-anchor-id="why-loss-functions-matter">Why Loss Functions Matter</h3>
<p>Training a segmentation model requires choosing an appropriate <strong>loss function</strong> that compares the predicted mask to the ground truth mask. The loss function is the mathematical measure that tells the model how wrong its predictions are, guiding the weight updates during training.</p>
<p><strong>The Challenge in Segmentation:</strong></p>
<p>Unlike image classification where we compare a single predicted label to a single true label, segmentation requires comparing entire images pixel-by-pixel. We’re not evaluating just one value—we must compare potentially millions of pixel predictions across the entire image.</p>
<p>Different loss functions emphasize different aspects of the prediction: - Some focus on <strong>pixel-wise accuracy</strong> (is each individual pixel correct?) - Others focus on <strong>region overlap</strong> (does the predicted flood extent match the true extent?) - Some emphasize <strong>boundary accuracy</strong> (are the edges of objects precisely delineated?)</p>
<p><strong>Why Choice Matters:</strong></p>
<p>The choice of loss function <strong>critically affects model behavior</strong>. Several loss functions are common in segmentation, each with different strengths. In the following sections, we’ll explore the main options and understand when to use each one for Earth Observation applications.</p>
</section>
<section id="challenge-class-imbalance-in-eo" class="level3">
<h3 class="anchored" data-anchor-id="challenge-class-imbalance-in-eo">Challenge: Class Imbalance in EO</h3>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>Common Imbalanced Scenarios
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>In segmentation of EO data, class imbalance is a typical issue:</strong></p>
<p><strong>Flood Mapping:</strong> - 95% non-flooded pixels, 5% flooded pixels - Think of mapping floods: the flooded pixels are usually <strong>far fewer</strong> than non-flooded</p>
<p><strong>Ship Detection:</strong> - 99.5% water/land, 0.5% ships</p>
<p><strong>Building Segmentation:</strong> - 80% background, 20% buildings</p>
<p><strong>Problem with Simple Accuracy:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model predicts: ALL pixels = "non-flooded"</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy: 95% ✓ (looks great!)</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># But: Completely useless - missed all floods!</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Result: Trivial but useless prediction</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why This Happens:</strong> In imbalanced cases, vanilla cross-entropy can be <strong>dominated by the majority class</strong>. A poor choice of loss might lead the model to predict all pixels as the majority class—achieving high accuracy but providing no useful information.</p>
<p><strong>Need Loss Functions That:</strong> - Handle severe class imbalance - Focus on minority (critical) class - Reward region overlap, not just pixel-wise correctness - Ensure boundaries (edges of floods, building outlines) are accurately captured</p>
</div>
</div>
</section>
<section id="loss-function-1-pixel-wise-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-1-pixel-wise-cross-entropy">Loss Function 1: Pixel-wise Cross-Entropy</h3>
<p><strong>How it Works:</strong> - Treat each pixel as an <strong>independent classification problem</strong> - Compare predicted class probability to true class using <strong>negative log-likelihood</strong> - Average loss across all pixels - Standard approach for multi-class segmentation (or binary cross-entropy for two classes)</p>
<p><strong>Formula (simplified):</strong> <span class="math display">\[
\text{Cross-Entropy} = -\sum_{i=1}^{N} y_{\text{true},i} \cdot \log(y_{\text{pred},i})
\]</span></p>
<p>Where <span class="math inline">\(N\)</span> is the total number of pixels.</p>
<p><strong>Advantages:</strong> - ✓ Standard, well-understood approach - ✓ Strong, stable gradients for learning - ✓ Works naturally with multi-class problems (softmax output) - ✓ Effective and straightforward</p>
<p><strong>Disadvantages:</strong> - ✗ Dominated by majority class in imbalanced data - ✗ Doesn’t directly optimize for spatial overlap or boundary alignment - ✗ Can effectively ignore minority classes - ✗ Focuses on pixel-level accuracy but not region-level correctness</p>
<p><strong>When to Use:</strong> Balanced datasets (~50/50 class distribution) or with class weighting</p>
</section>
<section id="weighted-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="weighted-cross-entropy">Weighted Cross-Entropy</h3>
<p><strong>Solution to Imbalance:</strong> Assign higher weight to under-represented classes so the model pays more attention to them</p>
<p><strong>Formula:</strong> <span class="math display">\[
\text{Weighted CE} = -\sum_{i=1}^{N} w_{\text{class}} \cdot y_{\text{true},i} \cdot \log(y_{\text{pred},i})
\]</span></p>
<p><strong>Example:</strong> - 95% background pixels → weight = 1.0 - 5% flood pixels → weight = 19.0 (inverse frequency: 95/5)</p>
<p><strong>Effect:</strong> - Model pays 19× more attention to flood pixels - Heavily penalized for missing flood pixels - <strong>Common remedy</strong> for class imbalance in segmentation</p>
<p><strong>Implementation (TensorFlow/Keras):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> tf.keras.losses.CategoricalCrossentropy(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    class_weight<span class="op">=</span>{<span class="dv">0</span>: <span class="fl">1.0</span>, <span class="dv">1</span>: <span class="fl">19.0</span>}  <span class="co"># background, flood</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>When Weighted CE Helps
</div>
</div>
<div class="callout-body-container callout-body">
<p>Weighted cross-entropy provides strong gradients for learning while addressing imbalance. However, it still focuses on <strong>pixel-wise accuracy</strong> and doesn’t directly ensure good overlap or boundary alignment—that’s where overlap-based losses come in.</p>
</div>
</div>
</section>
<section id="loss-function-2-dice-loss" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-2-dice-loss">Loss Function 2: Dice Loss</h3>
<p><strong>Concept:</strong> Measure overlap between prediction and ground truth regions<br>
<strong>Training Goal:</strong> Maximize Dice (or minimize 1 - Dice) to encourage the network to get the segmentation overlap as high as possible</p>
<p><strong>Formula:</strong> <span class="math display">\[
\text{Dice Coefficient} = \frac{2 \times |P \cap T|}{|P| + |T|}
\]</span></p>
<p><span class="math display">\[
\text{Dice Loss} = 1 - \text{Dice Coefficient}
\]</span></p>
<p>Where: - <span class="math inline">\(P\)</span> = predicted foreground pixels - <span class="math inline">\(T\)</span> = true foreground pixels - <span class="math inline">\(\cap\)</span> = intersection (overlap) - Essentially: <span class="math inline">\(\frac{2 \times (\text{intersection})}{(\text{sum of areas})}\)</span></p>
<p><strong>Interpretation:</strong> - Dice = 1.0: Perfect overlap - Dice = 0.5: 50% overlap - Dice = 0.0: No overlap - Loss = 0.0: Perfect (lower is better)</p>
<p><strong>Why for Imbalanced Data?</strong> - Focuses on the <strong>relative overlap</strong> of the object (minority class) - Treats foreground and background asymmetrically - <strong>Correctly segmenting one small flooded patch</strong> contributes as much to Dice as a large region of non-flood - Inherently handles class imbalance <strong>without manual weighting</strong> - Particularly well-suited when target objects occupy a <strong>small fraction</strong> of the image - Doesn’t let the model get complacent by only predicting the majority class, because it focuses on the overlap of the positive (target) class</p>
<p><strong>When False Positives and Negatives Need Equal Weight:</strong> Dice loss is known to be effective when <strong>false negatives</strong> (missing floods) and <strong>false positives</strong> (predicting flood where there isn’t) need to be weighted equally. It treats false negatives and false positives more equally than cross-entropy.</p>
<p><strong>Advantages:</strong> - ✓ Inherently robust to class imbalance - ✓ Directly optimizes overlap metric (F1-score for segmentation) - ✓ Excellent for small objects - ✓ No need to manually set class weights - ✓ Helps model not ignore small structures or minority classes</p>
<p><strong>Disadvantages:</strong> - ✗ Less stable gradients (can be noisy early in training) - ✗ May converge slower than cross-entropy - ✗ Requires careful implementation (avoid division by zero)</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Medical Imaging Parallel
</div>
</div>
<div class="callout-body-container callout-body">
<p>Many medical image segmentation models use Dice loss to segment <strong>tumors that occupy only a tiny area</strong> of the image—exactly the same challenge as segmenting small flood patches in vast satellite scenes.</p>
</div>
</div>
</section>
<section id="loss-function-3-iou-loss-jaccard-index" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-3-iou-loss-jaccard-index">Loss Function 3: IoU Loss (Jaccard Index)</h3>
<p><strong>Concept:</strong> Similar to Dice, directly optimizes the IoU metric<br>
<strong>Also Known As:</strong> Jaccard Index</p>
<p><strong>Formula:</strong> <span class="math display">\[
\text{IoU} = \frac{|P \cap T|}{|P \cup T|}
\]</span></p>
<p><span class="math display">\[
\text{IoU Loss} = 1 - \text{IoU}
\]</span></p>
<p>Where <span class="math inline">\(\cup\)</span> represents the union of predicted and true pixels.</p>
<p><strong>Difference from Dice:</strong> - <strong>IoU:</strong> Intersection / Union - <strong>Dice:</strong> 2 × Intersection / (Sum of areas) - Numerically different but conceptually similar - Related by: <span class="math inline">\(\text{Dice} = \frac{2 \times \text{IoU}}{1 + \text{IoU}}\)</span></p>
<p><strong>Properties:</strong> - <strong>Robust to class imbalance</strong> (like Dice) - Emphasizes <strong>boundary accuracy</strong>—maximizing IoU requires the predicted region to <strong>align well</strong> with the true region boundaries - Penalizes false positives and false negatives <strong>equally at the region level</strong> - Standard metric in segmentation challenges (evaluation)</p>
<p><strong>Why for EO?</strong> - Accurately capturing <strong>boundaries</strong> (edge of a flood, outline of a building) is often vital in geographic mapping - IoU-based loss directly rewards aligning shapes - Useful in applications like geographic mapping where <strong>boundary delineation is crucial</strong></p>
</section>
<section id="dice-vs-iou---when-to-choose" class="level3">
<h3 class="anchored" data-anchor-id="dice-vs-iou---when-to-choose">Dice vs IoU - When to Choose?</h3>
<p>Both Dice and IoU losses are very similar in concept—they both measure overlap between predicted and true regions—but they have subtle differences that can affect training:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Aspect</th>
<th>Dice Loss</th>
<th>IoU Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Formulation</strong></td>
<td>2×Intersection / Sum</td>
<td>Intersection / Union</td>
</tr>
<tr class="even">
<td><strong>Gradient</strong></td>
<td>More forgiving (2× numerator)</td>
<td>Stricter</td>
</tr>
<tr class="odd">
<td><strong>Training</strong></td>
<td>Smoother, more stable</td>
<td>Can be less stable</td>
</tr>
<tr class="even">
<td><strong>Evaluation</strong></td>
<td>Common in medical/EO</td>
<td>Standard in challenges</td>
</tr>
<tr class="odd">
<td><strong>Boundary Focus</strong></td>
<td>Moderate</td>
<td>Higher emphasis</td>
</tr>
</tbody>
</table>
<p><strong>Practical Guidance:</strong></p>
<p>In practice, both Dice and IoU work well for imbalanced EO data. Dice is slightly more popular in training due to its smoother gradients, while IoU is often used as an evaluation metric in segmentation challenges and competitions. The best approach is to try both on your specific dataset and compare results—the difference is often small, but one may work slightly better depending on your particular data characteristics and class distribution.</p>
<p>Very similar to Dice, IoU loss directly optimizes the IoU metric and has an interpretation closely tied to segmentation quality—it penalizes false positives and false negatives at the region level. This makes it useful in applications like geographic mapping where boundary delineation is crucial.</p>
</section>
<section id="loss-function-4-combined-losses" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-4-combined-losses">Loss Function 4: Combined Losses</h3>
<p><strong>Best of Both Worlds:</strong> Combine complementary loss functions<br>
<strong>In Practice:</strong> You don’t have to pick just one—many implementations use a combination</p>
<p><strong>Common Combination:</strong> <span class="math display">\[
\text{Total Loss} = \alpha \cdot \text{CE Loss} + \beta \cdot \text{Dice Loss}
\]</span></p>
<p>Where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are weighting factors (e.g., <span class="math inline">\(\alpha=0.5\)</span>, <span class="math inline">\(\beta=0.5\)</span>)</p>
<p><strong>Why Combine?</strong> - <strong>Cross-Entropy:</strong> Provides strong, stable gradients; ensures overall pixel-wise correctness - <strong>Dice:</strong> Focuses on overlap and handles imbalance; ensures region-level accuracy - Get the <strong>benefits of both</strong> loss functions</p>
<p><strong>Benefits:</strong> - Stable training from CE’s strong signal - Balanced optimization from Dice’s overlap focus - Often achieves <strong>best results in practice</strong> - Can improve both per-pixel accuracy AND overall region accuracy</p>
<p><strong>Implementation Example:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> combined_loss(y_true, y_pred):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    ce <span class="op">=</span> tf.keras.losses.categorical_crossentropy(y_true, y_pred)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    dice <span class="op">=</span> dice_loss(y_true, y_pred)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> ce <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> dice</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Other Advanced Losses
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some practitioners also use <strong>Focal Loss</strong> (a modified cross-entropy that down-weights easy/background examples) especially for <strong>extremely imbalanced cases</strong>, though it’s more common in object detection. The key takeaway: loss function choice significantly affects model behavior.</p>
</div>
</div>
</section>
<section id="loss-function-selection-guide" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-selection-guide">Loss Function Selection Guide</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Decision Framework for Loss Selection
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Start Here:</strong> 1. <strong>Is your data balanced</strong> (classes ~50/50)? - <strong>Yes</strong> → Standard Cross-Entropy (simple and effective) - <strong>No</strong> → Continue to next question</p>
<ol start="2" type="1">
<li><strong>Is the minority class critical</strong> (e.g., floods, damage, ships)?
<ul>
<li><strong>Yes</strong> → Dice or IoU Loss (inherently handle imbalance)</li>
<li><strong>Somewhat</strong> → Weighted Cross-Entropy</li>
</ul></li>
<li><strong>Need most stable training?</strong>
<ul>
<li><strong>Yes</strong> → Combined Loss (CE + Dice) for stability + imbalance handling</li>
<li><strong>No</strong> → Pure Dice/IoU is fine</li>
</ul></li>
<li><strong>Is boundary accuracy critical?</strong>
<ul>
<li><strong>Yes</strong> → IoU Loss or Combined approach</li>
<li><strong>Moderate</strong> → Dice is sufficient</li>
</ul></li>
</ol>
<p><strong>EO Common Practice:</strong> - <strong>Flood mapping:</strong> Dice or Combined (severe imbalance, critical boundaries) - <strong>Balanced land cover:</strong> Cross-Entropy (classes relatively balanced) - <strong>Building extraction:</strong> Dice or IoU (precise footprints matter) - <strong>Road extraction:</strong> Combined Loss (thin features, need continuity) - <strong>Ship detection:</strong> Dice (extreme imbalance, small objects)</p>
<p><strong>Golden Rule:</strong> Participants should learn <strong>not just to accept the default loss</strong>, but to <strong>think about the nature</strong> of their segmentation problem and pick (or tune) a loss accordingly for the best results.</p>
</div>
</div>
</section>
<section id="practical-example-flood-mapping-loss-selection" class="level3">
<h3 class="anchored" data-anchor-id="practical-example-flood-mapping-loss-selection">Practical Example: Flood Mapping Loss Selection</h3>
<p><strong>Scenario:</strong> - Dataset: 1000 Sentinel-1 SAR images from Central Luzon floods - Class distribution: 92% non-flooded, 8% flooded pixels (typical imbalance) - Goal: Precise flood extent mapping for disaster response</p>
<p><strong>Experiment Results:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 45%">
<col style="width: 33%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Loss Function</th>
<th>IoU Score</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Cross-Entropy</strong></td>
<td>0.12</td>
<td>Model predicts mostly non-flooded; misses actual floods (trivial solution)</td>
</tr>
<tr class="even">
<td><strong>Weighted CE</strong></td>
<td>0.54</td>
<td>Better; weight = 11.5× for flood class; some false positives</td>
</tr>
<tr class="odd">
<td><strong>Dice Loss</strong></td>
<td>0.68</td>
<td>Good recall; slightly noisy predictions; handles imbalance well</td>
</tr>
<tr class="even">
<td><strong>Combined (CE + Dice)</strong></td>
<td><strong>0.73</strong> ✓</td>
<td>Best balance of precision and recall; stable training</td>
</tr>
</tbody>
</table>
<p><strong>Winner:</strong> Combined Loss (0.5 × CE + 0.5 × Dice)</p>
<p><strong>Key Insight:</strong> For our flood mapping case (<strong>binary segmentation with severe imbalance</strong>), we might choose a <strong>Dice loss</strong> or a <strong>combined approach (Dice + Cross-Entropy)</strong> to handle the imbalance and get sharp boundaries. This demonstrates that the choice of loss can <strong>significantly affect model training</strong>—the right loss will push the model to correctly segment the minority class rather than achieving high but useless accuracy by predicting everything as the majority class.</p>
<hr>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Session 1 Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Semantic Segmentation:</strong> - ✓ Pixel-wise classification providing precise boundaries and complete spatial understanding - ✓ Differs from image classification (single labels) and object detection (bounding boxes) - ✓ Essential for EO applications requiring exact spatial extent and area calculations - ✓ Enables detailed thematic mapping and spatial pattern analysis</p>
<p><strong>U-Net Architecture:</strong> - ✓ Encoder-decoder structure with characteristic U-shape - ✓ <strong>Skip connections</strong> are the key innovation—preserve spatial details from encoder to decoder - ✓ Combines “what” (semantic context) with “where” (precise localization) - ✓ Proven architecture even with limited training data (hundreds to thousands of samples) - ✓ Widely adopted across EO community for high-accuracy segmentation - ✓ Uses same CNN building blocks from Day 2 (convolution, pooling, padding, activation)</p>
<p><strong>Loss Functions:</strong> - ✓ <strong>Cross-Entropy:</strong> Standard but sensitive to class imbalance; provides strong gradients - ✓ <strong>Weighted CE:</strong> Addresses imbalance through class weighting - ✓ <strong>Dice/IoU:</strong> Inherently handle imbalance, optimize region overlap, focus on minority class - ✓ <strong>Combined losses</strong> often achieve best performance in EO (e.g., CE + Dice) - ✓ <strong>Choice significantly impacts model behavior</strong>—can mean difference between useless and excellent results - ✓ Must consider data characteristics (balance, boundary importance, object size)</p>
<p><strong>Applications:</strong> - ✓ Flood mapping, land cover, buildings, roads, vegetation monitoring - ✓ U-Net achieves high accuracy even with small datasets - ✓ Outperforms older pixel-based and patch-based methods - ✓ Wide adoption across EO community - ✓ Proven results in Philippine disaster response contexts (Typhoon Ulysses, urban mapping)</p>
</div>
</div>
<hr>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<section id="core-references" class="level3">
<h3 class="anchored" data-anchor-id="core-references">Core References</h3>
<p><strong>Foundational Paper:</strong> - Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. <em>MICCAI 2015</em>. <a href="https://arxiv.org/abs/1505.04597">arXiv:1505.04597</a> - Original U-Net paper from medical imaging - Introduced skip connections concept - Demonstrated effectiveness with limited training data</p>
<p><strong>EO Applications:</strong> - Flood mapping with Sentinel-1 SAR and U-Net (disaster response) - Building extraction from high-resolution imagery (urban planning) - Land cover classification with multispectral data (environmental monitoring) - Road network extraction (infrastructure mapping)</p>
<p><strong>Loss Function References:</strong> - Dice Loss for handling class imbalance in segmentation - IoU (Jaccard) Loss for boundary accuracy - Combined loss strategies for optimal performance</p>
</section>
<section id="datasets-for-practice" class="level3">
<h3 class="anchored" data-anchor-id="datasets-for-practice">Datasets for Practice</h3>
<ul>
<li><strong>Sen1Floods11:</strong> Global flood dataset with Sentinel-1 SAR</li>
<li><strong>DeepGlobe Land Cover Challenge:</strong> Multi-class segmentation</li>
<li><strong>SpaceNet Building Detection:</strong> Urban building footprints</li>
<li><strong>Landcover.ai:</strong> High-resolution orthophotos (Poland)</li>
</ul>
</section>
<section id="tutorials" class="level3">
<h3 class="anchored" data-anchor-id="tutorials">Tutorials</h3>
<ul>
<li><a href="https://www.tensorflow.org/tutorials/images/segmentation">TensorFlow U-Net Tutorial</a></li>
<li><a href="https://pytorch.org/vision/stable/models.html#semantic-segmentation">PyTorch Semantic Segmentation Examples</a></li>
<li><a href="https://keras.io/examples/vision/oxford_pets_image_segmentation/">Keras U-Net for Satellite Imagery</a></li>
</ul>
</section>
<section id="philippine-eo-context" class="level3">
<h3 class="anchored" data-anchor-id="philippine-eo-context">Philippine EO Context</h3>
<ul>
<li><strong>PhilSA:</strong> Flood monitoring and disaster response initiatives</li>
<li><strong>DOST-ASTI DATOS:</strong> Automated rapid mapping for disasters</li>
<li><strong>NAMRIA:</strong> Geospatial data infrastructure</li>
</ul>
<hr>
</section>
</section>
<section id="preparation-for-session-2" class="level2">
<h2 class="anchored" data-anchor-id="preparation-for-session-2">Preparation for Session 2</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Next: Hands-on Flood Mapping Lab
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Session 2</strong> will put these concepts into practice with a complete U-Net implementation:</p>
<p><strong>What You’ll Do:</strong> 1. Load Sentinel-1 SAR data from Typhoon Ulysses (Central Luzon) 2. Build U-Net model in TensorFlow/PyTorch 3. Train with Dice Loss (or combined loss) 4. Evaluate performance using IoU, F1-score, precision, recall 5. Visualize flood predictions and create export maps</p>
<p><strong>Dataset:</strong> - ~500-1000 pre-processed SAR patches (256×256 pixels) - Binary flood masks (flooded / non-flooded) - Real flood event from major Philippine river basin</p>
<p><strong>Expected Results:</strong> - IoU &gt; 0.70 with properly trained model - Visual flood extent maps ready for GIS integration</p>
<p><strong>To Prepare:</strong> - Ensure Google Colab access - Check GPU availability (Runtime → Change runtime type → GPU) - Review Python and NumPy basics if needed - Have patience - model training takes time!</p>
<p><a href="../../day3/notebooks/Day3_Session2_Flood_Mapping_UNet.html" class="btn btn-outline-primary">Preview Session 2 →</a></p>
</div>
</div>
<hr>
</section>
<section id="discussion-questions" class="level2">
<h2 class="anchored" data-anchor-id="discussion-questions">Discussion Questions</h2>
<p>Before moving to the hands-on session, consider these questions:</p>
<ol type="1">
<li><p><strong>What EO applications in your work</strong> could benefit from semantic segmentation versus classification or detection?</p></li>
<li><p><strong>How would you validate</strong> segmentation results in the field, especially for disaster response applications?</p></li>
<li><p><strong>What challenges do you anticipate</strong> when working with limited training data for Philippine-specific contexts?</p></li>
<li><p><strong>How might transfer learning</strong> from models trained on other regions help with Philippine EO applications?</p></li>
</ol>
<hr>
<p><em>This session is part of the CoPhil 4-Day Advanced Training on AI/ML for Earth Observation, funded by the European Union under the Global Gateway initiative.</em></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="cophil-training-v1.0" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../day3/index.html" class="pagination-link" aria-label="Day 3: Deep Learning for Earth Observation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Day 3: Deep Learning for Earth Observation</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../day3/sessions/session2.html" class="pagination-link" aria-label="Session 2: Hands-on Flood Mapping with U-Net and Sentinel-1 SAR">
        <span class="nav-page-text">Session 2: Hands-on Flood Mapping with U-Net and Sentinel-1 SAR</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>CoPhil EO AI/ML Training Programme</p>
</div>   
    <div class="nav-footer-center">
<p>Funded by the European Union - Global Gateway Initiative</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:skotsopoulos@neuralio.ai">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://philsa.gov.ph">
      <i class="bi bi-globe" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>