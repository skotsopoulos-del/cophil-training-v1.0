<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="CoPhil Advanced Training Program">
<meta name="dcterms.date" content="2025-10-16">

<title>Session 1: Semantic Segmentation with U-Net for Earth Observation – CoPhil EO AI/ML Training</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../day3/sessions/session2.html" rel="next">
<link href="../../day3/index.html" rel="prev">
<link href="../../images/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c9822816d3895e59fda95a6fa7545fef.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-4a074efccdeff27617fbc72d37c1244e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c9822816d3895e59fda95a6fa7545fef.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-3775014fae9fc394bbda1d6ff89dd45e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-a15f5dce5650fb3fe5aba34e3b6df9a9.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-3775014fae9fc394bbda1d6ff89dd45e.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles/custom.css">
<link rel="stylesheet" href="../../styles/phase2-enhancements.css">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CoPhil EO AI/ML Training</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-training-days" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Training Days</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-training-days">    
        <li>
    <a class="dropdown-item" href="../../day1/index.html">
 <span class="dropdown-text">Day 1: EO Data &amp; Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day2/index.html">
 <span class="dropdown-text">Day 2: Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day3/index.html">
 <span class="dropdown-text">Day 3: Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day4/index.html">
 <span class="dropdown-text">Day 4: Advanced Topics</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="../../resources/setup.html">
 <span class="dropdown-text">Setup Guide</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/philippine-eo.html">
 <span class="dropdown-text">Philippine EO Links</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/cheatsheets.html">
 <span class="dropdown-text">Cheat Sheets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/faq.html">
 <span class="dropdown-text">FAQ</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/glossary.html">
 <span class="dropdown-text">Glossary</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/cophil-training-v1.0"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resources/downloads.html"> <i class="bi bi-download" role="img">
</i> 
<span class="menu-text">Materials</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day3/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day3/sessions/session1.html">Session 1: Semantic Segmentation with U-Net for Earth Observation</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Day 3: Deep Learning for Earth Observation</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Sessions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/sessions/session1.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Session 1: Semantic Segmentation with U-Net for Earth Observation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/sessions/session2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 2: Hands-on Flood Mapping with U-Net and Sentinel-1 SAR</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/sessions/session3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 3: Object Detection Techniques for Earth Observation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/sessions/session4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: Hands-on Object Detection from Sentinel Imagery</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Notebooks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/notebooks/Day3_Session2_Flood_Mapping_UNet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 2: Hands-on Flood Mapping with U-Net and Sentinel-1 SAR</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day3/notebooks/Day3_Session4_Object_Detection_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: Object Detection from Sentinel Imagery</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#session-1-semantic-segmentation-with-u-net" id="toc-session-1-semantic-segmentation-with-u-net" class="nav-link active" data-scroll-target="#session-1-semantic-segmentation-with-u-net">Session 1: Semantic Segmentation with U-Net</a>
  <ul class="collapse">
  <li><a href="#advanced-deep-learning-for-pixel-level-analysis" id="toc-advanced-deep-learning-for-pixel-level-analysis" class="nav-link" data-scroll-target="#advanced-deep-learning-for-pixel-level-analysis">Advanced Deep Learning for Pixel-Level Analysis</a></li>
  </ul></li>
  <li><a href="#session-overview" id="toc-session-overview" class="nav-link" data-scroll-target="#session-overview">Session Overview</a>
  <ul class="collapse">
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives">Learning Objectives</a></li>
  </ul></li>
  <li><a href="#presentation-slides" id="toc-presentation-slides" class="nav-link" data-scroll-target="#presentation-slides">Presentation Slides</a></li>
  <li><a href="#part-1-concept-of-semantic-segmentation" id="toc-part-1-concept-of-semantic-segmentation" class="nav-link" data-scroll-target="#part-1-concept-of-semantic-segmentation">Part 1: Concept of Semantic Segmentation</a>
  <ul class="collapse">
  <li><a href="#what-is-semantic-segmentation" id="toc-what-is-semantic-segmentation" class="nav-link" data-scroll-target="#what-is-semantic-segmentation">What is Semantic Segmentation?</a></li>
  <li><a href="#understanding-the-difference" id="toc-understanding-the-difference" class="nav-link" data-scroll-target="#understanding-the-difference">Understanding the Difference</a></li>
  <li><a href="#three-task-comparison" id="toc-three-task-comparison" class="nav-link" data-scroll-target="#three-task-comparison">Three Task Comparison</a></li>
  </ul></li>
  <li><a href="#part-2-u-net-architecture" id="toc-part-2-u-net-architecture" class="nav-link" data-scroll-target="#part-2-u-net-architecture">Part 2: U-Net Architecture</a>
  <ul class="collapse">
  <li><a href="#introduction-to-u-net" id="toc-introduction-to-u-net" class="nav-link" data-scroll-target="#introduction-to-u-net">Introduction to U-Net</a></li>
  <li><a href="#encoder-contracting-path" id="toc-encoder-contracting-path" class="nav-link" data-scroll-target="#encoder-contracting-path">Encoder (Contracting Path)</a></li>
  <li><a href="#bottleneck-layer" id="toc-bottleneck-layer" class="nav-link" data-scroll-target="#bottleneck-layer">Bottleneck Layer</a></li>
  <li><a href="#decoder-expansive-path" id="toc-decoder-expansive-path" class="nav-link" data-scroll-target="#decoder-expansive-path">Decoder (Expansive Path)</a></li>
  <li><a href="#skip-connections---the-key-innovation" id="toc-skip-connections---the-key-innovation" class="nav-link" data-scroll-target="#skip-connections---the-key-innovation">Skip Connections - The Key Innovation</a></li>
  <li><a href="#how-skip-connections-work" id="toc-how-skip-connections-work" class="nav-link" data-scroll-target="#how-skip-connections-work">How Skip Connections Work</a></li>
  <li><a href="#u-net-complete-architecture-summary" id="toc-u-net-complete-architecture-summary" class="nav-link" data-scroll-target="#u-net-complete-architecture-summary">U-Net Complete Architecture Summary</a></li>
  </ul></li>
  <li><a href="#part-3-applications-in-earth-observation" id="toc-part-3-applications-in-earth-observation" class="nav-link" data-scroll-target="#part-3-applications-in-earth-observation">Part 3: Applications in Earth Observation</a>
  <ul class="collapse">
  <li><a href="#why-u-net-is-popular-in-eo" id="toc-why-u-net-is-popular-in-eo" class="nav-link" data-scroll-target="#why-u-net-is-popular-in-eo">Why U-Net is Popular in EO</a></li>
  <li><a href="#application-1-flood-mapping" id="toc-application-1-flood-mapping" class="nav-link" data-scroll-target="#application-1-flood-mapping">Application 1: Flood Mapping</a></li>
  <li><a href="#application-2-land-cover-mapping" id="toc-application-2-land-cover-mapping" class="nav-link" data-scroll-target="#application-2-land-cover-mapping">Application 2: Land Cover Mapping</a></li>
  <li><a href="#application-3-road-network-extraction" id="toc-application-3-road-network-extraction" class="nav-link" data-scroll-target="#application-3-road-network-extraction">Application 3: Road Network Extraction</a></li>
  <li><a href="#application-4-building-footprint-delineation" id="toc-application-4-building-footprint-delineation" class="nav-link" data-scroll-target="#application-4-building-footprint-delineation">Application 4: Building Footprint Delineation</a></li>
  <li><a href="#application-5-vegetation-and-crop-monitoring" id="toc-application-5-vegetation-and-crop-monitoring" class="nav-link" data-scroll-target="#application-5-vegetation-and-crop-monitoring">Application 5: Vegetation and Crop Monitoring</a></li>
  </ul></li>
  <li><a href="#part-4-loss-functions-for-segmentation" id="toc-part-4-loss-functions-for-segmentation" class="nav-link" data-scroll-target="#part-4-loss-functions-for-segmentation">Part 4: Loss Functions for Segmentation</a>
  <ul class="collapse">
  <li><a href="#why-loss-functions-matter" id="toc-why-loss-functions-matter" class="nav-link" data-scroll-target="#why-loss-functions-matter">Why Loss Functions Matter</a></li>
  <li><a href="#challenge-class-imbalance-in-eo" id="toc-challenge-class-imbalance-in-eo" class="nav-link" data-scroll-target="#challenge-class-imbalance-in-eo">Challenge: Class Imbalance in EO</a></li>
  <li><a href="#loss-function-1-pixel-wise-cross-entropy" id="toc-loss-function-1-pixel-wise-cross-entropy" class="nav-link" data-scroll-target="#loss-function-1-pixel-wise-cross-entropy">Loss Function 1: Pixel-wise Cross-Entropy</a></li>
  <li><a href="#weighted-cross-entropy" id="toc-weighted-cross-entropy" class="nav-link" data-scroll-target="#weighted-cross-entropy">Weighted Cross-Entropy</a></li>
  <li><a href="#loss-function-2-dice-loss" id="toc-loss-function-2-dice-loss" class="nav-link" data-scroll-target="#loss-function-2-dice-loss">Loss Function 2: Dice Loss</a></li>
  <li><a href="#loss-function-3-iou-loss-jaccard-index" id="toc-loss-function-3-iou-loss-jaccard-index" class="nav-link" data-scroll-target="#loss-function-3-iou-loss-jaccard-index">Loss Function 3: IoU Loss (Jaccard Index)</a></li>
  <li><a href="#dice-vs-iou---when-to-choose" id="toc-dice-vs-iou---when-to-choose" class="nav-link" data-scroll-target="#dice-vs-iou---when-to-choose">Dice vs IoU - When to Choose?</a></li>
  <li><a href="#loss-function-4-combined-losses" id="toc-loss-function-4-combined-losses" class="nav-link" data-scroll-target="#loss-function-4-combined-losses">Loss Function 4: Combined Losses</a></li>
  <li><a href="#loss-function-selection-guide" id="toc-loss-function-selection-guide" class="nav-link" data-scroll-target="#loss-function-selection-guide">Loss Function Selection Guide</a></li>
  <li><a href="#practical-example-flood-mapping-loss-selection" id="toc-practical-example-flood-mapping-loss-selection" class="nav-link" data-scroll-target="#practical-example-flood-mapping-loss-selection">Practical Example: Flood Mapping Loss Selection</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a>
  <ul class="collapse">
  <li><a href="#core-references" id="toc-core-references" class="nav-link" data-scroll-target="#core-references">Core References</a></li>
  <li><a href="#datasets-for-practice" id="toc-datasets-for-practice" class="nav-link" data-scroll-target="#datasets-for-practice">Datasets for Practice</a></li>
  <li><a href="#tutorials" id="toc-tutorials" class="nav-link" data-scroll-target="#tutorials">Tutorials</a></li>
  <li><a href="#philippine-eo-context" id="toc-philippine-eo-context" class="nav-link" data-scroll-target="#philippine-eo-context">Philippine EO Context</a></li>
  </ul></li>
  <li><a href="#preparation-for-session-2" id="toc-preparation-for-session-2" class="nav-link" data-scroll-target="#preparation-for-session-2">Preparation for Session 2</a></li>
  <li><a href="#discussion-questions" id="toc-discussion-questions" class="nav-link" data-scroll-target="#discussion-questions">Discussion Questions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day3/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day3/sessions/session1.html">Session 1: Semantic Segmentation with U-Net for Earth Observation</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Session 1: Semantic Segmentation with U-Net for Earth Observation</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Advanced Deep Learning for Pixel-Level Analysis</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Instructor</div>
    <div class="quarto-title-meta-contents">
             <p>CoPhil Advanced Training Program </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Date</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 16, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<nav class="breadcrumb" aria-label="Breadcrumb">
<a href="../../index.html">Home</a> <span class="breadcrumb-separator" aria-hidden="true">›</span> <a href="../index.html">Day 3</a> <span class="breadcrumb-separator" aria-hidden="true">›</span> <span class="breadcrumb-current">Session 1</span>
</nav>
<section id="session-1-semantic-segmentation-with-u-net" class="level1 hero">
<h1>Session 1: Semantic Segmentation with U-Net</h1>
<section id="advanced-deep-learning-for-pixel-level-analysis" class="level3">
<h3 class="anchored" data-anchor-id="advanced-deep-learning-for-pixel-level-analysis">Advanced Deep Learning for Pixel-Level Analysis</h3>
<p>Master the U-Net architecture for precise Earth Observation segmentation tasks</p>
</section>
</section>
<section id="session-overview" class="level2">
<h2 class="anchored" data-anchor-id="session-overview">Session Overview</h2>
<div class="session-info">
<p><strong>Duration:</strong> 1.5 hours | <strong>Format:</strong> Lecture + Discussion | <strong>Platform:</strong> Presentation &amp; Slides</p>
</div>
<hr>
<p>This session introduces semantic segmentation as a pixel-wise classification task and explores the U-Net architecture—one of the most successful deep learning models for Earth Observation applications. You’ll understand how U-Net’s encoder-decoder structure with skip connections enables precise boundary delineation for tasks like flood mapping, land cover classification, and infrastructure extraction.</p>
<section id="learning-objectives" class="level3 learning-objectives">
<h3 class="anchored" data-anchor-id="learning-objectives">Learning Objectives</h3>
<p>By the end of this session, you will be able to:</p>
<ul>
<li><strong>Define</strong> semantic segmentation and distinguish it from classification and object detection</li>
<li><strong>Explain</strong> the U-Net architecture including encoder, decoder, and skip connections</li>
<li><strong>Describe</strong> how loss functions (Cross-Entropy, Dice, IoU) handle class imbalance</li>
<li><strong>Identify</strong> Earth Observation applications suited for semantic segmentation</li>
<li><strong>Evaluate</strong> when to use different loss functions for segmentation tasks</li>
</ul>
</section>
<hr>
</section>
<section id="presentation-slides" class="level2">
<h2 class="anchored" data-anchor-id="presentation-slides">Presentation Slides</h2>
<iframe src="../presentations/session1_unet_segmentation.html" width="100%" height="600" style="border: 1px solid #ccc; border-radius: 4px;">
</iframe>
<hr>
</section>
<section id="part-1-concept-of-semantic-segmentation" class="level2">
<h2 class="anchored" data-anchor-id="part-1-concept-of-semantic-segmentation">Part 1: Concept of Semantic Segmentation</h2>
<section id="what-is-semantic-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="what-is-semantic-segmentation">What is Semantic Segmentation?</h3>
<p><strong>Semantic segmentation</strong> is the task of classifying every pixel in an image into a category, producing a detailed, pixel-wise map of the image content. Unlike <strong>image classification</strong> (which assigns one label per image) or <strong>object detection</strong> (which locates objects with bounding boxes), segmentation provides a fine-grained understanding of the scene.</p>
<p>In segmentation, the output is an image where every pixel is labeled (e.g., water, building, forest, road), thus outlining the exact shapes and areas of these features. This level of detail is especially useful in geospatial contexts for creating thematic maps and analyzing spatial patterns.</p>
<p><strong>Example Comparison:</strong> - <strong>Classification:</strong> “Is this satellite patch urban or agricultural?” → Single label for entire image - <strong>Detection:</strong> “Where are the buildings?” → Bounding boxes around structures<br>
- <strong>Segmentation:</strong> “Label every pixel as building, road, vegetation, or water” → Complete pixel-level map</p>
<p>This pixel-level approach is crucial for Earth Observation tasks like mapping floods, land cover, or burn scars, where knowing the exact extent and shape of features matters. Unlike classification that might tell us a patch is “urban,” segmentation highlights exactly which pixels are buildings, roads, vegetation, water, etc., giving a much richer understanding of the scene.</p>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    A[Computer Vision Tasks] --&gt; B[Classification]
    A --&gt; C[Object Detection]
    A --&gt; D[Semantic Segmentation]
    
    B --&gt; B1["What's in this image?&lt;br/&gt;Output: Single label&lt;br/&gt;Granularity: Image-level"]
    C --&gt; C1["Where are the objects?&lt;br/&gt;Output: Bounding boxes&lt;br/&gt;Granularity: Object-level"]
    D --&gt; D1["Which pixels belong to what?&lt;br/&gt;Output: Pixel masks&lt;br/&gt;Granularity: Pixel-level"]
    
    style A fill:#0066cc,stroke:#003d7a,stroke-width:3px,color:#fff
    style D fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff
    style D1 fill:#00aa44,stroke:#006622,stroke-width:1px,color:#fff
</pre>
</div>
<p></p><figcaption> Computer Vision Task Hierarchy</figcaption> </figure><p></p>
</div>
</div>
</div>
</section>
<section id="understanding-the-difference" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-difference">Understanding the Difference</h3>
<p>In semantic segmentation, the output is an image where every pixel is labeled (e.g., water, building, forest), allowing <strong>precise delineation</strong> of different land cover types. This level of detail is especially useful in geospatial contexts for creating thematic maps and analyzing spatial patterns.</p>
<p>For example, in a satellite image we might label each pixel as water, building, forest, road, etc., thus outlining the exact shapes and areas of these features. Whereas an image classification might tell us an entire satellite patch is “urban” or “agriculture,” semantic segmentation can highlight <strong>exactly which pixels</strong> are buildings, roads, vegetation, water, etc., giving a much richer understanding of the scene.</p>
<p>We contrast these tasks visually and conceptually so the distinction is clear:</p>
<ul>
<li><strong>Image Classification</strong> answers: “Is this satellite patch urban or agricultural?” with a single label for the entire image</li>
<li><strong>Object Detection</strong> answers: “Where are the buildings?” by drawing bounding boxes around each structure</li>
<li><strong>Semantic Segmentation</strong> answers: “Label every pixel as water, forest, urban, or agriculture” producing a complete pixel-level classification map</li>
</ul>
<p>This pixel-level approach is crucial for Earth Observation tasks like mapping floods, land cover, or burn scars, where knowing the <strong>exact extent and shape</strong> of features matters for decision-making, planning, and response.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Why Semantic Segmentation for EO?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Semantic segmentation offers several critical advantages for Earth Observation applications that make it indispensable for many geospatial analysis tasks:</p>
<p><strong>Precise Delineation:</strong> Segmentation provides exact boundaries of features—the precise edge of flood extent, the exact boundary where forest stops and urban area begins, the specific outline of agricultural fields. This pixel-level precision is far superior to bounding boxes or image-level labels.</p>
<p><strong>Quantitative Analysis:</strong> With pixel-wise classification, we can calculate accurate areas down to the precision of individual pixels. For flood mapping, this means knowing exactly how many square kilometers are inundated. For forest monitoring, it means precise measurements of deforestation extent.</p>
<p><strong>Change Detection:</strong> Pixel-level comparison over time enables detailed change detection. We can identify exactly which pixels changed from forest to urban, or from dry land to water, enabling fine-grained temporal analysis.</p>
<p><strong>Thematic Mapping:</strong> Segmentation produces detailed land cover and land use maps where every location has a meaningful class label, creating rich thematic datasets for analysis, planning, and decision-making.</p>
<p><strong>Decision Support:</strong> The fine-grained information from segmentation directly supports disaster response and planning. For typhoon flood assessment, segmentation provides exact flood boundaries for targeted relief operations—identifying which specific buildings or roads are affected—not just a general “flooded” vs “not flooded” assessment for an entire region.</p>
</div>
</div>
</section>
<section id="three-task-comparison" class="level3">
<h3 class="anchored" data-anchor-id="three-task-comparison">Three Task Comparison</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 23%">
<col style="width: 28%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Classification</th>
<th>Object Detection</th>
<th>Semantic Segmentation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Question</strong></td>
<td>What’s in this image?</td>
<td>Where are objects?</td>
<td>Which pixels are what?</td>
</tr>
<tr class="even">
<td><strong>Output</strong></td>
<td>Single label</td>
<td>Bounding boxes + labels</td>
<td>Pixel-wise mask</td>
</tr>
<tr class="odd">
<td><strong>Granularity</strong></td>
<td>Image-level</td>
<td>Object-level</td>
<td>Pixel-level</td>
</tr>
<tr class="even">
<td><strong>Spatial Info</strong></td>
<td>None</td>
<td>Approximate (boxes)</td>
<td>Precise (pixels)</td>
</tr>
<tr class="odd">
<td><strong>Computation</strong></td>
<td>Fast</td>
<td>Moderate</td>
<td>Intensive</td>
</tr>
<tr class="even">
<td><strong>Use Case Example</strong></td>
<td>“Contains buildings”</td>
<td>“10 buildings detected”</td>
<td>“Building footprints mapped”</td>
</tr>
</tbody>
</table>
<hr>
</section>
</section>
<section id="part-2-u-net-architecture" class="level2">
<h2 class="anchored" data-anchor-id="part-2-u-net-architecture">Part 2: U-Net Architecture</h2>
<section id="introduction-to-u-net" class="level3">
<h3 class="anchored" data-anchor-id="introduction-to-u-net">Introduction to U-Net</h3>
<p><strong>U-Net</strong> was developed by Ronneberger et al.&nbsp;(2015) for biomedical image segmentation and has since become one of the most popular architectures for Earth Observation applications.</p>
<p><strong>Why “U-Net”?</strong> - Architecture shape resembles the letter “U” - Symmetric encoder-decoder structure - <strong>Key Innovation:</strong> Skip connections that preserve spatial information</p>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A[Input Image&lt;br/&gt;H × W × C] --&gt; B[Encoder&lt;br/&gt;Contracting Path]
    B --&gt; C[Bottleneck&lt;br/&gt;Most Compressed]
    C --&gt; D[Decoder&lt;br/&gt;Expansive Path]
    D --&gt; E[Output Mask&lt;br/&gt;H × W × Classes]
    
    B -.-&gt;|Skip Connection 1| D
    B -.-&gt;|Skip Connection 2| D
    B -.-&gt;|Skip Connection 3| D
    B -.-&gt;|Skip Connection 4| D
    
    B --&gt; B1["Feature Extraction&lt;br/&gt;Spatial dimension ↓&lt;br/&gt;Feature depth ↑"]
    C --&gt; C1["Global Context&lt;br/&gt;What is in image"]
    D --&gt; D1["Spatial Reconstruction&lt;br/&gt;Spatial dimension ↑&lt;br/&gt;Feature depth ↓"]
    
    style A fill:#0066cc,stroke:#003d7a,stroke-width:2px,color:#fff
    style E fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff
    style C fill:#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff
</pre>
</div>
<p></p><figcaption> U-Net Architecture Overview</figcaption> </figure><p></p>
</div>
</div>
</div>
</section>
<section id="encoder-contracting-path" class="level3">
<h3 class="anchored" data-anchor-id="encoder-contracting-path">Encoder (Contracting Path)</h3>
<p><strong>Purpose:</strong> Extract hierarchical features at multiple scales while progressively compressing spatial information</p>
<p>The encoder is a series of convolutional and pooling layers that progressively downsample the image, extracting higher-level features while reducing spatial resolution (just as we learned with CNNs on Day 2).</p>
<p><strong>Operations:</strong> 1. <strong>Convolution blocks:</strong> - Two 3×3 convolutional layers (with ReLU activations) - (Optional) Batch normalization - <strong>Note:</strong> Often uses “same” padding to preserve spatial dimensions through conv layers</p>
<ol start="2" type="1">
<li><strong>Downsampling:</strong>
<ul>
<li>2×2 max pooling (reduces spatial resolution)</li>
<li>Spatial dimensions halve</li>
<li>Feature channels double</li>
<li>Creates hierarchical representation</li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Connection to Day 2 Concepts
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Recall from Day 2:</strong> The encoder uses the same CNN building blocks you learned: - <strong>Convolution layers</strong> apply learnable filters to extract features - <strong>Padding</strong> (“same” padding) helps preserve spatial dimensions so feature maps align for skip connections - <strong>Pooling</strong> reduces dimensionality—recall from Day 2 that pooling without padding reduces image size, losing some detail - <strong>ReLU activation</strong> introduces non-linearity</p>
<p>For instance, using 3×3 convolutions (with ReLU activations) and 2×2 max-pooling, the encoder learns rich features but shrinks the image size at each step. As we move down the encoder, image details are compressed and abstracted, capturing the <strong>context</strong> of what is in the image.</p>
</div>
</div>
<p><strong>Example Progression:</strong></p>
<pre><code>Input:     256×256×3   (RGB satellite image)
Block 1:   256×256×64  (after convolutions)
Pool 1:    128×128×64  (after max pooling)
Block 2:   128×128×128 (after convolutions)
Pool 2:    64×64×128   (after max pooling)
Block 3:   64×64×256   (after convolutions)
Pool 3:    32×32×256   (after max pooling)
Block 4:   32×32×512   (after convolutions)
Pool 4:    16×16×512   (after max pooling)</code></pre>
<p><strong>Multi-Scale Learning:</strong> - <strong>Early layers:</strong> Capture fine details (edges, textures, small features) - <strong>Deep layers:</strong> Capture semantic meaning (water bodies, urban areas, forests)</p>
</section>
<section id="bottleneck-layer" class="level3">
<h3 class="anchored" data-anchor-id="bottleneck-layer">Bottleneck Layer</h3>
<p>The central part of U-Net is the <strong>bottleneck layer</strong> (the bottom of the “U”), where the feature representation is most compressed. This is where the network holds a condensed encoding of the image’s content—<strong>maximum context, minimum spatial detail</strong>—before the decoder begins expanding it.</p>
<p><strong>Characteristics of the Bottleneck:</strong></p>
<p>At this point, we have the smallest spatial dimensions (e.g., 16×16 pixels) but the largest number of feature channels (e.g., 1024). This creates a highly compressed representation of the entire image.</p>
<p><strong>What It Captures:</strong></p>
<p>The bottleneck captures <strong>global context</strong>—it understands <strong>what’s in the image</strong> at a semantic level. It contains information like “there is water, buildings, vegetation” but has lost the precise spatial information about <strong>where exactly</strong> these features are located. This trade-off is intentional: by compressing spatial dimensions while expanding feature depth, the encoder creates an abstract, semantic understanding of the scene that the decoder can then use to reconstruct precise pixel-wise predictions.</p>
</section>
<section id="decoder-expansive-path" class="level3">
<h3 class="anchored" data-anchor-id="decoder-expansive-path">Decoder (Expansive Path)</h3>
<p><strong>Purpose:</strong> Reconstruct spatial resolution using encoded features to construct precise pixel-wise predictions</p>
<p>The decoder performs the reverse of the encoder: it uses upsampling (e.g., transpose convolutions) to increase the spatial resolution, gradually building the output segmentation map.</p>
<p><strong>Operations:</strong> 1. <strong>Upsampling:</strong> - <strong>Transpose convolution</strong> (learnable filters for upsampling, sometimes called “deconvolution”) OR - <strong>Bilinear/nearest upsampling</strong> (simpler interpolations) + regular convolution - Doubles spatial dimensions - Halves feature channels - Mirrors encoder downsampling in reverse</p>
<ol start="2" type="1">
<li><strong>Skip Connection Concatenation:</strong>
<ul>
<li>Copy high-resolution feature maps from corresponding encoder layer</li>
<li>The feature maps from the encoder are copied and <strong>concatenated</strong> onto the decoder’s feature maps at corresponding levels</li>
<li>Fuse high-resolution spatial details with semantic understanding</li>
<li><strong>Critical:</strong> Feature map sizes must align (achieved through proper padding in encoder)</li>
</ul></li>
<li><strong>Convolution blocks:</strong>
<ul>
<li>Two 3×3 convolutional layers<br>
</li>
<li>ReLU activation</li>
<li>Refine combined features into sharper predictions</li>
</ul></li>
</ol>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Upsampling and Implementation Details
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Recall from Day 2:</strong> Upsampling is essentially the <strong>inverse of pooling</strong>—it increases spatial dimensions to expand the image back to full size.</p>
<p><strong>Two common approaches:</strong> - <strong>Transpose convolution:</strong> Learned transposed conv layers (sometimes called “deconvolution”) with trainable filters - <strong>Interpolation + Conv:</strong> Simpler interpolations (bilinear or nearest neighbor) followed by regular conv to refine</p>
<p><strong>Implementation Note:</strong> To make concatenation in skip connections seamless, we often use padding in convolutions to maintain equal sizes between encoder and decoder feature maps (Day 2 covered how “same” padding keeps dimensions). The <strong>original U-Net paper cropped feature maps</strong> instead, but modern frameworks simply pad zeros so that encoder outputs and decoder inputs align.</p>
<p>By the end of the decoder, a <strong>1×1 convolution</strong> produces the final segmentation map, with as many channels as target classes, so that each pixel receives a class label.</p>
</div>
</div>
<p><strong>Example Progression:</strong></p>
<pre><code>Bottleneck:  16×16×1024
Upsample 1:  32×32×512
Concat:      32×32×1024  (512 from decoder + 512 from encoder skip)
Conv Block:  32×32×512
Upsample 2:  64×64×256
Concat:      64×64×512   (256 from decoder + 256 from encoder skip)
Conv Block:  64×64×256
...
Final:       256×256×num_classes</code></pre>
</section>
<section id="skip-connections---the-key-innovation" class="level3">
<h3 class="anchored" data-anchor-id="skip-connections---the-key-innovation">Skip Connections - The Key Innovation</h3>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Why Skip Connections Matter
</div>
</div>
<div class="callout-body-container callout-body">
<p>A key innovation of U-Net is the <strong>skip connections</strong> linking matching encoder and decoder layers. The feature maps from the encoder (which contain fine-grained spatial details from earlier layers) are <strong>concatenated</strong> with the upsampled features in the decoder. This allows the model to “skip over” the bottleneck and directly inject high-resolution context into the decoding process.</p>
<p><strong>The Problem They Solve:</strong></p>
<p>Without skip connections, information is inevitably lost during the downsampling process (pooling operations). The decoder would have to reconstruct precise boundaries solely from the coarse, compressed features at the bottleneck. This results in blurry boundaries and loss of fine spatial detail—exactly what we want to avoid in Earth Observation applications.</p>
<p><strong>How Skip Connections Help:</strong></p>
<p>The skip connections preserve edges and small structures (e.g., the exact boundary of a flooded area or building outline) that might otherwise be lost during downsampling. The result is <strong>improved detail and accuracy</strong> in segmentation outputs, since the decoder doesn’t have to rely solely on the coarse feature maps after upsampling—it can leverage the original fine details as well.</p>
<p><strong>Best of Both Worlds:</strong></p>
<p>Crucially, U-Net’s decoder is fed by skip connections from the encoder: these skip connections provide high-resolution context to the decoder, ensuring that fine details (like precise boundaries) are preserved even after the image was compressed by the encoder.</p>
<p>In essence, the <strong>encoder captures what is in the image</strong> (context), and the <strong>decoder, aided by skips, ensures we know where those things are</strong> in the image (precise localization). By combining encoder and decoder features, U-Net captures both the <strong>what</strong> (context from the semantic understanding in the bottleneck) and the <strong>where</strong> (location from the high-resolution encoder features) for each class in the image.</p>
<p>Notably, U-Net implementations must handle the alignment of feature map sizes for concatenation—often using appropriate padding (“same” padding) on convolutions so that each encoder output matches the size of the corresponding decoder feature map.</p>
</div>
</div>
</section>
<section id="how-skip-connections-work" class="level3">
<h3 class="anchored" data-anchor-id="how-skip-connections-work">How Skip Connections Work</h3>
<p>Let’s walk through the skip connection mechanism step-by-step to understand how it preserves spatial information:</p>
<p><strong>The Process:</strong></p>
<ol type="1">
<li>As the encoder processes the input, it produces a feature map at a specific resolution, say 128×128×64 (128×128 spatial dimensions, 64 feature channels)</li>
<li>This feature map is <strong>copied and temporarily saved</strong> before any further processing</li>
<li>The encoder continues its downsampling path, applying pooling to reduce spatial dimensions further</li>
<li>The process continues through the bottleneck, where the representation is most compressed</li>
<li>The decoder begins upsampling, bringing the spatial dimensions back up. It produces, for example, a 128×128×32 feature map</li>
<li><strong>Concatenation happens:</strong> The decoder’s upsampled features (128×128×32) are combined channel-wise with the saved encoder features (128×128×64)</li>
<li>The <strong>result</strong> is a 128×128×96 combined feature map containing:
<ul>
<li><strong>High-level semantic context</strong> from the decoder path (understanding of what objects are present)</li>
<li><strong>Fine spatial details</strong> from the encoder path (precise localization of boundaries)</li>
</ul></li>
</ol>
<p><strong>Real-World Impact in Earth Observation:</strong></p>
<p>The difference is dramatic. In flood mapping applications, for instance: - <strong>Without skip connections:</strong> Flood boundary accuracy might be ±10 pixels (100-200 meters at 10m resolution) - <strong>With skip connections:</strong> Flood boundary accuracy improves to ±1-2 pixels (10-20 meters)</p>
<p>This precision is critical for applications requiring legal boundaries, property lines, or hazard zone delineation, where even small errors can have significant consequences for decision-making and resource allocation.</p>
</section>
<section id="u-net-complete-architecture-summary" class="level3">
<h3 class="anchored" data-anchor-id="u-net-complete-architecture-summary">U-Net Complete Architecture Summary</h3>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A[Input: 256×256×3] --&gt; B1[Conv + ReLU]
    B1 --&gt; B2[Conv + ReLU]
    B2 --&gt; C1[MaxPool ↓]
    B2 -.-&gt;|Skip 1| G1
    
    C1 --&gt; D1[Conv + ReLU]
    D1 --&gt; D2[Conv + ReLU]
    D2 --&gt; E1[MaxPool ↓]
    D2 -.-&gt;|Skip 2| F1
    
    E1 --&gt; F0[Bottleneck&lt;br/&gt;Conv Layers]
    
    F0 --&gt; F1[Upsample ↑]
    F1 --&gt; G1[Concatenate]
    G1 --&gt; H1[Conv + ReLU]
    
    H1 --&gt; I1[Upsample ↑]
    I1 --&gt; J1[Concatenate]
    J1 --&gt; K1[Conv + ReLU]
    
    K1 --&gt; L[Output: 256×256×Classes]
    
    style A fill:#0066cc,stroke:#003d7a,stroke-width:2px,color:#fff
    style F0 fill:#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff
    style L fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff
</pre>
</div>
<p></p><figcaption> U-Net Information Flow</figcaption> </figure><p></p>
</div>
</div>
</div>
<hr>
</section>
</section>
<section id="part-3-applications-in-earth-observation" class="level2">
<h2 class="anchored" data-anchor-id="part-3-applications-in-earth-observation">Part 3: Applications in Earth Observation</h2>
<section id="why-u-net-is-popular-in-eo" class="level3">
<h3 class="anchored" data-anchor-id="why-u-net-is-popular-in-eo">Why U-Net is Popular in EO</h3>
<p>U-Net has become a go-to architecture for many Earth Observation segmentation tasks due to its accuracy and efficiency in learning from limited data. Several key factors contribute to its widespread adoption:</p>
<p><strong>Data Efficiency:</strong></p>
<p>U-Net performs well with relatively modest amounts of training data—typically hundreds to thousands of training samples rather than the millions required by some other deep learning approaches. Data augmentation techniques (rotations, flips, which are particularly relevant for satellite nadir views) help further. This is <strong>critical</strong> when labeled EO data is expensive and time-consuming to acquire, requiring expert annotators and field validation.</p>
<p><strong>Spatial Precision:</strong></p>
<p>The skip connections preserve fine boundaries with remarkable accuracy, which is important for applications requiring legal boundaries, property lines, or precise hazard zone delineation. This enables accuracy from millimeter to meter level, depending on the input imagery resolution—essential for cadastral mapping, flood extent determination, and infrastructure monitoring.</p>
<p><strong>Multi-Scale Learning:</strong></p>
<p>The encoder’s hierarchical structure captures both local textures (in early layers) and global context (in deeper layers). This is essential for the varied scales of EO features, from small boats (a few pixels) to large water bodies (thousands of pixels). U-Net handles objects at multiple scales simultaneously within a single architecture.</p>
<p><strong>Transfer Learning Capability:</strong></p>
<p>U-Net encoders can leverage pre-trained weights from ImageNet or other large-scale datasets, enabling domain adaptation from natural images to satellite imagery. This significantly improves performance when labeled EO data is limited, allowing the model to start with general feature extraction capabilities and fine-tune to specific remote sensing tasks.</p>
</section>
<section id="application-1-flood-mapping" class="level3">
<h3 class="anchored" data-anchor-id="application-1-flood-mapping">Application 1: Flood Mapping</h3>
<p><strong>Use Case:</strong> Disaster response and damage assessment</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Philippine Context: Typhoon Flood Mapping
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Data Sources:</strong> - <strong>Sentinel-1 SAR</strong> (cloud-penetrating, all-weather capability) - <strong>Sentinel-2 optical</strong> (high resolution when clouds permit)</p>
<p><strong>Task:</strong> - Binary segmentation: Flooded vs Non-flooded pixels - Input: SAR backscatter (VV, VH polarizations) or optical RGB+NIR - Output: Precise flood extent mask</p>
<p><strong>Why U-Net Excels:</strong> - U-Net has been used to segment flooded areas in Sentinel-1 SAR and Sentinel-2 optical images with <strong>high accuracy</strong> - Studies have shown U-Net is effective in capturing flood patterns in SAR imagery, achieving high accuracy in delineating water from land - During floods, produces a <strong>binary map of floodwater vs.&nbsp;non-flood</strong> for each pixel, identifying flooded pixels vs.&nbsp;dry pixels across an entire region - Enables <strong>rapid assessment</strong> of flood extent for emergency response - Research shows U-Net achieves robust results even with relatively <strong>small training datasets</strong></p>
<p><strong>Benefits:</strong> - Rapid mapping within hours of satellite acquisition - Precise area calculations for damage assessment<br>
- Time-series monitoring of flood evolution and recession - Integration with GIS for evacuation planning and relief distribution</p>
<p><strong>Real Example:</strong> Typhoon Ulysses (2020) - Central Luzon floods mapped using U-Net on Sentinel-1 data, providing precise inundation extent for affected municipalities in the Pampanga River Basin.</p>
</div>
</div>
</section>
<section id="application-2-land-cover-mapping" class="level3">
<h3 class="anchored" data-anchor-id="application-2-land-cover-mapping">Application 2: Land Cover Mapping</h3>
<p><strong>Use Case:</strong> Environmental monitoring, urban planning, biodiversity assessment</p>
<p><strong>Data Sources:</strong> - Sentinel-2 multispectral (10m resolution) - Landsat 8/9 (30m, long time series) - High-resolution commercial imagery</p>
<p><strong>Task:</strong> - Multi-class segmentation: Water, Forest, Urban, Agriculture, Barren, Mangrove - Input: Multi-spectral bands (RGB, NIR, SWIR, Red Edge) - Output: Detailed land cover classification map</p>
<p><strong>U-Net’s Strength:</strong> - Combines <strong>broad context</strong> (distinguishing urban area from forest in general) with <strong>precise boundaries</strong> (exactly where forest stops and urban begins) - U-Net’s ability to preserve fine details helps <strong>delineate boundaries</strong> between different land cover types - Can outline exact shapes of urban districts, small water bodies, or forest edges - Research shows U-Net often <strong>outperforms older pixel-based or patch-based methods</strong> in remote sensing</p>
<p><strong>Benefits:</strong> - Pixel-accurate thematic maps - Change detection over time (deforestation, urbanization) - Biodiversity habitat assessments<br>
- Carbon stock estimation for climate reporting</p>
</section>
<section id="application-3-road-network-extraction" class="level3">
<h3 class="anchored" data-anchor-id="application-3-road-network-extraction">Application 3: Road Network Extraction</h3>
<p><strong>Use Case:</strong> Map updating, transportation planning, accessibility analysis</p>
<p><strong>Challenges:</strong> - Thin linear features difficult to detect - Occlusion by trees and shadows - Complex urban backgrounds - Need to maintain continuous structure</p>
<p><strong>U-Net Advantages:</strong> - Skip connections preserve <strong>road continuity</strong> (prevents gaps) - Learns to follow <strong>linear patterns</strong> across the image - Handles varying road widths (from highways to small paths) - Can trace continuous structures like roads and railways</p>
<p><strong>Task:</strong> - Binary segmentation: Road vs Background - Input: High-resolution aerial/satellite RGB or SAR - Output: Road network mask for vectorization</p>
<p><strong>Applications:</strong> - Automated map updating for rural areas - Transportation network planning - Accessibility analysis for disaster response</p>
</section>
<section id="application-4-building-footprint-delineation" class="level3">
<h3 class="anchored" data-anchor-id="application-4-building-footprint-delineation">Application 4: Building Footprint Delineation</h3>
<p><strong>Use Case:</strong> Urban mapping, population estimation, disaster risk assessment</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Philippine Application: Informal Settlement Detection
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Relevance:</strong> - Monitor unplanned urban growth in Metro Manila - Identify disaster-vulnerable communities<br>
- Support urban planning and housing programs</p>
<p><strong>Task:</strong> - Binary or multi-class: Building vs Background (or building types) - Input: Very high-resolution imagery (&lt;1m) or Sentinel-2 for large structures - Output: Building footprint polygons</p>
<p><strong>U-Net Performance:</strong> - With appropriate high-resolution data, U-Net can <strong>outline individual buildings</strong> or dense informal settlements, even with complex backgrounds - U-Net-based models have been used to extract buildings in urban areas and to map roads winding through forests or cities - Variants like <strong>Residual U-Net</strong> or <strong>Attention U-Net</strong> also popular for building segmentation - Core idea remains: encoder-decoder with skip connections for precise boundaries - Instead of just saying “there are buildings in this image,” we get a map of where each building is</p>
<p><strong>Benefits:</strong> - Automated mapping at scale - Pre/post disaster damage assessment - 3D city model generation (with height data) - Infrastructure planning - Aids urban planning and risk assessment</p>
</div>
</div>
</section>
<section id="application-5-vegetation-and-crop-monitoring" class="level3">
<h3 class="anchored" data-anchor-id="application-5-vegetation-and-crop-monitoring">Application 5: Vegetation and Crop Monitoring</h3>
<p><strong>Use Case:</strong> Precision agriculture, forestry, ecosystem health</p>
<p><strong>Data Sources:</strong> - Sentinel-2 multispectral (5-day revisit) - PlanetScope (3m daily coverage) - UAV imagery for field-scale monitoring</p>
<p><strong>Task:</strong> - Multi-class segmentation: Crop types (rice, corn, sugarcane, coconut) - Or binary: Vegetation vs Non-vegetation - Input: Multi-temporal + multi-spectral imagery - Output: Crop type map or vegetation mask</p>
<p><strong>U-Net Applications:</strong> - Identifying <strong>crop fields</strong> and <strong>forest cover</strong> at pixel level - Monitoring agricultural areas for food security - Tracking <strong>tree cover</strong> for forestry management - Detecting vegetation changes and health patterns</p>
<p><strong>Benefits:</strong> - Yield prediction and harvest planning - Irrigation requirement monitoring - Early disease detection - Deforestation and illegal logging tracking</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Research Evidence
</div>
</div>
<div class="callout-body-container callout-body">
<p>Across these examples, research and practice have shown <strong>U-Net achieves high segmentation accuracy</strong> in remote sensing. It has been demonstrated that U-Net can achieve <strong>robust results with relatively small training datasets</strong>, thanks to the efficiency of the architecture—one reason it was originally successful in medical imaging with limited training images.</p>
</div>
</div>
<hr>
</section>
</section>
<section id="part-4-loss-functions-for-segmentation" class="level2">
<h2 class="anchored" data-anchor-id="part-4-loss-functions-for-segmentation">Part 4: Loss Functions for Segmentation</h2>
<section id="why-loss-functions-matter" class="level3">
<h3 class="anchored" data-anchor-id="why-loss-functions-matter">Why Loss Functions Matter</h3>
<p>Training a segmentation model requires choosing an appropriate <strong>loss function</strong> that compares the predicted mask to the ground truth mask. The loss function is the mathematical measure that tells the model how wrong its predictions are, guiding the weight updates during training.</p>
<p><strong>The Challenge in Segmentation:</strong></p>
<p>Unlike image classification where we compare a single predicted label to a single true label, segmentation requires comparing entire images pixel-by-pixel. We’re not evaluating just one value—we must compare potentially millions of pixel predictions across the entire image.</p>
<p>Different loss functions emphasize different aspects of the prediction: - Some focus on <strong>pixel-wise accuracy</strong> (is each individual pixel correct?) - Others focus on <strong>region overlap</strong> (does the predicted flood extent match the true extent?) - Some emphasize <strong>boundary accuracy</strong> (are the edges of objects precisely delineated?)</p>
<p><strong>Why Choice Matters:</strong></p>
<p>The choice of loss function <strong>critically affects model behavior</strong>. Several loss functions are common in segmentation, each with different strengths. In the following sections, we’ll explore the main options and understand when to use each one for Earth Observation applications.</p>
</section>
<section id="challenge-class-imbalance-in-eo" class="level3">
<h3 class="anchored" data-anchor-id="challenge-class-imbalance-in-eo">Challenge: Class Imbalance in EO</h3>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>Common Imbalanced Scenarios
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>In segmentation of EO data, class imbalance is a typical issue:</strong></p>
<p><strong>Flood Mapping:</strong> - 95% non-flooded pixels, 5% flooded pixels - Think of mapping floods: the flooded pixels are usually <strong>far fewer</strong> than non-flooded</p>
<p><strong>Ship Detection:</strong> - 99.5% water/land, 0.5% ships</p>
<p><strong>Building Segmentation:</strong> - 80% background, 20% buildings</p>
<p><strong>Problem with Simple Accuracy:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model predicts: ALL pixels = "non-flooded"</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy: 95% ✓ (looks great!)</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># But: Completely useless - missed all floods!</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Result: Trivial but useless prediction</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Why This Happens:</strong> In imbalanced cases, vanilla cross-entropy can be <strong>dominated by the majority class</strong>. A poor choice of loss might lead the model to predict all pixels as the majority class—achieving high accuracy but providing no useful information.</p>
<p><strong>Need Loss Functions That:</strong> - Handle severe class imbalance - Focus on minority (critical) class - Reward region overlap, not just pixel-wise correctness - Ensure boundaries (edges of floods, building outlines) are accurately captured</p>
</div>
</div>
</section>
<section id="loss-function-1-pixel-wise-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-1-pixel-wise-cross-entropy">Loss Function 1: Pixel-wise Cross-Entropy</h3>
<p><strong>How it Works:</strong> - Treat each pixel as an <strong>independent classification problem</strong> - Compare predicted class probability to true class using <strong>negative log-likelihood</strong> - Average loss across all pixels - Standard approach for multi-class segmentation (or binary cross-entropy for two classes)</p>
<p><strong>Formula (simplified):</strong> <span class="math display">\[
\text{Cross-Entropy} = -\sum_{i=1}^{N} y_{\text{true},i} \cdot \log(y_{\text{pred},i})
\]</span></p>
<p>Where <span class="math inline">\(N\)</span> is the total number of pixels.</p>
<p><strong>Advantages:</strong> - ✓ Standard, well-understood approach - ✓ Strong, stable gradients for learning - ✓ Works naturally with multi-class problems (softmax output) - ✓ Effective and straightforward</p>
<p><strong>Disadvantages:</strong> - ✗ Dominated by majority class in imbalanced data - ✗ Doesn’t directly optimize for spatial overlap or boundary alignment - ✗ Can effectively ignore minority classes - ✗ Focuses on pixel-level accuracy but not region-level correctness</p>
<p><strong>When to Use:</strong> Balanced datasets (~50/50 class distribution) or with class weighting</p>
</section>
<section id="weighted-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="weighted-cross-entropy">Weighted Cross-Entropy</h3>
<p><strong>Solution to Imbalance:</strong> Assign higher weight to under-represented classes so the model pays more attention to them</p>
<p><strong>Formula:</strong> <span class="math display">\[
\text{Weighted CE} = -\sum_{i=1}^{N} w_{\text{class}} \cdot y_{\text{true},i} \cdot \log(y_{\text{pred},i})
\]</span></p>
<p><strong>Example:</strong> - 95% background pixels → weight = 1.0 - 5% flood pixels → weight = 19.0 (inverse frequency: 95/5)</p>
<p><strong>Effect:</strong> - Model pays 19× more attention to flood pixels - Heavily penalized for missing flood pixels - <strong>Common remedy</strong> for class imbalance in segmentation</p>
<p><strong>Implementation (TensorFlow/Keras):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> tf.keras.losses.CategoricalCrossentropy(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    class_weight<span class="op">=</span>{<span class="dv">0</span>: <span class="fl">1.0</span>, <span class="dv">1</span>: <span class="fl">19.0</span>}  <span class="co"># background, flood</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>When Weighted CE Helps
</div>
</div>
<div class="callout-body-container callout-body">
<p>Weighted cross-entropy provides strong gradients for learning while addressing imbalance. However, it still focuses on <strong>pixel-wise accuracy</strong> and doesn’t directly ensure good overlap or boundary alignment—that’s where overlap-based losses come in.</p>
</div>
</div>
</section>
<section id="loss-function-2-dice-loss" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-2-dice-loss">Loss Function 2: Dice Loss</h3>
<p><strong>Concept:</strong> Measure overlap between prediction and ground truth regions<br>
<strong>Training Goal:</strong> Maximize Dice (or minimize 1 - Dice) to encourage the network to get the segmentation overlap as high as possible</p>
<p><strong>Formula:</strong> <span class="math display">\[
\text{Dice Coefficient} = \frac{2 \times |P \cap T|}{|P| + |T|}
\]</span></p>
<p><span class="math display">\[
\text{Dice Loss} = 1 - \text{Dice Coefficient}
\]</span></p>
<p>Where: - <span class="math inline">\(P\)</span> = predicted foreground pixels - <span class="math inline">\(T\)</span> = true foreground pixels - <span class="math inline">\(\cap\)</span> = intersection (overlap) - Essentially: <span class="math inline">\(\frac{2 \times (\text{intersection})}{(\text{sum of areas})}\)</span></p>
<p><strong>Interpretation:</strong> - Dice = 1.0: Perfect overlap - Dice = 0.5: 50% overlap - Dice = 0.0: No overlap - Loss = 0.0: Perfect (lower is better)</p>
<p><strong>Why for Imbalanced Data?</strong> - Focuses on the <strong>relative overlap</strong> of the object (minority class) - Treats foreground and background asymmetrically - <strong>Correctly segmenting one small flooded patch</strong> contributes as much to Dice as a large region of non-flood - Inherently handles class imbalance <strong>without manual weighting</strong> - Particularly well-suited when target objects occupy a <strong>small fraction</strong> of the image - Doesn’t let the model get complacent by only predicting the majority class, because it focuses on the overlap of the positive (target) class</p>
<p><strong>When False Positives and Negatives Need Equal Weight:</strong> Dice loss is known to be effective when <strong>false negatives</strong> (missing floods) and <strong>false positives</strong> (predicting flood where there isn’t) need to be weighted equally. It treats false negatives and false positives more equally than cross-entropy.</p>
<p><strong>Advantages:</strong> - ✓ Inherently robust to class imbalance - ✓ Directly optimizes overlap metric (F1-score for segmentation) - ✓ Excellent for small objects - ✓ No need to manually set class weights - ✓ Helps model not ignore small structures or minority classes</p>
<p><strong>Disadvantages:</strong> - ✗ Less stable gradients (can be noisy early in training) - ✗ May converge slower than cross-entropy - ✗ Requires careful implementation (avoid division by zero)</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Medical Imaging Parallel
</div>
</div>
<div class="callout-body-container callout-body">
<p>Many medical image segmentation models use Dice loss to segment <strong>tumors that occupy only a tiny area</strong> of the image—exactly the same challenge as segmenting small flood patches in vast satellite scenes.</p>
</div>
</div>
</section>
<section id="loss-function-3-iou-loss-jaccard-index" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-3-iou-loss-jaccard-index">Loss Function 3: IoU Loss (Jaccard Index)</h3>
<p><strong>Concept:</strong> Similar to Dice, directly optimizes the IoU metric<br>
<strong>Also Known As:</strong> Jaccard Index</p>
<p><strong>Formula:</strong> <span class="math display">\[
\text{IoU} = \frac{|P \cap T|}{|P \cup T|}
\]</span></p>
<p><span class="math display">\[
\text{IoU Loss} = 1 - \text{IoU}
\]</span></p>
<p>Where <span class="math inline">\(\cup\)</span> represents the union of predicted and true pixels.</p>
<p><strong>Difference from Dice:</strong> - <strong>IoU:</strong> Intersection / Union - <strong>Dice:</strong> 2 × Intersection / (Sum of areas) - Numerically different but conceptually similar - Related by: <span class="math inline">\(\text{Dice} = \frac{2 \times \text{IoU}}{1 + \text{IoU}}\)</span></p>
<p><strong>Properties:</strong> - <strong>Robust to class imbalance</strong> (like Dice) - Emphasizes <strong>boundary accuracy</strong>—maximizing IoU requires the predicted region to <strong>align well</strong> with the true region boundaries - Penalizes false positives and false negatives <strong>equally at the region level</strong> - Standard metric in segmentation challenges (evaluation)</p>
<p><strong>Why for EO?</strong> - Accurately capturing <strong>boundaries</strong> (edge of a flood, outline of a building) is often vital in geographic mapping - IoU-based loss directly rewards aligning shapes - Useful in applications like geographic mapping where <strong>boundary delineation is crucial</strong></p>
</section>
<section id="dice-vs-iou---when-to-choose" class="level3">
<h3 class="anchored" data-anchor-id="dice-vs-iou---when-to-choose">Dice vs IoU - When to Choose?</h3>
<p>Both Dice and IoU losses are very similar in concept—they both measure overlap between predicted and true regions—but they have subtle differences that can affect training:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Aspect</th>
<th>Dice Loss</th>
<th>IoU Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Formulation</strong></td>
<td>2×Intersection / Sum</td>
<td>Intersection / Union</td>
</tr>
<tr class="even">
<td><strong>Gradient</strong></td>
<td>More forgiving (2× numerator)</td>
<td>Stricter</td>
</tr>
<tr class="odd">
<td><strong>Training</strong></td>
<td>Smoother, more stable</td>
<td>Can be less stable</td>
</tr>
<tr class="even">
<td><strong>Evaluation</strong></td>
<td>Common in medical/EO</td>
<td>Standard in challenges</td>
</tr>
<tr class="odd">
<td><strong>Boundary Focus</strong></td>
<td>Moderate</td>
<td>Higher emphasis</td>
</tr>
</tbody>
</table>
<p><strong>Practical Guidance:</strong></p>
<p>In practice, both Dice and IoU work well for imbalanced EO data. Dice is slightly more popular in training due to its smoother gradients, while IoU is often used as an evaluation metric in segmentation challenges and competitions. The best approach is to try both on your specific dataset and compare results—the difference is often small, but one may work slightly better depending on your particular data characteristics and class distribution.</p>
<p>Very similar to Dice, IoU loss directly optimizes the IoU metric and has an interpretation closely tied to segmentation quality—it penalizes false positives and false negatives at the region level. This makes it useful in applications like geographic mapping where boundary delineation is crucial.</p>
</section>
<section id="loss-function-4-combined-losses" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-4-combined-losses">Loss Function 4: Combined Losses</h3>
<p><strong>Best of Both Worlds:</strong> Combine complementary loss functions<br>
<strong>In Practice:</strong> You don’t have to pick just one—many implementations use a combination</p>
<p><strong>Common Combination:</strong> <span class="math display">\[
\text{Total Loss} = \alpha \cdot \text{CE Loss} + \beta \cdot \text{Dice Loss}
\]</span></p>
<p>Where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are weighting factors (e.g., <span class="math inline">\(\alpha=0.5\)</span>, <span class="math inline">\(\beta=0.5\)</span>)</p>
<p><strong>Why Combine?</strong> - <strong>Cross-Entropy:</strong> Provides strong, stable gradients; ensures overall pixel-wise correctness - <strong>Dice:</strong> Focuses on overlap and handles imbalance; ensures region-level accuracy - Get the <strong>benefits of both</strong> loss functions</p>
<p><strong>Benefits:</strong> - Stable training from CE’s strong signal - Balanced optimization from Dice’s overlap focus - Often achieves <strong>best results in practice</strong> - Can improve both per-pixel accuracy AND overall region accuracy</p>
<p><strong>Implementation Example:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> combined_loss(y_true, y_pred):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    ce <span class="op">=</span> tf.keras.losses.categorical_crossentropy(y_true, y_pred)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    dice <span class="op">=</span> dice_loss(y_true, y_pred)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> ce <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> dice</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Other Advanced Losses
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some practitioners also use <strong>Focal Loss</strong> (a modified cross-entropy that down-weights easy/background examples) especially for <strong>extremely imbalanced cases</strong>, though it’s more common in object detection. The key takeaway: loss function choice significantly affects model behavior.</p>
</div>
</div>
</section>
<section id="loss-function-selection-guide" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-selection-guide">Loss Function Selection Guide</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Decision Framework for Loss Selection
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Start Here:</strong> 1. <strong>Is your data balanced</strong> (classes ~50/50)? - <strong>Yes</strong> → Standard Cross-Entropy (simple and effective) - <strong>No</strong> → Continue to next question</p>
<ol start="2" type="1">
<li><strong>Is the minority class critical</strong> (e.g., floods, damage, ships)?
<ul>
<li><strong>Yes</strong> → Dice or IoU Loss (inherently handle imbalance)</li>
<li><strong>Somewhat</strong> → Weighted Cross-Entropy</li>
</ul></li>
<li><strong>Need most stable training?</strong>
<ul>
<li><strong>Yes</strong> → Combined Loss (CE + Dice) for stability + imbalance handling</li>
<li><strong>No</strong> → Pure Dice/IoU is fine</li>
</ul></li>
<li><strong>Is boundary accuracy critical?</strong>
<ul>
<li><strong>Yes</strong> → IoU Loss or Combined approach</li>
<li><strong>Moderate</strong> → Dice is sufficient</li>
</ul></li>
</ol>
<p><strong>EO Common Practice:</strong> - <strong>Flood mapping:</strong> Dice or Combined (severe imbalance, critical boundaries) - <strong>Balanced land cover:</strong> Cross-Entropy (classes relatively balanced) - <strong>Building extraction:</strong> Dice or IoU (precise footprints matter) - <strong>Road extraction:</strong> Combined Loss (thin features, need continuity) - <strong>Ship detection:</strong> Dice (extreme imbalance, small objects)</p>
<p><strong>Golden Rule:</strong> Participants should learn <strong>not just to accept the default loss</strong>, but to <strong>think about the nature</strong> of their segmentation problem and pick (or tune) a loss accordingly for the best results.</p>
</div>
</div>
</section>
<section id="practical-example-flood-mapping-loss-selection" class="level3">
<h3 class="anchored" data-anchor-id="practical-example-flood-mapping-loss-selection">Practical Example: Flood Mapping Loss Selection</h3>
<p><strong>Scenario:</strong> - Dataset: 1000 Sentinel-1 SAR images from Central Luzon floods - Class distribution: 92% non-flooded, 8% flooded pixels (typical imbalance) - Goal: Precise flood extent mapping for disaster response</p>
<p><strong>Experiment Results:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 45%">
<col style="width: 33%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Loss Function</th>
<th>IoU Score</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Cross-Entropy</strong></td>
<td>0.12</td>
<td>Model predicts mostly non-flooded; misses actual floods (trivial solution)</td>
</tr>
<tr class="even">
<td><strong>Weighted CE</strong></td>
<td>0.54</td>
<td>Better; weight = 11.5× for flood class; some false positives</td>
</tr>
<tr class="odd">
<td><strong>Dice Loss</strong></td>
<td>0.68</td>
<td>Good recall; slightly noisy predictions; handles imbalance well</td>
</tr>
<tr class="even">
<td><strong>Combined (CE + Dice)</strong></td>
<td><strong>0.73</strong> ✓</td>
<td>Best balance of precision and recall; stable training</td>
</tr>
</tbody>
</table>
<p><strong>Winner:</strong> Combined Loss (0.5 × CE + 0.5 × Dice)</p>
<p><strong>Key Insight:</strong> For our flood mapping case (<strong>binary segmentation with severe imbalance</strong>), we might choose a <strong>Dice loss</strong> or a <strong>combined approach (Dice + Cross-Entropy)</strong> to handle the imbalance and get sharp boundaries. This demonstrates that the choice of loss can <strong>significantly affect model training</strong>—the right loss will push the model to correctly segment the minority class rather than achieving high but useless accuracy by predicting everything as the majority class.</p>
<hr>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Session 1 Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Semantic Segmentation:</strong> - ✓ Pixel-wise classification providing precise boundaries and complete spatial understanding - ✓ Differs from image classification (single labels) and object detection (bounding boxes) - ✓ Essential for EO applications requiring exact spatial extent and area calculations - ✓ Enables detailed thematic mapping and spatial pattern analysis</p>
<p><strong>U-Net Architecture:</strong> - ✓ Encoder-decoder structure with characteristic U-shape - ✓ <strong>Skip connections</strong> are the key innovation—preserve spatial details from encoder to decoder - ✓ Combines “what” (semantic context) with “where” (precise localization) - ✓ Proven architecture even with limited training data (hundreds to thousands of samples) - ✓ Widely adopted across EO community for high-accuracy segmentation - ✓ Uses same CNN building blocks from Day 2 (convolution, pooling, padding, activation)</p>
<p><strong>Loss Functions:</strong> - ✓ <strong>Cross-Entropy:</strong> Standard but sensitive to class imbalance; provides strong gradients - ✓ <strong>Weighted CE:</strong> Addresses imbalance through class weighting - ✓ <strong>Dice/IoU:</strong> Inherently handle imbalance, optimize region overlap, focus on minority class - ✓ <strong>Combined losses</strong> often achieve best performance in EO (e.g., CE + Dice) - ✓ <strong>Choice significantly impacts model behavior</strong>—can mean difference between useless and excellent results - ✓ Must consider data characteristics (balance, boundary importance, object size)</p>
<p><strong>Applications:</strong> - ✓ Flood mapping, land cover, buildings, roads, vegetation monitoring - ✓ U-Net achieves high accuracy even with small datasets - ✓ Outperforms older pixel-based and patch-based methods - ✓ Wide adoption across EO community - ✓ Proven results in Philippine disaster response contexts (Typhoon Ulysses, urban mapping)</p>
</div>
</div>
<hr>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<section id="core-references" class="level3">
<h3 class="anchored" data-anchor-id="core-references">Core References</h3>
<p><strong>Foundational Paper:</strong> - Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. <em>MICCAI 2015</em>. <a href="https://arxiv.org/abs/1505.04597">arXiv:1505.04597</a> - Original U-Net paper from medical imaging - Introduced skip connections concept - Demonstrated effectiveness with limited training data</p>
<p><strong>EO Applications:</strong> - Flood mapping with Sentinel-1 SAR and U-Net (disaster response) - Building extraction from high-resolution imagery (urban planning) - Land cover classification with multispectral data (environmental monitoring) - Road network extraction (infrastructure mapping)</p>
<p><strong>Loss Function References:</strong> - Dice Loss for handling class imbalance in segmentation - IoU (Jaccard) Loss for boundary accuracy - Combined loss strategies for optimal performance</p>
</section>
<section id="datasets-for-practice" class="level3">
<h3 class="anchored" data-anchor-id="datasets-for-practice">Datasets for Practice</h3>
<ul>
<li><strong>Sen1Floods11:</strong> Global flood dataset with Sentinel-1 SAR</li>
<li><strong>DeepGlobe Land Cover Challenge:</strong> Multi-class segmentation</li>
<li><strong>SpaceNet Building Detection:</strong> Urban building footprints</li>
<li><strong>Landcover.ai:</strong> High-resolution orthophotos (Poland)</li>
</ul>
</section>
<section id="tutorials" class="level3">
<h3 class="anchored" data-anchor-id="tutorials">Tutorials</h3>
<ul>
<li><a href="https://www.tensorflow.org/tutorials/images/segmentation">TensorFlow U-Net Tutorial</a></li>
<li><a href="https://pytorch.org/vision/stable/models.html#semantic-segmentation">PyTorch Semantic Segmentation Examples</a></li>
<li><a href="https://keras.io/examples/vision/oxford_pets_image_segmentation/">Keras U-Net for Satellite Imagery</a></li>
</ul>
</section>
<section id="philippine-eo-context" class="level3">
<h3 class="anchored" data-anchor-id="philippine-eo-context">Philippine EO Context</h3>
<ul>
<li><strong>PhilSA:</strong> Flood monitoring and disaster response initiatives</li>
<li><strong>DOST-ASTI DATOS:</strong> Automated rapid mapping for disasters</li>
<li><strong>NAMRIA:</strong> Geospatial data infrastructure</li>
</ul>
<hr>
</section>
</section>
<section id="preparation-for-session-2" class="level2">
<h2 class="anchored" data-anchor-id="preparation-for-session-2">Preparation for Session 2</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Next: Hands-on Flood Mapping Lab
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Session 2</strong> will put these concepts into practice with a complete U-Net implementation:</p>
<p><strong>What You’ll Do:</strong> 1. Load Sentinel-1 SAR data from Typhoon Ulysses (Central Luzon) 2. Build U-Net model in TensorFlow/PyTorch 3. Train with Dice Loss (or combined loss) 4. Evaluate performance using IoU, F1-score, precision, recall 5. Visualize flood predictions and create export maps</p>
<p><strong>Dataset:</strong> - ~500-1000 pre-processed SAR patches (256×256 pixels) - Binary flood masks (flooded / non-flooded) - Real flood event from major Philippine river basin</p>
<p><strong>Expected Results:</strong> - IoU &gt; 0.70 with properly trained model - Visual flood extent maps ready for GIS integration</p>
<p><strong>To Prepare:</strong> - Ensure Google Colab access - Check GPU availability (Runtime → Change runtime type → GPU) - Review Python and NumPy basics if needed - Have patience - model training takes time!</p>
<p><a href="../../day3/notebooks/Day3_Session2_Flood_Mapping_UNet.html" class="btn btn-outline-primary">Preview Session 2 →</a></p>
</div>
</div>
<hr>
</section>
<section id="discussion-questions" class="level2">
<h2 class="anchored" data-anchor-id="discussion-questions">Discussion Questions</h2>
<p>Before moving to the hands-on session, consider these questions:</p>
<ol type="1">
<li><p><strong>What EO applications in your work</strong> could benefit from semantic segmentation versus classification or detection?</p></li>
<li><p><strong>How would you validate</strong> segmentation results in the field, especially for disaster response applications?</p></li>
<li><p><strong>What challenges do you anticipate</strong> when working with limited training data for Philippine-specific contexts?</p></li>
<li><p><strong>How might transfer learning</strong> from models trained on other regions help with Philippine EO applications?</p></li>
</ol>
<hr>
<div class="session-nav">
<p><a href="../../day3/index.html" class="btn btn-outline-secondary">← Back to Day 3 Overview</a> <a href="../../day3/notebooks/Day3_Session2_Flood_Mapping_UNet.html" class="btn btn-primary">Next: Session 2 - Flood Mapping Lab →</a></p>
</div>
<hr>
<p><em>This session is part of the CoPhil 4-Day Advanced Training on AI/ML for Earth Observation, funded by the European Union under the Global Gateway initiative.</em></p>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/DimitrisKasabalis\.github\.io\/cophil-training-v1\.0");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="cophil-training-v1.0" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../day3/index.html" class="pagination-link" aria-label="Day 3: Deep Learning for Earth Observation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Day 3: Deep Learning for Earth Observation</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../day3/sessions/session2.html" class="pagination-link" aria-label="Session 2: Hands-on Flood Mapping with U-Net and Sentinel-1 SAR">
        <span class="nav-page-text">Session 2: Hands-on Flood Mapping with U-Net and Sentinel-1 SAR</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Session 1: Semantic Segmentation with U-Net for Earth Observation"</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Advanced Deep Learning for Pixel-Level Analysis"</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> last-modified</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "CoPhil Advanced Training Program"</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">    toc-depth: 3</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">nav</span><span class="ot"> class</span><span class="op">=</span><span class="st">"breadcrumb"</span><span class="ot"> aria-label</span><span class="op">=</span><span class="st">"Breadcrumb"</span><span class="dt">&gt;</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">a</span><span class="ot"> href</span><span class="op">=</span><span class="st">"../../index.html"</span><span class="dt">&gt;</span>Home<span class="dt">&lt;/</span><span class="kw">a</span><span class="dt">&gt;</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">span</span><span class="ot"> class</span><span class="op">=</span><span class="st">"breadcrumb-separator"</span><span class="ot"> aria-hidden</span><span class="op">=</span><span class="st">"true"</span><span class="dt">&gt;</span>›<span class="dt">&lt;/</span><span class="kw">span</span><span class="dt">&gt;</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">a</span><span class="ot"> href</span><span class="op">=</span><span class="st">"../index.html"</span><span class="dt">&gt;</span>Day 3<span class="dt">&lt;/</span><span class="kw">a</span><span class="dt">&gt;</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">span</span><span class="ot"> class</span><span class="op">=</span><span class="st">"breadcrumb-separator"</span><span class="ot"> aria-hidden</span><span class="op">=</span><span class="st">"true"</span><span class="dt">&gt;</span>›<span class="dt">&lt;/</span><span class="kw">span</span><span class="dt">&gt;</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>  <span class="dt">&lt;</span><span class="kw">span</span><span class="ot"> class</span><span class="op">=</span><span class="st">"breadcrumb-current"</span><span class="dt">&gt;</span>Session 1<span class="dt">&lt;/</span><span class="kw">span</span><span class="dt">&gt;</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">nav</span><span class="dt">&gt;</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>::: {.hero}</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="fu"># Session 1: Semantic Segmentation with U-Net</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="fu">### Advanced Deep Learning for Pixel-Level Analysis</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>Master the U-Net architecture for precise Earth Observation segmentation tasks</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="fu">## Session Overview</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>::: {.session-info}</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>**Duration:** 1.5 hours | **Format:** Lecture + Discussion | **Platform:** Presentation &amp; Slides</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>This session introduces semantic segmentation as a pixel-wise classification task and explores the U-Net architecture—one of the most successful deep learning models for Earth Observation applications. You'll understand how U-Net's encoder-decoder structure with skip connections enables precise boundary delineation for tasks like flood mapping, land cover classification, and infrastructure extraction.</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>::: {.learning-objectives}</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="fu">### Learning Objectives</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>By the end of this session, you will be able to:</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Define** semantic segmentation and distinguish it from classification and object detection</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Explain** the U-Net architecture including encoder, decoder, and skip connections</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Describe** how loss functions (Cross-Entropy, Dice, IoU) handle class imbalance</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Identify** Earth Observation applications suited for semantic segmentation</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Evaluate** when to use different loss functions for segmentation tasks</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a><span class="fu">## Presentation Slides</span></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">iframe</span><span class="ot"> src</span><span class="op">=</span><span class="st">"../presentations/session1_unet_segmentation.html"</span><span class="ot"> width</span><span class="op">=</span><span class="st">"100%"</span><span class="ot"> height</span><span class="op">=</span><span class="st">"600"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"border: 1px solid #ccc; border-radius: 4px;"</span><span class="dt">&gt;&lt;/</span><span class="kw">iframe</span><span class="dt">&gt;</span></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 1: Concept of Semantic Segmentation</span></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a><span class="fu">### What is Semantic Segmentation?</span></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a>**Semantic segmentation** is the task of classifying every pixel in an image into a category, producing a detailed, pixel-wise map of the image content. Unlike **image classification** (which assigns one label per image) or **object detection** (which locates objects with bounding boxes), segmentation provides a fine-grained understanding of the scene.</span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a>In segmentation, the output is an image where every pixel is labeled (e.g., water, building, forest, road), thus outlining the exact shapes and areas of these features. This level of detail is especially useful in geospatial contexts for creating thematic maps and analyzing spatial patterns.</span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>**Example Comparison:**</span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classification:** "Is this satellite patch urban or agricultural?" → Single label for entire image</span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Detection:** "Where are the buildings?" → Bounding boxes around structures  </span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Segmentation:** "Label every pixel as building, road, vegetation, or water" → Complete pixel-level map</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>This pixel-level approach is crucial for Earth Observation tasks like mapping floods, land cover, or burn scars, where knowing the exact extent and shape of features matters. Unlike classification that might tell us a patch is "urban," segmentation highlights exactly which pixels are buildings, roads, vegetation, water, etc., giving a much richer understanding of the scene.</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">Computer Vision Task Hierarchy</span><span class="ot">"</span></span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>%%| fig-width: <span class="dv">100</span>%</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>graph TB</span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>    A[Computer Vision Tasks] --&gt; B[Classification]</span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>    A --&gt; C[Object Detection]</span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>    A --&gt; D[Semantic Segmentation]</span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>    B --&gt; B1[<span class="ot">"</span><span class="st">What's in this image?&lt;br/&gt;Output: Single label&lt;br/&gt;Granularity: Image-level</span><span class="ot">"</span>]</span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>    C --&gt; C1[<span class="ot">"</span><span class="st">Where are the objects?&lt;br/&gt;Output: Bounding boxes&lt;br/&gt;Granularity: Object-level</span><span class="ot">"</span>]</span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>    D --&gt; D1[<span class="ot">"</span><span class="st">Which pixels belong to what?&lt;br/&gt;Output: Pixel masks&lt;br/&gt;Granularity: Pixel-level</span><span class="ot">"</span>]</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a>    style A fill:<span class="co">#0066cc,stroke:#003d7a,stroke-width:3px,color:#fff</span></span>
<span id="cb6-90"><a href="#cb6-90" aria-hidden="true" tabindex="-1"></a>    style D fill:<span class="co">#00aa44,stroke:#006622,stroke-width:2px,color:#fff</span></span>
<span id="cb6-91"><a href="#cb6-91" aria-hidden="true" tabindex="-1"></a>    style D1 fill:<span class="co">#00aa44,stroke:#006622,stroke-width:1px,color:#fff</span></span>
<span id="cb6-92"><a href="#cb6-92" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-93"><a href="#cb6-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-94"><a href="#cb6-94" aria-hidden="true" tabindex="-1"></a><span class="fu">### Understanding the Difference</span></span>
<span id="cb6-95"><a href="#cb6-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-96"><a href="#cb6-96" aria-hidden="true" tabindex="-1"></a>In semantic segmentation, the output is an image where every pixel is labeled (e.g., water, building, forest), allowing **precise delineation** of different land cover types. This level of detail is especially useful in geospatial contexts for creating thematic maps and analyzing spatial patterns.</span>
<span id="cb6-97"><a href="#cb6-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-98"><a href="#cb6-98" aria-hidden="true" tabindex="-1"></a>For example, in a satellite image we might label each pixel as water, building, forest, road, etc., thus outlining the exact shapes and areas of these features. Whereas an image classification might tell us an entire satellite patch is "urban" or "agriculture," semantic segmentation can highlight **exactly which pixels** are buildings, roads, vegetation, water, etc., giving a much richer understanding of the scene. </span>
<span id="cb6-99"><a href="#cb6-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-100"><a href="#cb6-100" aria-hidden="true" tabindex="-1"></a>We contrast these tasks visually and conceptually so the distinction is clear:</span>
<span id="cb6-101"><a href="#cb6-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-102"><a href="#cb6-102" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Image Classification** answers: "Is this satellite patch urban or agricultural?" with a single label for the entire image</span>
<span id="cb6-103"><a href="#cb6-103" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Object Detection** answers: "Where are the buildings?" by drawing bounding boxes around each structure</span>
<span id="cb6-104"><a href="#cb6-104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Semantic Segmentation** answers: "Label every pixel as water, forest, urban, or agriculture" producing a complete pixel-level classification map</span>
<span id="cb6-105"><a href="#cb6-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-106"><a href="#cb6-106" aria-hidden="true" tabindex="-1"></a>This pixel-level approach is crucial for Earth Observation tasks like mapping floods, land cover, or burn scars, where knowing the **exact extent and shape** of features matters for decision-making, planning, and response.</span>
<span id="cb6-107"><a href="#cb6-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-108"><a href="#cb6-108" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb6-109"><a href="#cb6-109" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Semantic Segmentation for EO?</span></span>
<span id="cb6-110"><a href="#cb6-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-111"><a href="#cb6-111" aria-hidden="true" tabindex="-1"></a>Semantic segmentation offers several critical advantages for Earth Observation applications that make it indispensable for many geospatial analysis tasks:</span>
<span id="cb6-112"><a href="#cb6-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-113"><a href="#cb6-113" aria-hidden="true" tabindex="-1"></a>**Precise Delineation:**</span>
<span id="cb6-114"><a href="#cb6-114" aria-hidden="true" tabindex="-1"></a>Segmentation provides exact boundaries of features—the precise edge of flood extent, the exact boundary where forest stops and urban area begins, the specific outline of agricultural fields. This pixel-level precision is far superior to bounding boxes or image-level labels.</span>
<span id="cb6-115"><a href="#cb6-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-116"><a href="#cb6-116" aria-hidden="true" tabindex="-1"></a>**Quantitative Analysis:**</span>
<span id="cb6-117"><a href="#cb6-117" aria-hidden="true" tabindex="-1"></a>With pixel-wise classification, we can calculate accurate areas down to the precision of individual pixels. For flood mapping, this means knowing exactly how many square kilometers are inundated. For forest monitoring, it means precise measurements of deforestation extent.</span>
<span id="cb6-118"><a href="#cb6-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-119"><a href="#cb6-119" aria-hidden="true" tabindex="-1"></a>**Change Detection:**</span>
<span id="cb6-120"><a href="#cb6-120" aria-hidden="true" tabindex="-1"></a>Pixel-level comparison over time enables detailed change detection. We can identify exactly which pixels changed from forest to urban, or from dry land to water, enabling fine-grained temporal analysis.</span>
<span id="cb6-121"><a href="#cb6-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-122"><a href="#cb6-122" aria-hidden="true" tabindex="-1"></a>**Thematic Mapping:**</span>
<span id="cb6-123"><a href="#cb6-123" aria-hidden="true" tabindex="-1"></a>Segmentation produces detailed land cover and land use maps where every location has a meaningful class label, creating rich thematic datasets for analysis, planning, and decision-making.</span>
<span id="cb6-124"><a href="#cb6-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-125"><a href="#cb6-125" aria-hidden="true" tabindex="-1"></a>**Decision Support:**</span>
<span id="cb6-126"><a href="#cb6-126" aria-hidden="true" tabindex="-1"></a>The fine-grained information from segmentation directly supports disaster response and planning. For typhoon flood assessment, segmentation provides exact flood boundaries for targeted relief operations—identifying which specific buildings or roads are affected—not just a general "flooded" vs "not flooded" assessment for an entire region.</span>
<span id="cb6-127"><a href="#cb6-127" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-128"><a href="#cb6-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-129"><a href="#cb6-129" aria-hidden="true" tabindex="-1"></a><span class="fu">### Three Task Comparison</span></span>
<span id="cb6-130"><a href="#cb6-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-131"><a href="#cb6-131" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Aspect <span class="pp">|</span> Classification <span class="pp">|</span> Object Detection <span class="pp">|</span> Semantic Segmentation <span class="pp">|</span></span>
<span id="cb6-132"><a href="#cb6-132" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|---------------|------------------|----------------------|</span></span>
<span id="cb6-133"><a href="#cb6-133" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Question** <span class="pp">|</span> What's in this image? <span class="pp">|</span> Where are objects? <span class="pp">|</span> Which pixels are what? <span class="pp">|</span></span>
<span id="cb6-134"><a href="#cb6-134" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Output** <span class="pp">|</span> Single label <span class="pp">|</span> Bounding boxes + labels <span class="pp">|</span> Pixel-wise mask <span class="pp">|</span></span>
<span id="cb6-135"><a href="#cb6-135" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Granularity** <span class="pp">|</span> Image-level <span class="pp">|</span> Object-level <span class="pp">|</span> Pixel-level <span class="pp">|</span></span>
<span id="cb6-136"><a href="#cb6-136" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Spatial Info** <span class="pp">|</span> None <span class="pp">|</span> Approximate (boxes) <span class="pp">|</span> Precise (pixels) <span class="pp">|</span></span>
<span id="cb6-137"><a href="#cb6-137" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Computation** <span class="pp">|</span> Fast <span class="pp">|</span> Moderate <span class="pp">|</span> Intensive <span class="pp">|</span></span>
<span id="cb6-138"><a href="#cb6-138" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Use Case Example** <span class="pp">|</span> "Contains buildings" <span class="pp">|</span> "10 buildings detected" <span class="pp">|</span> "Building footprints mapped" <span class="pp">|</span></span>
<span id="cb6-139"><a href="#cb6-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-140"><a href="#cb6-140" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-141"><a href="#cb6-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-142"><a href="#cb6-142" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 2: U-Net Architecture</span></span>
<span id="cb6-143"><a href="#cb6-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-144"><a href="#cb6-144" aria-hidden="true" tabindex="-1"></a><span class="fu">### Introduction to U-Net</span></span>
<span id="cb6-145"><a href="#cb6-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-146"><a href="#cb6-146" aria-hidden="true" tabindex="-1"></a>**U-Net** was developed by Ronneberger et al. (2015) for biomedical image segmentation and has since become one of the most popular architectures for Earth Observation applications.</span>
<span id="cb6-147"><a href="#cb6-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-148"><a href="#cb6-148" aria-hidden="true" tabindex="-1"></a>**Why "U-Net"?**</span>
<span id="cb6-149"><a href="#cb6-149" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Architecture shape resembles the letter "U"</span>
<span id="cb6-150"><a href="#cb6-150" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Symmetric encoder-decoder structure</span>
<span id="cb6-151"><a href="#cb6-151" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Key Innovation:** Skip connections that preserve spatial information</span>
<span id="cb6-152"><a href="#cb6-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-155"><a href="#cb6-155" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb6-156"><a href="#cb6-156" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">U-Net Architecture Overview</span><span class="ot">"</span></span>
<span id="cb6-157"><a href="#cb6-157" aria-hidden="true" tabindex="-1"></a>%%| fig-width: <span class="dv">100</span>%</span>
<span id="cb6-158"><a href="#cb6-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-159"><a href="#cb6-159" aria-hidden="true" tabindex="-1"></a>graph TD</span>
<span id="cb6-160"><a href="#cb6-160" aria-hidden="true" tabindex="-1"></a>    A[Input Image&lt;br/&gt;H × W × C] --&gt; B[Encoder&lt;br/&gt;Contracting Path]</span>
<span id="cb6-161"><a href="#cb6-161" aria-hidden="true" tabindex="-1"></a>    B --&gt; C[Bottleneck&lt;br/&gt;Most Compressed]</span>
<span id="cb6-162"><a href="#cb6-162" aria-hidden="true" tabindex="-1"></a>    C --&gt; D[Decoder&lt;br/&gt;Expansive Path]</span>
<span id="cb6-163"><a href="#cb6-163" aria-hidden="true" tabindex="-1"></a>    D --&gt; E[Output Mask&lt;br/&gt;H × W × Classes]</span>
<span id="cb6-164"><a href="#cb6-164" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-165"><a href="#cb6-165" aria-hidden="true" tabindex="-1"></a>    B -.-&gt;|Skip Connection <span class="dv">1</span>| D</span>
<span id="cb6-166"><a href="#cb6-166" aria-hidden="true" tabindex="-1"></a>    B -.-&gt;|Skip Connection <span class="dv">2</span>| D</span>
<span id="cb6-167"><a href="#cb6-167" aria-hidden="true" tabindex="-1"></a>    B -.-&gt;|Skip Connection <span class="dv">3</span>| D</span>
<span id="cb6-168"><a href="#cb6-168" aria-hidden="true" tabindex="-1"></a>    B -.-&gt;|Skip Connection <span class="dv">4</span>| D</span>
<span id="cb6-169"><a href="#cb6-169" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-170"><a href="#cb6-170" aria-hidden="true" tabindex="-1"></a>    B --&gt; B1[<span class="ot">"</span><span class="st">Feature Extraction&lt;br/&gt;Spatial dimension ↓&lt;br/&gt;Feature depth ↑</span><span class="ot">"</span>]</span>
<span id="cb6-171"><a href="#cb6-171" aria-hidden="true" tabindex="-1"></a>    C --&gt; C1[<span class="ot">"</span><span class="st">Global Context&lt;br/&gt;What is in image</span><span class="ot">"</span>]</span>
<span id="cb6-172"><a href="#cb6-172" aria-hidden="true" tabindex="-1"></a>    D --&gt; D1[<span class="ot">"</span><span class="st">Spatial Reconstruction&lt;br/&gt;Spatial dimension ↑&lt;br/&gt;Feature depth ↓</span><span class="ot">"</span>]</span>
<span id="cb6-173"><a href="#cb6-173" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-174"><a href="#cb6-174" aria-hidden="true" tabindex="-1"></a>    style A fill:<span class="co">#0066cc,stroke:#003d7a,stroke-width:2px,color:#fff</span></span>
<span id="cb6-175"><a href="#cb6-175" aria-hidden="true" tabindex="-1"></a>    style E fill:<span class="co">#00aa44,stroke:#006622,stroke-width:2px,color:#fff</span></span>
<span id="cb6-176"><a href="#cb6-176" aria-hidden="true" tabindex="-1"></a>    style C fill:<span class="co">#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff</span></span>
<span id="cb6-177"><a href="#cb6-177" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-178"><a href="#cb6-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-179"><a href="#cb6-179" aria-hidden="true" tabindex="-1"></a><span class="fu">### Encoder (Contracting Path)</span></span>
<span id="cb6-180"><a href="#cb6-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-181"><a href="#cb6-181" aria-hidden="true" tabindex="-1"></a>**Purpose:** Extract hierarchical features at multiple scales while progressively compressing spatial information</span>
<span id="cb6-182"><a href="#cb6-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-183"><a href="#cb6-183" aria-hidden="true" tabindex="-1"></a>The encoder is a series of convolutional and pooling layers that progressively downsample the image, extracting higher-level features while reducing spatial resolution (just as we learned with CNNs on Day 2). </span>
<span id="cb6-184"><a href="#cb6-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-185"><a href="#cb6-185" aria-hidden="true" tabindex="-1"></a>**Operations:**</span>
<span id="cb6-186"><a href="#cb6-186" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Convolution blocks:**</span>
<span id="cb6-187"><a href="#cb6-187" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Two 3×3 convolutional layers (with ReLU activations)</span>
<span id="cb6-188"><a href="#cb6-188" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>(Optional) Batch normalization</span>
<span id="cb6-189"><a href="#cb6-189" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Note:** Often uses "same" padding to preserve spatial dimensions through conv layers</span>
<span id="cb6-190"><a href="#cb6-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-191"><a href="#cb6-191" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Downsampling:**</span>
<span id="cb6-192"><a href="#cb6-192" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>2×2 max pooling (reduces spatial resolution)</span>
<span id="cb6-193"><a href="#cb6-193" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Spatial dimensions halve</span>
<span id="cb6-194"><a href="#cb6-194" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Feature channels double</span>
<span id="cb6-195"><a href="#cb6-195" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Creates hierarchical representation</span>
<span id="cb6-196"><a href="#cb6-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-197"><a href="#cb6-197" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-198"><a href="#cb6-198" aria-hidden="true" tabindex="-1"></a><span class="fu">## Connection to Day 2 Concepts</span></span>
<span id="cb6-199"><a href="#cb6-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-200"><a href="#cb6-200" aria-hidden="true" tabindex="-1"></a>**Recall from Day 2:** The encoder uses the same CNN building blocks you learned:</span>
<span id="cb6-201"><a href="#cb6-201" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Convolution layers** apply learnable filters to extract features</span>
<span id="cb6-202"><a href="#cb6-202" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Padding** ("same" padding) helps preserve spatial dimensions so feature maps align for skip connections</span>
<span id="cb6-203"><a href="#cb6-203" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Pooling** reduces dimensionality—recall from Day 2 that pooling without padding reduces image size, losing some detail</span>
<span id="cb6-204"><a href="#cb6-204" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**ReLU activation** introduces non-linearity</span>
<span id="cb6-205"><a href="#cb6-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-206"><a href="#cb6-206" aria-hidden="true" tabindex="-1"></a>For instance, using 3×3 convolutions (with ReLU activations) and 2×2 max-pooling, the encoder learns rich features but shrinks the image size at each step. As we move down the encoder, image details are compressed and abstracted, capturing the **context** of what is in the image.</span>
<span id="cb6-207"><a href="#cb6-207" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-208"><a href="#cb6-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-209"><a href="#cb6-209" aria-hidden="true" tabindex="-1"></a>**Example Progression:**</span>
<span id="cb6-210"><a href="#cb6-210" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-211"><a href="#cb6-211" aria-hidden="true" tabindex="-1"></a><span class="in">Input:     256×256×3   (RGB satellite image)</span></span>
<span id="cb6-212"><a href="#cb6-212" aria-hidden="true" tabindex="-1"></a><span class="in">Block 1:   256×256×64  (after convolutions)</span></span>
<span id="cb6-213"><a href="#cb6-213" aria-hidden="true" tabindex="-1"></a><span class="in">Pool 1:    128×128×64  (after max pooling)</span></span>
<span id="cb6-214"><a href="#cb6-214" aria-hidden="true" tabindex="-1"></a><span class="in">Block 2:   128×128×128 (after convolutions)</span></span>
<span id="cb6-215"><a href="#cb6-215" aria-hidden="true" tabindex="-1"></a><span class="in">Pool 2:    64×64×128   (after max pooling)</span></span>
<span id="cb6-216"><a href="#cb6-216" aria-hidden="true" tabindex="-1"></a><span class="in">Block 3:   64×64×256   (after convolutions)</span></span>
<span id="cb6-217"><a href="#cb6-217" aria-hidden="true" tabindex="-1"></a><span class="in">Pool 3:    32×32×256   (after max pooling)</span></span>
<span id="cb6-218"><a href="#cb6-218" aria-hidden="true" tabindex="-1"></a><span class="in">Block 4:   32×32×512   (after convolutions)</span></span>
<span id="cb6-219"><a href="#cb6-219" aria-hidden="true" tabindex="-1"></a><span class="in">Pool 4:    16×16×512   (after max pooling)</span></span>
<span id="cb6-220"><a href="#cb6-220" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-221"><a href="#cb6-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-222"><a href="#cb6-222" aria-hidden="true" tabindex="-1"></a>**Multi-Scale Learning:**</span>
<span id="cb6-223"><a href="#cb6-223" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Early layers:** Capture fine details (edges, textures, small features)</span>
<span id="cb6-224"><a href="#cb6-224" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Deep layers:** Capture semantic meaning (water bodies, urban areas, forests)</span>
<span id="cb6-225"><a href="#cb6-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-226"><a href="#cb6-226" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bottleneck Layer</span></span>
<span id="cb6-227"><a href="#cb6-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-228"><a href="#cb6-228" aria-hidden="true" tabindex="-1"></a>The central part of U-Net is the **bottleneck layer** (the bottom of the "U"), where the feature representation is most compressed. This is where the network holds a condensed encoding of the image's content—**maximum context, minimum spatial detail**—before the decoder begins expanding it.</span>
<span id="cb6-229"><a href="#cb6-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-230"><a href="#cb6-230" aria-hidden="true" tabindex="-1"></a>**Characteristics of the Bottleneck:**</span>
<span id="cb6-231"><a href="#cb6-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-232"><a href="#cb6-232" aria-hidden="true" tabindex="-1"></a>At this point, we have the smallest spatial dimensions (e.g., 16×16 pixels) but the largest number of feature channels (e.g., 1024). This creates a highly compressed representation of the entire image.</span>
<span id="cb6-233"><a href="#cb6-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-234"><a href="#cb6-234" aria-hidden="true" tabindex="-1"></a>**What It Captures:**</span>
<span id="cb6-235"><a href="#cb6-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-236"><a href="#cb6-236" aria-hidden="true" tabindex="-1"></a>The bottleneck captures **global context**—it understands **what's in the image** at a semantic level. It contains information like "there is water, buildings, vegetation" but has lost the precise spatial information about **where exactly** these features are located. This trade-off is intentional: by compressing spatial dimensions while expanding feature depth, the encoder creates an abstract, semantic understanding of the scene that the decoder can then use to reconstruct precise pixel-wise predictions.</span>
<span id="cb6-237"><a href="#cb6-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-238"><a href="#cb6-238" aria-hidden="true" tabindex="-1"></a><span class="fu">### Decoder (Expansive Path)</span></span>
<span id="cb6-239"><a href="#cb6-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-240"><a href="#cb6-240" aria-hidden="true" tabindex="-1"></a>**Purpose:** Reconstruct spatial resolution using encoded features to construct precise pixel-wise predictions</span>
<span id="cb6-241"><a href="#cb6-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-242"><a href="#cb6-242" aria-hidden="true" tabindex="-1"></a>The decoder performs the reverse of the encoder: it uses upsampling (e.g., transpose convolutions) to increase the spatial resolution, gradually building the output segmentation map.</span>
<span id="cb6-243"><a href="#cb6-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-244"><a href="#cb6-244" aria-hidden="true" tabindex="-1"></a>**Operations:**</span>
<span id="cb6-245"><a href="#cb6-245" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Upsampling:**</span>
<span id="cb6-246"><a href="#cb6-246" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Transpose convolution** (learnable filters for upsampling, sometimes called "deconvolution") OR</span>
<span id="cb6-247"><a href="#cb6-247" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Bilinear/nearest upsampling** (simpler interpolations) + regular convolution</span>
<span id="cb6-248"><a href="#cb6-248" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Doubles spatial dimensions</span>
<span id="cb6-249"><a href="#cb6-249" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Halves feature channels</span>
<span id="cb6-250"><a href="#cb6-250" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Mirrors encoder downsampling in reverse</span>
<span id="cb6-251"><a href="#cb6-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-252"><a href="#cb6-252" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Skip Connection Concatenation:**</span>
<span id="cb6-253"><a href="#cb6-253" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Copy high-resolution feature maps from corresponding encoder layer</span>
<span id="cb6-254"><a href="#cb6-254" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>The feature maps from the encoder are copied and **concatenated** onto the decoder's feature maps at corresponding levels</span>
<span id="cb6-255"><a href="#cb6-255" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Fuse high-resolution spatial details with semantic understanding</span>
<span id="cb6-256"><a href="#cb6-256" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Critical:** Feature map sizes must align (achieved through proper padding in encoder)</span>
<span id="cb6-257"><a href="#cb6-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-258"><a href="#cb6-258" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Convolution blocks:**</span>
<span id="cb6-259"><a href="#cb6-259" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Two 3×3 convolutional layers  </span>
<span id="cb6-260"><a href="#cb6-260" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>ReLU activation</span>
<span id="cb6-261"><a href="#cb6-261" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Refine combined features into sharper predictions</span>
<span id="cb6-262"><a href="#cb6-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-263"><a href="#cb6-263" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb6-264"><a href="#cb6-264" aria-hidden="true" tabindex="-1"></a><span class="fu">## Upsampling and Implementation Details</span></span>
<span id="cb6-265"><a href="#cb6-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-266"><a href="#cb6-266" aria-hidden="true" tabindex="-1"></a>**Recall from Day 2:** Upsampling is essentially the **inverse of pooling**—it increases spatial dimensions to expand the image back to full size.</span>
<span id="cb6-267"><a href="#cb6-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-268"><a href="#cb6-268" aria-hidden="true" tabindex="-1"></a>**Two common approaches:**</span>
<span id="cb6-269"><a href="#cb6-269" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Transpose convolution:** Learned transposed conv layers (sometimes called "deconvolution") with trainable filters</span>
<span id="cb6-270"><a href="#cb6-270" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Interpolation + Conv:** Simpler interpolations (bilinear or nearest neighbor) followed by regular conv to refine</span>
<span id="cb6-271"><a href="#cb6-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-272"><a href="#cb6-272" aria-hidden="true" tabindex="-1"></a>**Implementation Note:** To make concatenation in skip connections seamless, we often use padding in convolutions to maintain equal sizes between encoder and decoder feature maps (Day 2 covered how "same" padding keeps dimensions). The **original U-Net paper cropped feature maps** instead, but modern frameworks simply pad zeros so that encoder outputs and decoder inputs align.</span>
<span id="cb6-273"><a href="#cb6-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-274"><a href="#cb6-274" aria-hidden="true" tabindex="-1"></a>By the end of the decoder, a **1×1 convolution** produces the final segmentation map, with as many channels as target classes, so that each pixel receives a class label.</span>
<span id="cb6-275"><a href="#cb6-275" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-276"><a href="#cb6-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-277"><a href="#cb6-277" aria-hidden="true" tabindex="-1"></a>**Example Progression:**</span>
<span id="cb6-278"><a href="#cb6-278" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-279"><a href="#cb6-279" aria-hidden="true" tabindex="-1"></a><span class="in">Bottleneck:  16×16×1024</span></span>
<span id="cb6-280"><a href="#cb6-280" aria-hidden="true" tabindex="-1"></a><span class="in">Upsample 1:  32×32×512</span></span>
<span id="cb6-281"><a href="#cb6-281" aria-hidden="true" tabindex="-1"></a><span class="in">Concat:      32×32×1024  (512 from decoder + 512 from encoder skip)</span></span>
<span id="cb6-282"><a href="#cb6-282" aria-hidden="true" tabindex="-1"></a><span class="in">Conv Block:  32×32×512</span></span>
<span id="cb6-283"><a href="#cb6-283" aria-hidden="true" tabindex="-1"></a><span class="in">Upsample 2:  64×64×256</span></span>
<span id="cb6-284"><a href="#cb6-284" aria-hidden="true" tabindex="-1"></a><span class="in">Concat:      64×64×512   (256 from decoder + 256 from encoder skip)</span></span>
<span id="cb6-285"><a href="#cb6-285" aria-hidden="true" tabindex="-1"></a><span class="in">Conv Block:  64×64×256</span></span>
<span id="cb6-286"><a href="#cb6-286" aria-hidden="true" tabindex="-1"></a><span class="in">...</span></span>
<span id="cb6-287"><a href="#cb6-287" aria-hidden="true" tabindex="-1"></a><span class="in">Final:       256×256×num_classes</span></span>
<span id="cb6-288"><a href="#cb6-288" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-289"><a href="#cb6-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-290"><a href="#cb6-290" aria-hidden="true" tabindex="-1"></a><span class="fu">### Skip Connections - The Key Innovation</span></span>
<span id="cb6-291"><a href="#cb6-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-292"><a href="#cb6-292" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb6-293"><a href="#cb6-293" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why Skip Connections Matter</span></span>
<span id="cb6-294"><a href="#cb6-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-295"><a href="#cb6-295" aria-hidden="true" tabindex="-1"></a>A key innovation of U-Net is the **skip connections** linking matching encoder and decoder layers. The feature maps from the encoder (which contain fine-grained spatial details from earlier layers) are **concatenated** with the upsampled features in the decoder. This allows the model to "skip over" the bottleneck and directly inject high-resolution context into the decoding process.</span>
<span id="cb6-296"><a href="#cb6-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-297"><a href="#cb6-297" aria-hidden="true" tabindex="-1"></a>**The Problem They Solve:**</span>
<span id="cb6-298"><a href="#cb6-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-299"><a href="#cb6-299" aria-hidden="true" tabindex="-1"></a>Without skip connections, information is inevitably lost during the downsampling process (pooling operations). The decoder would have to reconstruct precise boundaries solely from the coarse, compressed features at the bottleneck. This results in blurry boundaries and loss of fine spatial detail—exactly what we want to avoid in Earth Observation applications.</span>
<span id="cb6-300"><a href="#cb6-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-301"><a href="#cb6-301" aria-hidden="true" tabindex="-1"></a>**How Skip Connections Help:**</span>
<span id="cb6-302"><a href="#cb6-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-303"><a href="#cb6-303" aria-hidden="true" tabindex="-1"></a>The skip connections preserve edges and small structures (e.g., the exact boundary of a flooded area or building outline) that might otherwise be lost during downsampling. The result is **improved detail and accuracy** in segmentation outputs, since the decoder doesn't have to rely solely on the coarse feature maps after upsampling—it can leverage the original fine details as well.</span>
<span id="cb6-304"><a href="#cb6-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-305"><a href="#cb6-305" aria-hidden="true" tabindex="-1"></a>**Best of Both Worlds:**</span>
<span id="cb6-306"><a href="#cb6-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-307"><a href="#cb6-307" aria-hidden="true" tabindex="-1"></a>Crucially, U-Net's decoder is fed by skip connections from the encoder: these skip connections provide high-resolution context to the decoder, ensuring that fine details (like precise boundaries) are preserved even after the image was compressed by the encoder. </span>
<span id="cb6-308"><a href="#cb6-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-309"><a href="#cb6-309" aria-hidden="true" tabindex="-1"></a>In essence, the **encoder captures what is in the image** (context), and the **decoder, aided by skips, ensures we know where those things are** in the image (precise localization). By combining encoder and decoder features, U-Net captures both the **what** (context from the semantic understanding in the bottleneck) and the **where** (location from the high-resolution encoder features) for each class in the image. </span>
<span id="cb6-310"><a href="#cb6-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-311"><a href="#cb6-311" aria-hidden="true" tabindex="-1"></a>Notably, U-Net implementations must handle the alignment of feature map sizes for concatenation—often using appropriate padding ("same" padding) on convolutions so that each encoder output matches the size of the corresponding decoder feature map.</span>
<span id="cb6-312"><a href="#cb6-312" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-313"><a href="#cb6-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-314"><a href="#cb6-314" aria-hidden="true" tabindex="-1"></a><span class="fu">### How Skip Connections Work</span></span>
<span id="cb6-315"><a href="#cb6-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-316"><a href="#cb6-316" aria-hidden="true" tabindex="-1"></a>Let's walk through the skip connection mechanism step-by-step to understand how it preserves spatial information:</span>
<span id="cb6-317"><a href="#cb6-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-318"><a href="#cb6-318" aria-hidden="true" tabindex="-1"></a>**The Process:**</span>
<span id="cb6-319"><a href="#cb6-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-320"><a href="#cb6-320" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>As the encoder processes the input, it produces a feature map at a specific resolution, say 128×128×64 (128×128 spatial dimensions, 64 feature channels)</span>
<span id="cb6-321"><a href="#cb6-321" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>This feature map is **copied and temporarily saved** before any further processing</span>
<span id="cb6-322"><a href="#cb6-322" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The encoder continues its downsampling path, applying pooling to reduce spatial dimensions further</span>
<span id="cb6-323"><a href="#cb6-323" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>The process continues through the bottleneck, where the representation is most compressed</span>
<span id="cb6-324"><a href="#cb6-324" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>The decoder begins upsampling, bringing the spatial dimensions back up. It produces, for example, a 128×128×32 feature map</span>
<span id="cb6-325"><a href="#cb6-325" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Concatenation happens:** The decoder's upsampled features (128×128×32) are combined channel-wise with the saved encoder features (128×128×64)</span>
<span id="cb6-326"><a href="#cb6-326" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>The **result** is a 128×128×96 combined feature map containing:</span>
<span id="cb6-327"><a href="#cb6-327" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**High-level semantic context** from the decoder path (understanding of what objects are present)</span>
<span id="cb6-328"><a href="#cb6-328" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Fine spatial details** from the encoder path (precise localization of boundaries)</span>
<span id="cb6-329"><a href="#cb6-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-330"><a href="#cb6-330" aria-hidden="true" tabindex="-1"></a>**Real-World Impact in Earth Observation:**</span>
<span id="cb6-331"><a href="#cb6-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-332"><a href="#cb6-332" aria-hidden="true" tabindex="-1"></a>The difference is dramatic. In flood mapping applications, for instance:</span>
<span id="cb6-333"><a href="#cb6-333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Without skip connections:** Flood boundary accuracy might be ±10 pixels (100-200 meters at 10m resolution)</span>
<span id="cb6-334"><a href="#cb6-334" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**With skip connections:** Flood boundary accuracy improves to ±1-2 pixels (10-20 meters)</span>
<span id="cb6-335"><a href="#cb6-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-336"><a href="#cb6-336" aria-hidden="true" tabindex="-1"></a>This precision is critical for applications requiring legal boundaries, property lines, or hazard zone delineation, where even small errors can have significant consequences for decision-making and resource allocation.</span>
<span id="cb6-337"><a href="#cb6-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-338"><a href="#cb6-338" aria-hidden="true" tabindex="-1"></a><span class="fu">### U-Net Complete Architecture Summary</span></span>
<span id="cb6-339"><a href="#cb6-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-342"><a href="#cb6-342" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb6-343"><a href="#cb6-343" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">U-Net Information Flow</span><span class="ot">"</span></span>
<span id="cb6-344"><a href="#cb6-344" aria-hidden="true" tabindex="-1"></a>%%| fig-width: <span class="dv">100</span>%</span>
<span id="cb6-345"><a href="#cb6-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-346"><a href="#cb6-346" aria-hidden="true" tabindex="-1"></a>flowchart TD</span>
<span id="cb6-347"><a href="#cb6-347" aria-hidden="true" tabindex="-1"></a>    A[Input: <span class="dv">256</span>×<span class="dv">256</span>×<span class="dv">3</span>] --&gt; B1[Conv + ReLU]</span>
<span id="cb6-348"><a href="#cb6-348" aria-hidden="true" tabindex="-1"></a>    B1 --&gt; B2[Conv + ReLU]</span>
<span id="cb6-349"><a href="#cb6-349" aria-hidden="true" tabindex="-1"></a>    B2 --&gt; C1[MaxPool ↓]</span>
<span id="cb6-350"><a href="#cb6-350" aria-hidden="true" tabindex="-1"></a>    B2 -.-&gt;|Skip <span class="dv">1</span>| G1</span>
<span id="cb6-351"><a href="#cb6-351" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-352"><a href="#cb6-352" aria-hidden="true" tabindex="-1"></a>    C1 --&gt; D1[Conv + ReLU]</span>
<span id="cb6-353"><a href="#cb6-353" aria-hidden="true" tabindex="-1"></a>    D1 --&gt; D2[Conv + ReLU]</span>
<span id="cb6-354"><a href="#cb6-354" aria-hidden="true" tabindex="-1"></a>    D2 --&gt; E1[MaxPool ↓]</span>
<span id="cb6-355"><a href="#cb6-355" aria-hidden="true" tabindex="-1"></a>    D2 -.-&gt;|Skip <span class="dv">2</span>| F1</span>
<span id="cb6-356"><a href="#cb6-356" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-357"><a href="#cb6-357" aria-hidden="true" tabindex="-1"></a>    E1 --&gt; F0[Bottleneck&lt;br/&gt;Conv Layers]</span>
<span id="cb6-358"><a href="#cb6-358" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-359"><a href="#cb6-359" aria-hidden="true" tabindex="-1"></a>    F0 --&gt; F1[Upsample ↑]</span>
<span id="cb6-360"><a href="#cb6-360" aria-hidden="true" tabindex="-1"></a>    F1 --&gt; G1[Concatenate]</span>
<span id="cb6-361"><a href="#cb6-361" aria-hidden="true" tabindex="-1"></a>    G1 --&gt; H1[Conv + ReLU]</span>
<span id="cb6-362"><a href="#cb6-362" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-363"><a href="#cb6-363" aria-hidden="true" tabindex="-1"></a>    H1 --&gt; I1[Upsample ↑]</span>
<span id="cb6-364"><a href="#cb6-364" aria-hidden="true" tabindex="-1"></a>    I1 --&gt; J1[Concatenate]</span>
<span id="cb6-365"><a href="#cb6-365" aria-hidden="true" tabindex="-1"></a>    J1 --&gt; K1[Conv + ReLU]</span>
<span id="cb6-366"><a href="#cb6-366" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-367"><a href="#cb6-367" aria-hidden="true" tabindex="-1"></a>    K1 --&gt; L[Output: <span class="dv">256</span>×<span class="dv">256</span>×Classes]</span>
<span id="cb6-368"><a href="#cb6-368" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-369"><a href="#cb6-369" aria-hidden="true" tabindex="-1"></a>    style A fill:<span class="co">#0066cc,stroke:#003d7a,stroke-width:2px,color:#fff</span></span>
<span id="cb6-370"><a href="#cb6-370" aria-hidden="true" tabindex="-1"></a>    style F0 fill:<span class="co">#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff</span></span>
<span id="cb6-371"><a href="#cb6-371" aria-hidden="true" tabindex="-1"></a>    style L fill:<span class="co">#00aa44,stroke:#006622,stroke-width:2px,color:#fff</span></span>
<span id="cb6-372"><a href="#cb6-372" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-373"><a href="#cb6-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-374"><a href="#cb6-374" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-375"><a href="#cb6-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-376"><a href="#cb6-376" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 3: Applications in Earth Observation</span></span>
<span id="cb6-377"><a href="#cb6-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-378"><a href="#cb6-378" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why U-Net is Popular in EO</span></span>
<span id="cb6-379"><a href="#cb6-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-380"><a href="#cb6-380" aria-hidden="true" tabindex="-1"></a>U-Net has become a go-to architecture for many Earth Observation segmentation tasks due to its accuracy and efficiency in learning from limited data. Several key factors contribute to its widespread adoption:</span>
<span id="cb6-381"><a href="#cb6-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-382"><a href="#cb6-382" aria-hidden="true" tabindex="-1"></a>**Data Efficiency:**</span>
<span id="cb6-383"><a href="#cb6-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-384"><a href="#cb6-384" aria-hidden="true" tabindex="-1"></a>U-Net performs well with relatively modest amounts of training data—typically hundreds to thousands of training samples rather than the millions required by some other deep learning approaches. Data augmentation techniques (rotations, flips, which are particularly relevant for satellite nadir views) help further. This is **critical** when labeled EO data is expensive and time-consuming to acquire, requiring expert annotators and field validation.</span>
<span id="cb6-385"><a href="#cb6-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-386"><a href="#cb6-386" aria-hidden="true" tabindex="-1"></a>**Spatial Precision:**</span>
<span id="cb6-387"><a href="#cb6-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-388"><a href="#cb6-388" aria-hidden="true" tabindex="-1"></a>The skip connections preserve fine boundaries with remarkable accuracy, which is important for applications requiring legal boundaries, property lines, or precise hazard zone delineation. This enables accuracy from millimeter to meter level, depending on the input imagery resolution—essential for cadastral mapping, flood extent determination, and infrastructure monitoring.</span>
<span id="cb6-389"><a href="#cb6-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-390"><a href="#cb6-390" aria-hidden="true" tabindex="-1"></a>**Multi-Scale Learning:**</span>
<span id="cb6-391"><a href="#cb6-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-392"><a href="#cb6-392" aria-hidden="true" tabindex="-1"></a>The encoder's hierarchical structure captures both local textures (in early layers) and global context (in deeper layers). This is essential for the varied scales of EO features, from small boats (a few pixels) to large water bodies (thousands of pixels). U-Net handles objects at multiple scales simultaneously within a single architecture.</span>
<span id="cb6-393"><a href="#cb6-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-394"><a href="#cb6-394" aria-hidden="true" tabindex="-1"></a>**Transfer Learning Capability:**</span>
<span id="cb6-395"><a href="#cb6-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-396"><a href="#cb6-396" aria-hidden="true" tabindex="-1"></a>U-Net encoders can leverage pre-trained weights from ImageNet or other large-scale datasets, enabling domain adaptation from natural images to satellite imagery. This significantly improves performance when labeled EO data is limited, allowing the model to start with general feature extraction capabilities and fine-tune to specific remote sensing tasks.</span>
<span id="cb6-397"><a href="#cb6-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-398"><a href="#cb6-398" aria-hidden="true" tabindex="-1"></a><span class="fu">### Application 1: Flood Mapping</span></span>
<span id="cb6-399"><a href="#cb6-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-400"><a href="#cb6-400" aria-hidden="true" tabindex="-1"></a>**Use Case:** Disaster response and damage assessment</span>
<span id="cb6-401"><a href="#cb6-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-402"><a href="#cb6-402" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-403"><a href="#cb6-403" aria-hidden="true" tabindex="-1"></a><span class="fu">## Philippine Context: Typhoon Flood Mapping</span></span>
<span id="cb6-404"><a href="#cb6-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-405"><a href="#cb6-405" aria-hidden="true" tabindex="-1"></a>**Data Sources:**</span>
<span id="cb6-406"><a href="#cb6-406" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sentinel-1 SAR** (cloud-penetrating, all-weather capability)</span>
<span id="cb6-407"><a href="#cb6-407" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sentinel-2 optical** (high resolution when clouds permit)</span>
<span id="cb6-408"><a href="#cb6-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-409"><a href="#cb6-409" aria-hidden="true" tabindex="-1"></a>**Task:**</span>
<span id="cb6-410"><a href="#cb6-410" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Binary segmentation: Flooded vs Non-flooded pixels</span>
<span id="cb6-411"><a href="#cb6-411" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: SAR backscatter (VV, VH polarizations) or optical RGB+NIR</span>
<span id="cb6-412"><a href="#cb6-412" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: Precise flood extent mask</span>
<span id="cb6-413"><a href="#cb6-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-414"><a href="#cb6-414" aria-hidden="true" tabindex="-1"></a>**Why U-Net Excels:**</span>
<span id="cb6-415"><a href="#cb6-415" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>U-Net has been used to segment flooded areas in Sentinel-1 SAR and Sentinel-2 optical images with **high accuracy**</span>
<span id="cb6-416"><a href="#cb6-416" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Studies have shown U-Net is effective in capturing flood patterns in SAR imagery, achieving high accuracy in delineating water from land</span>
<span id="cb6-417"><a href="#cb6-417" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>During floods, produces a **binary map of floodwater vs. non-flood** for each pixel, identifying flooded pixels vs. dry pixels across an entire region</span>
<span id="cb6-418"><a href="#cb6-418" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables **rapid assessment** of flood extent for emergency response</span>
<span id="cb6-419"><a href="#cb6-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Research shows U-Net achieves robust results even with relatively **small training datasets**</span>
<span id="cb6-420"><a href="#cb6-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-421"><a href="#cb6-421" aria-hidden="true" tabindex="-1"></a>**Benefits:**</span>
<span id="cb6-422"><a href="#cb6-422" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Rapid mapping within hours of satellite acquisition</span>
<span id="cb6-423"><a href="#cb6-423" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Precise area calculations for damage assessment  </span>
<span id="cb6-424"><a href="#cb6-424" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Time-series monitoring of flood evolution and recession</span>
<span id="cb6-425"><a href="#cb6-425" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Integration with GIS for evacuation planning and relief distribution</span>
<span id="cb6-426"><a href="#cb6-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-427"><a href="#cb6-427" aria-hidden="true" tabindex="-1"></a>**Real Example:** Typhoon Ulysses (2020) - Central Luzon floods mapped using U-Net on Sentinel-1 data, providing precise inundation extent for affected municipalities in the Pampanga River Basin.</span>
<span id="cb6-428"><a href="#cb6-428" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-429"><a href="#cb6-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-430"><a href="#cb6-430" aria-hidden="true" tabindex="-1"></a><span class="fu">### Application 2: Land Cover Mapping</span></span>
<span id="cb6-431"><a href="#cb6-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-432"><a href="#cb6-432" aria-hidden="true" tabindex="-1"></a>**Use Case:** Environmental monitoring, urban planning, biodiversity assessment</span>
<span id="cb6-433"><a href="#cb6-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-434"><a href="#cb6-434" aria-hidden="true" tabindex="-1"></a>**Data Sources:**</span>
<span id="cb6-435"><a href="#cb6-435" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sentinel-2 multispectral (10m resolution)</span>
<span id="cb6-436"><a href="#cb6-436" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Landsat 8/9 (30m, long time series)</span>
<span id="cb6-437"><a href="#cb6-437" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High-resolution commercial imagery</span>
<span id="cb6-438"><a href="#cb6-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-439"><a href="#cb6-439" aria-hidden="true" tabindex="-1"></a>**Task:**</span>
<span id="cb6-440"><a href="#cb6-440" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-class segmentation: Water, Forest, Urban, Agriculture, Barren, Mangrove</span>
<span id="cb6-441"><a href="#cb6-441" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: Multi-spectral bands (RGB, NIR, SWIR, Red Edge)</span>
<span id="cb6-442"><a href="#cb6-442" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: Detailed land cover classification map</span>
<span id="cb6-443"><a href="#cb6-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-444"><a href="#cb6-444" aria-hidden="true" tabindex="-1"></a>**U-Net's Strength:**</span>
<span id="cb6-445"><a href="#cb6-445" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combines **broad context** (distinguishing urban area from forest in general) with **precise boundaries** (exactly where forest stops and urban begins)</span>
<span id="cb6-446"><a href="#cb6-446" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>U-Net's ability to preserve fine details helps **delineate boundaries** between different land cover types</span>
<span id="cb6-447"><a href="#cb6-447" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can outline exact shapes of urban districts, small water bodies, or forest edges</span>
<span id="cb6-448"><a href="#cb6-448" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Research shows U-Net often **outperforms older pixel-based or patch-based methods** in remote sensing</span>
<span id="cb6-449"><a href="#cb6-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-450"><a href="#cb6-450" aria-hidden="true" tabindex="-1"></a>**Benefits:**</span>
<span id="cb6-451"><a href="#cb6-451" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pixel-accurate thematic maps</span>
<span id="cb6-452"><a href="#cb6-452" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Change detection over time (deforestation, urbanization)</span>
<span id="cb6-453"><a href="#cb6-453" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Biodiversity habitat assessments  </span>
<span id="cb6-454"><a href="#cb6-454" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Carbon stock estimation for climate reporting</span>
<span id="cb6-455"><a href="#cb6-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-456"><a href="#cb6-456" aria-hidden="true" tabindex="-1"></a><span class="fu">### Application 3: Road Network Extraction</span></span>
<span id="cb6-457"><a href="#cb6-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-458"><a href="#cb6-458" aria-hidden="true" tabindex="-1"></a>**Use Case:** Map updating, transportation planning, accessibility analysis</span>
<span id="cb6-459"><a href="#cb6-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-460"><a href="#cb6-460" aria-hidden="true" tabindex="-1"></a>**Challenges:**</span>
<span id="cb6-461"><a href="#cb6-461" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Thin linear features difficult to detect</span>
<span id="cb6-462"><a href="#cb6-462" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Occlusion by trees and shadows</span>
<span id="cb6-463"><a href="#cb6-463" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Complex urban backgrounds</span>
<span id="cb6-464"><a href="#cb6-464" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Need to maintain continuous structure</span>
<span id="cb6-465"><a href="#cb6-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-466"><a href="#cb6-466" aria-hidden="true" tabindex="-1"></a>**U-Net Advantages:**</span>
<span id="cb6-467"><a href="#cb6-467" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Skip connections preserve **road continuity** (prevents gaps)</span>
<span id="cb6-468"><a href="#cb6-468" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learns to follow **linear patterns** across the image</span>
<span id="cb6-469"><a href="#cb6-469" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Handles varying road widths (from highways to small paths)</span>
<span id="cb6-470"><a href="#cb6-470" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can trace continuous structures like roads and railways</span>
<span id="cb6-471"><a href="#cb6-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-472"><a href="#cb6-472" aria-hidden="true" tabindex="-1"></a>**Task:**</span>
<span id="cb6-473"><a href="#cb6-473" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Binary segmentation: Road vs Background</span>
<span id="cb6-474"><a href="#cb6-474" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: High-resolution aerial/satellite RGB or SAR</span>
<span id="cb6-475"><a href="#cb6-475" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: Road network mask for vectorization</span>
<span id="cb6-476"><a href="#cb6-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-477"><a href="#cb6-477" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb6-478"><a href="#cb6-478" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Automated map updating for rural areas</span>
<span id="cb6-479"><a href="#cb6-479" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transportation network planning</span>
<span id="cb6-480"><a href="#cb6-480" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Accessibility analysis for disaster response</span>
<span id="cb6-481"><a href="#cb6-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-482"><a href="#cb6-482" aria-hidden="true" tabindex="-1"></a><span class="fu">### Application 4: Building Footprint Delineation</span></span>
<span id="cb6-483"><a href="#cb6-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-484"><a href="#cb6-484" aria-hidden="true" tabindex="-1"></a>**Use Case:** Urban mapping, population estimation, disaster risk assessment</span>
<span id="cb6-485"><a href="#cb6-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-486"><a href="#cb6-486" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb6-487"><a href="#cb6-487" aria-hidden="true" tabindex="-1"></a><span class="fu">## Philippine Application: Informal Settlement Detection</span></span>
<span id="cb6-488"><a href="#cb6-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-489"><a href="#cb6-489" aria-hidden="true" tabindex="-1"></a>**Relevance:**</span>
<span id="cb6-490"><a href="#cb6-490" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Monitor unplanned urban growth in Metro Manila</span>
<span id="cb6-491"><a href="#cb6-491" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Identify disaster-vulnerable communities  </span>
<span id="cb6-492"><a href="#cb6-492" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Support urban planning and housing programs</span>
<span id="cb6-493"><a href="#cb6-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-494"><a href="#cb6-494" aria-hidden="true" tabindex="-1"></a>**Task:**</span>
<span id="cb6-495"><a href="#cb6-495" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Binary or multi-class: Building vs Background (or building types)</span>
<span id="cb6-496"><a href="#cb6-496" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: Very high-resolution imagery (&lt;1m) or Sentinel-2 for large structures</span>
<span id="cb6-497"><a href="#cb6-497" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: Building footprint polygons</span>
<span id="cb6-498"><a href="#cb6-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-499"><a href="#cb6-499" aria-hidden="true" tabindex="-1"></a>**U-Net Performance:**</span>
<span id="cb6-500"><a href="#cb6-500" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>With appropriate high-resolution data, U-Net can **outline individual buildings** or dense informal settlements, even with complex backgrounds</span>
<span id="cb6-501"><a href="#cb6-501" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>U-Net-based models have been used to extract buildings in urban areas and to map roads winding through forests or cities</span>
<span id="cb6-502"><a href="#cb6-502" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Variants like **Residual U-Net** or **Attention U-Net** also popular for building segmentation</span>
<span id="cb6-503"><a href="#cb6-503" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Core idea remains: encoder-decoder with skip connections for precise boundaries</span>
<span id="cb6-504"><a href="#cb6-504" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Instead of just saying "there are buildings in this image," we get a map of where each building is</span>
<span id="cb6-505"><a href="#cb6-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-506"><a href="#cb6-506" aria-hidden="true" tabindex="-1"></a>**Benefits:**</span>
<span id="cb6-507"><a href="#cb6-507" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Automated mapping at scale</span>
<span id="cb6-508"><a href="#cb6-508" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pre/post disaster damage assessment</span>
<span id="cb6-509"><a href="#cb6-509" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>3D city model generation (with height data)</span>
<span id="cb6-510"><a href="#cb6-510" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Infrastructure planning</span>
<span id="cb6-511"><a href="#cb6-511" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Aids urban planning and risk assessment</span>
<span id="cb6-512"><a href="#cb6-512" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-513"><a href="#cb6-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-514"><a href="#cb6-514" aria-hidden="true" tabindex="-1"></a><span class="fu">### Application 5: Vegetation and Crop Monitoring</span></span>
<span id="cb6-515"><a href="#cb6-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-516"><a href="#cb6-516" aria-hidden="true" tabindex="-1"></a>**Use Case:** Precision agriculture, forestry, ecosystem health</span>
<span id="cb6-517"><a href="#cb6-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-518"><a href="#cb6-518" aria-hidden="true" tabindex="-1"></a>**Data Sources:**</span>
<span id="cb6-519"><a href="#cb6-519" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sentinel-2 multispectral (5-day revisit)</span>
<span id="cb6-520"><a href="#cb6-520" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PlanetScope (3m daily coverage)</span>
<span id="cb6-521"><a href="#cb6-521" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>UAV imagery for field-scale monitoring</span>
<span id="cb6-522"><a href="#cb6-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-523"><a href="#cb6-523" aria-hidden="true" tabindex="-1"></a>**Task:**</span>
<span id="cb6-524"><a href="#cb6-524" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Multi-class segmentation: Crop types (rice, corn, sugarcane, coconut)</span>
<span id="cb6-525"><a href="#cb6-525" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Or binary: Vegetation vs Non-vegetation</span>
<span id="cb6-526"><a href="#cb6-526" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: Multi-temporal + multi-spectral imagery</span>
<span id="cb6-527"><a href="#cb6-527" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: Crop type map or vegetation mask</span>
<span id="cb6-528"><a href="#cb6-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-529"><a href="#cb6-529" aria-hidden="true" tabindex="-1"></a>**U-Net Applications:**</span>
<span id="cb6-530"><a href="#cb6-530" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Identifying **crop fields** and **forest cover** at pixel level</span>
<span id="cb6-531"><a href="#cb6-531" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Monitoring agricultural areas for food security</span>
<span id="cb6-532"><a href="#cb6-532" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Tracking **tree cover** for forestry management</span>
<span id="cb6-533"><a href="#cb6-533" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Detecting vegetation changes and health patterns</span>
<span id="cb6-534"><a href="#cb6-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-535"><a href="#cb6-535" aria-hidden="true" tabindex="-1"></a>**Benefits:**</span>
<span id="cb6-536"><a href="#cb6-536" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Yield prediction and harvest planning</span>
<span id="cb6-537"><a href="#cb6-537" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Irrigation requirement monitoring</span>
<span id="cb6-538"><a href="#cb6-538" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Early disease detection</span>
<span id="cb6-539"><a href="#cb6-539" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Deforestation and illegal logging tracking</span>
<span id="cb6-540"><a href="#cb6-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-541"><a href="#cb6-541" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-542"><a href="#cb6-542" aria-hidden="true" tabindex="-1"></a><span class="fu">## Research Evidence</span></span>
<span id="cb6-543"><a href="#cb6-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-544"><a href="#cb6-544" aria-hidden="true" tabindex="-1"></a>Across these examples, research and practice have shown **U-Net achieves high segmentation accuracy** in remote sensing. It has been demonstrated that U-Net can achieve **robust results with relatively small training datasets**, thanks to the efficiency of the architecture—one reason it was originally successful in medical imaging with limited training images.</span>
<span id="cb6-545"><a href="#cb6-545" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-546"><a href="#cb6-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-547"><a href="#cb6-547" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-548"><a href="#cb6-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-549"><a href="#cb6-549" aria-hidden="true" tabindex="-1"></a><span class="fu">## Part 4: Loss Functions for Segmentation</span></span>
<span id="cb6-550"><a href="#cb6-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-551"><a href="#cb6-551" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Loss Functions Matter</span></span>
<span id="cb6-552"><a href="#cb6-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-553"><a href="#cb6-553" aria-hidden="true" tabindex="-1"></a>Training a segmentation model requires choosing an appropriate **loss function** that compares the predicted mask to the ground truth mask. The loss function is the mathematical measure that tells the model how wrong its predictions are, guiding the weight updates during training.</span>
<span id="cb6-554"><a href="#cb6-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-555"><a href="#cb6-555" aria-hidden="true" tabindex="-1"></a>**The Challenge in Segmentation:**</span>
<span id="cb6-556"><a href="#cb6-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-557"><a href="#cb6-557" aria-hidden="true" tabindex="-1"></a>Unlike image classification where we compare a single predicted label to a single true label, segmentation requires comparing entire images pixel-by-pixel. We're not evaluating just one value—we must compare potentially millions of pixel predictions across the entire image.</span>
<span id="cb6-558"><a href="#cb6-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-559"><a href="#cb6-559" aria-hidden="true" tabindex="-1"></a>Different loss functions emphasize different aspects of the prediction:</span>
<span id="cb6-560"><a href="#cb6-560" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Some focus on **pixel-wise accuracy** (is each individual pixel correct?)</span>
<span id="cb6-561"><a href="#cb6-561" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Others focus on **region overlap** (does the predicted flood extent match the true extent?)</span>
<span id="cb6-562"><a href="#cb6-562" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Some emphasize **boundary accuracy** (are the edges of objects precisely delineated?)</span>
<span id="cb6-563"><a href="#cb6-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-564"><a href="#cb6-564" aria-hidden="true" tabindex="-1"></a>**Why Choice Matters:**</span>
<span id="cb6-565"><a href="#cb6-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-566"><a href="#cb6-566" aria-hidden="true" tabindex="-1"></a>The choice of loss function **critically affects model behavior**. Several loss functions are common in segmentation, each with different strengths. In the following sections, we'll explore the main options and understand when to use each one for Earth Observation applications.</span>
<span id="cb6-567"><a href="#cb6-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-568"><a href="#cb6-568" aria-hidden="true" tabindex="-1"></a><span class="fu">### Challenge: Class Imbalance in EO</span></span>
<span id="cb6-569"><a href="#cb6-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-570"><a href="#cb6-570" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb6-571"><a href="#cb6-571" aria-hidden="true" tabindex="-1"></a><span class="fu">## Common Imbalanced Scenarios</span></span>
<span id="cb6-572"><a href="#cb6-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-573"><a href="#cb6-573" aria-hidden="true" tabindex="-1"></a>**In segmentation of EO data, class imbalance is a typical issue:**</span>
<span id="cb6-574"><a href="#cb6-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-575"><a href="#cb6-575" aria-hidden="true" tabindex="-1"></a>**Flood Mapping:**</span>
<span id="cb6-576"><a href="#cb6-576" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>95% non-flooded pixels, 5% flooded pixels</span>
<span id="cb6-577"><a href="#cb6-577" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Think of mapping floods: the flooded pixels are usually **far fewer** than non-flooded</span>
<span id="cb6-578"><a href="#cb6-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-579"><a href="#cb6-579" aria-hidden="true" tabindex="-1"></a>**Ship Detection:**</span>
<span id="cb6-580"><a href="#cb6-580" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>99.5% water/land, 0.5% ships</span>
<span id="cb6-581"><a href="#cb6-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-582"><a href="#cb6-582" aria-hidden="true" tabindex="-1"></a>**Building Segmentation:**</span>
<span id="cb6-583"><a href="#cb6-583" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>80% background, 20% buildings</span>
<span id="cb6-584"><a href="#cb6-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-585"><a href="#cb6-585" aria-hidden="true" tabindex="-1"></a>**Problem with Simple Accuracy:**</span>
<span id="cb6-586"><a href="#cb6-586" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb6-587"><a href="#cb6-587" aria-hidden="true" tabindex="-1"></a><span class="co"># Model predicts: ALL pixels = "non-flooded"</span></span>
<span id="cb6-588"><a href="#cb6-588" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy: 95% ✓ (looks great!)</span></span>
<span id="cb6-589"><a href="#cb6-589" aria-hidden="true" tabindex="-1"></a><span class="co"># But: Completely useless - missed all floods!</span></span>
<span id="cb6-590"><a href="#cb6-590" aria-hidden="true" tabindex="-1"></a><span class="co"># Result: Trivial but useless prediction</span></span>
<span id="cb6-591"><a href="#cb6-591" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-592"><a href="#cb6-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-593"><a href="#cb6-593" aria-hidden="true" tabindex="-1"></a>**Why This Happens:**</span>
<span id="cb6-594"><a href="#cb6-594" aria-hidden="true" tabindex="-1"></a>In imbalanced cases, vanilla cross-entropy can be **dominated by the majority class**. A poor choice of loss might lead the model to predict all pixels as the majority class—achieving high accuracy but providing no useful information.</span>
<span id="cb6-595"><a href="#cb6-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-596"><a href="#cb6-596" aria-hidden="true" tabindex="-1"></a>**Need Loss Functions That:**</span>
<span id="cb6-597"><a href="#cb6-597" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Handle severe class imbalance</span>
<span id="cb6-598"><a href="#cb6-598" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Focus on minority (critical) class</span>
<span id="cb6-599"><a href="#cb6-599" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reward region overlap, not just pixel-wise correctness</span>
<span id="cb6-600"><a href="#cb6-600" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ensure boundaries (edges of floods, building outlines) are accurately captured</span>
<span id="cb6-601"><a href="#cb6-601" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-602"><a href="#cb6-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-603"><a href="#cb6-603" aria-hidden="true" tabindex="-1"></a><span class="fu">### Loss Function 1: Pixel-wise Cross-Entropy</span></span>
<span id="cb6-604"><a href="#cb6-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-605"><a href="#cb6-605" aria-hidden="true" tabindex="-1"></a>**How it Works:**</span>
<span id="cb6-606"><a href="#cb6-606" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Treat each pixel as an **independent classification problem**</span>
<span id="cb6-607"><a href="#cb6-607" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Compare predicted class probability to true class using **negative log-likelihood**</span>
<span id="cb6-608"><a href="#cb6-608" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Average loss across all pixels</span>
<span id="cb6-609"><a href="#cb6-609" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Standard approach for multi-class segmentation (or binary cross-entropy for two classes)</span>
<span id="cb6-610"><a href="#cb6-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-611"><a href="#cb6-611" aria-hidden="true" tabindex="-1"></a>**Formula (simplified):**</span>
<span id="cb6-612"><a href="#cb6-612" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-613"><a href="#cb6-613" aria-hidden="true" tabindex="-1"></a>\text{Cross-Entropy} = -\sum_{i=1}^{N} y_{\text{true},i} \cdot \log(y_{\text{pred},i})</span>
<span id="cb6-614"><a href="#cb6-614" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-615"><a href="#cb6-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-616"><a href="#cb6-616" aria-hidden="true" tabindex="-1"></a>Where $N$ is the total number of pixels.</span>
<span id="cb6-617"><a href="#cb6-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-618"><a href="#cb6-618" aria-hidden="true" tabindex="-1"></a>**Advantages:**</span>
<span id="cb6-619"><a href="#cb6-619" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Standard, well-understood approach</span>
<span id="cb6-620"><a href="#cb6-620" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Strong, stable gradients for learning</span>
<span id="cb6-621"><a href="#cb6-621" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Works naturally with multi-class problems (softmax output)</span>
<span id="cb6-622"><a href="#cb6-622" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Effective and straightforward</span>
<span id="cb6-623"><a href="#cb6-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-624"><a href="#cb6-624" aria-hidden="true" tabindex="-1"></a>**Disadvantages:**</span>
<span id="cb6-625"><a href="#cb6-625" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✗ Dominated by majority class in imbalanced data</span>
<span id="cb6-626"><a href="#cb6-626" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✗ Doesn't directly optimize for spatial overlap or boundary alignment</span>
<span id="cb6-627"><a href="#cb6-627" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✗ Can effectively ignore minority classes</span>
<span id="cb6-628"><a href="#cb6-628" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✗ Focuses on pixel-level accuracy but not region-level correctness</span>
<span id="cb6-629"><a href="#cb6-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-630"><a href="#cb6-630" aria-hidden="true" tabindex="-1"></a>**When to Use:** Balanced datasets (~50/50 class distribution) or with class weighting</span>
<span id="cb6-631"><a href="#cb6-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-632"><a href="#cb6-632" aria-hidden="true" tabindex="-1"></a><span class="fu">### Weighted Cross-Entropy</span></span>
<span id="cb6-633"><a href="#cb6-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-634"><a href="#cb6-634" aria-hidden="true" tabindex="-1"></a>**Solution to Imbalance:** Assign higher weight to under-represented classes so the model pays more attention to them</span>
<span id="cb6-635"><a href="#cb6-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-636"><a href="#cb6-636" aria-hidden="true" tabindex="-1"></a>**Formula:**</span>
<span id="cb6-637"><a href="#cb6-637" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-638"><a href="#cb6-638" aria-hidden="true" tabindex="-1"></a>\text{Weighted CE} = -\sum_{i=1}^{N} w_{\text{class}} \cdot y_{\text{true},i} \cdot \log(y_{\text{pred},i})</span>
<span id="cb6-639"><a href="#cb6-639" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-640"><a href="#cb6-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-641"><a href="#cb6-641" aria-hidden="true" tabindex="-1"></a>**Example:**</span>
<span id="cb6-642"><a href="#cb6-642" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>95% background pixels → weight = 1.0</span>
<span id="cb6-643"><a href="#cb6-643" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>5% flood pixels → weight = 19.0 (inverse frequency: 95/5)</span>
<span id="cb6-644"><a href="#cb6-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-645"><a href="#cb6-645" aria-hidden="true" tabindex="-1"></a>**Effect:**</span>
<span id="cb6-646"><a href="#cb6-646" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model pays 19× more attention to flood pixels</span>
<span id="cb6-647"><a href="#cb6-647" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Heavily penalized for missing flood pixels</span>
<span id="cb6-648"><a href="#cb6-648" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Common remedy** for class imbalance in segmentation</span>
<span id="cb6-649"><a href="#cb6-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-650"><a href="#cb6-650" aria-hidden="true" tabindex="-1"></a>**Implementation (TensorFlow/Keras):**</span>
<span id="cb6-651"><a href="#cb6-651" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb6-652"><a href="#cb6-652" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> tf.keras.losses.CategoricalCrossentropy(</span>
<span id="cb6-653"><a href="#cb6-653" aria-hidden="true" tabindex="-1"></a>    class_weight<span class="op">=</span>{<span class="dv">0</span>: <span class="fl">1.0</span>, <span class="dv">1</span>: <span class="fl">19.0</span>}  <span class="co"># background, flood</span></span>
<span id="cb6-654"><a href="#cb6-654" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-655"><a href="#cb6-655" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-656"><a href="#cb6-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-657"><a href="#cb6-657" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-658"><a href="#cb6-658" aria-hidden="true" tabindex="-1"></a><span class="fu">## When Weighted CE Helps</span></span>
<span id="cb6-659"><a href="#cb6-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-660"><a href="#cb6-660" aria-hidden="true" tabindex="-1"></a>Weighted cross-entropy provides strong gradients for learning while addressing imbalance. However, it still focuses on **pixel-wise accuracy** and doesn't directly ensure good overlap or boundary alignment—that's where overlap-based losses come in.</span>
<span id="cb6-661"><a href="#cb6-661" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-662"><a href="#cb6-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-663"><a href="#cb6-663" aria-hidden="true" tabindex="-1"></a><span class="fu">### Loss Function 2: Dice Loss</span></span>
<span id="cb6-664"><a href="#cb6-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-665"><a href="#cb6-665" aria-hidden="true" tabindex="-1"></a>**Concept:** Measure overlap between prediction and ground truth regions  </span>
<span id="cb6-666"><a href="#cb6-666" aria-hidden="true" tabindex="-1"></a>**Training Goal:** Maximize Dice (or minimize 1 - Dice) to encourage the network to get the segmentation overlap as high as possible</span>
<span id="cb6-667"><a href="#cb6-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-668"><a href="#cb6-668" aria-hidden="true" tabindex="-1"></a>**Formula:**</span>
<span id="cb6-669"><a href="#cb6-669" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-670"><a href="#cb6-670" aria-hidden="true" tabindex="-1"></a>\text{Dice Coefficient} = \frac{2 \times |P \cap T|}{|P| + |T|}</span>
<span id="cb6-671"><a href="#cb6-671" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-672"><a href="#cb6-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-673"><a href="#cb6-673" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-674"><a href="#cb6-674" aria-hidden="true" tabindex="-1"></a>\text{Dice Loss} = 1 - \text{Dice Coefficient}</span>
<span id="cb6-675"><a href="#cb6-675" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-676"><a href="#cb6-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-677"><a href="#cb6-677" aria-hidden="true" tabindex="-1"></a>Where:</span>
<span id="cb6-678"><a href="#cb6-678" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$P$ = predicted foreground pixels</span>
<span id="cb6-679"><a href="#cb6-679" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$T$ = true foreground pixels</span>
<span id="cb6-680"><a href="#cb6-680" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\cap$ = intersection (overlap)</span>
<span id="cb6-681"><a href="#cb6-681" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Essentially: $\frac{2 \times (\text{intersection})}{(\text{sum of areas})}$</span>
<span id="cb6-682"><a href="#cb6-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-683"><a href="#cb6-683" aria-hidden="true" tabindex="-1"></a>**Interpretation:**</span>
<span id="cb6-684"><a href="#cb6-684" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dice = 1.0: Perfect overlap</span>
<span id="cb6-685"><a href="#cb6-685" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dice = 0.5: 50% overlap</span>
<span id="cb6-686"><a href="#cb6-686" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dice = 0.0: No overlap</span>
<span id="cb6-687"><a href="#cb6-687" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Loss = 0.0: Perfect (lower is better)</span>
<span id="cb6-688"><a href="#cb6-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-689"><a href="#cb6-689" aria-hidden="true" tabindex="-1"></a>**Why for Imbalanced Data?**</span>
<span id="cb6-690"><a href="#cb6-690" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Focuses on the **relative overlap** of the object (minority class)</span>
<span id="cb6-691"><a href="#cb6-691" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Treats foreground and background asymmetrically</span>
<span id="cb6-692"><a href="#cb6-692" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Correctly segmenting one small flooded patch** contributes as much to Dice as a large region of non-flood</span>
<span id="cb6-693"><a href="#cb6-693" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Inherently handles class imbalance **without manual weighting**</span>
<span id="cb6-694"><a href="#cb6-694" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Particularly well-suited when target objects occupy a **small fraction** of the image</span>
<span id="cb6-695"><a href="#cb6-695" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Doesn't let the model get complacent by only predicting the majority class, because it focuses on the overlap of the positive (target) class</span>
<span id="cb6-696"><a href="#cb6-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-697"><a href="#cb6-697" aria-hidden="true" tabindex="-1"></a>**When False Positives and Negatives Need Equal Weight:**</span>
<span id="cb6-698"><a href="#cb6-698" aria-hidden="true" tabindex="-1"></a>Dice loss is known to be effective when **false negatives** (missing floods) and **false positives** (predicting flood where there isn't) need to be weighted equally. It treats false negatives and false positives more equally than cross-entropy.</span>
<span id="cb6-699"><a href="#cb6-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-700"><a href="#cb6-700" aria-hidden="true" tabindex="-1"></a>**Advantages:**</span>
<span id="cb6-701"><a href="#cb6-701" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Inherently robust to class imbalance</span>
<span id="cb6-702"><a href="#cb6-702" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Directly optimizes overlap metric (F1-score for segmentation)</span>
<span id="cb6-703"><a href="#cb6-703" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Excellent for small objects</span>
<span id="cb6-704"><a href="#cb6-704" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ No need to manually set class weights</span>
<span id="cb6-705"><a href="#cb6-705" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Helps model not ignore small structures or minority classes</span>
<span id="cb6-706"><a href="#cb6-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-707"><a href="#cb6-707" aria-hidden="true" tabindex="-1"></a>**Disadvantages:**</span>
<span id="cb6-708"><a href="#cb6-708" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✗ Less stable gradients (can be noisy early in training)</span>
<span id="cb6-709"><a href="#cb6-709" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✗ May converge slower than cross-entropy</span>
<span id="cb6-710"><a href="#cb6-710" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✗ Requires careful implementation (avoid division by zero)</span>
<span id="cb6-711"><a href="#cb6-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-712"><a href="#cb6-712" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb6-713"><a href="#cb6-713" aria-hidden="true" tabindex="-1"></a><span class="fu">## Medical Imaging Parallel</span></span>
<span id="cb6-714"><a href="#cb6-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-715"><a href="#cb6-715" aria-hidden="true" tabindex="-1"></a>Many medical image segmentation models use Dice loss to segment **tumors that occupy only a tiny area** of the image—exactly the same challenge as segmenting small flood patches in vast satellite scenes.</span>
<span id="cb6-716"><a href="#cb6-716" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-717"><a href="#cb6-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-718"><a href="#cb6-718" aria-hidden="true" tabindex="-1"></a><span class="fu">### Loss Function 3: IoU Loss (Jaccard Index)</span></span>
<span id="cb6-719"><a href="#cb6-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-720"><a href="#cb6-720" aria-hidden="true" tabindex="-1"></a>**Concept:** Similar to Dice, directly optimizes the IoU metric  </span>
<span id="cb6-721"><a href="#cb6-721" aria-hidden="true" tabindex="-1"></a>**Also Known As:** Jaccard Index</span>
<span id="cb6-722"><a href="#cb6-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-723"><a href="#cb6-723" aria-hidden="true" tabindex="-1"></a>**Formula:**</span>
<span id="cb6-724"><a href="#cb6-724" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-725"><a href="#cb6-725" aria-hidden="true" tabindex="-1"></a>\text{IoU} = \frac{|P \cap T|}{|P \cup T|}</span>
<span id="cb6-726"><a href="#cb6-726" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-727"><a href="#cb6-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-728"><a href="#cb6-728" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-729"><a href="#cb6-729" aria-hidden="true" tabindex="-1"></a>\text{IoU Loss} = 1 - \text{IoU}</span>
<span id="cb6-730"><a href="#cb6-730" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-731"><a href="#cb6-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-732"><a href="#cb6-732" aria-hidden="true" tabindex="-1"></a>Where $\cup$ represents the union of predicted and true pixels.</span>
<span id="cb6-733"><a href="#cb6-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-734"><a href="#cb6-734" aria-hidden="true" tabindex="-1"></a>**Difference from Dice:**</span>
<span id="cb6-735"><a href="#cb6-735" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**IoU:** Intersection / Union</span>
<span id="cb6-736"><a href="#cb6-736" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dice:** 2 × Intersection / (Sum of areas)</span>
<span id="cb6-737"><a href="#cb6-737" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Numerically different but conceptually similar</span>
<span id="cb6-738"><a href="#cb6-738" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Related by: $\text{Dice} = \frac{2 \times \text{IoU}}{1 + \text{IoU}}$</span>
<span id="cb6-739"><a href="#cb6-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-740"><a href="#cb6-740" aria-hidden="true" tabindex="-1"></a>**Properties:**</span>
<span id="cb6-741"><a href="#cb6-741" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Robust to class imbalance** (like Dice)</span>
<span id="cb6-742"><a href="#cb6-742" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Emphasizes **boundary accuracy**—maximizing IoU requires the predicted region to **align well** with the true region boundaries</span>
<span id="cb6-743"><a href="#cb6-743" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Penalizes false positives and false negatives **equally at the region level**</span>
<span id="cb6-744"><a href="#cb6-744" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Standard metric in segmentation challenges (evaluation)</span>
<span id="cb6-745"><a href="#cb6-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-746"><a href="#cb6-746" aria-hidden="true" tabindex="-1"></a>**Why for EO?**</span>
<span id="cb6-747"><a href="#cb6-747" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Accurately capturing **boundaries** (edge of a flood, outline of a building) is often vital in geographic mapping</span>
<span id="cb6-748"><a href="#cb6-748" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>IoU-based loss directly rewards aligning shapes</span>
<span id="cb6-749"><a href="#cb6-749" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Useful in applications like geographic mapping where **boundary delineation is crucial**</span>
<span id="cb6-750"><a href="#cb6-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-751"><a href="#cb6-751" aria-hidden="true" tabindex="-1"></a><span class="fu">### Dice vs IoU - When to Choose?</span></span>
<span id="cb6-752"><a href="#cb6-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-753"><a href="#cb6-753" aria-hidden="true" tabindex="-1"></a>Both Dice and IoU losses are very similar in concept—they both measure overlap between predicted and true regions—but they have subtle differences that can affect training:</span>
<span id="cb6-754"><a href="#cb6-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-755"><a href="#cb6-755" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Aspect <span class="pp">|</span> Dice Loss <span class="pp">|</span> IoU Loss <span class="pp">|</span></span>
<span id="cb6-756"><a href="#cb6-756" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|-----------|----------|</span></span>
<span id="cb6-757"><a href="#cb6-757" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Formulation** <span class="pp">|</span> 2×Intersection / Sum <span class="pp">|</span> Intersection / Union <span class="pp">|</span></span>
<span id="cb6-758"><a href="#cb6-758" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Gradient** <span class="pp">|</span> More forgiving (2× numerator) <span class="pp">|</span> Stricter <span class="pp">|</span></span>
<span id="cb6-759"><a href="#cb6-759" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Training** <span class="pp">|</span> Smoother, more stable <span class="pp">|</span> Can be less stable <span class="pp">|</span></span>
<span id="cb6-760"><a href="#cb6-760" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Evaluation** <span class="pp">|</span> Common in medical/EO <span class="pp">|</span> Standard in challenges <span class="pp">|</span></span>
<span id="cb6-761"><a href="#cb6-761" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Boundary Focus** <span class="pp">|</span> Moderate <span class="pp">|</span> Higher emphasis <span class="pp">|</span></span>
<span id="cb6-762"><a href="#cb6-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-763"><a href="#cb6-763" aria-hidden="true" tabindex="-1"></a>**Practical Guidance:**</span>
<span id="cb6-764"><a href="#cb6-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-765"><a href="#cb6-765" aria-hidden="true" tabindex="-1"></a>In practice, both Dice and IoU work well for imbalanced EO data. Dice is slightly more popular in training due to its smoother gradients, while IoU is often used as an evaluation metric in segmentation challenges and competitions. The best approach is to try both on your specific dataset and compare results—the difference is often small, but one may work slightly better depending on your particular data characteristics and class distribution.</span>
<span id="cb6-766"><a href="#cb6-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-767"><a href="#cb6-767" aria-hidden="true" tabindex="-1"></a>Very similar to Dice, IoU loss directly optimizes the IoU metric and has an interpretation closely tied to segmentation quality—it penalizes false positives and false negatives at the region level. This makes it useful in applications like geographic mapping where boundary delineation is crucial.</span>
<span id="cb6-768"><a href="#cb6-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-769"><a href="#cb6-769" aria-hidden="true" tabindex="-1"></a><span class="fu">### Loss Function 4: Combined Losses</span></span>
<span id="cb6-770"><a href="#cb6-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-771"><a href="#cb6-771" aria-hidden="true" tabindex="-1"></a>**Best of Both Worlds:** Combine complementary loss functions  </span>
<span id="cb6-772"><a href="#cb6-772" aria-hidden="true" tabindex="-1"></a>**In Practice:** You don't have to pick just one—many implementations use a combination</span>
<span id="cb6-773"><a href="#cb6-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-774"><a href="#cb6-774" aria-hidden="true" tabindex="-1"></a>**Common Combination:**</span>
<span id="cb6-775"><a href="#cb6-775" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-776"><a href="#cb6-776" aria-hidden="true" tabindex="-1"></a>\text{Total Loss} = \alpha \cdot \text{CE Loss} + \beta \cdot \text{Dice Loss}</span>
<span id="cb6-777"><a href="#cb6-777" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb6-778"><a href="#cb6-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-779"><a href="#cb6-779" aria-hidden="true" tabindex="-1"></a>Where $\alpha$ and $\beta$ are weighting factors (e.g., $\alpha=0.5$, $\beta=0.5$)</span>
<span id="cb6-780"><a href="#cb6-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-781"><a href="#cb6-781" aria-hidden="true" tabindex="-1"></a>**Why Combine?**</span>
<span id="cb6-782"><a href="#cb6-782" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cross-Entropy:** Provides strong, stable gradients; ensures overall pixel-wise correctness</span>
<span id="cb6-783"><a href="#cb6-783" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dice:** Focuses on overlap and handles imbalance; ensures region-level accuracy</span>
<span id="cb6-784"><a href="#cb6-784" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Get the **benefits of both** loss functions</span>
<span id="cb6-785"><a href="#cb6-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-786"><a href="#cb6-786" aria-hidden="true" tabindex="-1"></a>**Benefits:**</span>
<span id="cb6-787"><a href="#cb6-787" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stable training from CE's strong signal</span>
<span id="cb6-788"><a href="#cb6-788" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Balanced optimization from Dice's overlap focus</span>
<span id="cb6-789"><a href="#cb6-789" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Often achieves **best results in practice**</span>
<span id="cb6-790"><a href="#cb6-790" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can improve both per-pixel accuracy AND overall region accuracy</span>
<span id="cb6-791"><a href="#cb6-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-792"><a href="#cb6-792" aria-hidden="true" tabindex="-1"></a>**Implementation Example:**</span>
<span id="cb6-793"><a href="#cb6-793" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb6-794"><a href="#cb6-794" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> combined_loss(y_true, y_pred):</span>
<span id="cb6-795"><a href="#cb6-795" aria-hidden="true" tabindex="-1"></a>    ce <span class="op">=</span> tf.keras.losses.categorical_crossentropy(y_true, y_pred)</span>
<span id="cb6-796"><a href="#cb6-796" aria-hidden="true" tabindex="-1"></a>    dice <span class="op">=</span> dice_loss(y_true, y_pred)</span>
<span id="cb6-797"><a href="#cb6-797" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> ce <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> dice</span>
<span id="cb6-798"><a href="#cb6-798" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb6-799"><a href="#cb6-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-800"><a href="#cb6-800" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb6-801"><a href="#cb6-801" aria-hidden="true" tabindex="-1"></a><span class="fu">## Other Advanced Losses</span></span>
<span id="cb6-802"><a href="#cb6-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-803"><a href="#cb6-803" aria-hidden="true" tabindex="-1"></a>Some practitioners also use **Focal Loss** (a modified cross-entropy that down-weights easy/background examples) especially for **extremely imbalanced cases**, though it's more common in object detection. The key takeaway: loss function choice significantly affects model behavior.</span>
<span id="cb6-804"><a href="#cb6-804" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-805"><a href="#cb6-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-806"><a href="#cb6-806" aria-hidden="true" tabindex="-1"></a><span class="fu">### Loss Function Selection Guide</span></span>
<span id="cb6-807"><a href="#cb6-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-808"><a href="#cb6-808" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb6-809"><a href="#cb6-809" aria-hidden="true" tabindex="-1"></a><span class="fu">## Decision Framework for Loss Selection</span></span>
<span id="cb6-810"><a href="#cb6-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-811"><a href="#cb6-811" aria-hidden="true" tabindex="-1"></a>**Start Here:**</span>
<span id="cb6-812"><a href="#cb6-812" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Is your data balanced** (classes ~50/50)?</span>
<span id="cb6-813"><a href="#cb6-813" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Yes** → Standard Cross-Entropy (simple and effective)</span>
<span id="cb6-814"><a href="#cb6-814" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**No** → Continue to next question</span>
<span id="cb6-815"><a href="#cb6-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-816"><a href="#cb6-816" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Is the minority class critical** (e.g., floods, damage, ships)?</span>
<span id="cb6-817"><a href="#cb6-817" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Yes** → Dice or IoU Loss (inherently handle imbalance)</span>
<span id="cb6-818"><a href="#cb6-818" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Somewhat** → Weighted Cross-Entropy</span>
<span id="cb6-819"><a href="#cb6-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-820"><a href="#cb6-820" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Need most stable training?**</span>
<span id="cb6-821"><a href="#cb6-821" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Yes** → Combined Loss (CE + Dice) for stability + imbalance handling</span>
<span id="cb6-822"><a href="#cb6-822" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**No** → Pure Dice/IoU is fine</span>
<span id="cb6-823"><a href="#cb6-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-824"><a href="#cb6-824" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Is boundary accuracy critical?**</span>
<span id="cb6-825"><a href="#cb6-825" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Yes** → IoU Loss or Combined approach</span>
<span id="cb6-826"><a href="#cb6-826" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Moderate** → Dice is sufficient</span>
<span id="cb6-827"><a href="#cb6-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-828"><a href="#cb6-828" aria-hidden="true" tabindex="-1"></a>**EO Common Practice:**</span>
<span id="cb6-829"><a href="#cb6-829" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Flood mapping:** Dice or Combined (severe imbalance, critical boundaries)</span>
<span id="cb6-830"><a href="#cb6-830" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Balanced land cover:** Cross-Entropy (classes relatively balanced)</span>
<span id="cb6-831"><a href="#cb6-831" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Building extraction:** Dice or IoU (precise footprints matter)</span>
<span id="cb6-832"><a href="#cb6-832" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Road extraction:** Combined Loss (thin features, need continuity)</span>
<span id="cb6-833"><a href="#cb6-833" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ship detection:** Dice (extreme imbalance, small objects)</span>
<span id="cb6-834"><a href="#cb6-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-835"><a href="#cb6-835" aria-hidden="true" tabindex="-1"></a>**Golden Rule:**</span>
<span id="cb6-836"><a href="#cb6-836" aria-hidden="true" tabindex="-1"></a>Participants should learn **not just to accept the default loss**, but to **think about the nature** of their segmentation problem and pick (or tune) a loss accordingly for the best results.</span>
<span id="cb6-837"><a href="#cb6-837" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-838"><a href="#cb6-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-839"><a href="#cb6-839" aria-hidden="true" tabindex="-1"></a><span class="fu">### Practical Example: Flood Mapping Loss Selection</span></span>
<span id="cb6-840"><a href="#cb6-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-841"><a href="#cb6-841" aria-hidden="true" tabindex="-1"></a>**Scenario:**</span>
<span id="cb6-842"><a href="#cb6-842" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dataset: 1000 Sentinel-1 SAR images from Central Luzon floods</span>
<span id="cb6-843"><a href="#cb6-843" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Class distribution: 92% non-flooded, 8% flooded pixels (typical imbalance)</span>
<span id="cb6-844"><a href="#cb6-844" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Goal: Precise flood extent mapping for disaster response</span>
<span id="cb6-845"><a href="#cb6-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-846"><a href="#cb6-846" aria-hidden="true" tabindex="-1"></a>**Experiment Results:**</span>
<span id="cb6-847"><a href="#cb6-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-848"><a href="#cb6-848" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Loss Function <span class="pp">|</span> IoU Score <span class="pp">|</span> Notes <span class="pp">|</span></span>
<span id="cb6-849"><a href="#cb6-849" aria-hidden="true" tabindex="-1"></a><span class="pp">|---------------|-----------|-------|</span></span>
<span id="cb6-850"><a href="#cb6-850" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Cross-Entropy** <span class="pp">|</span> 0.12 <span class="pp">|</span> Model predicts mostly non-flooded; misses actual floods (trivial solution) <span class="pp">|</span></span>
<span id="cb6-851"><a href="#cb6-851" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Weighted CE** <span class="pp">|</span> 0.54 <span class="pp">|</span> Better; weight = 11.5× for flood class; some false positives <span class="pp">|</span></span>
<span id="cb6-852"><a href="#cb6-852" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Dice Loss** <span class="pp">|</span> 0.68 <span class="pp">|</span> Good recall; slightly noisy predictions; handles imbalance well <span class="pp">|</span></span>
<span id="cb6-853"><a href="#cb6-853" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Combined (CE + Dice)** | **0.73** ✓ <span class="pp">|</span> Best balance of precision and recall; stable training <span class="pp">|</span></span>
<span id="cb6-854"><a href="#cb6-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-855"><a href="#cb6-855" aria-hidden="true" tabindex="-1"></a>**Winner:** Combined Loss (0.5 × CE + 0.5 × Dice)</span>
<span id="cb6-856"><a href="#cb6-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-857"><a href="#cb6-857" aria-hidden="true" tabindex="-1"></a>**Key Insight:**</span>
<span id="cb6-858"><a href="#cb6-858" aria-hidden="true" tabindex="-1"></a>For our flood mapping case (**binary segmentation with severe imbalance**), we might choose a **Dice loss** or a **combined approach (Dice + Cross-Entropy)** to handle the imbalance and get sharp boundaries. This demonstrates that the choice of loss can **significantly affect model training**—the right loss will push the model to correctly segment the minority class rather than achieving high but useless accuracy by predicting everything as the majority class.</span>
<span id="cb6-859"><a href="#cb6-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-860"><a href="#cb6-860" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-861"><a href="#cb6-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-862"><a href="#cb6-862" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key Takeaways</span></span>
<span id="cb6-863"><a href="#cb6-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-864"><a href="#cb6-864" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb6-865"><a href="#cb6-865" aria-hidden="true" tabindex="-1"></a><span class="fu">## Session 1 Summary</span></span>
<span id="cb6-866"><a href="#cb6-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-867"><a href="#cb6-867" aria-hidden="true" tabindex="-1"></a>**Semantic Segmentation:**</span>
<span id="cb6-868"><a href="#cb6-868" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Pixel-wise classification providing precise boundaries and complete spatial understanding</span>
<span id="cb6-869"><a href="#cb6-869" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Differs from image classification (single labels) and object detection (bounding boxes)</span>
<span id="cb6-870"><a href="#cb6-870" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Essential for EO applications requiring exact spatial extent and area calculations</span>
<span id="cb6-871"><a href="#cb6-871" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Enables detailed thematic mapping and spatial pattern analysis</span>
<span id="cb6-872"><a href="#cb6-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-873"><a href="#cb6-873" aria-hidden="true" tabindex="-1"></a>**U-Net Architecture:**</span>
<span id="cb6-874"><a href="#cb6-874" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Encoder-decoder structure with characteristic U-shape</span>
<span id="cb6-875"><a href="#cb6-875" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ **Skip connections** are the key innovation—preserve spatial details from encoder to decoder</span>
<span id="cb6-876"><a href="#cb6-876" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Combines "what" (semantic context) with "where" (precise localization)</span>
<span id="cb6-877"><a href="#cb6-877" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Proven architecture even with limited training data (hundreds to thousands of samples)</span>
<span id="cb6-878"><a href="#cb6-878" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Widely adopted across EO community for high-accuracy segmentation</span>
<span id="cb6-879"><a href="#cb6-879" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Uses same CNN building blocks from Day 2 (convolution, pooling, padding, activation)</span>
<span id="cb6-880"><a href="#cb6-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-881"><a href="#cb6-881" aria-hidden="true" tabindex="-1"></a>**Loss Functions:**</span>
<span id="cb6-882"><a href="#cb6-882" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ **Cross-Entropy:** Standard but sensitive to class imbalance; provides strong gradients</span>
<span id="cb6-883"><a href="#cb6-883" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ **Weighted CE:** Addresses imbalance through class weighting</span>
<span id="cb6-884"><a href="#cb6-884" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ **Dice/IoU:** Inherently handle imbalance, optimize region overlap, focus on minority class</span>
<span id="cb6-885"><a href="#cb6-885" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ **Combined losses** often achieve best performance in EO (e.g., CE + Dice)</span>
<span id="cb6-886"><a href="#cb6-886" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ **Choice significantly impacts model behavior**—can mean difference between useless and excellent results</span>
<span id="cb6-887"><a href="#cb6-887" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Must consider data characteristics (balance, boundary importance, object size)</span>
<span id="cb6-888"><a href="#cb6-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-889"><a href="#cb6-889" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb6-890"><a href="#cb6-890" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Flood mapping, land cover, buildings, roads, vegetation monitoring</span>
<span id="cb6-891"><a href="#cb6-891" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ U-Net achieves high accuracy even with small datasets</span>
<span id="cb6-892"><a href="#cb6-892" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Outperforms older pixel-based and patch-based methods</span>
<span id="cb6-893"><a href="#cb6-893" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Wide adoption across EO community</span>
<span id="cb6-894"><a href="#cb6-894" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✓ Proven results in Philippine disaster response contexts (Typhoon Ulysses, urban mapping)</span>
<span id="cb6-895"><a href="#cb6-895" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-896"><a href="#cb6-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-897"><a href="#cb6-897" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-898"><a href="#cb6-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-899"><a href="#cb6-899" aria-hidden="true" tabindex="-1"></a><span class="fu">## Resources</span></span>
<span id="cb6-900"><a href="#cb6-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-901"><a href="#cb6-901" aria-hidden="true" tabindex="-1"></a><span class="fu">### Core References</span></span>
<span id="cb6-902"><a href="#cb6-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-903"><a href="#cb6-903" aria-hidden="true" tabindex="-1"></a>**Foundational Paper:**</span>
<span id="cb6-904"><a href="#cb6-904" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. *MICCAI 2015*. <span class="co">[</span><span class="ot">arXiv:1505.04597</span><span class="co">](https://arxiv.org/abs/1505.04597)</span></span>
<span id="cb6-905"><a href="#cb6-905" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Original U-Net paper from medical imaging</span>
<span id="cb6-906"><a href="#cb6-906" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Introduced skip connections concept</span>
<span id="cb6-907"><a href="#cb6-907" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Demonstrated effectiveness with limited training data</span>
<span id="cb6-908"><a href="#cb6-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-909"><a href="#cb6-909" aria-hidden="true" tabindex="-1"></a>**EO Applications:**</span>
<span id="cb6-910"><a href="#cb6-910" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Flood mapping with Sentinel-1 SAR and U-Net (disaster response)</span>
<span id="cb6-911"><a href="#cb6-911" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Building extraction from high-resolution imagery (urban planning)</span>
<span id="cb6-912"><a href="#cb6-912" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Land cover classification with multispectral data (environmental monitoring)</span>
<span id="cb6-913"><a href="#cb6-913" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Road network extraction (infrastructure mapping)</span>
<span id="cb6-914"><a href="#cb6-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-915"><a href="#cb6-915" aria-hidden="true" tabindex="-1"></a>**Loss Function References:**</span>
<span id="cb6-916"><a href="#cb6-916" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dice Loss for handling class imbalance in segmentation</span>
<span id="cb6-917"><a href="#cb6-917" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>IoU (Jaccard) Loss for boundary accuracy</span>
<span id="cb6-918"><a href="#cb6-918" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Combined loss strategies for optimal performance</span>
<span id="cb6-919"><a href="#cb6-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-920"><a href="#cb6-920" aria-hidden="true" tabindex="-1"></a><span class="fu">### Datasets for Practice</span></span>
<span id="cb6-921"><a href="#cb6-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-922"><a href="#cb6-922" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sen1Floods11:** Global flood dataset with Sentinel-1 SAR</span>
<span id="cb6-923"><a href="#cb6-923" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DeepGlobe Land Cover Challenge:** Multi-class segmentation</span>
<span id="cb6-924"><a href="#cb6-924" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SpaceNet Building Detection:** Urban building footprints</span>
<span id="cb6-925"><a href="#cb6-925" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Landcover.ai:** High-resolution orthophotos (Poland)</span>
<span id="cb6-926"><a href="#cb6-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-927"><a href="#cb6-927" aria-hidden="true" tabindex="-1"></a><span class="fu">### Tutorials</span></span>
<span id="cb6-928"><a href="#cb6-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-929"><a href="#cb6-929" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">TensorFlow U-Net Tutorial</span><span class="co">](https://www.tensorflow.org/tutorials/images/segmentation)</span></span>
<span id="cb6-930"><a href="#cb6-930" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">PyTorch Semantic Segmentation Examples</span><span class="co">](https://pytorch.org/vision/stable/models.html#semantic-segmentation)</span></span>
<span id="cb6-931"><a href="#cb6-931" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Keras U-Net for Satellite Imagery</span><span class="co">](https://keras.io/examples/vision/oxford_pets_image_segmentation/)</span></span>
<span id="cb6-932"><a href="#cb6-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-933"><a href="#cb6-933" aria-hidden="true" tabindex="-1"></a><span class="fu">### Philippine EO Context</span></span>
<span id="cb6-934"><a href="#cb6-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-935"><a href="#cb6-935" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**PhilSA:** Flood monitoring and disaster response initiatives</span>
<span id="cb6-936"><a href="#cb6-936" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DOST-ASTI DATOS:** Automated rapid mapping for disasters</span>
<span id="cb6-937"><a href="#cb6-937" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**NAMRIA:** Geospatial data infrastructure</span>
<span id="cb6-938"><a href="#cb6-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-939"><a href="#cb6-939" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-940"><a href="#cb6-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-941"><a href="#cb6-941" aria-hidden="true" tabindex="-1"></a><span class="fu">## Preparation for Session 2</span></span>
<span id="cb6-942"><a href="#cb6-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-943"><a href="#cb6-943" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb6-944"><a href="#cb6-944" aria-hidden="true" tabindex="-1"></a><span class="fu">## Next: Hands-on Flood Mapping Lab</span></span>
<span id="cb6-945"><a href="#cb6-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-946"><a href="#cb6-946" aria-hidden="true" tabindex="-1"></a>**Session 2** will put these concepts into practice with a complete U-Net implementation:</span>
<span id="cb6-947"><a href="#cb6-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-948"><a href="#cb6-948" aria-hidden="true" tabindex="-1"></a>**What You'll Do:**</span>
<span id="cb6-949"><a href="#cb6-949" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Load Sentinel-1 SAR data from Typhoon Ulysses (Central Luzon)</span>
<span id="cb6-950"><a href="#cb6-950" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Build U-Net model in TensorFlow/PyTorch</span>
<span id="cb6-951"><a href="#cb6-951" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Train with Dice Loss (or combined loss)</span>
<span id="cb6-952"><a href="#cb6-952" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Evaluate performance using IoU, F1-score, precision, recall</span>
<span id="cb6-953"><a href="#cb6-953" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Visualize flood predictions and create export maps</span>
<span id="cb6-954"><a href="#cb6-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-955"><a href="#cb6-955" aria-hidden="true" tabindex="-1"></a>**Dataset:**</span>
<span id="cb6-956"><a href="#cb6-956" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>~500-1000 pre-processed SAR patches (256×256 pixels)</span>
<span id="cb6-957"><a href="#cb6-957" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Binary flood masks (flooded / non-flooded)</span>
<span id="cb6-958"><a href="#cb6-958" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Real flood event from major Philippine river basin</span>
<span id="cb6-959"><a href="#cb6-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-960"><a href="#cb6-960" aria-hidden="true" tabindex="-1"></a>**Expected Results:**</span>
<span id="cb6-961"><a href="#cb6-961" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>IoU &gt; 0.70 with properly trained model</span>
<span id="cb6-962"><a href="#cb6-962" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Visual flood extent maps ready for GIS integration</span>
<span id="cb6-963"><a href="#cb6-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-964"><a href="#cb6-964" aria-hidden="true" tabindex="-1"></a>**To Prepare:**</span>
<span id="cb6-965"><a href="#cb6-965" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ensure Google Colab access</span>
<span id="cb6-966"><a href="#cb6-966" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Check GPU availability (Runtime → Change runtime type → GPU)</span>
<span id="cb6-967"><a href="#cb6-967" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Review Python and NumPy basics if needed</span>
<span id="cb6-968"><a href="#cb6-968" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Have patience - model training takes time!</span>
<span id="cb6-969"><a href="#cb6-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-970"><a href="#cb6-970" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Preview Session 2 →</span><span class="co">](../notebooks/Day3_Session2_Flood_Mapping_UNet.ipynb)</span>{.btn .btn-outline-primary}</span>
<span id="cb6-971"><a href="#cb6-971" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-972"><a href="#cb6-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-973"><a href="#cb6-973" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-974"><a href="#cb6-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-975"><a href="#cb6-975" aria-hidden="true" tabindex="-1"></a><span class="fu">## Discussion Questions</span></span>
<span id="cb6-976"><a href="#cb6-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-977"><a href="#cb6-977" aria-hidden="true" tabindex="-1"></a>Before moving to the hands-on session, consider these questions:</span>
<span id="cb6-978"><a href="#cb6-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-979"><a href="#cb6-979" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**What EO applications in your work** could benefit from semantic segmentation versus classification or detection?</span>
<span id="cb6-980"><a href="#cb6-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-981"><a href="#cb6-981" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**How would you validate** segmentation results in the field, especially for disaster response applications?</span>
<span id="cb6-982"><a href="#cb6-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-983"><a href="#cb6-983" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**What challenges do you anticipate** when working with limited training data for Philippine-specific contexts?</span>
<span id="cb6-984"><a href="#cb6-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-985"><a href="#cb6-985" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**How might transfer learning** from models trained on other regions help with Philippine EO applications?</span>
<span id="cb6-986"><a href="#cb6-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-987"><a href="#cb6-987" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-988"><a href="#cb6-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-989"><a href="#cb6-989" aria-hidden="true" tabindex="-1"></a>::: {.session-nav}</span>
<span id="cb6-990"><a href="#cb6-990" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">← Back to Day 3 Overview</span><span class="co">](../index.qmd)</span>{.btn .btn-outline-secondary}</span>
<span id="cb6-991"><a href="#cb6-991" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Next: Session 2 - Flood Mapping Lab →</span><span class="co">](../notebooks/Day3_Session2_Flood_Mapping_UNet.ipynb)</span>{.btn .btn-primary}</span>
<span id="cb6-992"><a href="#cb6-992" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb6-993"><a href="#cb6-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-994"><a href="#cb6-994" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb6-995"><a href="#cb6-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-996"><a href="#cb6-996" aria-hidden="true" tabindex="-1"></a>*This session is part of the CoPhil 4-Day Advanced Training on AI/ML for Earth Observation, funded by the European Union under the Global Gateway initiative.*</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>CoPhil EO AI/ML Training Programme</p>
</div>   
    <div class="nav-footer-center">
<p>Funded by the European Union - Global Gateway Initiative</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:info@philsa.gov.ph">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://philsa.gov.ph">
      <i class="bi bi-globe" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>