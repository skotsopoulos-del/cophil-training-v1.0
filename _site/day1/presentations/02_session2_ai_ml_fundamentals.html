<!DOCTYPE html>
<html lang="en"><head>
<link href="../../images/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Stylianos Kotsopoulos">
  <title>CoPhil EO AI/ML Training – Core Concepts of AI/ML for Earth Observation</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-534cd8e3a96973385dffff3f4709048d.css">
  <link rel="stylesheet" href="custom.scss">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-509191933"></script>

  <script type="text/javascript">

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-509191933', { 'anonymize_ip': true});
  </script>
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Core Concepts of AI/ML for Earth Observation</h1>
  <p class="subtitle">CoPhil EO AI/ML Training - Day 1, Session 2</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Stylianos Kotsopoulos 
</div>
        <p class="quarto-title-affiliation">
            EU-Philippines CoPhil Programme
          </p>
    </div>
</div>

</section>
<section>
<section id="welcome-to-session-2" class="title-slide slide level1 center" data-background-color="#1e3a8a">
<h1>Welcome to Session 2</h1>

</section>
<section id="session-objectives" class="slide level2 center">
<h2>Session Objectives</h2>
<ul>
<li class="fragment">Understand <strong>what AI/ML means</strong> in Earth Observation context</li>
<li class="fragment">Learn the <strong>end-to-end workflow</strong> for ML projects</li>
<li class="fragment">Distinguish <strong>supervised</strong> vs <strong>unsupervised</strong> learning</li>
<li class="fragment">Grasp <strong>deep learning</strong> and neural network basics</li>
<li class="fragment">Explore <strong>2025 AI innovations</strong> (foundation models, data-centric AI)</li>
</ul>
<div class="fragment">
<p><strong>Duration:</strong> 2 hours</p>
</div>
<aside class="notes">
<p>This session covers fundamental AI/ML concepts tailored to EO applications. Mostly conceptual, but essential foundation for hands-on work in Sessions 3-4 and throughout Day 2-4.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="session-roadmap" class="slide level2 center">
<h2>Session Roadmap</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>Time</th>
<th>Topic</th>
<th>Duration</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>00-10 min</td>
<td>What is AI/ML?</td>
<td>10 min</td>
</tr>
<tr class="even">
<td>10-35 min</td>
<td>EO Workflow &amp; Data Pipeline</td>
<td>25 min</td>
</tr>
<tr class="odd">
<td>35-60 min</td>
<td>Supervised vs Unsupervised Learning</td>
<td>25 min</td>
</tr>
<tr class="even">
<td><strong>60-65 min</strong></td>
<td><strong>☕ Break</strong></td>
<td><strong>5 min</strong></td>
</tr>
<tr class="odd">
<td>65-90 min</td>
<td>Deep Learning &amp; Neural Networks</td>
<td>25 min</td>
</tr>
<tr class="even">
<td>90-110 min</td>
<td>Data-Centric AI &amp; 2025 Updates</td>
<td>20 min</td>
</tr>
<tr class="odd">
<td>110-120 min</td>
<td>Q&amp;A &amp; Summary</td>
<td>10 min</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p><strong>Timing:</strong> 2 minutes</p>
<p>This session is more conceptual than Session 1. Focus on building intuition and mental models. Hands-on practice comes in Sessions 3-4 and Days 2-4.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="why-aiml-for-earth-observation" class="slide level2 center">
<h2>Why AI/ML for Earth Observation?</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Traditional Approach</strong></p>
<ul>
<li>Manual interpretation</li>
<li>Rule-based classification</li>
<li>Simple thresholds</li>
<li>Time-consuming</li>
<li>Hard to scale</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>AI/ML Approach</strong></p>
<ul>
<li>Automated pattern recognition</li>
<li>Learn from examples</li>
<li>Complex decision boundaries</li>
<li>Fast processing</li>
<li>Scalable to large areas</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>ML can process years of satellite data in hours!</strong></p>
</div>
<aside class="notes">
<p>Machine learning allows us to automatically recognize patterns in satellite imagery without hard-coding rules for every scenario. This is transformative for large-area, time-series analysis.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="what-is-aiml" class="title-slide slide level1 center" data-background-color="#0f766e">
<h1>What is AI/ML?</h1>

</section>
<section id="defining-the-terms" class="slide level2 center">
<h2>Defining the Terms</h2>

<img data-src="../../diagrams_export/diagram_006_day1_sessions_session2_1.png" style="width:85.0%" class="r-stretch quarto-figure-center"><p class="caption">AI, ML, and Deep Learning Hierarchy with EO Applications</p><ul>
<li class="fragment"><strong>Artificial Intelligence (AI):</strong> Broad field of making machines “smart”</li>
<li class="fragment"><strong>Machine Learning (ML):</strong> Subset of AI where algorithms learn from data</li>
<li class="fragment"><strong>Deep Learning (DL):</strong> Subset of ML using neural networks with many layers</li>
</ul>
<aside class="notes">
<p>AI is the broadest term. Machine Learning is a subset where computer algorithms learn patterns from data without being explicitly programmed. Deep Learning uses neural networks. This comprehensive diagram shows the nested hierarchy with specific algorithms at each level (Random Forest, CNNs, RNNs, etc.) and their Earth Observation applications like land cover classification and time series analysis.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="machine-learning-in-simple-terms" class="slide level2 center">
<h2>Machine Learning in Simple Terms</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Traditional Programming</strong></p>
<pre><code>Rules + Data → Results</code></pre>
<ul>
<li>Programmer writes explicit rules</li>
<li>Fixed logic</li>
<li>Hard to handle complexity</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Machine Learning</strong></p>
<pre><code>Data + Results → Rules</code></pre>
<ul>
<li>Algorithm learns rules from examples</li>
<li>Adaptive</li>
<li>Handles complex patterns</li>
</ul>
</div></div>
<aside class="notes">
<p>In traditional programming, we tell computers what to do step-by-step. In ML, we show examples and the algorithm figures out the pattern.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="ml-in-earth-observation-context" class="slide level2 center">
<h2>ML in Earth Observation Context</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Example: Forest vs Non-Forest</strong></p>
<p><strong>Traditional:</strong></p>
<pre><code>IF NDVI &gt; 0.6 THEN Forest
ELSE Non-Forest</code></pre>
<p>Simple, but breaks easily</p>
</div><div class="column" style="width:50%;">
<p><strong>Machine Learning:</strong></p>
<ul>
<li>Show 1000 examples of forest pixels</li>
<li>Show 1000 examples of non-forest</li>
<li>Algorithm learns complex patterns</li>
<li>Works in diverse conditions</li>
</ul>
</div></div>
<!-- ![](images/classification_example.jpg){fig-align="center" width="60%"} -->
<aside class="notes">
<p>A simple NDVI threshold might work in one region but fail in another. ML can learn the nuanced patterns that distinguish forest from non-forest across different conditions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="the-aiml-workflow-for-eo" class="title-slide slide level1 center" data-background-color="#0f766e">
<h1>The AI/ML Workflow for EO</h1>

</section>
<section id="end-to-end-ml-workflow" class="slide level2 center">
<h2>End-to-End ML Workflow</h2>

<img data-src="images/ml_workflow.png" class="quarto-figure quarto-figure-center r-stretch" style="width:90.0%"><aside class="notes">
<p>This is the typical workflow for any ML project in Earth Observation. Understanding these steps is crucial for successful implementation.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="step-1-problem-definition" class="slide level2 center">
<h2>Step 1: Problem Definition</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Key Questions</strong></p>
<ul>
<li>What exactly are we trying to achieve?</li>
<li>What decisions will this support?</li>
<li>What level of accuracy is needed?</li>
<li>What resources are available?</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>EO Examples</strong></p>
<ul>
<li>Map rice paddy extent</li>
<li>Detect flooded areas after typhoon</li>
<li>Classify land cover types</li>
<li>Estimate crop yield</li>
<li>Monitor deforestation</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>Clear problem definition = 50% of success</strong></p>
</div>
<aside class="notes">
<p>Being clear on the question helps design the solution. “We want to classify land cover in Palawan” is much more actionable than “We want to use AI.”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="step-2-data-acquisition" class="slide level2 center">
<h2>Step 2: Data Acquisition</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Satellite Imagery</strong></p>
<ul>
<li>Sentinel-1/2 (covered in Session 1!)</li>
<li>Landsat</li>
<li>Planet</li>
<li>High-resolution commercial</li>
<li>Multiple dates/seasons</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Ground Truth / Labels</strong></p>
<ul>
<li>Field surveys</li>
<li>GPS points</li>
<li>Existing maps</li>
<li>Photo interpretation</li>
<li>Expert knowledge</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>Challenge:</strong> Getting quality labels is often hardest part</p>
</div>
<aside class="notes">
<p>Data acquisition includes both satellite images and the ground truth labels needed to train supervised models. The quality and quantity of labels directly impact model performance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="step-3-data-preprocessing" class="slide level2 center">
<h2>Step 3: Data Preprocessing</h2>
<p><strong>For Satellite Imagery:</strong></p>
<ul>
<li class="fragment">Atmospheric correction (use Level-2A!)</li>
<li class="fragment">Cloud masking</li>
<li class="fragment">Geometric correction</li>
<li class="fragment">Radiometric calibration</li>
<li class="fragment">Co-registration (multiple sensors)</li>
<li class="fragment">Temporal compositing</li>
</ul>
<div class="fragment">
<p><strong>“Garbage In, Garbage Out”</strong> - preprocessing matters!</p>
</div>
<aside class="notes">
<p>Well-prepared input data is crucial. Even the best model will fail if fed cloudy, misaligned, or uncorrected images.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="preprocessing-example" class="slide level2 center">
<h2>Preprocessing Example</h2>

<img data-src="images/Cloud_removal_before_after.webp" style="width:90.0%" class="r-stretch quarto-figure-center"><p class="caption">Cloud Removal Before and After Comparison</p><div class="columns">
<div class="column" style="width:50%;">
<p><strong>Before Preprocessing:</strong> - Clouds present - Atmospheric haze - Different acquisition dates</p>
</div><div class="column" style="width:50%;">
<p><strong>After Preprocessing:</strong> - Clouds masked - Atmospherically corrected - Temporal composite created</p>
</div></div>
<aside class="notes">
<p>Preprocessing transforms raw satellite data into analysis-ready products. This side-by-side comparison shows the dramatic improvement from cloud masking and creating temporal composites - essential steps before any ML analysis.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="step-4-feature-engineering" class="slide level2 center">
<h2>Step 4: Feature Engineering</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>What are Features?</strong></p>
<ul>
<li>Input variables for the model</li>
<li>Derived from raw data</li>
<li>Informative for the task</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>EO Features</strong></p>
<ul>
<li>Spectral bands (Blue, Red, NIR, etc.)</li>
<li>Spectral indices (NDVI, NDWI)</li>
<li>Texture measures</li>
<li>Temporal statistics</li>
<li>Topography (elevation, slope)</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>Deep Learning:</strong> Often learns features automatically!</p>
</div>
<aside class="notes">
<p>For traditional ML like Random Forest, we engineer features. For deep learning, the network learns features automatically from raw pixels.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="common-eo-features" class="slide level2 center">
<h2>Common EO Features</h2>
<table class="caption-top">
<colgroup>
<col style="width: 32%">
<col style="width: 25%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Feature Type</strong></th>
<th><strong>Examples</strong></th>
<th><strong>What They Capture</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Spectral Bands</strong></td>
<td>B2, B3, B4, B8</td>
<td>Reflectance at different wavelengths</td>
</tr>
<tr class="even">
<td><strong>Vegetation Indices</strong></td>
<td>NDVI, EVI, SAVI</td>
<td>Vegetation health, density</td>
</tr>
<tr class="odd">
<td><strong>Water Indices</strong></td>
<td>NDWI, MNDWI</td>
<td>Water presence, moisture</td>
</tr>
<tr class="even">
<td><strong>Texture</strong></td>
<td>GLCM variance, entropy</td>
<td>Spatial patterns</td>
</tr>
<tr class="odd">
<td><strong>Temporal</strong></td>
<td>Mean, std over time</td>
<td>Phenology, seasonality</td>
</tr>
<tr class="even">
<td><strong>Topographic</strong></td>
<td>Elevation, slope, aspect</td>
<td>Terrain characteristics</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>Different features highlight different aspects of the landscape. Vegetation indices emphasize green biomass, water indices highlight water bodies, etc.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="step-5-model-selection-training" class="slide level2 center">
<h2>Step 5: Model Selection &amp; Training</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Model Selection</strong></p>
<p>Choose based on:</p>
<ul>
<li>Problem type (classification vs regression)</li>
<li>Data size</li>
<li>Interpretability needs</li>
<li>Computational resources</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Common EO Models</strong></p>
<ul>
<li>Random Forest</li>
<li>Support Vector Machines</li>
<li>Convolutional Neural Networks</li>
<li>U-Net (segmentation)</li>
<li>Recurrent networks (time series)</li>
</ul>
</div></div>
<aside class="notes">
<p>Model choice depends on your specific problem, available data, and resources. We’ll cover Random Forest on Day 2 and CNNs on Day 3.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="training-process" class="slide level2 center">
<h2>Training Process</h2>

<img data-src="images/training_process.png" class="quarto-figure quarto-figure-center r-stretch" style="width:75.0%"><ol type="1">
<li class="fragment"><strong>Split data:</strong> Training set (70-80%) &amp; Validation set (20-30%)</li>
<li class="fragment"><strong>Feed training data</strong> to model</li>
<li class="fragment"><strong>Model learns patterns</strong> by adjusting internal parameters</li>
<li class="fragment"><strong>Validate</strong> on unseen validation data</li>
<li class="fragment"><strong>Iterate:</strong> Adjust model or data if needed</li>
</ol>
<aside class="notes">
<p>Training involves feeding labeled examples to the model. The model adjusts its internal parameters to minimize errors on the training data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="step-6-validation-evaluation" class="slide level2 center">
<h2>Step 6: Validation &amp; Evaluation</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Why Validate?</strong></p>
<ul>
<li>Ensure model generalizes</li>
<li>Detect overfitting</li>
<li>Compare different models</li>
<li>Build confidence</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Evaluation Metrics</strong></p>
<ul>
<li>Overall Accuracy</li>
<li>Confusion Matrix</li>
<li>Precision &amp; Recall</li>
<li>F1-Score</li>
<li>Kappa coefficient</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>Use independent test data - never validate on training data!</strong></p>
</div>
<aside class="notes">
<p>Rigorous validation using held-out data ensures the model works on new, unseen examples - not just memorizing training data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="confusion-matrix-example" class="slide level2 center">
<h2>Confusion Matrix Example</h2>

<img data-src="images/confusion_matrix.png" class="quarto-figure quarto-figure-center r-stretch" style="width:60.0%"><div class="columns">
<div class="column" style="width:50%;">
<p><strong>What it shows:</strong></p>
<ul>
<li>True Positives (correct predictions)</li>
<li>False Positives (type I error)</li>
<li>False Negatives (type II error)</li>
<li>True Negatives</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Derived Metrics:</strong></p>
<ul>
<li>Precision = TP / (TP + FP)</li>
<li>Recall = TP / (TP + FN)</li>
<li>Accuracy = (TP + TN) / Total</li>
</ul>
</div></div>
<aside class="notes">
<p>The confusion matrix shows where your model is making mistakes. This helps identify which classes are being confused.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="step-7-deployment" class="slide level2 center">
<h2>Step 7: Deployment</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Deployment Options</strong></p>
<ul>
<li>Generate full maps</li>
<li>Near real-time monitoring</li>
<li>Operational pipelines</li>
<li>Decision support systems</li>
<li>Web applications</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Considerations</strong></p>
<ul>
<li>Model retraining schedule</li>
<li>Computational requirements</li>
<li>User interface</li>
<li>Data updates</li>
<li>Maintenance plan</li>
</ul>
</div></div>
<aside class="notes">
<p>If the model is satisfactory, deploy it for operational use. This might mean generating maps for entire regions or setting up automatic processing of new satellite images.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="workflow-is-iterative" class="slide level2 center">
<h2>Workflow is Iterative</h2>

<img data-src="images/iterative_workflow.png" class="quarto-figure quarto-figure-center r-stretch" style="width:70.0%"><ul>
<li class="fragment"><strong>Poor validation?</strong> → Go back to data acquisition or model selection</li>
<li class="fragment"><strong>New data available?</strong> → Retrain model</li>
<li class="fragment"><strong>Requirements change?</strong> → Redefine problem</li>
<li class="fragment"><strong>Continuous improvement</strong> is key</li>
</ul>
<aside class="notes">
<p>Real projects are iterative. You often loop back: if validation is poor, you might need more data, different features, or a different model.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="types-of-machine-learning" class="title-slide slide level1 center" data-background-color="#1e3a8a">
<h1>Types of Machine Learning</h1>

</section>
<section id="main-ml-paradigms" class="slide level2 center">
<h2>Main ML Paradigms</h2>

<img data-src="images/ml_types.png" class="quarto-figure quarto-figure-center r-stretch" style="width:75.0%"><ol type="1">
<li class="fragment"><strong>Supervised Learning</strong> (most common in EO)</li>
<li class="fragment"><strong>Unsupervised Learning</strong> (exploratory analysis)</li>
<li class="fragment"><strong>Semi-supervised Learning</strong> (combines both)</li>
<li class="fragment"><strong>Reinforcement Learning</strong> (less common in EO)</li>
</ol>
<aside class="notes">
<p>We’ll focus on supervised and unsupervised learning as these are most relevant for Earth Observation applications.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="supervised-learning" class="title-slide slide level1 center" data-background-color="#0f766e">
<h1>Supervised Learning</h1>

</section>
<section id="what-is-supervised-learning" class="slide level2 center">
<h2>What is Supervised Learning?</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Definition</strong></p>
<ul>
<li>Learning from <strong>labeled data</strong></li>
<li>Known input-output pairs</li>
<li>Model learns mapping from inputs to outputs</li>
<li>Like learning with an answer key</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="images/supervised_learning.png" style="width:100.0%"></p>
</div></div>
<div class="fragment">
<p><strong>Requires ground truth labels for training</strong></p>
</div>
<aside class="notes">
<p>Supervised learning is the most common in EO. The algorithm is given examples with known outcomes (labels) and learns to predict labels for new, unseen data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="two-types-of-supervised-learning" class="slide level2 center">
<h2>Two Types of Supervised Learning</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Classification</strong></p>
<ul>
<li>Predict <strong>categorical</strong> labels</li>
<li>Discrete classes</li>
<li>Example outputs: “Forest”, “Water”, “Urban”</li>
</ul>
<p><img data-src="images/classification.jpg" style="width:100.0%"></p>
</div><div class="column" style="width:50%;">
<p><strong>Regression</strong></p>
<ul>
<li>Predict <strong>continuous</strong> values</li>
<li>Numeric outputs</li>
<li>Example outputs: 25.3 tons/hectare, 15.2°C</li>
</ul>
<p><img data-src="images/regression.jpg" style="width:100.0%"></p>
</div></div>
<aside class="notes">
<p>Classification assigns data to categories. Regression predicts numeric values. Both require labeled training data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="classification-examples-in-eo" class="slide level2 center">
<h2>Classification Examples in EO</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Land Cover Classification</strong></p>
<p><img data-src="images/land_cover_classification.jpg" style="width:100.0%"></p>
<ul>
<li>Forest, agriculture, urban, water</li>
<li>Pixel-wise or object-based</li>
<li>Multi-class problem</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Crop Type Mapping</strong></p>
<p><img data-src="images/crop_type_map.jpg" style="width:100.0%"></p>
<ul>
<li>Rice, corn, sugarcane</li>
<li>Seasonal patterns important</li>
<li>Supports agricultural planning</li>
</ul>
</div></div>
<aside class="notes">
<p>Land cover classification is the classic EO supervised learning task. Each pixel or region is assigned to a class like forest, water, or urban.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="regression-examples-in-eo" class="slide level2 center">
<h2>Regression Examples in EO</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Biomass Estimation</strong></p>
<p><img data-src="images/biomass_estimation.jpg" style="width:100.0%"></p>
<ul>
<li>Predict tons of biomass per hectare</li>
<li>Important for carbon accounting</li>
<li>Uses SAR and optical data</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Crop Yield Prediction</strong></p>
<p><img data-src="images/yield_prediction.jpg" style="width:100.0%"></p>
<ul>
<li>Predict tons per hectare</li>
<li>Seasonal NDVI time series</li>
<li>Supports food security planning</li>
</ul>
</div></div>
<aside class="notes">
<p>Regression tasks predict continuous values like biomass density, crop yield, soil moisture, or sea surface temperature from satellite data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="common-supervised-algorithms" class="slide level2 center">
<h2>Common Supervised Algorithms</h2>
<table class="caption-top">
<colgroup>
<col style="width: 29%">
<col style="width: 29%">
<col style="width: 41%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Algorithm</strong></th>
<th><strong>Strengths</strong></th>
<th><strong>EO Applications</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Random Forest</strong></td>
<td>Handles high dimensions, robust</td>
<td>Land cover, crop classification</td>
</tr>
<tr class="even">
<td><strong>SVM</strong></td>
<td>Effective in high dimensions</td>
<td>Binary classification, change detection</td>
</tr>
<tr class="odd">
<td><strong>Neural Networks</strong></td>
<td>Learns complex patterns</td>
<td>Image classification, segmentation</td>
</tr>
<tr class="even">
<td><strong>Decision Trees</strong></td>
<td>Interpretable</td>
<td>Quick classifications</td>
</tr>
<tr class="odd">
<td><strong>k-NN</strong></td>
<td>Simple, non-parametric</td>
<td>Local classifications</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>Different algorithms have different strengths. Random Forest is popular in EO for its robustness and ability to handle many features.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="supervised-learning-requirements" class="slide level2 center">
<h2>Supervised Learning Requirements</h2>
<p><strong>Essential:</strong></p>
<ol type="1">
<li class="fragment"><strong>Training data</strong> with known labels</li>
<li class="fragment"><strong>Representative samples</strong> covering all classes</li>
<li class="fragment"><strong>Sufficient quantity</strong> (varies by algorithm)</li>
<li class="fragment"><strong>Quality labels</strong> (accurate, consistent)</li>
<li class="fragment"><strong>Independent validation</strong> data</li>
</ol>
<div class="fragment">
<p><strong>Challenge:</strong> Getting quality labels is often the bottleneck!</p>
</div>
<aside class="notes">
<p>Supervised learning needs ground truth. For land cover, this might be field surveys, GPS points, or careful photo interpretation. Quality matters more than quantity!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="unsupervised-learning" class="title-slide slide level1 center" data-background-color="#0f766e">
<h1>Unsupervised Learning</h1>

</section>
<section id="what-is-unsupervised-learning" class="slide level2 center">
<h2>What is Unsupervised Learning?</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Definition</strong></p>
<ul>
<li>Learning from <strong>unlabeled data</strong></li>
<li>No known outputs</li>
<li>Discover hidden patterns</li>
<li>Like sorting without instructions</li>
</ul>
</div><div class="column" style="width:50%;">
<p><img data-src="images/unsupervised_learning.png" style="width:100.0%"></p>
</div></div>
<div class="fragment">
<p><strong>Useful for exploratory analysis and finding structure</strong></p>
</div>
<aside class="notes">
<p>Unsupervised learning finds patterns or groupings inherent in the data without being told what to look for.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="clustering-main-unsupervised-technique" class="slide level2 center">
<h2>Clustering: Main Unsupervised Technique</h2>

<img data-src="images/clustering_example.png" class="quarto-figure quarto-figure-center r-stretch" style="width:70.0%"><ul>
<li class="fragment"><strong>Group similar pixels</strong> based on spectral characteristics</li>
<li class="fragment">Algorithm decides number of clusters (or you specify)</li>
<li class="fragment"><strong>Analyst interprets</strong> what each cluster means</li>
<li class="fragment">Example: “Cluster 3 looks like water, Cluster 7 looks like forest”</li>
</ul>
<aside class="notes">
<p>K-means clustering is a common unsupervised method. It groups pixels with similar reflectance, but you have to interpret what those groups mean.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="unsupervised-eo-applications" class="slide level2 center">
<h2>Unsupervised EO Applications</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Change Detection</strong></p>
<ul>
<li>Cluster “before” and “after” images</li>
<li>Identify changed areas</li>
<li>No labels needed</li>
</ul>
<p><strong>Anomaly Detection</strong></p>
<ul>
<li>Find unusual pixels</li>
<li>Potential forest disturbance</li>
<li>Data quality issues</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Initial Exploration</strong></p>
<ul>
<li>Quick overview of spectral classes</li>
<li>Inform supervised approach</li>
<li>Generate training samples</li>
</ul>
<p><strong>Dimensionality Reduction</strong></p>
<ul>
<li>PCA, t-SNE</li>
<li>Visualize high-dimensional data</li>
<li>Feature extraction</li>
</ul>
</div></div>
<aside class="notes">
<p>Unsupervised methods are useful for quick initial analysis or when you don’t have ground truth labels. Results need interpretation though.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="supervised-vs-unsupervised" class="slide level2 center">
<h2>Supervised vs Unsupervised</h2>
<table class="caption-top">
<colgroup>
<col style="width: 26%">
<col style="width: 34%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Aspect</strong></th>
<th><strong>Supervised</strong></th>
<th><strong>Unsupervised</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Labels</strong></td>
<td>Required</td>
<td>Not needed</td>
</tr>
<tr class="even">
<td><strong>Accuracy</strong></td>
<td>Generally higher</td>
<td>Lower, needs interpretation</td>
</tr>
<tr class="odd">
<td><strong>Use Case</strong></td>
<td>Precise classification</td>
<td>Exploration, pattern discovery</td>
</tr>
<tr class="even">
<td><strong>Effort</strong></td>
<td>High (collecting labels)</td>
<td>Low (no labels)</td>
</tr>
<tr class="odd">
<td><strong>Output</strong></td>
<td>Predefined classes</td>
<td>Discovered clusters</td>
</tr>
<tr class="even">
<td><strong>Control</strong></td>
<td>High (you define classes)</td>
<td>Low (algorithm decides groups)</td>
</tr>
</tbody>
</table>
<aside class="notes">
<p>Supervised methods generally yield more accurate results when good training data is available. Unsupervised is useful when labels are unavailable or for exploratory work.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="which-to-choose" class="slide level2 center">
<h2>Which to Choose?</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Use Supervised When:</strong></p>
<ul>
<li>You have ground truth labels</li>
<li>Need specific classes</li>
<li>Accuracy is critical</li>
<li>Operational application</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Use Unsupervised When:</strong></p>
<ul>
<li>No labels available</li>
<li>Exploratory analysis</li>
<li>Discovering unknown patterns</li>
<li>Quick initial assessment</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>In practice:</strong> Often combine both approaches!</p>
</div>
<aside class="notes">
<p>Most operational EO applications use supervised learning because accuracy and specific class definitions are important. Unsupervised helps with initial exploration.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="minute-break" class="slide level2 center" data-background-color="#7c3aed">
<h2>☕ 5-Minute Break</h2>
<div class="r-fit-text">
<p><strong>Stretch Break</strong></p>
<p>Stand up • Grab water • Back in 5 minutes</p>
</div>
<aside class="notes">
<p><strong>Timing:</strong> 5 minutes</p>
<p><strong>Instructor Actions:</strong> - Announce break - Mention we’ll dive into deep learning next - Be available for quick questions</p>
<p><strong>When Resuming:</strong> - Quick recap: “We’ve covered ML basics, workflow, supervised/unsupervised. Now: deep learning!”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2 center">

</section></section>
<section>
<section id="introduction-to-deep-learning" class="title-slide slide level1 center" data-background-color="#1e3a8a">
<h1>Introduction to Deep Learning</h1>

</section>
<section id="what-is-deep-learning" class="slide level2 center">
<h2>What is Deep Learning?</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><strong>Deep Learning = Neural Networks with Many Layers</strong></p>
<ul>
<li>Subset of machine learning</li>
<li>“Deep” refers to multiple layers</li>
<li>Automatically learns features</li>
<li>Excels at image analysis</li>
<li>Data-hungry</li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="images/deep_learning_layers.png" style="width:100.0%"></p>
</div></div>
<aside class="notes">
<p>Deep learning is essentially about neural networks with many layers (dozens or even hundreds). These “deep” networks can capture very complex relationships.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="neural-networks-building-blocks" class="slide level2 center">
<h2>Neural Networks: Building Blocks</h2>

<img data-src="images/neuron_diagram.png" class="quarto-figure quarto-figure-center r-stretch" style="width:70.0%"><p><strong>Artificial Neuron:</strong></p>
<ol type="1">
<li class="fragment">Takes multiple inputs</li>
<li class="fragment">Multiplies each by a <strong>weight</strong></li>
<li class="fragment">Adds a <strong>bias</strong></li>
<li class="fragment">Applies <strong>activation function</strong></li>
<li class="fragment">Produces output</li>
</ol>
<aside class="notes">
<p>A single neuron is like a logistic regression unit. It takes inputs, applies weights, and uses an activation function to produce an output.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="neural-network-architecture" class="slide level2 center">
<h2>Neural Network Architecture</h2>

<img data-src="images/neural_network_architecture.png" class="quarto-figure quarto-figure-center r-stretch" style="width:80.0%"><div class="columns">
<div class="column" style="width:50%;">
<p><strong>Layers:</strong></p>
<ul>
<li><strong>Input Layer:</strong> Receives data (e.g., pixel values)</li>
<li><strong>Hidden Layers:</strong> Process and transform</li>
<li><strong>Output Layer:</strong> Final prediction</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Connections:</strong></p>
<ul>
<li>Each neuron connects to next layer</li>
<li>Weights on connections</li>
<li>Information flows forward</li>
</ul>
</div></div>
<aside class="notes">
<p>Neurons are organized into layers. The input layer receives data (pixel values), hidden layers progressively extract features, and the output layer makes predictions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="key-concepts" class="slide level2 center">
<h2>Key Concepts</h2>
<p><strong>Activation Functions</strong></p>
<ul>
<li class="fragment">Introduce non-linearity</li>
<li class="fragment">Common: ReLU, Sigmoid, Tanh</li>
<li class="fragment">Allow network to learn complex patterns</li>
</ul>
<p><strong>Weights and Biases</strong></p>
<ul>
<li class="fragment">Parameters the network learns</li>
<li class="fragment">Millions of parameters in deep networks</li>
<li class="fragment">Adjusted during training</li>
</ul>
<p><strong>Forward Propagation</strong></p>
<ul>
<li class="fragment">Data flows input → output</li>
<li class="fragment">Generate prediction</li>
</ul>
<aside class="notes">
<p>Activation functions are crucial - they allow neural networks to learn non-linear relationships. Without them, the network would just be linear regression!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="how-neural-networks-learn" class="slide level2 center">
<h2>How Neural Networks Learn</h2>

<img data-src="images/training_loop.png" class="quarto-figure quarto-figure-center r-stretch" style="width:75.0%"><ol type="1">
<li class="fragment"><strong>Forward pass:</strong> Input data, get prediction</li>
<li class="fragment"><strong>Calculate loss:</strong> How wrong is the prediction?</li>
<li class="fragment"><strong>Backpropagation:</strong> Calculate gradients</li>
<li class="fragment"><strong>Update weights:</strong> Adjust to reduce error</li>
<li class="fragment"><strong>Repeat:</strong> Thousands of times (epochs)</li>
</ol>
<aside class="notes">
<p>Training adjusts weights to minimize error. This happens through backpropagation - computing gradients and updating weights in the direction that reduces loss.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="loss-functions" class="slide level2 center">
<h2>Loss Functions</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Classification</strong></p>
<p><strong>Cross-Entropy Loss</strong></p>
<ul>
<li>Measures classification error</li>
<li>Higher penalty for confident wrong predictions</li>
<li>Standard for multi-class problems</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Regression</strong></p>
<p><strong>Mean Squared Error</strong></p>
<p><span class="math display">\[MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2\]</span></p>
<ul>
<li>Measures prediction error</li>
<li>Squared difference from true value</li>
</ul>
</div></div>
<aside class="notes">
<p>Loss functions quantify “how bad” predictions are. The training process tries to minimize this loss by adjusting weights.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="optimizers" class="slide level2 center">
<h2>Optimizers</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Stochastic Gradient Descent (SGD)</strong></p>
<ul>
<li>Basic optimizer</li>
<li>Updates weights based on gradients</li>
<li>Learning rate controls step size</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Adam Optimizer</strong></p>
<ul>
<li>Adaptive learning rates</li>
<li>Faster convergence</li>
<li>Most popular for deep learning</li>
<li>Generally works well</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>You don’t need to implement these - frameworks do it for you!</strong></p>
</div>
<aside class="notes">
<p>Optimizers determine how weights are updated. Adam is the most popular because it adapts learning rates automatically and generally converges faster than SGD.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="convolutional-neural-networks-cnns" class="slide level2 center">
<h2>Convolutional Neural Networks (CNNs)</h2>

<img data-src="images/cnn_architecture.png" class="quarto-figure quarto-figure-center r-stretch" style="width:85.0%"><p><strong>Specialized for images:</strong></p>
<ul>
<li class="fragment"><strong>Convolutional layers:</strong> Detect spatial patterns</li>
<li class="fragment"><strong>Pooling layers:</strong> Reduce dimensionality</li>
<li class="fragment"><strong>Fully connected layers:</strong> Final classification</li>
<li class="fragment">Automatically learn features (edges, textures, objects)</li>
</ul>
<aside class="notes">
<p>CNNs are neural networks specialized for grid data like images. They use convolutional layers to automatically extract spatial features.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="how-cnns-process-images" class="slide level2 center">
<h2>How CNNs Process Images</h2>

<img data-src="images/cnn_feature_extraction.png" class="quarto-figure quarto-figure-center r-stretch" style="width:80.0%"><p><strong>Hierarchical Feature Learning:</strong></p>
<ul>
<li class="fragment"><strong>Early layers:</strong> Detect edges, simple patterns</li>
<li class="fragment"><strong>Middle layers:</strong> Detect textures, parts</li>
<li class="fragment"><strong>Later layers:</strong> Detect objects, scenes</li>
<li class="fragment"><strong>No manual feature engineering needed!</strong></li>
</ul>
<aside class="notes">
<p>CNNs learn increasingly complex features at each layer. Early layers detect edges, later layers detect whole objects. This happens automatically during training!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="cnns-in-earth-observation" class="slide level2 center">
<h2>CNNs in Earth Observation</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Applications:</strong></p>
<ul>
<li>Image classification</li>
<li>Object detection (ships, buildings)</li>
<li>Semantic segmentation (pixel-wise)</li>
<li>Change detection</li>
<li>Super-resolution</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Advantages:</strong></p>
<ul>
<li>Learn features automatically</li>
<li>Handle spatial context</li>
<li>State-of-the-art performance</li>
<li>Transfer learning possible</li>
</ul>
</div></div>

<img data-src="images/cnn_eo_examples.jpg" class="quarto-figure quarto-figure-center r-stretch" style="width:70.0%"><aside class="notes">
<p>CNNs have achieved state-of-the-art results in many EO tasks. They can learn to recognize complex patterns without manual feature engineering.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="popular-cnn-architectures-for-eo" class="slide level2 center">
<h2>Popular CNN Architectures for EO</h2>
<table class="caption-top">
<colgroup>
<col style="width: 27%">
<col style="width: 15%">
<col style="width: 29%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Architecture</strong></th>
<th><strong>Year</strong></th>
<th><strong>Key Innovation</strong></th>
<th><strong>EO Use Cases</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>ResNet</strong></td>
<td>2015</td>
<td>Residual connections</td>
<td>Classification, backbone for detection</td>
</tr>
<tr class="even">
<td><strong>U-Net</strong></td>
<td>2015</td>
<td>Skip connections</td>
<td>Semantic segmentation, flood mapping</td>
</tr>
<tr class="odd">
<td><strong>EfficientNet</strong></td>
<td>2019</td>
<td>Compound scaling</td>
<td>Efficient classification, mobile deployment</td>
</tr>
<tr class="even">
<td><strong>DeepLabv3+</strong></td>
<td>2018</td>
<td>Atrous convolution</td>
<td>Land cover segmentation</td>
</tr>
<tr class="odd">
<td><strong>YOLOv8</strong></td>
<td>2023</td>
<td>Real-time detection</td>
<td>Object detection, ship/vehicle counting</td>
</tr>
</tbody>
</table>
<div class="fragment">
<p><strong>ResNet and EfficientNet are most popular backbones for EO</strong></p>
</div>
<aside class="notes">
<p>These are proven architectures widely used in EO. ResNet-50 is often the starting point for transfer learning. U-Net dominates semantic segmentation tasks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="resnet-residual-networks" class="slide level2 center">
<h2>ResNet: Residual Networks</h2>

<img data-src="images/resnet_residual_block.png" class="quarto-figure quarto-figure-center r-stretch" style="width:70.0%"><div class="columns">
<div class="column" style="width:60%;">
<p><strong>Key Innovation: Skip Connections</strong></p>
<ul>
<li>Allows training very deep networks (50, 101, 152 layers)</li>
<li>Solves vanishing gradient problem</li>
<li>Identity mapping preserves information</li>
</ul>
<p><strong>Common Variants:</strong> - ResNet-50 (25M parameters) - ResNet-101 (44M parameters) - ResNet-152 (60M parameters)</p>
</div><div class="column" style="width:40%;">
<p><strong>EO Applications:</strong></p>
<ul>
<li>Pre-trained on ImageNet</li>
<li>Fine-tune for EO tasks</li>
<li>Backbone for object detection</li>
<li>Transfer learning baseline</li>
</ul>
<p><strong>Performance:</strong> - Top-5 error: 3.57% (ImageNet) - Works well with 10k+ images</p>
</div></div>
<aside class="notes">
<p>ResNet revolutionized deep learning by enabling training of very deep networks. Skip connections allow gradients to flow directly through the network.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="u-net-for-semantic-segmentation" class="slide level2 center">
<h2>U-Net for Semantic Segmentation</h2>

<img data-src="images/unet_architecture.png" class="quarto-figure quarto-figure-center r-stretch" style="width:80.0%"><p><strong>Architecture:</strong> - <strong>Encoder (contracting path):</strong> Captures context - <strong>Decoder (expanding path):</strong> Enables precise localization - <strong>Skip connections:</strong> Combine low &amp; high-level features</p>
<p><strong>Why Dominant in EO:</strong> - Works with small datasets (hundreds of images) - Precise pixel-wise predictions - Perfect for segmentation tasks</p>
<div class="fragment">
<p><strong>EO Applications:</strong> Flood mapping, land cover, building footprints, crop fields</p>
</div>
<aside class="notes">
<p>U-Net is THE architecture for semantic segmentation in EO. Originally designed for biomedical image segmentation, it’s now standard for pixel-wise classification tasks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="deep-learning-frameworks" class="slide level2 center">
<h2>Deep Learning Frameworks</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>TensorFlow / Keras</strong></p>
<p><img data-src="images/tensorflow_logo.png" width="150"></p>
<ul>
<li>Google’s framework</li>
<li>High-level Keras API</li>
<li>Production-ready</li>
<li>Large ecosystem</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>PyTorch</strong></p>
<p><img data-src="images/pytorch_logo.png" width="150"></p>
<ul>
<li>Facebook’s framework</li>
<li>Pythonic and intuitive</li>
<li>Popular in research</li>
<li>Flexible</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>We’ll use TensorFlow/Keras in this training</strong></p>
</div>
<aside class="notes">
<p>You don’t implement backpropagation yourself - frameworks like TensorFlow and PyTorch handle the math. You just define the architecture and provide data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="deep-learning-considerations" class="slide level2 center">
<h2>Deep Learning Considerations</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Advantages:</strong></p>
<ul>
<li>Automatic feature learning</li>
<li>State-of-the-art accuracy</li>
<li>Handles complex patterns</li>
<li>Scales to big data</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Challenges:</strong></p>
<ul>
<li>Requires lots of training data</li>
<li>Computationally intensive (need GPUs)</li>
<li>Less interpretable (“black box”)</li>
<li>Harder to debug</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>Start simple (Random Forest), move to DL when you have data and compute</strong></p>
</div>
<aside class="notes">
<p>Deep learning is powerful but data-hungry and computationally expensive. For many EO tasks, simpler models like Random Forest work well with less data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="benchmark-datasets-for-eo" class="title-slide slide level1 center" data-background-color="#0f766e">
<h1>Benchmark Datasets for EO</h1>

</section>
<section id="why-benchmark-datasets-matter" class="slide level2 center">
<h2>Why Benchmark Datasets Matter</h2>
<ol type="1">
<li class="fragment"><strong>Standardized Evaluation</strong> - Compare algorithms objectively</li>
<li class="fragment"><strong>Training Resources</strong> - Pre-labeled data for model training</li>
<li class="fragment"><strong>Transfer Learning</strong> - Pre-train on large datasets, fine-tune locally</li>
<li class="fragment"><strong>Research Reproducibility</strong> - Enable comparison across studies</li>
<li class="fragment"><strong>Community Building</strong> - Shared resources accelerate progress</li>
</ol>
<div class="fragment">
<p><strong>You don’t need to label everything from scratch!</strong></p>
</div>
<aside class="notes">
<p>Benchmark datasets are crucial for EO ML. They provide labeled training data and enable fair comparison of methods across research groups worldwide.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="eurosat-land-cover-classification" class="slide level2 center">
<h2>EuroSAT: Land Cover Classification</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><strong>Specifications:</strong> - <strong>Images:</strong> 27,000 labeled patches - <strong>Classes:</strong> 10 land cover types - <strong>Size:</strong> 64×64 pixels - <strong>Bands:</strong> All 13 Sentinel-2 bands - <strong>Source:</strong> European cities</p>
<p><strong>10 Classes:</strong> Annual Crop • Forest • Herbaceous Vegetation • Highway • Industrial • Pasture • Permanent Crop • Residential • River • Sea/Lake</p>
</div><div class="column" style="width:40%;">
<p><img data-src="images/eurosat_classes.png" style="width:100.0%"></p>
<p><strong>Achievement:</strong> <strong>98.57% accuracy</strong> with CNNs</p>
<p><strong>Why Popular:</strong> - Sentinel-2 based - Balanced classes - Easy to use</p>
</div></div>
<aside class="notes">
<p>EuroSAT is one of the most popular benchmarks for EO classification. Based on Sentinel-2, making it highly relevant for operational applications. Great starting point for CNN experiments.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="bigearthnet-large-scale-multi-label" class="slide level2 center">
<h2>BigEarthNet: Large-Scale Multi-Label</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Massive Scale:</strong> - <strong>Images:</strong> 590,326 Sentinel-2 patches - <strong>Coverage:</strong> 10 European countries - <strong>Labels:</strong> 43 land cover classes - <strong>Multi-label:</strong> Multiple classes per image - <strong>Multi-modal:</strong> Optical + SAR version</p>
<p><strong>Real-World Complexity:</strong> - Forest + Water - Urban + Agricultural - Reflects actual landscapes</p>
</div><div class="column" style="width:50%;">
<p><img data-src="images/bigearth_multilabel.jpg" style="width:100.0%"></p>
<p><strong>Why Different:</strong></p>
<p>Unlike EuroSAT (single label), BigEarthNet has multiple overlapping classes - more realistic!</p>
<p><strong>Access:</strong> - bigearth.net - TensorFlow Datasets - Papers With Code</p>
</div></div>
<aside class="notes">
<p>BigEarthNet’s multi-label nature makes it more challenging but also more realistic. Essential for semantic segmentation research and testing advanced architectures.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="xview-object-detection-benchmark" class="slide level2 center">
<h2>xView: Object Detection Benchmark</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Specifications:</strong> - <strong>Objects:</strong> &gt;1 million annotated - <strong>Classes:</strong> 60 object types - <strong>Resolution:</strong> 0.3m (WorldView-3) - <strong>Area:</strong> &gt;1,400 km² - <strong>Annotations:</strong> Bounding boxes</p>
<p><strong>Object Categories:</strong> - Buildings &amp; infrastructure - Vehicles (cars, trucks, aircraft) - Ships &amp; maritime - Storage tanks - Construction equipment</p>
</div><div class="column" style="width:50%;">
<p><img data-src="images/xview_samples.jpg" style="width:100.0%"></p>
<p><strong>Created for disaster response</strong></p>
<p><strong>Applications:</strong> - YOLO training - Faster R-CNN - Small object detection - Infrastructure mapping</p>
</div></div>
<aside class="notes">
<p>xView is THE benchmark for object detection in satellite imagery. Created for disaster response applications, now widely used for testing detection algorithms like YOLO and Faster R-CNN.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="philippine-data-resources" class="slide level2 center">
<h2>Philippine Data Resources</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>PRiSM (PhilRice)</strong> - Rice area maps (wet/dry season) - Planting dates &amp; growth stages - Yield estimates - Since 2014 - https://prism.philrice.gov.ph/</p>
<p><strong>PhilSA Products</strong> - Flood extent maps (DATOS) - Mangrove extent mapping - Land cover classifications - Disaster damage assessments</p>
</div><div class="column" style="width:50%;">
<p><strong>DOST-ASTI Outputs</strong> - DATOS rapid flood mapping - Hazard susceptibility maps - AI-powered damage assessment - hazardhunter.georisk.gov.ph</p>
<p><strong>NAMRIA Geoportal</strong> - National land cover (2020) - Topographic basemaps - Administrative boundaries - Digital Elevation Models - www.geoportal.gov.ph</p>
</div></div>
<div class="fragment">
<p><strong>Use these as training/validation data - don’t start from scratch!</strong></p>
</div>
<aside class="notes">
<p>Philippine agencies have produced operational EO products that can serve as training or validation data for your ML models. Leverage existing work!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="data-centric-ai-2025-innovations" class="title-slide slide level1 center" data-background-color="#1e3a8a">
<h1>Data-Centric AI &amp; 2025 Innovations</h1>

</section>
<section id="paradigm-shift-model-centric-vs-data-centric" class="slide level2 center">
<h2>Paradigm Shift: Model-Centric vs Data-Centric</h2>

<img data-src="images/model_vs_data_centric.png" class="quarto-figure quarto-figure-center r-stretch" style="width:80.0%"><div class="columns">
<div class="column" style="width:50%;">
<p><strong>Model-Centric (Traditional)</strong></p>
<ul>
<li>Focus on improving algorithms</li>
<li>Keep data fixed</li>
<li>Try different models</li>
<li>Tune hyperparameters</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Data-Centric (Modern)</strong></p>
<ul>
<li>Focus on improving data</li>
<li>Keep model fixed</li>
<li>Clean and augment data</li>
<li>Better annotations</li>
</ul>
</div></div>
<aside class="notes">
<p>A lot of early ML progress focused on model algorithms. Data-Centric AI, popularized by Andrew Ng, advocates that improving your data often yields bigger gains.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="why-data-centric-matters-for-eo" class="slide level2 center">
<h2>Why Data-Centric Matters for EO</h2>
<p><strong>EO-Specific Data Challenges:</strong></p>
<ul>
<li class="fragment">Cloud contamination</li>
<li class="fragment">Atmospheric effects</li>
<li class="fragment">Sensor artifacts and noise</li>
<li class="fragment">Label uncertainty</li>
<li class="fragment">Geographic variability</li>
<li class="fragment">Temporal dynamics</li>
<li class="fragment">Class imbalance</li>
</ul>
<div class="fragment">
<p><strong>“Better data beats a cleverer model” in most cases</strong></p>
</div>
<aside class="notes">
<p>In EO, the “food” you feed your AI matters more than fancy model tweaks. Cloudy images, mislabeled points, or biased samples can derail any model.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="research-data-efficiency" class="slide level2 center">
<h2>2025 Research: Data Efficiency</h2>

<img data-src="images/data_efficiency_research.png" class="quarto-figure quarto-figure-center r-stretch" style="width:70.0%"><p><strong>Key Finding (ArXiv 2025):</strong></p>
<ul>
<li class="fragment">Some EO datasets reach <strong>optimal accuracy with &lt;20% of temporal instances</strong></li>
<li class="fragment"><strong>Single band</strong> from single modality can be sufficient</li>
<li class="fragment">Data efficiency crucial for operational systems</li>
<li class="fragment">Quality over quantity</li>
</ul>
<aside class="notes">
<p>Recent research shows you don’t always need all available data. Smart selection of temporal instances and bands can achieve similar accuracy with much less data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="four-pillars-of-data-centric-ai" class="slide level2 center">
<h2>Four Pillars of Data-Centric AI</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>1. Data Quality</strong></p>
<p><img data-src="images/data_quality.jpg" style="width:100.0%"></p>
<ul>
<li>Cloud/shadow removal</li>
<li>Atmospheric correction</li>
<li>Sensor calibration</li>
<li>Geometric accuracy</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>2. Data Quantity</strong></p>
<p><img data-src="images/data_quantity.jpg" style="width:100.0%"></p>
<ul>
<li>Sufficient training samples</li>
<li>Balanced classes</li>
<li>Data augmentation</li>
<li>Transfer learning</li>
</ul>
</div></div>
</section>
<section id="four-pillars-continued" class="slide level2 center">
<h2>Four Pillars (Continued)</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>3. Data Diversity</strong></p>
<p><img data-src="images/data_diversity.jpg" style="width:100.0%"></p>
<ul>
<li>Multiple seasons</li>
<li>Different regions</li>
<li>Various conditions</li>
<li>Class variations</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>4. Label Quality</strong></p>
<p><img data-src="images/label_quality.jpg" style="width:100.0%"></p>
<ul>
<li>Clear definitions</li>
<li>Consistent protocols</li>
<li>Expert validation</li>
<li>Accurate geolocation</li>
</ul>
</div></div>
<aside class="notes">
<p>These four aspects - quality, quantity, diversity, and labels - determine model success more than architectural choices.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="data-quality" class="title-slide slide level1 center" data-background-color="#0f766e">
<h1>Data Quality</h1>

</section>
<section id="data-quality-in-eo" class="slide level2 center">
<h2>Data Quality in EO</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Common Issues:</strong></p>
<ul>
<li>Clouds and shadows</li>
<li>Haze and aerosols</li>
<li>Sensor artifacts (striping, banding)</li>
<li>Geometric misalignment</li>
<li>Radiometric inconsistencies</li>
<li>Mixed pixels at boundaries</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Solutions:</strong></p>
<ul>
<li>Use Level-2A products</li>
<li>Rigorous cloud masking</li>
<li>Quality flag filtering</li>
<li>Multi-temporal compositing</li>
<li>Validation checks</li>
<li>Document preprocessing</li>
</ul>
</div></div>
<aside class="notes">
<p>Satellite data can be noisy. Cloud masking, using atmospherically corrected products, and careful preprocessing are essential.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="quality-example-cloud-masking" class="slide level2 center">
<h2>Quality Example: Cloud Masking</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Without Cloud Masking</strong></p>
<p><img data-src="images/with_clouds.jpg" style="width:100.0%"></p>
<ul>
<li>Clouds misclassified</li>
<li>Shadows cause errors</li>
<li>Poor model performance</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>With Proper Masking</strong></p>
<p><img data-src="images/clouds_masked.jpg" style="width:100.0%"></p>
<ul>
<li>Clean training data</li>
<li>Accurate classifications</li>
<li>Better generalization</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>One cloudy image can ruin your training data!</strong></p>
</div>
<aside class="notes">
<p>Even a few cloudy training samples can teach the model wrong patterns. Rigorous cloud masking is non-negotiable.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="data-quantity" class="title-slide slide level1 center" data-background-color="#0f766e">
<h1>Data Quantity</h1>

</section>
<section id="how-much-data-do-you-need" class="slide level2 center">
<h2>How Much Data Do You Need?</h2>
<p><strong>Depends on:</strong></p>
<ul>
<li class="fragment">Model complexity (DL needs more)</li>
<li class="fragment">Problem difficulty</li>
<li class="fragment">Class separability</li>
<li class="fragment">Available features</li>
</ul>
<p><strong>General Guidelines:</strong></p>
<ul>
<li class="fragment"><strong>Traditional ML:</strong> 100s to 1000s of samples per class</li>
<li class="fragment"><strong>Deep Learning:</strong> 1000s to 10,000s per class</li>
<li class="fragment"><strong>Transfer Learning:</strong> Can work with 100s per class</li>
</ul>
<aside class="notes">
<p>Deep learning is data-hungry. Random Forest can work with smaller datasets. Transfer learning (starting from pre-trained models) reduces data requirements.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="data-augmentation" class="slide level2 center">
<h2>Data Augmentation</h2>

<img data-src="images/data_augmentation.jpg" class="quarto-figure quarto-figure-center r-stretch" style="width:80.0%"><p><strong>Techniques:</strong></p>
<ul>
<li class="fragment">Rotation (90°, 180°, 270°)</li>
<li class="fragment">Flipping (horizontal, vertical)</li>
<li class="fragment">Brightness/contrast adjustment</li>
<li class="fragment">Adding noise</li>
<li class="fragment">Elastic deformations</li>
</ul>
<div class="fragment">
<p><strong>Result:</strong> 10x more training samples from existing data!</p>
</div>
<aside class="notes">
<p>Data augmentation synthetically increases dataset size by creating modified versions of existing samples. This helps models generalize better.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="transfer-learning" class="slide level2 center">
<h2>Transfer Learning</h2>

<img data-src="images/transfer_learning.png" class="quarto-figure quarto-figure-center r-stretch" style="width:75.0%"><div class="columns">
<div class="column" style="width:50%;">
<p><strong>Concept:</strong></p>
<ul>
<li>Start with model pre-trained on large dataset</li>
<li>Fine-tune on your specific task</li>
<li>Requires much less data</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>EO Applications:</strong></p>
<ul>
<li>Use ImageNet pre-trained models</li>
<li>NASA-IBM Geospatial Foundation Model</li>
<li>Domain-specific pre-training</li>
</ul>
</div></div>
<aside class="notes">
<p>Transfer learning leverages knowledge learned on large datasets and adapts it to your specific problem with much less data.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="data-diversity" class="title-slide slide level1 center" data-background-color="#0f766e">
<h1>Data Diversity</h1>

</section>
<section id="why-diversity-matters" class="slide level2 center">
<h2>Why Diversity Matters</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Problem: Biased Training</strong></p>
<p><img data-src="images/biased_training.jpg" style="width:100.0%"></p>
<ul>
<li>All samples from one season</li>
<li>One geographic region only</li>
<li>Similar conditions</li>
<li><strong>Result:</strong> Model fails elsewhere</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Solution: Diverse Training</strong></p>
<p><img data-src="images/diverse_training.jpg" style="width:100.0%"></p>
<ul>
<li>Multiple seasons</li>
<li>Different regions</li>
<li>Various conditions</li>
<li><strong>Result:</strong> Model generalizes</li>
</ul>
</div></div>
<aside class="notes">
<p>Models trained on narrow datasets often fail when deployed in different conditions. Diversity in training data leads to better generalization.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="sources-of-diversity-needed" class="slide level2 center">
<h2>Sources of Diversity Needed</h2>
<p><strong>Temporal Diversity:</strong></p>
<ul>
<li class="fragment">Different seasons (wet/dry)</li>
<li class="fragment">Multiple years</li>
<li class="fragment">Phenological stages</li>
</ul>
<p><strong>Geographic Diversity:</strong></p>
<ul>
<li class="fragment">Different regions</li>
<li class="fragment">Various elevations</li>
<li class="fragment">Coastal vs inland</li>
</ul>
<p><strong>Atmospheric Diversity:</strong></p>
<ul>
<li class="fragment">Clear vs hazy days</li>
<li class="fragment">Different solar angles</li>
<li class="fragment">Seasonal lighting</li>
</ul>
<p><strong>Class Diversity:</strong></p>
<ul>
<li class="fragment">Variations within classes</li>
<li class="fragment">Edge cases</li>
<li class="fragment">Transitional zones</li>
</ul>
<aside class="notes">
<p>For robust models, ensure your training data covers the range of conditions the model will encounter in operational use.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="example-urban-classification" class="slide level2 center">
<h2>Example: Urban Classification</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Poor Diversity</strong></p>
<ul>
<li>Only Metro Manila samples</li>
<li>Only concrete roofs</li>
<li>Only high-density areas</li>
<li><strong>Fails</strong> in other cities</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Good Diversity</strong></p>
<ul>
<li>Large cities, small towns</li>
<li>Various roof materials (concrete, metal, nipa)</li>
<li>Different architectural styles</li>
<li>Different densities</li>
<li><strong>Works</strong> across Philippines</li>
</ul>
</div></div>
<aside class="notes">
<p>If training only on Metro Manila, the model might not recognize small towns or rural settlements with different characteristics.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="label-quality" class="title-slide slide level1 center" data-background-color="#0f766e">
<h1>Label Quality</h1>

</section>
<section id="label-quality-is-critical" class="slide level2 center">
<h2>Label Quality is Critical</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Common Label Issues:</strong></p>
<ul>
<li>Mislabeled samples</li>
<li>Positional errors (GPS drift)</li>
<li>Temporal mismatch (old labels, new image)</li>
<li>Ambiguous classes</li>
<li>Inconsistent definitions</li>
<li>Mixed pixels</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Impact:</strong></p>
<ul>
<li>Model learns wrong patterns</li>
<li>Contradictory signals</li>
<li>Poor generalization</li>
<li>Low confidence predictions</li>
<li>Wasted compute</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>One bad label can corrupt model learning!</strong></p>
</div>
<aside class="notes">
<p>Ground truth labels might have errors - GPS inaccuracy, outdated information, or human mistakes. These errors propagate to model predictions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="label-quality-best-practices" class="slide level2 center">
<h2>Label Quality Best Practices</h2>
<p><strong>1. Clear Class Definitions</strong></p>
<ul>
<li class="fragment">Write explicit criteria</li>
<li class="fragment">Provide examples</li>
<li class="fragment">Define edge cases</li>
<li class="fragment">Document ambiguities</li>
</ul>
<p><strong>2. Consistent Protocols</strong></p>
<ul>
<li class="fragment">Standard operating procedures</li>
<li class="fragment">Same interpretation rules</li>
<li class="fragment">Calibration sessions</li>
<li class="fragment">Regular training for labelers</li>
</ul>
<p><strong>3. Multiple Annotators</strong></p>
<ul>
<li class="fragment">Independent labeling</li>
<li class="fragment">Compare for consistency</li>
<li class="fragment">Resolve disagreements</li>
<li class="fragment">Build consensus labels</li>
</ul>
<p><strong>4. Expert Validation</strong></p>
<ul>
<li class="fragment">Domain experts review samples</li>
<li class="fragment">Random quality checks</li>
<li class="fragment">Iterative improvement</li>
</ul>
<aside class="notes">
<p>Invest time in defining classes clearly and training labelers. Consistency matters more than speed.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="label-quality-example" class="slide level2 center">
<h2>Label Quality Example</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Poor Labels</strong></p>
<p><img data-src="images/poor_labels.jpg" style="width:100.0%"></p>
<ul>
<li>“Forest” defined inconsistently</li>
<li>Mixed with shrubland</li>
<li>Temporal mismatch</li>
<li>Positional errors</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>High-Quality Labels</strong></p>
<p><img data-src="images/good_labels.jpg" style="width:100.0%"></p>
<ul>
<li>Clear forest definition</li>
<li>Careful boundary delineation</li>
<li>Image-label temporal match</li>
<li>Validated position</li>
</ul>
</div></div>
<aside class="notes">
<p>High-quality labels are worth the effort. A smaller dataset with accurate labels often outperforms a larger dataset with noisy labels.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="alam-project-addressing-labels" class="slide level2 center">
<h2>ALaM Project: Addressing Labels</h2>

<img data-src="images/alam_project.jpg" class="quarto-figure quarto-figure-center r-stretch" style="width:70.0%"><p><strong>DOST-ASTI’s Automated Labeling Machine</strong></p>
<ul>
<li class="fragment">Automates labeling process</li>
<li class="fragment">Crowdsourcing capabilities</li>
<li class="fragment">Expert validation workflow</li>
<li class="fragment"><strong>Addresses EO’s biggest bottleneck</strong></li>
</ul>
<aside class="notes">
<p>Remember from Session 1: DOST-ASTI’s ALaM project specifically addresses the label quality and quantity challenge through automation and crowdsourcing.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="practical-data-centric-tips" class="title-slide slide level1 center" data-background-color="#0f766e">
<h1>Practical Data-Centric Tips</h1>

</section>
<section id="data-centric-workflow" class="slide level2 center">
<h2>Data-Centric Workflow</h2>
<p><strong>Before Training:</strong></p>
<ol type="1">
<li class="fragment"><strong>Audit your data:</strong> Visualize samples, check distributions</li>
<li class="fragment"><strong>Clean aggressively:</strong> Remove clouds, fix labels, filter outliers</li>
<li class="fragment"><strong>Balance classes:</strong> Address imbalances through sampling or augmentation</li>
<li class="fragment"><strong>Document everything:</strong> Track data sources, preprocessing, versions</li>
</ol>
<p><strong>During Training:</strong></p>
<ol start="5" type="1">
<li class="fragment"><strong>Analyze errors:</strong> Which samples does model get wrong?</li>
<li class="fragment"><strong>Identify patterns:</strong> Are errors systematic? (e.g., all in one region)</li>
<li class="fragment"><strong>Fix data:</strong> Add more diverse samples, improve labels</li>
<li class="fragment"><strong>Iterate:</strong> Retrain with better data</li>
</ol>
<aside class="notes">
<p>Data-centric approach means continuously improving data quality based on model feedback. Look at errors to understand what data you’re missing.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="data-quality-checklist" class="slide level2 center">
<h2>Data Quality Checklist</h2>
<ul class="task-list">
<li class="fragment"><label><input type="checkbox">Atmospherically corrected (Level-2A)?</label></li>
<li class="fragment"><label><input type="checkbox">Clouds and shadows masked?</label></li>
<li class="fragment"><label><input type="checkbox">Geometric alignment verified?</label></li>
<li class="fragment"><label><input type="checkbox">Temporal consistency checked?</label></li>
<li class="fragment"><label><input type="checkbox">Label accuracy validated?</label></li>
<li class="fragment"><label><input type="checkbox">Classes clearly defined?</label></li>
<li class="fragment"><label><input type="checkbox">Training data balanced?</label></li>
<li class="fragment"><label><input type="checkbox">Geographic diversity ensured?</label></li>
<li class="fragment"><label><input type="checkbox">Seasonal coverage adequate?</label></li>
<li class="fragment"><label><input type="checkbox">Edge cases included?</label></li>
<li class="fragment"><label><input type="checkbox">Quality flags documented?</label></li>
</ul>
<aside class="notes">
<p>Use this checklist before training any model. Addressing data issues upfront saves time and improves results.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="case-study-better-data-better-results" class="slide level2 center">
<h2>Case Study: Better Data = Better Results</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Scenario:</strong></p>
<p>Coral reef mapping project</p>
<p><strong>Initial Results:</strong></p>
<ul>
<li>70% accuracy</li>
<li>Fails in turbid water</li>
<li>Confuses reef with sand</li>
</ul>
<p><strong>Problem Identified:</strong></p>
<p>All training data from clear water</p>
</div><div class="column" style="width:50%;">
<p><strong>Data-Centric Solution:</strong></p>
<ol type="1">
<li>Add turbid water samples</li>
<li>Include reef-sand transition zones</li>
<li>More diverse depths</li>
<li>Improve label precision</li>
</ol>
<p><strong>New Results:</strong></p>
<ul>
<li><strong>90% accuracy</strong></li>
<li>Works in turbid water</li>
<li>Better boundary detection</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>10x improvement from better data, same model!</strong></p>
</div>
<aside class="notes">
<p>Real example of how data improvements had bigger impact than model tuning. The data was the key, not the algorithm.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="developments" class="title-slide slide level1 center" data-background-color="#1e3a8a">
<h1>2025 Developments</h1>

</section>
<section id="foundation-models-for-eo" class="slide level2 center">
<h2>Foundation Models for EO</h2>

<img data-src="images/foundation_models.png" class="quarto-figure quarto-figure-center r-stretch" style="width:75.0%"><p><strong>What are Foundation Models?</strong></p>
<ul>
<li class="fragment">Large models pre-trained on massive EO datasets</li>
<li class="fragment">Learn general representations</li>
<li class="fragment">Fine-tune for specific tasks</li>
<li class="fragment"><strong>Dramatically reduce labeled data needs</strong></li>
</ul>
<p><strong>Examples (2025):</strong></p>
<ul>
<li class="fragment"><strong>Google AlphaEarth Foundations</strong> (DeepMind, 2025) - 1.4 trillion embeddings/year in GEE</li>
<li class="fragment"><strong>NASA-IBM Geospatial Foundation Model</strong> (open-source, Aug 2024)</li>
<li class="fragment"><strong>Prithvi</strong> (IBM/NASA/ESA collaboration)</li>
<li class="fragment"><strong>Clay Foundation Model</strong> (open-source)</li>
<li class="fragment">Planet Labs + Anthropic Claude integration</li>
</ul>
<aside class="notes">
<p><strong>Timing:</strong> 4 minutes</p>
<p><strong>Key Points:</strong> - <strong>2025 Update:</strong> Foundation models are THE major innovation in EO AI - <strong>Google AlphaEarth Foundations:</strong> Virtual satellite model, 10x10m resolution, integrates Sentinel-1/2 + Landsat + radar, available in Earth Engine, 16x less storage than other AI systems - NASA-IBM model released August 2024 as open-source - Trained on massive Sentinel-2 datasets (1 billion parameters) - Can be fine-tuned with just hundreds of labeled samples (vs thousands before) - <strong>Philippine Application:</strong> Use foundation models to jumpstart projects with limited labeled data - AlphaEarth embeddings already in GEE!</p>
<p><strong>Example:</strong> “Instead of manually labeling 10,000 images for rice mapping, use AlphaEarth embeddings in GEE or fine-tune Prithvi with just 500 samples and achieve similar accuracy”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="on-board-ai-processing" class="slide level2 center">
<h2>On-Board AI Processing</h2>

<img data-src="images/onboard_ai.jpg" class="quarto-figure quarto-figure-center r-stretch" style="width:70.0%"><div class="columns">
<div class="column" style="width:50%;">
<p><strong>ESA Φsat-2 (Launched 2024)</strong></p>
<ul>
<li>22×10×33 cm CubeSat</li>
<li>Onboard AI computer (Intel Myriad X VPU)</li>
<li>Real-time cloud detection</li>
<li>Process before downlink</li>
<li><strong>Saves bandwidth</strong></li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Satellogic Edge Computing</strong></p>
<ul>
<li>“AI First” satellites</li>
<li>Onboard GPUs</li>
<li>Real-time processing</li>
<li>Immediate insights</li>
<li>Ship/object detection</li>
</ul>
</div></div>
<aside class="notes">
<p><strong>Timing:</strong> 3 minutes</p>
<p><strong>Key Points:</strong> - <strong>2025 Update:</strong> On-board AI is operational on multiple satellites - ESA’s Φsat-2 launched 2024 with Intel AI chip - Processes images on-orbit before transmitting - Use case: Only download cloud-free portions, save 90% bandwidth - Future: Real-time disaster detection from space</p>
<p><strong>Philippine Relevance:</strong> “Imagine typhoon damage detected and reported automatically from orbit within minutes, not hours”</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="self-supervised-learning" class="slide level2 center">
<h2>Self-Supervised Learning</h2>
<div class="columns">
<div class="column" style="width:60%;">
<p><strong>Concept:</strong></p>
<ul>
<li>Learn from <strong>unlabeled data</strong></li>
<li>Define pretext tasks (e.g., predict missing patches)</li>
<li>Model learns useful representations</li>
<li>Fine-tune with small labeled dataset</li>
</ul>
<p><strong>Why Important for EO:</strong></p>
<ul>
<li>Abundance of unlabeled satellite imagery</li>
<li>High cost of labeling</li>
<li>Improves transferability</li>
</ul>
</div><div class="column" style="width:40%;">
<p><img data-src="images/self_supervised.png" style="width:100.0%"></p>
</div></div>
<aside class="notes">
<p>Self-supervised learning is particularly relevant for EO due to abundance of unlabeled imagery. Models learn useful features without expensive labeling.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="explainable-ai-xai" class="slide level2 center">
<h2>Explainable AI (XAI)</h2>

<img data-src="images/xai_methods.png" class="quarto-figure quarto-figure-center r-stretch" style="width:75.0%"><p><strong>Why XAI Matters:</strong></p>
<ul>
<li class="fragment">Understand model decisions</li>
<li class="fragment">Build trust in AI systems</li>
<li class="fragment">Debug and improve models</li>
<li class="fragment">Regulatory compliance</li>
</ul>
<p><strong>Methods:</strong></p>
<ul>
<li class="fragment"><strong>SHAP:</strong> Feature importance</li>
<li class="fragment"><strong>LIME:</strong> Local explanations</li>
<li class="fragment"><strong>Grad-CAM:</strong> Visual attention maps</li>
<li class="fragment"><strong>Saliency Maps:</strong> What pixels matter?</li>
</ul>
<aside class="notes">
<p>As AI systems make important decisions (disaster response, resource allocation), understanding why they make those decisions becomes crucial.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="summary-key-takeaways" class="title-slide slide level1 center" data-background-color="#1e3a8a">
<h1>Summary &amp; Key Takeaways</h1>

</section>
<section id="what-we-covered" class="slide level2 center">
<h2>What We Covered</h2>
<ol type="1">
<li class="fragment"><strong>AI/ML Basics:</strong> What it is and why it’s powerful for EO</li>
<li class="fragment"><strong>ML Workflow:</strong> 7-step process from problem to deployment</li>
<li class="fragment"><strong>Supervised Learning:</strong> Classification and regression with labeled data</li>
<li class="fragment"><strong>Unsupervised Learning:</strong> Clustering and pattern discovery</li>
<li class="fragment"><strong>Deep Learning:</strong> Neural networks and CNNs for images</li>
<li class="fragment"><strong>Data-Centric AI:</strong> Quality, quantity, diversity, labels</li>
<li class="fragment"><strong>2025 Trends:</strong> Foundation models, on-board AI, XAI</li>
</ol>
<aside class="notes">
<p>We’ve covered the fundamental concepts you need to understand before diving into hands-on implementation.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="key-takeaways" class="slide level2 center">
<h2>Key Takeaways</h2>
<p><strong>1. Focus on Data First</strong></p>
<ul>
<li class="fragment">Quality beats quantity</li>
<li class="fragment">Diversity enables generalization</li>
<li class="fragment">Good labels are gold</li>
</ul>
<p><strong>2. Start Simple</strong></p>
<ul>
<li class="fragment">Try traditional ML before deep learning</li>
<li class="fragment">Random Forest is often enough</li>
<li class="fragment">Add complexity only when needed</li>
</ul>
<p><strong>3. Iterate Continuously</strong></p>
<ul>
<li class="fragment">Analyze errors</li>
<li class="fragment">Improve data</li>
<li class="fragment">Retrain models</li>
<li class="fragment">Deployment is not the end</li>
</ul>
<aside class="notes">
<p>These principles will serve you well throughout your AI/ML journey. Data quality and iterative improvement are more important than fancy algorithms.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="practical-advice" class="slide level2 center">
<h2>Practical Advice</h2>
<p><strong>For Your Projects:</strong></p>
<ul>
<li class="fragment"><strong>Define the problem clearly</strong> before collecting data</li>
<li class="fragment"><strong>Invest in high-quality training data</strong> - it’s worth it</li>
<li class="fragment"><strong>Validate rigorously</strong> on independent data</li>
<li class="fragment"><strong>Document everything</strong> (data sources, preprocessing, model versions)</li>
<li class="fragment"><strong>Start with baselines</strong> (simple models, existing methods)</li>
<li class="fragment"><strong>Iterate based on errors</strong> - let failures guide improvements</li>
<li class="fragment"><strong>Consider operational constraints</strong> early</li>
</ul>
<aside class="notes">
<p>These practical tips come from real-world experience. Following them will save you time and frustration.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-data-centric-mindset" class="slide level2 center">
<h2>The Data-Centric Mindset</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>When model performs poorly, ask:</strong></p>
<ol type="1">
<li>Is my data clean?</li>
<li>Are labels accurate?</li>
<li>Is training data representative?</li>
<li>Do I have enough diversity?</li>
<li>Are there systematic biases?</li>
</ol>
</div><div class="column" style="width:50%;">
<p><strong>Before trying:</strong></p>
<ul>
<li>More complex model</li>
<li>More epochs</li>
<li>Different hyperparameters</li>
<li>New architecture</li>
</ul>
<p><strong>Check your data first!</strong></p>
</div></div>
<div class="fragment">
<p><strong>“Better data beats a cleverer model” - Andrew Ng</strong></p>
</div>
<aside class="notes">
<p>Adopt a data-centric mindset. When models underperform, investigate data issues before blaming the algorithm.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="connection-to-sessions-3-4" class="slide level2 center">
<h2>Connection to Sessions 3 &amp; 4</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Session 3: Python Basics</strong></p>
<ul>
<li>Load and explore data</li>
<li>GeoPandas (vector)</li>
<li>Rasterio (raster)</li>
<li><strong>Foundation for all ML work</strong></li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Session 4: Google Earth Engine</strong></p>
<ul>
<li>Access Sentinel data at scale</li>
<li>Cloud masking (data quality!)</li>
<li>Temporal compositing</li>
<li>Export for ML workflows</li>
</ul>
</div></div>
<div class="fragment">
<p><strong>Everything builds on these concepts!</strong></p>
</div>
<aside class="notes">
<p>The hands-on sessions this afternoon put these concepts into practice. You’ll actually work with data and see these principles in action.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="looking-ahead-days-2-4" class="slide level2 center">
<h2>Looking Ahead: Days 2-4</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Day 2:</strong></p>
<ul>
<li>Random Forest classification</li>
<li>Land cover mapping</li>
<li>CNN basics</li>
<li>TensorFlow/Keras intro</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Days 3-4:</strong></p>
<ul>
<li>U-Net for segmentation</li>
<li>Flood mapping (DRR focus)</li>
<li>Time series with LSTMs</li>
<li>Foundation models</li>
<li>Explainable AI</li>
</ul>
</div></div>
<aside class="notes">
<p>Over the next three days, we’ll implement these concepts in real EO applications for DRR, CCA, and NRM.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="resources-for-continued-learning" class="slide level2 center">
<h2>Resources for Continued Learning</h2>
<p><strong>Online Courses:</strong></p>
<ul>
<li class="fragment">NASA ARSET: ML for Earth Science</li>
<li class="fragment">EO College: Introduction to ML for EO</li>
<li class="fragment">Coursera: Machine Learning (Andrew Ng)</li>
<li class="fragment">Fast.ai: Practical Deep Learning</li>
</ul>
<p><strong>Papers &amp; Tutorials:</strong></p>
<ul>
<li class="fragment">“Data-Centric ML for Earth Observation” (ArXiv 2025)</li>
<li class="fragment">Google Earth Engine tutorials</li>
<li class="fragment">TensorFlow Earth Observation tutorials</li>
</ul>
<p><strong>Communities:</strong></p>
<ul>
<li class="fragment">SkAI-Pinas network</li>
<li class="fragment">Digital Space Campus (CoPhil)</li>
<li class="fragment">DIMER model repository</li>
</ul>
<aside class="notes">
<p>These resources will support your continued learning after the training. The Digital Space Campus will have all our materials for reference.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="session-summary" class="slide level2 center">
<h2>Session Summary</h2>
<p><strong>What We Covered:</strong></p>
<p>✅ AI/ML/DL definitions and relationships<br>
✅ End-to-end ML workflow for EO<br>
✅ Supervised learning (classification, regression)<br>
✅ Unsupervised learning (clustering)<br>
✅ Deep learning &amp; CNNs for satellite imagery<br>
✅ Data-centric AI philosophy<br>
✅ <strong>2025 innovations:</strong> Foundation models, on-board AI</p>
<aside class="notes">
<p><strong>Timing:</strong> 2 minutes</p>
<p>You now have conceptual foundation for all ML work in this course. Sessions 3-4 today and Days 2-4 will implement these concepts.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="qa" class="slide level2 center">
<h2>Q&amp;A</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>AI/ML Concepts</strong></p>
<ul>
<li>Supervised vs unsupervised?</li>
<li>When to use deep learning?</li>
<li>Foundation models for my use case?</li>
</ul>
</div><div class="column" style="width:50%;">
<p><strong>Practical Questions</strong></p>
<ul>
<li>Data quality challenges?</li>
<li>Label collection strategies?</li>
<li>Computing requirements?</li>
</ul>
</div></div>
<aside class="notes">
<p><strong>Timing:</strong> 5-8 minutes for Q&amp;A</p>
<p><strong>Common Questions:</strong> - “Do I need a GPU?” → Not for Random Forest, yes for deep learning - “How many labels do I need?” → Depends: 100s with foundation models, 1000s for CNN from scratch - “Which algorithm should I use?” → Start simple (RF), then deep learning if needed - “Can I use foundation models for Philippines?” → Yes! They’re global and open-source</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section></section>
<section>
<section id="next-session-3" class="title-slide slide level1 center" data-background-color="#0f766e">
<h1>Next: Session 3</h1>

</section>
<section id="hands-on-python-for-geospatial-data" class="slide level2 center">
<h2>Hands-on Python for Geospatial Data</h2>
<p><strong>Coming up after 15-minute break:</strong></p>
<ul>
<li>Google Colab environment setup</li>
<li>GeoPandas for vector data</li>
<li>Rasterio for raster data</li>
<li>Work with Philippine boundaries</li>
<li>Load and visualize Sentinel-2 imagery</li>
<li>Calculate NDVI</li>
</ul>
<div class="fragment">
<p><strong>Get ready to code! 💻</strong></p>
</div>
</section></section>
<section>
<section id="thank-you" class="title-slide slide level1 center" data-background-color="#1e3a8a">
<h1>Thank You!</h1>

</section>
<section id="resources" class="slide level2 center">
<h2>Resources</h2>
<p><strong>Foundation Models:</strong><br>
NASA-IBM Geospatial: <a href="https://huggingface.co/ibm-nasa-geospatial" class="uri">https://huggingface.co/ibm-nasa-geospatial</a><br>
Prithvi: <a href="https://github.com/NASA-IMPACT/Prithvi" class="uri">https://github.com/NASA-IMPACT/Prithvi</a><br>
Clay: <a href="https://clay-foundation.github.io" class="uri">https://clay-foundation.github.io</a></p>
<p><strong>Learning:</strong><br>
NASA ARSET: <a href="https://appliedsciences.nasa.gov/arset" class="uri">https://appliedsciences.nasa.gov/arset</a><br>
EO College: <a href="https://eo-college.org" class="uri">https://eo-college.org</a><br>
SkAI-Pinas: <a href="https://asti.dost.gov.ph/skai-pinas" class="uri">https://asti.dost.gov.ph/skai-pinas</a></p>
<aside class="notes">
<p><strong>Session 2 Complete!</strong></p>
<p>15-minute break before Session 3. Make sure participants have: - Google Colab access working - GEE account registration started (will finalize in Session 4)</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>


</section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p>Day 1 Session 2 | AI/ML Fundamentals | 20-23 October 2025</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'h.v',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    <script src="https://utteranc.es/client.js" repo="cophil-training-v1.0" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
    </script>
    

</body></html>