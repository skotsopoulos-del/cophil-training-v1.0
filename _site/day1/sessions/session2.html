<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-10-16">

<title>Session 2: Core Concepts of AI/ML for Earth Observation – CoPhil EO AI/ML Training</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../day1/sessions/session3.html" rel="next">
<link href="../../day1/sessions/session1.html" rel="prev">
<link href="../../images/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c9822816d3895e59fda95a6fa7545fef.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-3775014fae9fc394bbda1d6ff89dd45e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-509191933"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-509191933', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>
<meta name="mermaid-theme" content="default">
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">


<link rel="stylesheet" href="../../styles/custom.css">
<link rel="stylesheet" href="../../styles/phase2-enhancements.css">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CoPhil EO AI/ML Training</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-training-days" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Training Days</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-training-days">    
        <li>
    <a class="dropdown-item" href="../../day1/index.html">
 <span class="dropdown-text">Day 1: EO Data &amp; Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day2/index.html">
 <span class="dropdown-text">Day 2: Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day3/index.html">
 <span class="dropdown-text">Day 3: Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day4/index.html">
 <span class="dropdown-text">Day 4: Advanced Topics</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="../../resources/setup.html">
 <span class="dropdown-text">Setup Guide</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/philippine-eo.html">
 <span class="dropdown-text">Philippine EO Links</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/cheatsheets.html">
 <span class="dropdown-text">Cheat Sheets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/faq.html">
 <span class="dropdown-text">FAQ</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/glossary.html">
 <span class="dropdown-text">Glossary</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../resources/downloads.html"> <i class="bi bi-download" role="img">
</i> 
<span class="menu-text">Materials</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day1/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day1/sessions/session2.html">Session 2: Core Concepts of AI/ML for Earth Observation</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Day 1: EO Data &amp; AI/ML Fundamentals</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Sessions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/sessions/session1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 1: Copernicus Sentinel Data Deep Dive &amp; Philippine EO Ecosystem</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/sessions/session2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Session 2: Core Concepts of AI/ML for Earth Observation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/sessions/session3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 3: Hands-on Python for Geospatial Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/sessions/session4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: Introduction to Google Earth Engine</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Notebooks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/notebooks/notebook1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notebook 1: Python for Geospatial Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/notebooks/notebook2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Notebook 2: Google Earth Engine</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/notebooks/Day1_Session3_Python_Geospatial_Data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Day 1, Session 3: Python for Geospatial Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day1/notebooks/Day1_Session4_Google_Earth_Engine.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Day 1, Session 4: Google Earth Engine Python API</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#session-overview" id="toc-session-overview" class="nav-link active" data-scroll-target="#session-overview">Session Overview</a>
  <ul class="collapse">
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link" data-scroll-target="#learning-objectives">Learning Objectives</a></li>
  </ul></li>
  <li><a href="#presentation-slides" id="toc-presentation-slides" class="nav-link" data-scroll-target="#presentation-slides">Presentation Slides</a></li>
  <li><a href="#part-1-what-is-aiml" id="toc-part-1-what-is-aiml" class="nav-link" data-scroll-target="#part-1-what-is-aiml">Part 1: What is AI/ML?</a>
  <ul class="collapse">
  <li><a href="#defining-the-terms" id="toc-defining-the-terms" class="nav-link" data-scroll-target="#defining-the-terms">Defining the Terms</a></li>
  <li><a href="#why-ml-for-earth-observation" id="toc-why-ml-for-earth-observation" class="nav-link" data-scroll-target="#why-ml-for-earth-observation">Why ML for Earth Observation?</a></li>
  </ul></li>
  <li><a href="#part-2-the-aiml-workflow-for-earth-observation" id="toc-part-2-the-aiml-workflow-for-earth-observation" class="nav-link" data-scroll-target="#part-2-the-aiml-workflow-for-earth-observation">Part 2: The AI/ML Workflow for Earth Observation</a>
  <ul class="collapse">
  <li><a href="#step-1-problem-definition" id="toc-step-1-problem-definition" class="nav-link" data-scroll-target="#step-1-problem-definition">Step 1: Problem Definition</a></li>
  <li><a href="#step-2-data-acquisition" id="toc-step-2-data-acquisition" class="nav-link" data-scroll-target="#step-2-data-acquisition">Step 2: Data Acquisition</a></li>
  <li><a href="#step-3-data-pre-processing" id="toc-step-3-data-pre-processing" class="nav-link" data-scroll-target="#step-3-data-pre-processing">Step 3: Data Pre-processing</a></li>
  <li><a href="#step-4-feature-engineering" id="toc-step-4-feature-engineering" class="nav-link" data-scroll-target="#step-4-feature-engineering">Step 4: Feature Engineering</a></li>
  <li><a href="#step-5-model-selection-and-training" id="toc-step-5-model-selection-and-training" class="nav-link" data-scroll-target="#step-5-model-selection-and-training">Step 5: Model Selection and Training</a></li>
  <li><a href="#step-6-validation-and-evaluation" id="toc-step-6-validation-and-evaluation" class="nav-link" data-scroll-target="#step-6-validation-and-evaluation">Step 6: Validation and Evaluation</a></li>
  <li><a href="#step-7-deployment-and-operationalization" id="toc-step-7-deployment-and-operationalization" class="nav-link" data-scroll-target="#step-7-deployment-and-operationalization">Step 7: Deployment and Operationalization</a></li>
  </ul></li>
  <li><a href="#part-3-types-of-machine-learning" id="toc-part-3-types-of-machine-learning" class="nav-link" data-scroll-target="#part-3-types-of-machine-learning">Part 3: Types of Machine Learning</a>
  <ul class="collapse">
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link" data-scroll-target="#supervised-learning">Supervised Learning</a></li>
  <li><a href="#unsupervised-learning" id="toc-unsupervised-learning" class="nav-link" data-scroll-target="#unsupervised-learning">Unsupervised Learning</a></li>
  </ul></li>
  <li><a href="#part-4-deep-learning-architectures-for-earth-observation" id="toc-part-4-deep-learning-architectures-for-earth-observation" class="nav-link" data-scroll-target="#part-4-deep-learning-architectures-for-earth-observation">Part 4: Deep Learning Architectures for Earth Observation</a>
  <ul class="collapse">
  <li><a href="#what-is-deep-learning" id="toc-what-is-deep-learning" class="nav-link" data-scroll-target="#what-is-deep-learning">What is Deep Learning?</a></li>
  <li><a href="#neural-network-fundamentals" id="toc-neural-network-fundamentals" class="nav-link" data-scroll-target="#neural-network-fundamentals">Neural Network Fundamentals</a></li>
  <li><a href="#convolutional-neural-networks-cnns" id="toc-convolutional-neural-networks-cnns" class="nav-link" data-scroll-target="#convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</a></li>
  <li><a href="#u-net-and-semantic-segmentation" id="toc-u-net-and-semantic-segmentation" class="nav-link" data-scroll-target="#u-net-and-semantic-segmentation">U-Net and Semantic Segmentation</a></li>
  <li><a href="#deeplab-family" id="toc-deeplab-family" class="nav-link" data-scroll-target="#deeplab-family">DeepLab Family</a></li>
  <li><a href="#vision-transformers-vits" id="toc-vision-transformers-vits" class="nav-link" data-scroll-target="#vision-transformers-vits">Vision Transformers (ViTs)</a></li>
  <li><a href="#hybrid-architectures-unetformer" id="toc-hybrid-architectures-unetformer" class="nav-link" data-scroll-target="#hybrid-architectures-unetformer">Hybrid Architectures: UNetFormer</a></li>
  <li><a href="#temporal-models-for-time-series" id="toc-temporal-models-for-time-series" class="nav-link" data-scroll-target="#temporal-models-for-time-series">Temporal Models for Time Series</a></li>
  <li><a href="#object-detection-architectures" id="toc-object-detection-architectures" class="nav-link" data-scroll-target="#object-detection-architectures">Object Detection Architectures</a></li>
  <li><a href="#multi-modal-architectures" id="toc-multi-modal-architectures" class="nav-link" data-scroll-target="#multi-modal-architectures">Multi-Modal Architectures</a></li>
  <li><a href="#foundation-models-for-earth-observation" id="toc-foundation-models-for-earth-observation" class="nav-link" data-scroll-target="#foundation-models-for-earth-observation">Foundation Models for Earth Observation</a></li>
  <li><a href="#training-strategies" id="toc-training-strategies" class="nav-link" data-scroll-target="#training-strategies">Training Strategies</a></li>
  </ul></li>
  <li><a href="#part-5-benchmark-datasets-for-training-and-validation" id="toc-part-5-benchmark-datasets-for-training-and-validation" class="nav-link" data-scroll-target="#part-5-benchmark-datasets-for-training-and-validation">Part 5: Benchmark Datasets for Training and Validation</a>
  <ul class="collapse">
  <li><a href="#patch-level-classification-datasets" id="toc-patch-level-classification-datasets" class="nav-link" data-scroll-target="#patch-level-classification-datasets">Patch-Level Classification Datasets</a></li>
  <li><a href="#object-detection-datasets" id="toc-object-detection-datasets" class="nav-link" data-scroll-target="#object-detection-datasets">Object Detection Datasets</a></li>
  <li><a href="#semantic-segmentation-datasets" id="toc-semantic-segmentation-datasets" class="nav-link" data-scroll-target="#semantic-segmentation-datasets">Semantic Segmentation Datasets</a></li>
  <li><a href="#scene-classification-datasets" id="toc-scene-classification-datasets" class="nav-link" data-scroll-target="#scene-classification-datasets">Scene Classification Datasets</a></li>
  <li><a href="#time-series-datasets" id="toc-time-series-datasets" class="nav-link" data-scroll-target="#time-series-datasets">Time Series Datasets</a></li>
  <li><a href="#philippine-specific-data-resources" id="toc-philippine-specific-data-resources" class="nav-link" data-scroll-target="#philippine-specific-data-resources">Philippine-Specific Data Resources</a></li>
  </ul></li>
  <li><a href="#part-6-data-centric-ai-in-earth-observation" id="toc-part-6-data-centric-ai-in-earth-observation" class="nav-link" data-scroll-target="#part-6-data-centric-ai-in-earth-observation">Part 6: Data-Centric AI in Earth Observation</a>
  <ul class="collapse">
  <li><a href="#the-paradigm-shift-2025" id="toc-the-paradigm-shift-2025" class="nav-link" data-scroll-target="#the-paradigm-shift-2025">The Paradigm Shift (2025)</a></li>
  <li><a href="#pillar-1-data-quality" id="toc-pillar-1-data-quality" class="nav-link" data-scroll-target="#pillar-1-data-quality">Pillar 1: Data Quality</a></li>
  <li><a href="#pillar-2-data-quantity" id="toc-pillar-2-data-quantity" class="nav-link" data-scroll-target="#pillar-2-data-quantity">Pillar 2: Data Quantity</a></li>
  <li><a href="#pillar-3-data-diversity" id="toc-pillar-3-data-diversity" class="nav-link" data-scroll-target="#pillar-3-data-diversity">Pillar 3: Data Diversity</a></li>
  <li><a href="#pillar-4-annotation-strategy" id="toc-pillar-4-annotation-strategy" class="nav-link" data-scroll-target="#pillar-4-annotation-strategy">Pillar 4: Annotation Strategy</a></li>
  <li><a href="#examples-data-centric-success-stories" id="toc-examples-data-centric-success-stories" class="nav-link" data-scroll-target="#examples-data-centric-success-stories">2025 Examples: Data-Centric Success Stories</a></li>
  </ul></li>
  <li><a href="#part-7-explainable-ai-xai-for-earth-observation" id="toc-part-7-explainable-ai-xai-for-earth-observation" class="nav-link" data-scroll-target="#part-7-explainable-ai-xai-for-earth-observation">Part 7: Explainable AI (XAI) for Earth Observation</a>
  <ul class="collapse">
  <li><a href="#why-xai-matters-in-eo" id="toc-why-xai-matters-in-eo" class="nav-link" data-scroll-target="#why-xai-matters-in-eo">Why XAI Matters in EO</a></li>
  <li><a href="#xai-methods-for-eo" id="toc-xai-methods-for-eo" class="nav-link" data-scroll-target="#xai-methods-for-eo">XAI Methods for EO</a></li>
  <li><a href="#applications-in-eo" id="toc-applications-in-eo" class="nav-link" data-scroll-target="#applications-in-eo">Applications in EO</a></li>
  <li><a href="#challenges-and-trade-offs" id="toc-challenges-and-trade-offs" class="nav-link" data-scroll-target="#challenges-and-trade-offs">Challenges and Trade-Offs</a></li>
  <li><a href="#best-practices-for-xai-in-eo" id="toc-best-practices-for-xai-in-eo" class="nav-link" data-scroll-target="#best-practices-for-xai-in-eo">Best Practices for XAI in EO</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#discussion-questions" id="toc-discussion-questions" class="nav-link" data-scroll-target="#discussion-questions">Discussion Questions</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading">Further Reading</a>
  <ul class="collapse">
  <li><a href="#foundational-concepts" id="toc-foundational-concepts" class="nav-link" data-scroll-target="#foundational-concepts">Foundational Concepts</a></li>
  <li><a href="#deep-learning-architectures-1" id="toc-deep-learning-architectures-1" class="nav-link" data-scroll-target="#deep-learning-architectures-1">Deep Learning Architectures</a></li>
  <li><a href="#deep-learning-for-eo" id="toc-deep-learning-for-eo" class="nav-link" data-scroll-target="#deep-learning-for-eo">Deep Learning for EO</a></li>
  <li><a href="#foundation-models" id="toc-foundation-models" class="nav-link" data-scroll-target="#foundation-models">Foundation Models</a></li>
  <li><a href="#self-supervised-learning-1" id="toc-self-supervised-learning-1" class="nav-link" data-scroll-target="#self-supervised-learning-1">Self-Supervised Learning</a></li>
  <li><a href="#data-centric-ai" id="toc-data-centric-ai" class="nav-link" data-scroll-target="#data-centric-ai">Data-Centric AI</a></li>
  <li><a href="#explainable-ai-1" id="toc-explainable-ai-1" class="nav-link" data-scroll-target="#explainable-ai-1">Explainable AI</a></li>
  <li><a href="#eo-specific-ml" id="toc-eo-specific-ml" class="nav-link" data-scroll-target="#eo-specific-ml">EO-Specific ML</a></li>
  <li><a href="#benchmark-datasets-1" id="toc-benchmark-datasets-1" class="nav-link" data-scroll-target="#benchmark-datasets-1">Benchmark Datasets</a></li>
  <li><a href="#philippine-ai-initiatives" id="toc-philippine-ai-initiatives" class="nav-link" data-scroll-target="#philippine-ai-initiatives">Philippine AI Initiatives</a></li>
  <li><a href="#recent-advances" id="toc-recent-advances" class="nav-link" data-scroll-target="#recent-advances">Recent Advances</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day1/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day1/sessions/session2.html">Session 2: Core Concepts of AI/ML for Earth Observation</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Session 2: Core Concepts of AI/ML for Earth Observation</h1>
<p class="subtitle lead">Understanding the fundamentals of machine learning for satellite data analysis</p>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Date</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 16, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<nav class="breadcrumb" aria-label="Breadcrumb">
<a href="../../index.html">Home</a> <span class="breadcrumb-separator" aria-hidden="true">›</span> <a href="../index.html">Day 1</a> <span class="breadcrumb-separator" aria-hidden="true">›</span> <span class="breadcrumb-current">Session 2</span>
</nav>
<div class="session-info">
<p><strong>Duration:</strong> 2 hours | <strong>Format:</strong> Lecture + Conceptual Exercises | <strong>Platform:</strong> Presentation</p>
</div>
<section id="session-overview" class="level2">
<h2 class="anchored" data-anchor-id="session-overview">Session Overview</h2>
<p>This session provides a comprehensive introduction to Artificial Intelligence and Machine Learning concepts specifically tailored for Earth Observation applications. You’ll learn the complete AI/ML workflow, understand different learning paradigms, explore deep learning architectures including CNNs, Vision Transformers, and temporal models, discover benchmark datasets, and understand why data quality matters more than model complexity in 2025’s data-centric AI paradigm.</p>
<hr>
<p>The session integrates cutting-edge methodologies with concrete Philippine case studies, from DOST-ASTI’s 10-20 minute flood detection using AI to PhilSA’s mangrove mapping efforts, demonstrating operational AI/ML systems already serving disaster risk reduction and natural resource management needs.</p>
<section id="learning-objectives" class="level3 learning-objectives">
<h3 class="anchored" data-anchor-id="learning-objectives">Learning Objectives</h3>
<p>By the end of this session, you will be able to:</p>
<ul>
<li><strong>Define</strong> AI and ML in the context of Earth Observation</li>
<li><strong>Describe</strong> the complete AI/ML workflow from problem definition to deployment</li>
<li><strong>Distinguish</strong> between supervised and unsupervised learning with EO examples</li>
<li><strong>Explain</strong> classification vs.&nbsp;regression tasks in satellite data analysis</li>
<li><strong>Identify</strong> major deep learning architectures: CNNs, U-Net, Vision Transformers, RNNs/LSTMs, object detection networks</li>
<li><strong>Understand</strong> key components: neurons, layers, activation functions, loss functions, optimizers</li>
<li><strong>Compare</strong> different model architectures and when to apply each</li>
<li><strong>Recognize</strong> benchmark datasets used for training and evaluation</li>
<li><strong>Articulate</strong> the data-centric AI paradigm and its importance for EO</li>
<li><strong>Apply</strong> best practices for data quality, quantity, diversity, and annotation</li>
<li><strong>Explain</strong> Explainable AI (XAI) and why it matters for operational systems</li>
</ul>
</section>
<hr>
</section>
<section id="presentation-slides" class="level2">
<h2 class="anchored" data-anchor-id="presentation-slides">Presentation Slides</h2>
<iframe src="../presentations/02_session2_ai_ml_fundamentals.html" width="100%" height="600" style="border: 1px solid #ccc; border-radius: 4px;">
</iframe>
<hr>
</section>
<section id="part-1-what-is-aiml" class="level2">
<h2 class="anchored" data-anchor-id="part-1-what-is-aiml">Part 1: What is AI/ML?</h2>
<section id="defining-the-terms" class="level3">
<h3 class="anchored" data-anchor-id="defining-the-terms">Defining the Terms</h3>
<p><strong>Artificial Intelligence (AI):</strong></p>
<ul>
<li>Broad field focused on creating intelligent machines</li>
<li>Systems that can perceive, reason, learn, and act</li>
<li>Includes everything from rule-based systems to machine learning</li>
<li>In EO: Enables automated interpretation of petabytes of satellite data</li>
</ul>
<p><strong>Machine Learning (ML):</strong></p>
<ul>
<li>Subset of AI focused on learning from data</li>
<li>Algorithms that improve performance through experience</li>
<li><strong>Key distinction:</strong> No explicit programming of rules</li>
<li><strong>Deep Learning:</strong> Subset of ML using multi-layered neural networks</li>
</ul>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    subgraph AI["ARTIFICIAL INTELLIGENCE&lt;br/&gt;Creating intelligent machines"]
        subgraph ML["MACHINE LEARNING&lt;br/&gt;Learning from data without explicit programming"]
            subgraph DL["DEEP LEARNING&lt;br/&gt;Multi-layered neural networks"]
                DL1[Convolutional&lt;br/&gt;Neural Networks&lt;br/&gt;CNNs]
                DL2[Recurrent&lt;br/&gt;Neural Networks&lt;br/&gt;RNNs/LSTMs]
                DL3[Transformers&lt;br/&gt;Vision Transformers]
                DL4[GANs &amp; VAEs&lt;br/&gt;Generative Models]
            end
            ML1[Random Forest]
            ML2[Support Vector&lt;br/&gt;Machines]
            ML3[Decision Trees]
            ML4[K-Means&lt;br/&gt;Clustering]
        end
        AI1[Expert Systems&lt;br/&gt;Rule-based]
        AI2[Fuzzy Logic]
        AI3[Genetic&lt;br/&gt;Algorithms]
    end

    DL1 -.-&gt;|EO Apps| EO1[Land Cover&lt;br/&gt;Classification]
    DL1 -.-&gt;|EO Apps| EO2[Semantic&lt;br/&gt;Segmentation]
    DL2 -.-&gt;|EO Apps| EO3[Time Series&lt;br/&gt;Crop Monitoring]
    DL3 -.-&gt;|EO Apps| EO4[Change&lt;br/&gt;Detection]

    style AI fill:#e6f3ff,stroke:#0066cc,stroke-width:3px
    style ML fill:#fff4e6,stroke:#ff8800,stroke-width:2px
    style DL fill:#e6ffe6,stroke:#00aa44,stroke-width:2px
    style DL1 fill:#00cc66,stroke:#008844,stroke-width:1px,color:#fff
    style DL2 fill:#00cc66,stroke:#008844,stroke-width:1px,color:#fff
    style DL3 fill:#00cc66,stroke:#008844,stroke-width:1px,color:#fff
</pre>
</div>
<p></p><figcaption> AI, Machine Learning, and Deep Learning Relationship</figcaption> </figure><p></p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>The ML Difference
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Traditional Programming:</strong></p>
<pre><code>Rules + Data → Output</code></pre>
<p><strong>Machine Learning:</strong></p>
<pre><code>Data + Desired Output → Rules (Model)</code></pre>
<p>In EO: Instead of coding “if NIR &gt; 0.6 and Red &lt; 0.3, then forest”, ML learns the pattern from labeled examples.</p>
</div>
</div>
</section>
<section id="why-ml-for-earth-observation" class="level3">
<h3 class="anchored" data-anchor-id="why-ml-for-earth-observation">Why ML for Earth Observation?</h3>
<p><strong>Challenges that ML addresses:</strong></p>
<ol type="1">
<li><strong>Scale:</strong> NASA’s Earth Science Data Systems exceeded 148 PB in 2023, projected 250 PB in 2025 - impossible to manually analyze</li>
<li><strong>Complexity:</strong> Multispectral, multi-temporal, spatial patterns humans can’t easily detect</li>
<li><strong>Consistency:</strong> Automated processing ensures reproducible results across time and space</li>
<li><strong>Speed:</strong> Real-time disaster mapping requires immediate analysis (DOST-ASTI DATOS: 10-20 minute flood response)</li>
<li><strong>Multi-modal fusion:</strong> Integrating optical, SAR, LiDAR data for robust monitoring</li>
</ol>
<p><strong>Traditional vs.&nbsp;ML approaches:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 40%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Task</th>
<th>Traditional</th>
<th>ML Approach</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Water detection</strong></td>
<td>Manual NDWI threshold</td>
<td>Learn optimal threshold + texture from examples</td>
</tr>
<tr class="even">
<td><strong>Land cover</strong></td>
<td>Rule-based classification</td>
<td>Random Forest or CNN with training samples</td>
</tr>
<tr class="odd">
<td><strong>Flood mapping</strong></td>
<td>Expert visual interpretation</td>
<td>U-Net segmentation trained on labeled floods</td>
</tr>
<tr class="even">
<td><strong>Crop monitoring</strong></td>
<td>Fixed vegetation index thresholds</td>
<td>LSTM time series model learning phenology</td>
</tr>
<tr class="odd">
<td><strong>Building detection</strong></td>
<td>Manual digitization</td>
<td>YOLO or Faster R-CNN object detection</td>
</tr>
<tr class="even">
<td><strong>Deforestation</strong></td>
<td>Visual comparison of dates</td>
<td>Siamese networks for change detection</td>
</tr>
</tbody>
</table>
<div class="philippine-context">
<p><strong>Philippine Operational Systems:</strong></p>
<p>The Philippines demonstrates successful AI/ML deployment:</p>
<ul>
<li><strong>DATOS (DOST-ASTI):</strong> AI-powered flood mapping from Sentinel-1 SAR achieves 10-20 minute response time during typhoons</li>
<li><strong>PRiSM (PhilRice-IRRI):</strong> Operational since 2014, first satellite-based rice monitoring in Southeast Asia</li>
<li><strong>SkAI-Pinas (DOST):</strong> National AI framework addressing the gap between abundant remote sensing data and sustainable AI pipelines</li>
<li><strong>DIMER Model Repository (DOST-ASTI):</strong> Democratizing access to trained models for Philippine contexts</li>
</ul>
</div>
<hr>
</section>
</section>
<section id="part-2-the-aiml-workflow-for-earth-observation" class="level2">
<h2 class="anchored" data-anchor-id="part-2-the-aiml-workflow-for-earth-observation">Part 2: The AI/ML Workflow for Earth Observation</h2>
<p>Understanding the complete workflow is essential for successful EO projects. Each step matters, and according to 2024 research, most underperforming models suffer from data issues rather than algorithm deficiencies.</p>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    A[1. Problem Definition&lt;br/&gt;What question?&lt;br/&gt;What output?] --&gt; B[2. Data Acquisition&lt;br/&gt;Satellite imagery&lt;br/&gt;Ground truth&lt;br/&gt;Ancillary data]

    B --&gt; C[3. Data Preprocessing&lt;br/&gt;Atmospheric correction&lt;br/&gt;Cloud masking&lt;br/&gt;Normalization]

    C --&gt; D[4. Data Annotation&lt;br/&gt;Label training samples&lt;br/&gt;Quality control&lt;br/&gt;Class balancing]

    D --&gt; E[5. Feature Engineering&lt;br/&gt;Spectral indices&lt;br/&gt;Texture features&lt;br/&gt;Temporal metrics]

    E --&gt; F{6. Train/Val/Test&lt;br/&gt;Split}
    F --&gt;|70%| G[Training Set]
    F --&gt;|15%| H[Validation Set]
    F --&gt;|15%| I[Test Set]

    G --&gt; J[7. Model Training&lt;br/&gt;Select architecture&lt;br/&gt;Set hyperparameters&lt;br/&gt;Train on GPU]

    H --&gt; K[8. Model Validation&lt;br/&gt;Tune hyperparameters&lt;br/&gt;Monitor overfitting&lt;br/&gt;Early stopping]

    J --&gt; K
    K --&gt;|Iterate| J

    K --&gt; L{Performance&lt;br/&gt;Acceptable?}
    L --&gt;|No| M[Improve Data&lt;br/&gt;More samples&lt;br/&gt;Better labels&lt;br/&gt;Data augmentation]
    M --&gt; D

    L --&gt;|Yes| N[9. Model Testing&lt;br/&gt;Final evaluation&lt;br/&gt;Unseen test set&lt;br/&gt;Confusion matrix]

    I --&gt; N

    N --&gt; O[10. Deployment&lt;br/&gt;Production system&lt;br/&gt;Monitoring&lt;br/&gt;Maintenance]

    O --&gt; P{Model Drift?&lt;br/&gt;Performance&lt;br/&gt;degraded?}
    P --&gt;|Yes| Q[Retrain with&lt;br/&gt;new data]
    Q --&gt; D
    P --&gt;|No| O

    style A fill:#0066cc,stroke:#003d7a,stroke-width:2px,color:#fff
    style B fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff
    style C fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff
    style D fill:#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff
    style J fill:#cc00cc,stroke:#880088,stroke-width:2px,color:#fff
    style K fill:#cc00cc,stroke:#880088,stroke-width:2px,color:#fff
    style O fill:#009999,stroke:#006666,stroke-width:2px,color:#fff
</pre>
</div>
<p></p><figcaption> Complete AI/ML Workflow for Earth Observation</figcaption> </figure><p></p>
</div>
</div>
</div>
<section id="step-1-problem-definition" class="level3">
<h3 class="anchored" data-anchor-id="step-1-problem-definition">Step 1: Problem Definition</h3>
<p><strong>Define clearly what you want to achieve:</strong></p>
<ul>
<li>What question are you answering? (e.g., “Where are mangroves declining?”)</li>
<li>What output do you need? (map, time series, alert system?)</li>
<li>What accuracy is acceptable?</li>
<li>What constraints exist? (time, computational resources, data availability)</li>
<li>What is the operational context and who will use the outputs?</li>
</ul>
<div class="philippine-context">
<p><strong>Philippine Example: PRiSM Rice Monitoring</strong></p>
<p><strong>Problem:</strong> Provide timely rice area and production estimates for food security planning and disaster response</p>
<p><strong>Clear definition:</strong> - Multi-class: rice wet season, rice dry season, non-rice agriculture, non-agriculture - 10m spatial resolution (Sentinel-1 SAR + Sentinel-2 optical) - Temporal: Per-season mapping (wet: June-Nov, dry: Dec-May) - Accuracy target: &gt;90% for policy-level decisions - Operational: Automated processing, quarterly updates to DA/PCIC - Cloud-penetrating capability essential for monsoon season</p>
</div>
</section>
<section id="step-2-data-acquisition" class="level3">
<h3 class="anchored" data-anchor-id="step-2-data-acquisition">Step 2: Data Acquisition</h3>
<p><strong>Gather all necessary data:</strong></p>
<ul>
<li><strong>Satellite imagery:</strong> Sentinel-1/2, Landsat, commercial VHR, hyperspectral</li>
<li><strong>Ground truth:</strong> Field surveys, high-res imagery interpretation, existing maps, crowdsourced data</li>
<li><strong>Ancillary data:</strong> DEM, climate, administrative boundaries, road networks, socioeconomic data</li>
</ul>
<p><strong>Data volume considerations:</strong> - NASA’s Earth Science Data Systems: 148 PB (2023) → 205 PB (2024) → 250 PB (2025) - Sentinel constellation generates thousands of terabytes daily - 1,052 active EO satellites as of 2024</p>
<p><strong>Data sources for Philippines:</strong></p>
<ul>
<li><strong>CoPhil Mirror Site (2025):</strong> Local, high-bandwidth access to Sentinel data covering entire archipelago</li>
<li><strong>Copernicus Data Space Ecosystem:</strong> STAC-compliant catalogues, API-driven access</li>
<li><strong>Google Earth Engine:</strong> Harmonized Sentinel-2 surface reflectance, Sentinel-1 GRD collections</li>
<li><strong>PhilSA SIYASAT:</strong> NovaSAR-1 X-band SAR data</li>
<li><strong>Diwata-1/2 microsatellites:</strong> Philippine-operated disaster monitoring</li>
<li><strong>NAMRIA Geoportal:</strong> Land cover basemaps, topographic data</li>
<li><strong>PAGASA:</strong> Climate and meteorological data</li>
</ul>
</section>
<section id="step-3-data-pre-processing" class="level3">
<h3 class="anchored" data-anchor-id="step-3-data-pre-processing">Step 3: Data Pre-processing</h3>
<p><strong>Critical step - “Garbage in, garbage out”</strong></p>
<p>Data pre-processing is foundational and directly impacts downstream analysis accuracy. Proper preprocessing ensures data quality, consistency, and comparability across time and sensors.</p>
<p><strong>For satellite imagery:</strong></p>
<p><strong>Atmospheric Correction:</strong> - <strong>Purpose:</strong> Remove atmospheric effects (scattering, absorption) - <strong>Convert:</strong> Top-of-Atmosphere (TOA) reflectance → Surface reflectance (SR) - <strong>Sentinel-2:</strong> Use Level-2A products (Sen2Cor algorithm) - <strong>HLS Products:</strong> NASA’s Harmonized Landsat Sentinel-2 applies LaSRC + BRDF normalization - <strong>Essential for:</strong> Multi-temporal comparisons, quantitative biophysical parameter retrieval</p>
<p><strong>Cloud Masking:</strong> - <strong>Sentinel-2 SCL:</strong> Scene Classification Layer (clouds, shadows, snow, water, vegetation) - <strong>Machine learning approaches:</strong> U-Net architectures for pixel-wise cloud segmentation - <strong>Multi-temporal approaches:</strong> Leverage temporal patterns to identify clouds - <strong>Gap filling:</strong> Temporal interpolation, spatial interpolation, deep learning reconstruction (Prithvi-EO-2.0)</p>
<p><strong>SAR-Specific Preprocessing (Sentinel-1):</strong> - <strong>Orbit file application:</strong> Precise geolocation - <strong>Radiometric calibration:</strong> Convert DN to sigma nought (σ⁰), beta nought (β⁰), or gamma nought (γ⁰) - <strong>De-bursting:</strong> Remove black boundaries between sub-swaths in TOPS mode - <strong>Speckle filtering:</strong> Lee, Frost, Gamma-MAP filters; CNN-based despecklers preserve edges - <strong>Terrain correction (RTC):</strong> Orthorectification using DEM (SRTM, Copernicus DEM) - <strong>Multi-temporal filtering:</strong> Leverage temporal stack to reduce speckle</p>
<p><strong>Normalization and Scaling:</strong> - <strong>Min-max normalization:</strong> Scale to [0, 1] range - <strong>Z-score standardization:</strong> Center to mean=0, std=1 (common for deep learning) - <strong>Percentile clipping:</strong> Reduce impact of outliers (e.g., 2nd and 98th percentiles) - <strong>Per-band normalization:</strong> Account for different dynamic ranges across spectral bands</p>
<p><strong>For training labels:</strong></p>
<ul>
<li><strong>Quality control:</strong> Verify label accuracy through multiple reviewers</li>
<li><strong>Coordinate alignment:</strong> Ensure labels match imagery timing and location</li>
<li><strong>Class balancing:</strong> Ensure adequate samples per class</li>
<li><strong>Format standardization:</strong> Convert to ML-ready format (GeoTIFF, TFRecord, COG)</li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>Pre-processing Pitfalls
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Common errors that degrade model performance:</strong></p>
<ul>
<li>Using Top-of-Atmosphere instead of surface reflectance</li>
<li>Temporal mismatch: 2020 imagery with 2018 labels</li>
<li>Incomplete cloud masking leaving cloud shadows</li>
<li>Mixed pixels at boundaries (especially for validation)</li>
<li>Inconsistent band ordering across scenes</li>
<li>Ignoring spatial autocorrelation (random train-test splits can lead to 28% overoptimistic performance)</li>
<li>Not applying same preprocessing to training and deployment data</li>
</ul>
</div>
</div>
</section>
<section id="step-4-feature-engineering" class="level3">
<h3 class="anchored" data-anchor-id="step-4-feature-engineering">Step 4: Feature Engineering</h3>
<p><strong>Deriving informative variables from raw data</strong></p>
<p>Feature engineering transforms raw satellite data into informative representations that enhance model performance. This step is crucial for traditional ML algorithms and beneficial even for deep learning.</p>
<p><strong>For traditional ML (Random Forest, SVM):</strong></p>
<p><strong>Spectral Indices:</strong> - <strong>Vegetation:</strong> NDVI, EVI, SAVI, NDRE, GNDVI, LAI - <strong>Water:</strong> NDWI, MNDWI, NDMI - <strong>Built-up:</strong> NDBI, Bare Soil Index (BSI) - <strong>Burn:</strong> NBR, dNBR</p>
<p><strong>Textural Features (GLCM):</strong> - Contrast, Correlation, Energy, Homogeneity, Entropy, Dissimilarity, Variance - Window sizes: 3×3, 5×5, 7×7 - Multiple directions: 0°, 45°, 90°, 135°</p>
<p><strong>Temporal Features:</strong> - Statistical: Mean, median, std dev, min, max, percentiles (10th, 25th, 75th, 90th) - Coefficient of Variation (CV): Normalized variability measure - Amplitude: Difference between peak and minimum - Phenological: Start of season (SOS), Peak of season (POS), End of season (EOS) - Trends: Linear regression slopes, breakpoint detection</p>
<p><strong>Multi-Modal Features:</strong> - <strong>Optical-SAR fusion:</strong> Concatenate optical indices with SAR backscatter - <strong>Derived ratios:</strong> VV/VH polarization ratio, optical/SAR combinations - <strong>SAR texture:</strong> GLCM features from backscatter - <strong>Interferometric:</strong> Coherence from InSAR</p>
<p><strong>Example: Forest classification features</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Spectral indices</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>NDVI <span class="op">=</span> (NIR <span class="op">-</span> Red) <span class="op">/</span> (NIR <span class="op">+</span> Red)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>NDWI <span class="op">=</span> (Green <span class="op">-</span> NIR) <span class="op">/</span> (Green <span class="op">+</span> NIR)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>EVI <span class="op">=</span> <span class="fl">2.5</span> <span class="op">*</span> (NIR <span class="op">-</span> Red) <span class="op">/</span> (NIR <span class="op">+</span> <span class="dv">6</span><span class="op">*</span>Red <span class="op">-</span> <span class="fl">7.5</span><span class="op">*</span>Blue <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># SAR features</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>VV_VH_ratio <span class="op">=</span> VV_backscatter <span class="op">/</span> VH_backscatter</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>SAR_texture <span class="op">=</span> GLCM_contrast(VH_backscatter)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Temporal</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>NDVI_mean <span class="op">=</span> mean(NDVI_time_series)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>NDVI_cv <span class="op">=</span> std(NDVI_time_series) <span class="op">/</span> NDVI_mean</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Topographic</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>Elevation, Slope, Aspect</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Result: Input feature vector per pixel</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> [Red, Green, Blue, NIR, SWIR1, SWIR2, NDVI, EVI, NDWI,</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>     VV, VH, VV_VH_ratio, SAR_texture, NDVI_mean, NDVI_cv,</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>     Elevation, Slope, Aspect]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>For deep learning (CNNs, U-Net, Vision Transformers):</strong></p>
<ul>
<li>Less manual feature engineering needed</li>
<li>Networks automatically learn features from raw pixels</li>
<li>Still benefit from good input data (cloud-free, calibrated, normalized)</li>
<li>Multi-spectral bands as input channels</li>
<li>Consider temporal stacking for multi-date analysis</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>2024 Research Insight: Feature Selection
</div>
</div>
<div class="callout-body-container callout-body">
<p>Seven unsupervised dimensionality reduction algorithms tested on hyperspectral data from HYPSO-1 satellite showed that:</p>
<ul>
<li>Careful feature selection can achieve optimal accuracy with &lt;20% of temporal instances</li>
<li>Single band from single sensor can be sufficient for specific tasks</li>
<li><strong>Implication:</strong> Smart data selection &gt; brute force data collection</li>
<li>Use PCA, MNF, or tree-based feature importance for efficient selection</li>
</ul>
</div>
</div>
</section>
<section id="step-5-model-selection-and-training" class="level3">
<h3 class="anchored" data-anchor-id="step-5-model-selection-and-training">Step 5: Model Selection and Training</h3>
<p><strong>Choose appropriate algorithm:</strong></p>
<p><strong>Consider:</strong></p>
<ul>
<li>Task type (classification, regression, segmentation, object detection)</li>
<li>Data size (deep learning needs more data; transfer learning reduces requirements)</li>
<li>Interpretability requirements (operational systems often need explainability)</li>
<li>Computational resources (edge devices vs.&nbsp;cloud platforms)</li>
<li>Deployment constraints (inference speed, model size)</li>
</ul>
<p><strong>Common EO algorithms:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 11%">
<col style="width: 18%">
<col style="width: 22%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Type</th>
<th>Best For</th>
<th>Data Needs</th>
<th>Key Strengths</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Random Forest</strong></td>
<td>Ensemble</td>
<td>Classification, feature importance, baseline</td>
<td>Medium (100s-1000s)</td>
<td>Robust, interpretable, handles high dimensions</td>
</tr>
<tr class="even">
<td><strong>SVM</strong></td>
<td>Kernel</td>
<td>Binary classification, small data</td>
<td>Small-Medium</td>
<td>Effective high-dimensional spaces</td>
</tr>
<tr class="odd">
<td><strong>XGBoost/LightGBM</strong></td>
<td>Gradient Boosting</td>
<td>Tabular features, yield prediction</td>
<td>Medium</td>
<td>High performance on structured data</td>
</tr>
<tr class="even">
<td><strong>CNN</strong></td>
<td>Deep Learning</td>
<td>Image classification, automatic features</td>
<td>Large (10,000s+)</td>
<td>Spatial awareness, hierarchical learning</td>
</tr>
<tr class="odd">
<td><strong>U-Net</strong></td>
<td>Deep Learning</td>
<td>Semantic segmentation (pixel-wise)</td>
<td>Large</td>
<td>Skip connections, works with limited data</td>
</tr>
<tr class="even">
<td><strong>ResNet</strong></td>
<td>Deep Learning</td>
<td>Very deep networks, complex classification</td>
<td>Large</td>
<td>Residual connections avoid vanishing gradients</td>
</tr>
<tr class="odd">
<td><strong>Vision Transformer</strong></td>
<td>Deep Learning</td>
<td>Global context, spatial relationships</td>
<td>Very Large</td>
<td>Attention mechanisms, long-range dependencies</td>
</tr>
<tr class="even">
<td><strong>LSTM/GRU</strong></td>
<td>Deep Learning</td>
<td>Time series prediction, phenology</td>
<td>Large</td>
<td>Temporal pattern learning</td>
</tr>
<tr class="odd">
<td><strong>YOLO/Faster R-CNN</strong></td>
<td>Deep Learning</td>
<td>Object detection (buildings, ships)</td>
<td>Large</td>
<td>Real-time detection, bounding boxes</td>
</tr>
</tbody>
</table>
<p><strong>Training process:</strong></p>
<ol type="1">
<li><strong>Split data:</strong> 70-80% training, 10-15% validation, 10-15% testing
<ul>
<li><strong>Spatial cross-validation:</strong> Essential for EO to avoid spatial leakage</li>
<li>Spatial k-fold or buffered leave-one-out</li>
</ul></li>
<li><strong>Feed training data:</strong> Algorithm adjusts parameters to minimize error</li>
<li><strong>Monitor validation:</strong> Track performance on held-out validation set</li>
<li><strong>Hyperparameter tuning:</strong> Optimize learning rate, batch size, architecture parameters</li>
<li><strong>Early stopping:</strong> Stop when validation performance plateaus</li>
<li><strong>Final evaluation:</strong> Test on completely independent test set</li>
</ol>
<p><strong>Transfer Learning:</strong> - Pre-train on large dataset (ImageNet, SatViT, Prithvi) - Fine-tune on task-specific data - Reduces training data requirements by 10-100× - Faster convergence and better generalization</p>
<div class="philippine-context">
<p><strong>Philippine Example: Poverty Mapping with Transfer Learning</strong></p>
<p>Study using satellite imagery, nighttime lights, and OpenStreetMap data: - Transfer learning from ImageNet improved performance by 14.1% for tropical cyclone impact areas - Requires careful hyperparameter tuning for generalization across regions - Cost-effective approach for limited labeled data scenarios</p>
</div>
</section>
<section id="step-6-validation-and-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="step-6-validation-and-evaluation">Step 6: Validation and Evaluation</h3>
<p><strong>Rigorous testing on independent data</strong></p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Never Test on Training Data!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Testing on data the model has seen gives falsely optimistic results. Always use held-out test data. For EO, random train-test splits can overestimate performance by up to 28% due to spatial autocorrelation.</p>
</div>
</div>
<p><strong>Classification metrics:</strong></p>
<ul>
<li><strong>Overall Accuracy (OA):</strong> Percentage of correctly classified pixels</li>
<li><strong>Confusion Matrix:</strong> Shows which classes are confused with each other</li>
<li><strong>Per-Class Metrics:</strong>
<ul>
<li>Producer’s Accuracy (Recall): How many ground truth samples were correctly classified</li>
<li>User’s Accuracy (Precision): How many predicted samples are actually correct</li>
</ul></li>
<li><strong>F1-Score:</strong> Harmonic mean of precision and recall (2 × Precision × Recall / (Precision + Recall))</li>
<li><strong>Kappa Coefficient:</strong> Agreement accounting for chance</li>
<li><strong>Matthews Correlation Coefficient (MCC):</strong> Balanced measure even for imbalanced classes</li>
</ul>
<p><strong>Semantic Segmentation metrics:</strong></p>
<ul>
<li><strong>IoU (Intersection over Union):</strong> Area of overlap / Area of union
<ul>
<li>IoU &gt; 0.5: Generally acceptable</li>
<li>IoU &gt; 0.7: High-quality segmentation</li>
<li>IoU &gt; 0.9: Excellent agreement</li>
</ul></li>
<li><strong>Mean IoU (mIoU):</strong> Average IoU across all classes</li>
<li><strong>Dice Coefficient:</strong> 2 × IoU / (1 + IoU), less harsh penalty than IoU</li>
<li><strong>Pixel Accuracy:</strong> Simple but biased toward majority class</li>
<li><strong>Boundary F1 Score:</strong> Precision/recall on boundary pixels</li>
</ul>
<p><strong>Regression metrics:</strong></p>
<ul>
<li><strong>RMSE (Root Mean Squared Error):</strong> Average prediction error, penalizes large errors</li>
<li><strong>MAE (Mean Absolute Error):</strong> Average absolute deviation, less sensitive to outliers</li>
<li><strong>R² (Coefficient of Determination):</strong> Proportion of variance explained (0.88 = 88% explained)</li>
</ul>
<p><strong>Object Detection metrics:</strong></p>
<ul>
<li><strong>Precision/Recall:</strong> At various IoU thresholds (0.5, 0.75)</li>
<li><strong>Average Precision (AP):</strong> Area under precision-recall curve</li>
<li><strong>Mean Average Precision (mAP):</strong> Mean AP across classes</li>
</ul>
<p><strong>Philippine Example: Flood mapping evaluation</strong></p>
<pre><code>Confusion Matrix (DOST-ASTI DATOS flood detection):
                Predicted
              | Flood | No Flood |
Actual Flood  |  450  |   50     |  Producer's Acc (Recall): 90%
Actual No Flood|  30   |  1470    |  Producer's Acc: 98%

User's Accuracy (Precision): 93.8%   96.7%
Overall Accuracy: 96%
F1-Score (Flood class): 91.8%</code></pre>
<p><strong>Spatial Validation Best Practices:</strong></p>
<ol type="1">
<li><strong>Spatial k-fold cross-validation:</strong> Divide data into spatially homogeneous clusters</li>
<li><strong>Buffered leave-one-out:</strong> Create buffer zones around test samples</li>
<li><strong>Independent geographic testing:</strong> Test on completely different regions</li>
<li><strong>Temporal validation:</strong> Train on one time period, test on another</li>
</ol>
</section>
<section id="step-7-deployment-and-operationalization" class="level3">
<h3 class="anchored" data-anchor-id="step-7-deployment-and-operationalization">Step 7: Deployment and Operationalization</h3>
<p><strong>Making the model operational:</strong></p>
<p><strong>Deployment strategies:</strong></p>
<ol type="1">
<li><strong>Batch processing:</strong> Apply model to large archives (entire countries, multi-year time series)</li>
<li><strong>Near real-time:</strong> Process new satellite acquisitions automatically (disaster response)</li>
<li><strong>On-demand:</strong> User-triggered analysis (web portals, APIs)</li>
<li><strong>Edge processing:</strong> On-board satellite AI (ESA Φsat-2, launched 2024)</li>
</ol>
<p><strong>On-Board AI Processing (2024 Breakthrough):</strong> - ESA Φsat-2: 22cm CubeSat with on-board AI - Processes imagery directly in orbit - Cloud filtering: Only clear, usable images sent to Earth - Reduces data transmission costs, enables real-time event detection - <strong>Rationale:</strong> With 1,052 active EO satellites generating thousands of terabytes daily, traditional communication cannot relay this volume</p>
<p><strong>Operational considerations:</strong></p>
<ul>
<li><strong>Scalability:</strong> Can it handle regional/national scale?</li>
<li><strong>Automation:</strong> Minimize manual intervention</li>
<li><strong>Performance monitoring:</strong> Track accuracy over time, detect distribution shifts</li>
<li><strong>Model retraining:</strong> Update as conditions change or new data becomes available</li>
<li><strong>Model versioning:</strong> Maintain reproducibility and track improvements</li>
<li><strong>Integration:</strong> Connect to decision support systems, early warning platforms</li>
<li><strong>API development:</strong> Create accessible interfaces for inference</li>
<li><strong>Cloud deployment:</strong> Google Earth Engine, AWS SageMaker, Azure ML, Vertex AI</li>
</ul>
<p><strong>Philippine context:</strong></p>
<ul>
<li><strong>DOST-ASTI AIPI platform:</strong> For model deployment and user-facing AI interfaces</li>
<li><strong>DIMER repository:</strong> For model sharing and democratizing access to trained models</li>
<li><strong>ALaM (Automated Labeling Machine):</strong> Combining automated labeling with crowdsourcing for continuous data quality improvement</li>
<li><strong>Integration with LGU disaster response protocols:</strong> DATOS outputs delivered to local government units</li>
<li><strong>Delivery via PhilSA Digital Space Campus:</strong> Training and capacity building</li>
<li><strong>CoPhil Data Centre (2025):</strong> Cloud-native distribution with API-driven access</li>
</ul>
<hr>
</section>
</section>
<section id="part-3-types-of-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="part-3-types-of-machine-learning">Part 3: Types of Machine Learning</h2>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    subgraph Supervised["SUPERVISED LEARNING&lt;br/&gt;Learning from labeled examples"]
        S1[Classification&lt;br/&gt;Discrete categories]
        S2[Regression&lt;br/&gt;Continuous values]
        S3[Object Detection&lt;br/&gt;Locate + classify]
        S4[Semantic Segmentation&lt;br/&gt;Pixel-wise classification]

        S1 --&gt; S1A[Land Cover&lt;br/&gt;Forest, Water, Urban]
        S2 --&gt; S2A[Biomass Estimation&lt;br/&gt;Predict AGB in tons/ha]
        S3 --&gt; S3A[Building Detection&lt;br/&gt;YOLO, Faster R-CNN]
        S4 --&gt; S4A[Flood Mapping&lt;br/&gt;U-Net segmentation]
    end

    subgraph Unsupervised["UNSUPERVISED LEARNING&lt;br/&gt;Finding patterns without labels"]
        U1[Clustering&lt;br/&gt;Group similar pixels]
        U2[Dimensionality&lt;br/&gt;Reduction]
        U3[Anomaly&lt;br/&gt;Detection]

        U1 --&gt; U1A[K-Means&lt;br/&gt;ISODATA&lt;br/&gt;Spectral clusters]
        U2 --&gt; U2A[PCA&lt;br/&gt;MNF Transform&lt;br/&gt;Band reduction]
        U3 --&gt; U3A[Outlier Detection&lt;br/&gt;Change hotspots]
    end

    subgraph SemiSupervised["SEMI-SUPERVISED&lt;br/&gt;Combine labeled + unlabeled"]
        SS1[Self-training&lt;br/&gt;Pseudo-labeling]
        SS2[Co-training&lt;br/&gt;Multiple views]
        SS1 --&gt; SS1A[Bootstrap from&lt;br/&gt;limited labels]
    end

    subgraph Reinforcement["REINFORCEMENT LEARNING&lt;br/&gt;Learn from interaction"]
        R1[Agent-Environment&lt;br/&gt;Interaction]
        R1 --&gt; R1A[Drone Path&lt;br/&gt;Planning]
    end

    style Supervised fill:#e6ffe6,stroke:#00aa44,stroke-width:3px
    style Unsupervised fill:#ffe6e6,stroke:#cc0044,stroke-width:3px
    style SemiSupervised fill:#fff4e6,stroke:#ff8800,stroke-width:3px
    style Reinforcement fill:#e6e6ff,stroke:#6666cc,stroke-width:3px
</pre>
</div>
<p></p><figcaption> Machine Learning Paradigms for Earth Observation</figcaption> </figure><p></p>
</div>
</div>
</div>
<section id="supervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="supervised-learning">Supervised Learning</h3>
<p><strong>Learning from labeled data</strong></p>
<p>The algorithm is given: - <strong>Input:</strong> Satellite image or features - <strong>Output:</strong> Known label (class or value) - <strong>Goal:</strong> Learn mapping from input to output</p>
<p>Supervised learning is the predominant approach in Earth Observation, where models learn from labeled training data to make predictions on new, unseen data.</p>
<section id="classification-tasks" class="level4">
<h4 class="anchored" data-anchor-id="classification-tasks">Classification Tasks</h4>
<p><strong>Predicting categorical labels</strong></p>
<p><strong>Common Algorithms:</strong> - <strong>Random Forest (RF):</strong> Ensemble method, best performance in object-based classification; robust to noise - <strong>Support Vector Machines (SVM):</strong> Effective for high-dimensional spaces; performs well with limited training samples - <strong>CNNs (Convolutional Neural Networks):</strong> Deep learning for automatic feature learning and complex patterns - <strong>Vision Transformers (ViT):</strong> Attention mechanisms for global context and long-range dependencies</p>
<p><strong>EO Examples:</strong></p>
<ol type="1">
<li><strong>Land Cover Classification</strong>
<ul>
<li>Input: Sentinel-2 pixel values (13 bands)</li>
<li>Output: Forest, Water, Urban, Agriculture, Bare soil, Wetlands</li>
<li>Algorithm: Random Forest, CNN, Vision Transformer</li>
<li>Datasets: EuroSAT (27,000 images, 10 classes, 98.57% accuracy)</li>
</ul></li>
<li><strong>Cloud Detection</strong>
<ul>
<li>Input: Multi-band imagery (blue, cirrus bands effective)</li>
<li>Output: Cloud vs.&nbsp;Clear, Cloud shadow, Cirrus</li>
<li>Algorithm: Threshold, Random Forest, U-Net for pixel-wise segmentation</li>
<li>Sentinel-2 SCL: Scene Classification Layer with 11 classes</li>
</ul></li>
<li><strong>Crop Type Mapping</strong>
<ul>
<li>Input: Multi-temporal NDVI, SAR backscatter</li>
<li>Output: Rice, Corn, Sugarcane, Coconut, Vegetables</li>
<li>Algorithm: Random Forest, LSTM (for temporal patterns)</li>
<li>Multi-temporal data &gt; single-date imagery for capturing phenology</li>
</ul></li>
</ol>
<div class="philippine-context">
<p><strong>Philippine Case Study: PhilSA-DENR Mangrove Mapping</strong></p>
<p><strong>Task:</strong> AI-based mangrove forest identification nationwide</p>
<p><strong>Technology:</strong> - Google Earth Engine platform - Mangrove Vegetation Index (MVI) - Sentinel-1 and Sentinel-2 fusion - Multi-temporal Landsat 8 and Sentinel-2 data</p>
<p><strong>Approach:</strong> - U-Net deep learning architecture - Binary classification: mangrove vs.&nbsp;non-mangrove - SAR for cloud-penetrating capability during monsoon</p>
<p><strong>Performance:</strong> - Accuracy: 99.73% (Myanmar study using similar approach) - Random Forest classifier also effective with high temporal resolution (5-day Sentinel-2) - Support Vector Machine shows high accuracy for mangrove discrimination</p>
<p><strong>Applications:</strong> - Blue carbon programs and carbon stock monitoring - Ecosystem service valuation - Conservation planning and restoration monitoring - Palawan multi-spatiotemporal analysis with Markov chain future trend prediction</p>
<p><strong>Result:</strong> Operational nationwide mangrove extent maps with regular updates</p>
</div>
<p><strong>Object Detection:</strong></p>
<p>Unlike pixel-level classification, object detection identifies and localizes specific objects of interest with bounding boxes.</p>
<p><strong>Popular Architectures:</strong> - <strong>YOLO (You Only Look Once):</strong> Real-time detection, single-stage architecture, fast inference - <strong>Faster R-CNN:</strong> Two-stage detector with region proposals, high accuracy - <strong>RetinaNet:</strong> Single-stage with focal loss for handling class imbalance - <strong>EfficientDet:</strong> Scalable architecture balancing accuracy and efficiency</p>
<p><strong>Key Considerations:</strong> - Scale variation: Objects appear at different sizes depending on altitude and resolution - Dense object detection: Multiple overlapping objects in urban or agricultural scenes - Small object detection: Challenging for standard architectures (e.g., individual trees, vehicles)</p>
<p><strong>Applications:</strong> - Building footprint extraction (SpaceNet, xView datasets) - Ship detection in maritime surveillance - Vehicle counting in traffic monitoring - Individual tree crown delineation</p>
<p><strong>Benchmark Dataset: xView</strong> - &gt;1 million objects - 60 classes - &gt;1,400 km² coverage - 0.3m resolution (WorldView-3) - Purpose: Disaster response, overhead imagery analysis</p>
</section>
<section id="regression-tasks" class="level4">
<h4 class="anchored" data-anchor-id="regression-tasks">Regression Tasks</h4>
<p><strong>Predicting continuous values</strong></p>
<p><strong>EO Examples:</strong></p>
<ol type="1">
<li><strong>Biomass Estimation</strong>
<ul>
<li>Input: Sentinel-1 SAR backscatter, Sentinel-2 vegetation indices, LiDAR (GEDI)</li>
<li>Output: Forest biomass (tons per hectare), Above-ground biomass (AGB)</li>
<li>Algorithm: Random Forest Regression (most popular: R² = 0.70, RMSE = 25.38 Mg C ha⁻¹)</li>
<li>Multi-sensor integration: LiDAR + SAR + Optical improves accuracy</li>
<li><strong>Note:</strong> SAR saturates at high biomass levels; LiDAR methods achieve ~90% agreement with field data</li>
</ul></li>
<li><strong>Soil Moisture Prediction</strong>
<ul>
<li>Input: Sentinel-1 VV/VH polarization, temperature, NDVI</li>
<li>Output: Volumetric soil moisture (%)</li>
<li>Algorithm: Neural network regression, Random Forest</li>
</ul></li>
<li><strong>Crop Yield Forecasting</strong>
<ul>
<li>Input: NDVI time series, EVI, NDMI, weather data (temperature, precipitation), soil properties</li>
<li>Output: Expected yield (tons per hectare)</li>
<li>Algorithm: LSTM regression (preferred in &gt;40% of studies), Random Forest</li>
<li>Performance: R² &gt; 0.93 for corn and soybean, RMSE &lt; 0.075 for NDVI estimation</li>
<li>Temporal considerations: Early-season (higher uncertainty) vs.&nbsp;end-of-season (more accurate, less actionable)</li>
</ul></li>
</ol>
<div class="philippine-context">
<p><strong>Philippine Operational System: PRiSM Rice Yield Prediction</strong></p>
<p><strong>Overview:</strong> Philippine Rice Information System, operational since 2014</p>
<p><strong>Technology:</strong> - Synthetic Aperture Radar (SAR): Day and night, cloud-penetrating - Crop modeling and cloud computing - UAV imagery and smartphone field surveys - Statistical data integration</p>
<p><strong>Data Sources:</strong> - Remote sensing satellites (Sentinel-1, RADARSAT) - Vegetation indices (NDVI, EVI) - Weather data from PAGASA - Historical yield records</p>
<p><strong>Modeling:</strong> - LSTM networks for temporal modeling of SAR backscatter and vegetation indices - Random Forest for integrating multi-source data - Phenology-based models</p>
<p><strong>Partners:</strong> - International Rice Research Institute (IRRI) - technology development - Philippine Rice Research Institute (PhilRice) - operations since 2018 - Department of Agriculture (DA) - policy formulation and planning</p>
<p><strong>Applications:</strong> - Food security planning and policy formulation - Disaster response (typhoon impact assessment) - Crop insurance (PCIC integration) - Per-season mapping: Wet season (June-Nov), Dry season (Dec-May)</p>
<p><strong>Significance:</strong> First satellite-based rice monitoring system in Southeast Asia, model for regional applications</p>
</div>
<p><strong>Key difference from classification:</strong> - Output is a number on a continuous scale rather than discrete classes - Loss functions measure distance from true value (MSE, MAE, RMSE) - Evaluation uses regression metrics (R², RMSE, MAE) - Predictions can be interpolated and extrapolated</p>
</section>
</section>
<section id="unsupervised-learning" class="level3">
<h3 class="anchored" data-anchor-id="unsupervised-learning">Unsupervised Learning</h3>
<p><strong>Finding patterns in unlabeled data</strong></p>
<p>The algorithm receives: - <strong>Input:</strong> Satellite imagery or features - <strong>No labels provided</strong> - <strong>Goal:</strong> Discover inherent structure or groupings</p>
<p>Unsupervised learning techniques do not require labeled training data, making them valuable for exploratory analysis, data reduction, and scenarios where ground-truth is unavailable or expensive to obtain.</p>
<section id="clustering" class="level4">
<h4 class="anchored" data-anchor-id="clustering">Clustering</h4>
<p><strong>Grouping similar pixels/regions together</strong></p>
<p><strong>Common algorithm: k-means</strong></p>
<ol type="1">
<li>Specify number of clusters (k)</li>
<li>Algorithm iteratively groups pixels with similar spectral characteristics</li>
<li>Result: Image segmented into k clusters</li>
<li><strong>Human interpretation needed:</strong> “Cluster 1 looks like water, Cluster 2 like forest…”</li>
</ol>
<p><strong>Other Clustering Methods:</strong></p>
<p><strong>Hierarchical Clustering:</strong> - Builds tree-like structure (dendrogram) of nested clusters - Agglomerative (bottom-up) or Divisive (top-down) - No need to specify number of clusters a priori - Applications: Multi-scale land cover analysis, ecological zone identification</p>
<p><strong>DBSCAN (Density-Based Spatial Clustering):</strong> - Groups points based on density - Identifies clusters of arbitrary shape - Robust to outliers - Applications: Urban area detection, anomaly detection in satellite imagery</p>
<p><strong>EO Applications:</strong></p>
<ul>
<li><strong>Exploratory analysis:</strong> “How many distinct spectral classes in this region?”</li>
<li><strong>Change detection:</strong> Cluster before/after images to find anomalies</li>
<li><strong>Image segmentation:</strong> Group similar pixels for object-based analysis</li>
<li><strong>InSAR time series:</strong> K-means with PCA for identifying spatially and temporally coherent displacement phenomena</li>
</ul>
</section>
<section id="dimensionality-reduction" class="level4">
<h4 class="anchored" data-anchor-id="dimensionality-reduction">Dimensionality Reduction</h4>
<p><strong>Principal Component Analysis (PCA):</strong> - Linear transformation projecting high-dimensional data onto orthogonal axes of maximum variance - Applications: Hyperspectral data compression, feature extraction, noise reduction, change detection - <strong>Workflow:</strong> Center data → Compute covariance → Calculate eigenvalues/eigenvectors → Select top K components - <strong>Benefits:</strong> Reduces computational requirements, removes redundancy, enhances signal-to-noise ratio - First few PCs capture most variance</p>
<p><strong>Independent Component Analysis (ICA):</strong> - Separates multivariate signal into independent, non-Gaussian components - Applications: Mixed pixel decomposition, blind source separation, endmember extraction in hyperspectral imagery</p>
<p><strong>t-SNE and UMAP:</strong> - Non-linear dimensionality reduction for visualization and exploratory analysis - Preserves local structure, reveals clusters and patterns in 2D/3D - Applications: Visualization of high-dimensional feature spaces, exploration of spectral diversity - <strong>Limitations:</strong> Computationally expensive, hyperparameter sensitive, not suitable for new data projection (t-SNE)</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>When to Use Unsupervised Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Advantages:</strong> - No need for expensive labeled data - Can discover unexpected patterns - Good for initial data exploration - Data reduction for preprocessing</p>
<p><strong>Limitations:</strong> - Results need interpretation - No guarantee clusters match desired classes - Often less accurate than supervised methods for specific tasks - Difficult to evaluate objectively without labels - Determining optimal number of clusters can be challenging</p>
<p><strong>Best Practice:</strong> Use unsupervised methods for exploration, then refine with supervised learning for operational applications</p>
</div>
</div>
<p><strong>Comparison Example:</strong></p>
<p><strong>Supervised (Land Cover Classification):</strong> - Provide 1000 labeled samples: forest, water, urban - Train Random Forest - Result: Every pixel assigned forest/water/urban - Evaluation: 90% accuracy against test labels</p>
<p><strong>Unsupervised (k-means Clustering):</strong> - No labels provided - Run k-means with k=3 - Result: Three clusters emerge - Interpretation: Cluster A=water, B=vegetation, C=mixed urban/bare - Evaluation: Subjective or requires labels anyway</p>
<p><strong>Integration Strategy:</strong> 1. Use clustering to create initial training samples 2. Apply dimensionality reduction (PCA) before classification 3. Combine unsupervised pre-training with supervised fine-tuning 4. Use clustering for quality control of labeled data</p>
<hr>
</section>
</section>
</section>
<section id="part-4-deep-learning-architectures-for-earth-observation" class="level2">
<h2 class="anchored" data-anchor-id="part-4-deep-learning-architectures-for-earth-observation">Part 4: Deep Learning Architectures for Earth Observation</h2>
<section id="what-is-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="what-is-deep-learning">What is Deep Learning?</h3>
<p><strong>Deep Learning = Neural Networks with Many Layers</strong></p>
<ul>
<li>Subset of machine learning</li>
<li>Inspired by biological neurons</li>
<li>Multiple processing layers extract progressively abstract features</li>
<li>Dominant approach for image analysis since ~2012</li>
<li>Revolutionized Earth Observation, enabling automated feature learning and state-of-the-art performance</li>
</ul>
<p><strong>Why “deep”?</strong> - Refers to depth: many hidden layers - Modern networks: 10s to 100s of layers (ResNet-152 has 152 layers) - Enables learning complex, hierarchical representations - Lower layers: Edges, textures - Middle layers: Patterns, shapes - Higher layers: Semantic concepts, objects</p>
<p><strong>Key Advantages for EO:</strong> - Automatic feature extraction from raw pixels - Spatial awareness through convolutional operations - Multi-scale analysis capabilities - Handles large, complex datasets - Transfer learning reduces data requirements</p>
</section>
<section id="neural-network-fundamentals" class="level3">
<h3 class="anchored" data-anchor-id="neural-network-fundamentals">Neural Network Fundamentals</h3>
<section id="the-artificial-neuron" class="level4">
<h4 class="anchored" data-anchor-id="the-artificial-neuron">The Artificial Neuron</h4>
<p><strong>Building block of neural networks:</strong></p>
<pre><code>Inputs (x1, x2, x3) → [Weighted Sum + Bias] → Activation Function → Output</code></pre>
<p><strong>Mathematical operation:</strong></p>
<ol type="1">
<li><strong>Weighted sum:</strong> <code>z = w1*x1 + w2*x2 + w3*x3 + b</code></li>
<li><strong>Activation function:</strong> <code>output = activation(z)</code></li>
</ol>
<p><strong>Example: Detecting bright pixels</strong></p>
<pre><code>Inputs: [Red=0.8, Green=0.7, NIR=0.9]
Weights: [w1=1.0, w2=1.0, w3=1.0]
Bias: b = -2.0

z = 1.0*0.8 + 1.0*0.7 + 1.0*0.9 - 2.0 = 0.4
output = ReLU(0.4) = 0.4  (indicates moderately bright)</code></pre>
</section>
<section id="network-architecture" class="level4">
<h4 class="anchored" data-anchor-id="network-architecture">Network Architecture</h4>
<p><strong>Layers of neurons:</strong></p>
<ol type="1">
<li><strong>Input Layer:</strong> Receives raw data (e.g., pixel values from all spectral bands)</li>
<li><strong>Hidden Layers:</strong> Process and transform data through learned representations</li>
<li><strong>Output Layer:</strong> Produces final prediction (class probabilities or continuous values)</li>
</ol>
<p><strong>For a simple image classification:</strong></p>
<pre><code>Input Layer (256 neurons = 16×16 image)
   ↓
Hidden Layer 1 (128 neurons with ReLU)
   ↓
Hidden Layer 2 (64 neurons with ReLU)
   ↓
Output Layer (5 neurons = 5 classes, softmax activation)</code></pre>
<p>Each connection has a <strong>weight</strong> - the network learns optimal weights through training via backpropagation.</p>
</section>
<section id="activation-functions" class="level4">
<h4 class="anchored" data-anchor-id="activation-functions">Activation Functions</h4>
<p><strong>Introduce non-linearity - crucial for learning complex patterns</strong></p>
<p><strong>Common activation functions:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 14%">
<col style="width: 20%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Function</th>
<th>Equation</th>
<th>Range</th>
<th>Use Case</th>
<th>Properties</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>ReLU</strong></td>
<td><code>max(0, x)</code></td>
<td>[0, ∞)</td>
<td>Hidden layers (most common)</td>
<td>Simple, efficient, avoids vanishing gradient</td>
</tr>
<tr class="even">
<td><strong>Sigmoid</strong></td>
<td><code>1 / (1 + e^-x)</code></td>
<td>(0, 1)</td>
<td>Binary classification output</td>
<td>Smooth, probabilistic interpretation</td>
</tr>
<tr class="odd">
<td><strong>Softmax</strong></td>
<td><code>e^xi / Σe^xj</code></td>
<td>(0, 1), sum=1</td>
<td>Multi-class classification output</td>
<td>Converts logits to probabilities</td>
</tr>
<tr class="even">
<td><strong>Tanh</strong></td>
<td><code>(e^x - e^-x) / (e^x + e^-x)</code></td>
<td>(-1, 1)</td>
<td>Hidden layers (older networks)</td>
<td>Zero-centered, smooth</td>
</tr>
<tr class="odd">
<td><strong>LeakyReLU</strong></td>
<td><code>max(αx, x), α=0.01</code></td>
<td>(-∞, ∞)</td>
<td>Hidden layers</td>
<td>Allows small negative gradient</td>
</tr>
<tr class="even">
<td><strong>GELU</strong></td>
<td><code>x * Φ(x)</code></td>
<td>(-∞, ∞)</td>
<td>Transformers</td>
<td>Smooth, stochastic regularization</td>
</tr>
</tbody>
</table>
<p><strong>Why activation functions matter:</strong></p>
<p>Without non-linearity, multiple layers would collapse to a single linear transformation - no benefit from depth! Networks would only learn linear decision boundaries.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>ReLU: The Default Choice
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>ReLU (Rectified Linear Unit)</strong> has become standard for hidden layers because:</p>
<ul>
<li>Simple: <code>f(x) = max(0, x)</code></li>
<li>Computationally efficient</li>
<li>Avoids vanishing gradient problem that plagued sigmoid/tanh</li>
<li>Empirically performs very well across diverse problems</li>
<li>Sparsity: ~50% of neurons set to zero, acting as automatic feature selection</li>
</ul>
</div>
</div>
</section>
<section id="loss-functions" class="level4">
<h4 class="anchored" data-anchor-id="loss-functions">Loss Functions</h4>
<p><strong>Measure how wrong the model’s predictions are</strong></p>
<p>The model’s objective: <strong>minimize the loss function</strong> through gradient descent optimization.</p>
<p><strong>For classification:</strong></p>
<p><strong>Categorical Cross-Entropy (Log Loss):</strong></p>
<pre><code>Loss = -Σ(y_true * log(y_pred))</code></pre>
<ul>
<li>Penalizes confident wrong predictions heavily</li>
<li>Encourages high probability for correct class</li>
<li>Standard for multi-class classification</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code>True class: Forest (encoded as [1, 0, 0, 0, 0])
Prediction: [0.7, 0.1, 0.1, 0.05, 0.05]  ← Good, 70% on forest
Loss = -1*log(0.7) = 0.36

Prediction: [0.2, 0.3, 0.4, 0.05, 0.05]  ← Bad, only 20% on forest
Loss = -1*log(0.2) = 1.61  (much higher penalty)</code></pre>
<p><strong>Binary Cross-Entropy:</strong> - For binary classification (e.g., flood vs.&nbsp;no-flood) - Similar principle, optimized for two classes</p>
<p><strong>Focal Loss:</strong> - Addresses class imbalance by down-weighting well-classified examples - Focuses training on hard examples - Used in RetinaNet object detection</p>
<p><strong>For semantic segmentation:</strong></p>
<p><strong>Dice Loss:</strong> - Based on Dice coefficient: 2×IoU / (1 + IoU) - Differentiable, suitable for optimization - More balanced for small objects - Often used in medical imaging and EO segmentation</p>
<p><strong>IoU Loss:</strong> - Directly optimizes intersection over union - Less strict than Dice for small discrepancies</p>
<p><strong>For regression:</strong></p>
<p><strong>Mean Squared Error (MSE):</strong></p>
<pre><code>Loss = (1/n) * Σ(y_true - y_pred)²</code></pre>
<p><strong>Example: Biomass prediction:</strong></p>
<pre><code>True: 150 tons/ha
Prediction: 140 tons/ha
Error: 10 tons/ha
Squared Error: 100
MSE = 100 (if single sample)</code></pre>
<p><strong>Mean Absolute Error (MAE):</strong> - Less sensitive to outliers than MSE - More robust when errors follow non-Gaussian distribution</p>
<p><strong>Huber Loss:</strong> - Combination of MSE (small errors) and MAE (large errors) - Robust to outliers while maintaining smooth gradient</p>
</section>
<section id="optimizers" class="level4">
<h4 class="anchored" data-anchor-id="optimizers">Optimizers</h4>
<p><strong>Algorithms that adjust weights to minimize loss</strong></p>
<p><strong>The process:</strong></p>
<ol type="1">
<li>Calculate loss on current batch of data</li>
<li>Compute gradients (via backpropagation): how should each weight change?</li>
<li>Update weights in direction that reduces loss</li>
<li>Repeat thousands/millions of times across epochs</li>
</ol>
<p><strong>Common optimizers:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 25%">
<col style="width: 28%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Optimizer</th>
<th>Description</th>
<th>Learning Rate</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>SGD</strong></td>
<td>Stochastic Gradient Descent</td>
<td>Constant or scheduled</td>
<td>Simple, well-understood, good for fine-tuning</td>
</tr>
<tr class="even">
<td><strong>Adam</strong></td>
<td>Adaptive Moment Estimation</td>
<td>Adaptive per parameter</td>
<td>Default choice, usually works well</td>
</tr>
<tr class="odd">
<td><strong>AdamW</strong></td>
<td>Adam with Weight Decay</td>
<td>Adaptive</td>
<td>Improved generalization, Transformers</td>
</tr>
<tr class="even">
<td><strong>RMSprop</strong></td>
<td>Root Mean Square Propagation</td>
<td>Adaptive</td>
<td>Good for RNNs/LSTMs</td>
</tr>
<tr class="odd">
<td><strong>AdaGrad</strong></td>
<td>Adaptive Gradient</td>
<td>Adaptive</td>
<td>Features vary in frequency</td>
</tr>
</tbody>
</table>
<p><strong>Adam is most popular</strong> because: - Adapts learning rate per parameter - Combines benefits of momentum (accelerates convergence) and adaptive learning - Requires minimal tuning - Works well across diverse problems - Default hyperparameters (lr=0.001, β1=0.9, β2=0.999) often sufficient</p>
<p><strong>Learning Rate Scheduling:</strong> - <strong>Step Decay:</strong> Reduce learning rate at fixed intervals - <strong>Cosine Annealing:</strong> Smooth decay following cosine function - <strong>Warm-up:</strong> Gradually increase learning rate at beginning - <strong>ReduceLROnPlateau:</strong> Reduce when validation loss plateaus</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Training Terminology
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Epoch:</strong> One complete pass through the entire training dataset</p>
<p><strong>Batch:</strong> Subset of training data processed together before updating weights</p>
<p><strong>Iteration:</strong> One weight update (one batch processed)</p>
<p><strong>Example:</strong> - Training data: 10,000 samples - Batch size: 100 - 1 epoch = 100 iterations (10,000 / 100) - Training for 50 epochs = 5,000 iterations</p>
<p><strong>Typical batch sizes for EO:</strong> - Images: 16-64 (limited by GPU memory) - Patches: 32-128 - Time series samples: 64-256</p>
</div>
</div>
</section>
<section id="the-training-process" class="level4">
<h4 class="anchored" data-anchor-id="the-training-process">The Training Process</h4>
<p><strong>Iterative improvement:</strong></p>
<pre><code>1. Initialize weights (random or pre-trained)
2. For each epoch:
    For each batch:
        a. Forward pass: Compute predictions
        b. Calculate loss
        c. Backward pass: Compute gradients (backpropagation)
        d. Update weights using optimizer
    e. Evaluate on validation set
    f. Save checkpoint if best performance
3. Stop when validation performance plateaus (early stopping)</code></pre>
<p><strong>Monitoring training:</strong></p>
<ul>
<li><strong>Training loss should decrease</strong> - model learning patterns in training data</li>
<li><strong>Validation loss should decrease</strong> - model generalizing to new data</li>
<li><strong>If validation loss increases while training loss decreases:</strong> Overfitting! Apply regularization.</li>
<li><strong>If both losses remain high:</strong> Underfitting. Need more capacity or better features.</li>
</ul>
<p><strong>Regularization techniques:</strong> - <strong>Dropout:</strong> Randomly deactivate neurons during training - <strong>Weight Decay (L2):</strong> Penalize large weights - <strong>Data Augmentation:</strong> Artificially expand training data - <strong>Early Stopping:</strong> Stop training when validation loss stops improving - <strong>Batch Normalization:</strong> Normalize activations, improves stability</p>
</section>
</section>
<section id="convolutional-neural-networks-cnns" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h3>
<p><strong>CNNs are the foundation of modern computer vision and EO analysis</strong></p>
<section id="why-cnns-excel-at-eo" class="level4">
<h4 class="anchored" data-anchor-id="why-cnns-excel-at-eo">Why CNNs Excel at EO</h4>
<p><strong>Traditional ML:</strong> - Manual feature engineering needed (NDVI, GLCM textures) - Limited ability to capture spatial patterns - Each pixel treated somewhat independently - Features fixed before training</p>
<p><strong>CNNs:</strong> - <strong>Automatic feature extraction</strong> from raw pixels - <strong>Spatial awareness</strong> through convolutional filters - <strong>Hierarchical learning:</strong> edges → textures → objects → scenes - <strong>Translation invariance:</strong> Detects patterns anywhere in image - <strong>Parameter sharing:</strong> Same filters applied across entire image (efficiency) - <strong>Multi-scale analysis:</strong> Through pooling and different kernel sizes</p>
</section>
<section id="cnn-architecture-components" class="level4">
<h4 class="anchored" data-anchor-id="cnn-architecture-components">CNN Architecture Components</h4>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    A[Input Image&lt;br/&gt;Sentinel-2&lt;br/&gt;64x64x13 bands] --&gt; B[Conv Layer 1&lt;br/&gt;32 filters 3x3&lt;br/&gt;ReLU activation]

    B --&gt; C[Max Pooling&lt;br/&gt;2x2&lt;br/&gt;32x32x32]

    C --&gt; D[Conv Layer 2&lt;br/&gt;64 filters 3x3&lt;br/&gt;ReLU activation]

    D --&gt; E[Max Pooling&lt;br/&gt;2x2&lt;br/&gt;16x16x64]

    E --&gt; F[Conv Layer 3&lt;br/&gt;128 filters 3x3&lt;br/&gt;ReLU activation]

    F --&gt; G[Max Pooling&lt;br/&gt;2x2&lt;br/&gt;8x8x128]

    G --&gt; H[Flatten&lt;br/&gt;8192 neurons]

    H --&gt; I[Dense Layer&lt;br/&gt;256 neurons&lt;br/&gt;ReLU + Dropout]

    I --&gt; J[Output Layer&lt;br/&gt;Softmax&lt;br/&gt;6 classes]

    J --&gt; K[Predictions&lt;br/&gt;Forest: 0.85&lt;br/&gt;Water: 0.05&lt;br/&gt;Urban: 0.03&lt;br/&gt;Agriculture: 0.04&lt;br/&gt;Bare: 0.02&lt;br/&gt;Wetlands: 0.01]

    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px
    style B fill:#ffe6e6,stroke:#cc0044,stroke-width:2px
    style C fill:#fff4e6,stroke:#ff8800,stroke-width:2px
    style D fill:#ffe6e6,stroke:#cc0044,stroke-width:2px
    style E fill:#fff4e6,stroke:#ff8800,stroke-width:2px
    style F fill:#ffe6e6,stroke:#cc0044,stroke-width:2px
    style G fill:#fff4e6,stroke:#ff8800,stroke-width:2px
    style I fill:#e6ffe6,stroke:#00aa44,stroke-width:2px
    style J fill:#e6e6ff,stroke:#6666cc,stroke-width:2px
    style K fill:#ccffcc,stroke:#00aa44,stroke-width:2px
</pre>
</div>
<p></p><figcaption> Convolutional Neural Network Architecture for Land Cover Classification</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>Convolutional Layers:</strong> - Apply learnable filters (kernels) across image - Each filter detects specific patterns (edges, textures, shapes) - Example: 3×3 kernel slides across image, computing dot product - Multiple filters per layer (e.g., 64, 128, 256 filters) - Stride controls movement (stride=1: every pixel, stride=2: every other pixel) - Padding preserves spatial dimensions</p>
<p><strong>Pooling Layers:</strong> - Reduce spatial dimensions - Max pooling: Take maximum value in window (common) - Average pooling: Take average value - Increases receptive field - Provides translation invariance - Reduces computational cost</p>
<p><strong>Fully Connected Layers:</strong> - Traditional neural network layers at end - Flatten spatial features - Perform final classification - Often replaced by Global Average Pooling in modern architectures</p>
</section>
<section id="popular-cnn-architectures-for-eo" class="level4">
<h4 class="anchored" data-anchor-id="popular-cnn-architectures-for-eo">Popular CNN Architectures for EO</h4>
<p><strong>VGG Networks (VGG16, VGG19):</strong> - Deep architecture with small (3×3) convolutional filters - Simple, uniform design - 16 or 19 layers - Large memory footprint - <strong>EO Application:</strong> VGG16 with instance normalization applied for LULC classification - Good for transfer learning from ImageNet</p>
<p><strong>ResNet (Residual Networks):</strong> - <strong>Innovation:</strong> Skip connections address vanishing gradient problem - Enables very deep networks (50, 101, 152 layers) - Residual blocks: <code>output = F(x) + x</code> - <strong>Performance:</strong> ResNet-18 and ResNet-50 widely used as encoders in semantic segmentation - <strong>EO Success:</strong> U-Net with ResNet encoder achieved precision 0.943 and recall 0.954 for building extraction - Winner architecture in many EO competitions</p>
<p><strong>Inception/GoogLeNet:</strong> - Multi-scale feature extraction - Parallel convolutions with different kernel sizes (1×1, 3×3, 5×5) - Computationally efficient through 1×1 bottleneck layers - <strong>EO Application:</strong> Multi-scale land cover classification</p>
<p><strong>EfficientNet:</strong> - <strong>Innovation:</strong> Compound scaling of depth, width, and resolution - Optimal balance between accuracy and computational efficiency - EfficientNet-B0 to B7 variants - <strong>EO Application:</strong> Increasingly popular for resource-constrained applications - Mobile deployment, edge computing</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Transfer Learning for EO
</div>
</div>
<div class="callout-body-container callout-body">
<p>Pre-trained CNNs (trained on ImageNet) are widely used in EO:</p>
<p><strong>Advantages:</strong> - Reduce training time - Improve performance with limited data - Lower layers learn generic features (edges, textures) applicable across domains</p>
<p><strong>Considerations:</strong> - ImageNet uses RGB images; EO often has more bands - Solutions: Use only RGB bands, or initialize additional channels with pre-trained weights - Fine-tune all layers or freeze early layers</p>
<p><strong>Recent Research (2024):</strong> Self-supervised pre-training on RS data (SatMAE, SatViT) offers modest improvements over ImageNet in few-shot settings, especially when pre-trained on domain-specific EO data.</p>
</div>
</div>
</section>
</section>
<section id="u-net-and-semantic-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="u-net-and-semantic-segmentation">U-Net and Semantic Segmentation</h3>
<section id="u-net-architecture" class="level4">
<h4 class="anchored" data-anchor-id="u-net-architecture">U-Net Architecture</h4>
<p><strong>Description:</strong> Encoder-decoder architecture with skip connections, originally designed for biomedical image segmentation but widely adopted for Earth Observation.</p>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    subgraph Encoder["ENCODER (Contracting Path)"]
        A[Input&lt;br/&gt;SAR Image&lt;br/&gt;256x256x2&lt;br/&gt;VV, VH] --&gt; B[Conv 3x3&lt;br/&gt;ReLU&lt;br/&gt;64 filters]
        B --&gt; C[Conv 3x3&lt;br/&gt;ReLU&lt;br/&gt;64 filters]
        C --&gt; D[Max Pool 2x2&lt;br/&gt;128x128x64]

        D --&gt; E[Conv 3x3&lt;br/&gt;128 filters]
        E --&gt; F[Conv 3x3&lt;br/&gt;128 filters]
        F --&gt; G[Max Pool 2x2&lt;br/&gt;64x64x128]

        G --&gt; H[Conv 3x3&lt;br/&gt;256 filters]
        H --&gt; I[Conv 3x3&lt;br/&gt;256 filters]
        I --&gt; J[Max Pool 2x2&lt;br/&gt;32x32x256]
    end

    subgraph Bottleneck["BOTTLENECK"]
        J --&gt; K[Conv 3x3&lt;br/&gt;512 filters]
        K --&gt; L[Conv 3x3&lt;br/&gt;512 filters]
    end

    subgraph Decoder["DECODER (Expanding Path)"]
        L --&gt; M[Up-Conv 2x2&lt;br/&gt;256 filters&lt;br/&gt;64x64x256]
        M --&gt; N[Concatenate&lt;br/&gt;with I]
        N --&gt; O[Conv 3x3&lt;br/&gt;256 filters]
        O --&gt; P[Conv 3x3&lt;br/&gt;256 filters]

        P --&gt; Q[Up-Conv 2x2&lt;br/&gt;128 filters&lt;br/&gt;128x128x128]
        Q --&gt; R[Concatenate&lt;br/&gt;with F]
        R --&gt; S[Conv 3x3&lt;br/&gt;128 filters]
        S --&gt; T[Conv 3x3&lt;br/&gt;128 filters]

        T --&gt; U[Up-Conv 2x2&lt;br/&gt;64 filters&lt;br/&gt;256x256x64]
        U --&gt; V[Concatenate&lt;br/&gt;with C]
        V --&gt; W[Conv 3x3&lt;br/&gt;64 filters]
        W --&gt; X[Conv 3x3&lt;br/&gt;64 filters]
    end

    X --&gt; Y[Conv 1x1&lt;br/&gt;2 classes&lt;br/&gt;Sigmoid]
    Y --&gt; Z[Output&lt;br/&gt;Flood Mask&lt;br/&gt;256x256x2&lt;br/&gt;Water/No-Water]

    I -.-&gt;|Skip Connection| N
    F -.-&gt;|Skip Connection| R
    C -.-&gt;|Skip Connection| V

    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px
    style Encoder fill:#ffe6e6,stroke:#cc0044,stroke-width:2px
    style Bottleneck fill:#fff4e6,stroke:#ff8800,stroke-width:2px
    style Decoder fill:#e6ffe6,stroke:#00aa44,stroke-width:2px
    style Z fill:#ccffcc,stroke:#00aa44,stroke-width:3px
</pre>
</div>
<p></p><figcaption> U-Net Architecture for Semantic Segmentation (Flood Mapping Example)</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>Architecture:</strong> - <strong>Encoder (Contracting Path):</strong> Progressively downsamples input (convolutional + pooling), capturing context - <strong>Decoder (Expanding Path):</strong> Upsamples features (transpose convolution), enabling precise localization - <strong>Skip Connections:</strong> Concatenate encoder features with decoder at same resolution, preserving spatial information - Fully symmetric structure</p>
<p><strong>Why U-Net Works Well:</strong> - Skip connections preserve fine-grained spatial information lost during downsampling - Works with relatively small training datasets (important for EO where labels are expensive) - End-to-end pixel-wise predictions - Multi-scale feature fusion</p>
<p><strong>Applications in EO:</strong> - Land cover semantic segmentation - Building footprint extraction - Road network mapping - Crop field delineation - Water body detection - Flood extent mapping</p>
<div class="philippine-context">
<p><strong>Philippine Case Study: Benguet Province Deforestation</strong></p>
<p><strong>Study Details:</strong> - Location: Benguet Province tropical montane forest - Time period: 2015 to early 2022 - Total deforestation detected: 417.93 km² - <strong>Significance:</strong> First deep learning application in Southeast Asian montane forests</p>
<p><strong>Methods:</strong> - Sentinel-1 SAR and Sentinel-2 optical fusion - U-Net deep learning architecture - Comparison with Random Forest and K-Nearest Neighbors</p>
<p><strong>Performance:</strong> - <strong>Accuracy: 99.73%</strong> for binary forest/non-forest classification - Outperformed traditional ML methods - Validated effectiveness of multi-sensor data fusion - Demonstrated U-Net suitability for tropical conditions</p>
<p><strong>Technology Advantages:</strong> - SAR cloud-penetrating capability fills observational gap in tropics - Multi-temporal analysis detects gradual and abrupt changes - Automated processing enables continuous monitoring - Scalable to national level</p>
</div>
<p><strong>U-Net Variants and Improvements:</strong></p>
<p><strong>UNet++:</strong> - Nested skip connections for improved gradient flow - Dense skip pathways - Better feature aggregation</p>
<p><strong>Attention U-Net:</strong> - Incorporates attention mechanisms to focus on relevant features - Attention gates highlight salient features - Improved performance on complex scenes</p>
<p><strong>3D U-Net:</strong> - Extends to volumetric data or multi-temporal stacks - Temporal convolutions for time series - Applications: Crop monitoring, change detection</p>
<p><strong>U-Net with Advanced Encoders:</strong> - <strong>U-Net with ResNet encoder:</strong> Combines U-Net decoder with ResNet encoder - <strong>U-Net with SK-ResNeXt encoder:</strong> Integrates selective kernel and ResNeXt for enhanced feature extraction - <strong>UNetFormer:</strong> Hybrid CNN encoder + Transformer decoder (discussed below)</p>
<p><strong>Performance Metrics:</strong> - Mean IoU (mIoU): Average IoU across all classes - F1-Score per class - Boundary accuracy for precise delineation</p>
</section>
</section>
<section id="deeplab-family" class="level3">
<h3 class="anchored" data-anchor-id="deeplab-family">DeepLab Family</h3>
<p><strong>DeepLab:</strong> State-of-the-art semantic segmentation architecture family</p>
<p><strong>Key Innovations:</strong></p>
<p><strong>Atrous (Dilated) Convolutions:</strong> - Enlarge receptive field without losing resolution - Insert “holes” in convolution kernel - Capture multi-scale context efficiently</p>
<p><strong>Atrous Spatial Pyramid Pooling (ASPP):</strong> - Parallel atrous convolutions with different rates - Captures features at multiple scales - Aggregates information from different receptive fields</p>
<p><strong>Encoder-Decoder Structure (DeepLabv3+):</strong> - Similar to U-Net philosophy - Combines ASPP with decoder for refined boundaries</p>
<p><strong>Performance:</strong> - DeepLabv3+ shows superior performance compared to standard U-Net on many benchmarks - Improved Mean IoU - Better boundary delineation</p>
<p><strong>EO Applications:</strong> - Large-scale land cover mapping - Urban scene segmentation - Agricultural field boundaries</p>
<p><strong>Comparison: U-Net vs.&nbsp;DeepLab</strong> - <strong>U-Net:</strong> Better with limited data, simpler architecture, faster training - <strong>DeepLab:</strong> Better overall performance, more complex, requires more data - <strong>Both:</strong> Widely used in EO, choice depends on data availability and computational resources</p>
</section>
<section id="vision-transformers-vits" class="level3">
<h3 class="anchored" data-anchor-id="vision-transformers-vits">Vision Transformers (ViTs)</h3>
<p><strong>Paradigm shift from convolutions to self-attention mechanisms</strong></p>
<section id="fundamentals" class="level4">
<h4 class="anchored" data-anchor-id="fundamentals">Fundamentals</h4>
<p><strong>Architecture:</strong></p>
<ol type="1">
<li><strong>Patch Embedding:</strong> Divide image into fixed-size patches (e.g., 16×16 pixels)</li>
<li><strong>Linear Projection:</strong> Flatten patches and project to embedding dimension (e.g., 768-D)</li>
<li><strong>Positional Encoding:</strong> Add learnable position information to preserve spatial relationships</li>
<li><strong>Transformer Encoder:</strong> Stack of multi-head self-attention and feed-forward layers</li>
<li><strong>Classification Head:</strong> MLP (Multi-Layer Perceptron) for final prediction</li>
</ol>
<p><strong>Self-Attention Mechanism:</strong> - Models relationships between all image patches - Each patch “attends to” all other patches - Learns which patches are relevant for prediction - Captures long-range dependencies - Adaptively focuses on informative regions</p>
<p><strong>Mathematical Formulation:</strong></p>
<pre><code>Attention(Q, K, V) = softmax(QK^T / √d_k) V

Q = Query (what am I looking for?)
K = Key (what do I contain?)
V = Value (what information do I pass?)</code></pre>
<p><strong>Multi-Head Attention:</strong> - Multiple attention mechanisms in parallel - Each head learns different relationships - Aggregate outputs for richer representation</p>
</section>
<section id="advantages-for-eo" class="level4">
<h4 class="anchored" data-anchor-id="advantages-for-eo">Advantages for EO</h4>
<p><strong>Global Context Modeling:</strong> - Attention mechanism captures relationships across entire image from early layers - CNNs build up receptive field gradually through layers - Particularly valuable for EO where context matters (e.g., urban vs.&nbsp;rural forest)</p>
<p><strong>Long-Range Dependencies:</strong> - Can relate distant image regions - Example: Recognizing rice paddy requires context of surrounding infrastructure, water bodies</p>
<p><strong>Effective for Large-Scale Imagery:</strong> - Scales well to high-resolution satellite images - Efficient self-attention variants reduce computational cost</p>
<p><strong>Strong Transfer Learning:</strong> - Pre-trained ViTs transfer well across tasks - SatViT: Pre-trained on 1.3 million satellite-derived RS images</p>
<p><strong>Handles Variable Input Sizes:</strong> - Flexible patch-based approach - Can adapt to different image resolutions</p>
</section>
<section id="variants-for-remote-sensing" class="level4">
<h4 class="anchored" data-anchor-id="variants-for-remote-sensing">Variants for Remote Sensing</h4>
<p><strong>Swin Transformer:</strong> - <strong>Innovation:</strong> Hierarchical architecture with shifted windows - Local attention within windows (efficient computation) - Shifted window scheme enables cross-window connections - Multi-scale feature representation (like CNN feature pyramids) - State-of-the-art performance on many EO benchmarks</p>
<p><strong>ViT with Spectral Adaptation:</strong> - Modified patch embedding for multi-spectral inputs - Handles variable number of spectral bands (not just RGB) - Pre-training on large satellite image datasets - Applications: Hyperspectral classification, multi-sensor fusion</p>
<p><strong>SatViT:</strong> - Pre-trained on 1.3 million satellite-derived RS images - Domain-specific Vision Transformer for remote sensing - Improved transfer learning performance over ImageNet pre-training - Publicly available for EO community</p>
<p><strong>MS-CLIP (IBM, 2024):</strong> - First vision-language model for multi-spectral Sentinel-2 data - Adapts CLIP dual-encoder architecture for 10+ spectral bands - Enables zero-shot classification and image-text retrieval - <strong>Example:</strong> “Show me images with dense vegetation” without explicit classification</p>
</section>
<section id="challenges" class="level4">
<h4 class="anchored" data-anchor-id="challenges">Challenges</h4>
<p><strong>Data Requirements:</strong> - ViTs require large training datasets (millions of samples) - Less effective with small datasets compared to CNNs - Solution: Transfer learning from pre-trained models (SatViT, ImageNet)</p>
<p><strong>Computational Cost:</strong> - Self-attention quadratic complexity in number of patches - Memory intensive for high-resolution images - Solutions: Swin Transformer (local attention), efficient attention mechanisms</p>
<p><strong>Interpretability:</strong> - Attention maps provide some interpretability - Can visualize which patches model focuses on - Still less intuitive than CNN filter visualizations</p>
</section>
</section>
<section id="hybrid-architectures-unetformer" class="level3">
<h3 class="anchored" data-anchor-id="hybrid-architectures-unetformer">Hybrid Architectures: UNetFormer</h3>
<p><strong>UNetFormer:</strong> Combines CNN encoders with Transformer decoders</p>
<p><strong>Description:</strong> Hybrid architecture leveraging strengths of both CNNs and Transformers</p>
<p><strong>Key Features:</strong> - <strong>CNN Encoder:</strong> ResNet18 captures local spatial features efficiently - <strong>Transformer Decoder:</strong> Models global context and long-range dependencies - <strong>Hybrid Design:</strong> Balances computational efficiency with modeling capacity - Skip connections from encoder to decoder (like U-Net)</p>
<p><strong>Performance:</strong> - State-of-the-art on remote sensing semantic segmentation benchmarks - Particularly effective for urban scene imagery - Outperforms pure CNN and pure Transformer approaches on many tasks</p>
<p><strong>Related Architectures:</strong> - <strong>UNeXt:</strong> Efficient network optimizing depth, width, and resolution - <strong>UNetFormer with boundary enhancement:</strong> Multi-scale approach for improved edge detection - <strong>Segformer:</strong> Transformer encoder + lightweight decoder</p>
<p><strong>When to Use:</strong> - Complex EO scenes requiring both local detail and global context - Semantic segmentation tasks with diverse object scales - When computational resources allow (more expensive than standard U-Net)</p>
</section>
<section id="temporal-models-for-time-series" class="level3">
<h3 class="anchored" data-anchor-id="temporal-models-for-time-series">Temporal Models for Time Series</h3>
<p><strong>Multi-temporal satellite data captures dynamic processes - temporal models extract these patterns</strong></p>
<section id="lstm-and-gru" class="level4">
<h4 class="anchored" data-anchor-id="lstm-and-gru">LSTM and GRU</h4>
<p><strong>LSTM (Long Short-Term Memory):</strong> - <strong>Purpose:</strong> Temporal pattern learning in sequential data - <strong>Architecture:</strong> Recurrent neural network with gating mechanisms - <strong>Gates:</strong> Input gate, forget gate, output gate, cell state - <strong>Advantage:</strong> Learns long-term dependencies, avoids vanishing gradient problem of vanilla RNNs</p>
<p><strong>Applications in EO:</strong> - <strong>Time series classification:</strong> Crop type mapping from multi-temporal NDVI - <strong>Phenology monitoring:</strong> Extracting growing season characteristics - <strong>Yield prediction:</strong> Forecasting crop yields from vegetation index time series - <strong>Change detection:</strong> Detecting disturbances in forest time series - <strong>Weather forecasting:</strong> Climate variables prediction</p>
<p><strong>GRU (Gated Recurrent Unit):</strong> - Simplified version of LSTM - Fewer parameters (faster training) - Often comparable performance to LSTM - Good choice when computational resources are limited</p>
<p><strong>Performance:</strong> - <strong>Most Used:</strong> RNNs applied in &gt;22% of EO time series studies - <strong>LSTMs Preferred:</strong> Used in &gt;40% of crop yield prediction studies - <strong>Accuracy:</strong> R² &gt; 0.93 for corn and soybean yield prediction</p>
<div class="philippine-context">
<p><strong>Philippine Application: Crop Yield Forecasting with LSTM</strong></p>
<p><strong>PRiSM Enhanced with Deep Learning:</strong></p>
<p><strong>Data:</strong> - Multi-temporal Sentinel-1 SAR backscatter (VV, VH polarizations) - Sentinel-2 NDVI time series - PAGASA weather data (rainfall, temperature) - Historical yield records from PhilRice</p>
<p><strong>Approach:</strong> - LSTM network processes time series sequentially - Captures phenological patterns (planting, vegetative growth, reproductive phase, maturity) - Integrates weather variables as auxiliary inputs - Trained on multi-year data across provinces</p>
<p><strong>Performance:</strong> - Earlier and more accurate yield forecasts than statistical models - Mid-season prediction (2-3 months before harvest) with acceptable accuracy - Integration with PRiSM for operational deployment</p>
<p><strong>Applications:</strong> - Food security early warning - Crop insurance (PCIC) - Agricultural planning and market stabilization - Disaster impact assessment</p>
</div>
</section>
<section id="convlstm" class="level4">
<h4 class="anchored" data-anchor-id="convlstm">ConvLSTM</h4>
<p><strong>ConvLSTM:</strong> Combines spatial convolutions with temporal LSTM</p>
<p><strong>Architecture:</strong> - Replaces matrix multiplications in LSTM with convolutional operations - Preserves spatial structure throughout temporal modeling - Input: 3D tensor (time, height, width) - Output: Spatial predictions over time</p>
<p><strong>Advantages:</strong> - Captures both spatial and temporal patterns simultaneously - More parameter efficient than separate spatial and temporal models - End-to-end learning</p>
<p><strong>Applications:</strong> - Weather forecasting and precipitation nowcasting - Flood prediction from time series of meteorological variables - Crop monitoring with spatial context - Spatiotemporal land cover change</p>
</section>
<section id="temporal-attention" class="level4">
<h4 class="anchored" data-anchor-id="temporal-attention">Temporal Attention</h4>
<p><strong>Lightweight Temporal Attention Encoder (L-TAE):</strong> - <strong>Innovation:</strong> Distributes channels among compact attention heads operating in parallel - Outperforms RNNs with fewer parameters and reduced computational complexity - Particularly effective for satellite image time series (SITS) classification</p>
<p><strong>Multi-Head Temporal Attention:</strong> - Each head attends to different temporal patterns - Learn complementary temporal representations - Aggregate outputs for final prediction</p>
<p><strong>Advantages over LSTMs:</strong> - Parallelizable (faster training) - Direct access to all time steps (no sequential bottleneck) - Attention weights provide interpretability (which dates are important?)</p>
<p><strong>Applications:</strong> - Crop type classification from Sentinel-2 time series - Land cover change detection - Phenology extraction</p>
</section>
<section id="temporal-transformers" class="level4">
<h4 class="anchored" data-anchor-id="temporal-transformers">Temporal Transformers</h4>
<p><strong>Transformer for Time Series:</strong> - Self-attention over temporal sequence - Positional encoding preserves temporal order - Can model arbitrarily long sequences</p>
<p><strong>TiMo (2025):</strong> - <strong>Description:</strong> Spatiotemporal vision transformer foundation model - <strong>Innovation:</strong> Hierarchical gyroscope attention mechanism - Captures evolving multi-scale patterns across time and space - Pre-trained on large satellite image time series datasets</p>
<p><strong>Advantages:</strong> - Global temporal context from first layer - Handles variable-length sequences - State-of-the-art performance on temporal EO tasks</p>
<p><strong>Challenges:</strong> - Requires large amounts of training data - Computationally expensive - Best suited for long time series (many observations)</p>
</section>
</section>
<section id="object-detection-architectures" class="level3">
<h3 class="anchored" data-anchor-id="object-detection-architectures">Object Detection Architectures</h3>
<p><strong>Object detection identifies and localizes specific objects with bounding boxes</strong></p>
<section id="yolo-you-only-look-once" class="level4">
<h4 class="anchored" data-anchor-id="yolo-you-only-look-once">YOLO (You Only Look Once)</h4>
<p><strong>Characteristics:</strong> - <strong>Real-time detection:</strong> Single-pass architecture, processes entire image once - <strong>Fast inference:</strong> 30-60+ FPS depending on variant - <strong>Good accuracy-speed trade-off:</strong> Suitable for operational systems - <strong>Single-stage detector:</strong> Predicts bounding boxes and class probabilities directly</p>
<p><strong>Versions:</strong> - <strong>YOLOv3:</strong> Introduced multi-scale predictions - <strong>YOLOv4:</strong> Enhanced training techniques, better accuracy - <strong>YOLOv5:</strong> Popular, well-documented, easy to use (Ultralytics implementation) - <strong>YOLOv6-v8:</strong> Latest, best performance, improved small object detection - <strong>YOLO-NAS:</strong> Neural Architecture Search, state-of-the-art accuracy</p>
<p><strong>Applications in EO:</strong> - <strong>Building detection:</strong> Rapid mapping of structures for disaster damage assessment - <strong>Vehicle detection:</strong> Traffic monitoring, parking lot analysis - <strong>Ship detection:</strong> Maritime surveillance, illegal fishing monitoring - <strong>Small object detection:</strong> Improved in recent versions (important for vehicles, individual trees)</p>
<p><strong>Advantages:</strong> - Fast training and inference - Good generalization - Easy to deploy - Active community and pre-trained models</p>
</section>
<section id="r-cnn-family" class="level4">
<h4 class="anchored" data-anchor-id="r-cnn-family">R-CNN Family</h4>
<p><strong>Faster R-CNN:</strong> - <strong>Architecture:</strong> Two-stage detector - <strong>Stage 1:</strong> Region Proposal Network (RPN) generates candidate object locations - <strong>Stage 2:</strong> Classifies proposals and refines bounding boxes - <strong>Advantage:</strong> High accuracy, especially for diverse object sizes - <strong>Disadvantage:</strong> Slower than single-stage detectors like YOLO</p>
<p><strong>Mask R-CNN:</strong> - <strong>Extension of Faster R-CNN:</strong> Adds instance segmentation branch - <strong>Output:</strong> Bounding box + pixel-level mask for each object - <strong>Applications:</strong> - Building footprints with precise boundaries - Individual tree crown delineation - Object-level change detection - Counting objects (vehicles, animals) with high precision</p>
<p><strong>Performance:</strong> - Generally higher accuracy than YOLO - Better for complex scenes with occlusions - Preferred when accuracy is more important than speed</p>
</section>
<section id="retinanet" class="level4">
<h4 class="anchored" data-anchor-id="retinanet">RetinaNet</h4>
<p><strong>Key Innovation:</strong> - <strong>Focal Loss:</strong> Addresses class imbalance by down-weighting well-classified examples - Focuses training on hard examples - Single-stage detector</p>
<p><strong>Advantages:</strong> - Excellent for imbalanced datasets (common in EO: rare objects like ships, rare land cover classes) - Competitive accuracy with two-stage detectors - Faster than R-CNN family</p>
<p><strong>Applications:</strong> - Rare object detection (e.g., informal settlements, landslides) - Multi-class detection with imbalanced classes</p>
</section>
<section id="efficientdet" class="level4">
<h4 class="anchored" data-anchor-id="efficientdet">EfficientDet</h4>
<p><strong>Key Innovation:</strong> - <strong>Compound scaling:</strong> Jointly scales resolution, depth, and width - <strong>BiFPN (Bi-directional Feature Pyramid Network):</strong> Efficient multi-scale feature fusion</p>
<p><strong>Advantages:</strong> - Optimal balance between accuracy and efficiency - Scalable (EfficientDet-D0 to D7) - Suitable for deployment on resource-constrained devices</p>
<p><strong>Applications:</strong> - Edge computing and mobile deployment - Operational systems requiring fast inference</p>
<p><strong>Comparison Table:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 17%">
<col style="width: 24%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Architecture</th>
<th>Speed</th>
<th>Accuracy</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>YOLO</strong></td>
<td>Very Fast</td>
<td>Good</td>
<td>Real-time applications, rapid mapping</td>
</tr>
<tr class="even">
<td><strong>Faster R-CNN</strong></td>
<td>Slow</td>
<td>High</td>
<td>High-accuracy requirements, diverse object sizes</td>
</tr>
<tr class="odd">
<td><strong>Mask R-CNN</strong></td>
<td>Slow</td>
<td>High</td>
<td>Instance segmentation, precise boundaries</td>
</tr>
<tr class="even">
<td><strong>RetinaNet</strong></td>
<td>Moderate</td>
<td>High</td>
<td>Imbalanced datasets, rare objects</td>
</tr>
<tr class="odd">
<td><strong>EfficientDet</strong></td>
<td>Fast-Moderate</td>
<td>High</td>
<td>Balanced accuracy/speed, deployment</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="multi-modal-architectures" class="level3">
<h3 class="anchored" data-anchor-id="multi-modal-architectures">Multi-Modal Architectures</h3>
<p><strong>Integrating data from multiple sensors for robust monitoring</strong></p>
<section id="optical-sar-fusion" class="level4">
<h4 class="anchored" data-anchor-id="optical-sar-fusion">Optical-SAR Fusion</h4>
<p><strong>Complementary Information:</strong> - <strong>Optical:</strong> Rich spectral information (13 bands for Sentinel-2), sensitive to biochemical properties - <strong>SAR:</strong> Structural information (backscatter), penetrates clouds, sensitive to moisture and geometry</p>
<p><strong>Fusion Strategies:</strong></p>
<p><strong>Early Fusion (Input-level):</strong> - Concatenate inputs at beginning - Example: Stack Sentinel-2 bands with Sentinel-1 VV/VH as additional channels - Simple, but assumes features align semantically - <strong>Advantage:</strong> Single model processes all modalities - <strong>Disadvantage:</strong> May not capture modality-specific patterns optimally</p>
<p><strong>Late Fusion (Decision-level):</strong> - Separate models for each modality - Combine predictions (average, weighted average, voting) - <strong>Advantage:</strong> Each modality processed optimally - <strong>Disadvantage:</strong> Doesn’t exploit inter-modality relationships</p>
<p><strong>Intermediate Fusion (Feature-level):</strong> - Merge features at middle layers - Learn joint representations - <strong>Advantage:</strong> Balances early and late fusion benefits - <strong>Disadvantage:</strong> More complex architecture design</p>
<p><strong>Recent Approaches:</strong></p>
<p><strong>Progressive Fusion Learning:</strong> - Gradually integrates multimodal information - Addresses semantic misalignment between modalities - Applications: Building extraction with optical + SAR</p>
<p><strong>M2Caps (Multi-modal Capsule Networks, 2024):</strong> - Capsule networks for optical-SAR fusion - Applications: Land cover classification - Handles appearance disparities between modalities</p>
<p><strong>Bi-modal Contrastive Learning:</strong> - Self-supervised approach for joint representation - Pre-training on unlabeled optical-SAR pairs - Fine-tune for specific tasks (crop classification, change detection)</p>
<p><strong>Transformer Temporal-Spatial Model (TTSM):</strong> - Synergizes SAR and optical time-series for vegetation monitoring - <strong>Performance:</strong> R² &gt; 0.88 for vegetation reconstruction - Handles missing data in one modality</p>
<div class="philippine-context">
<p><strong>Philippine Application: All-Weather Rice Monitoring</strong></p>
<p><strong>PRiSM Multi-Sensor Approach:</strong></p>
<p><strong>Challenge:</strong> Philippines has &gt;60% cloud cover during monsoon season (June-November)</p>
<p><strong>Solution:</strong> Sentinel-1 SAR + Sentinel-2 optical fusion</p>
<p><strong>Methodology:</strong> - <strong>Sentinel-1 SAR:</strong> Primary data source during wet season (cloud-penetrating) - <strong>Sentinel-2 optical:</strong> Complementary data during dry season and cloud-free periods - <strong>Feature-level fusion:</strong> Combine SAR backscatter (VV, VH) with optical indices (NDVI, EVI) - <strong>Random Forest classifier:</strong> Trained on fused features</p>
<p><strong>Benefits:</strong> - Year-round monitoring regardless of weather - Higher accuracy than single-sensor approach - Reduced data gaps - Continuous rice area tracking</p>
<p><strong>Operational Impact:</strong> - Reliable per-season mapping even during typhoons - Supports disaster damage assessment - Improved yield prediction with temporal SAR backscatter patterns</p>
</div>
<p><strong>Challenges:</strong> - Modality alignment: Different imaging mechanisms (reflectance vs.&nbsp;backscatter) - Semantic misalignment: Features may not correspond across modalities - Optimal fusion level depends on task and data availability - Increased computational cost</p>
<p><strong>Applications:</strong> - All-weather land cover classification - Crop monitoring during cloudy seasons - Building extraction (optical for spectral, SAR for structure) - Flood mapping (SAR for water extent, optical for pre-event land cover) - Forest biomass estimation (optical for species, SAR for structure)</p>
</section>
</section>
<section id="foundation-models-for-earth-observation" class="level3">
<h3 class="anchored" data-anchor-id="foundation-models-for-earth-observation">Foundation Models for Earth Observation</h3>
<p><strong>Foundation models are large, pre-trained models adaptable to various downstream tasks</strong></p>
<p>Emerged as transformative trend in EO 2023-2025, dramatically reducing resources required for environmental monitoring.</p>
<section id="prithvi-family-ibm-nasa" class="level4">
<h4 class="anchored" data-anchor-id="prithvi-family-ibm-nasa">Prithvi Family (IBM-NASA)</h4>
<p><strong>Prithvi-EO-1.0 (August 2023):</strong> - <strong>Scale:</strong> 100 million parameters - <strong>Training Data:</strong> NASA’s Harmonized Landsat Sentinel-2 (HLS) dataset - <strong>Pre-training Strategy:</strong> Masked autoencoder (MAE) - self-supervised learning on unlabeled imagery - <strong>Significance:</strong> World’s largest geospatial AI model at release - <strong>Availability:</strong> Open-source on Hugging Face</p>
<p><strong>Prithvi-EO-2.0 (December 2024):</strong> - <strong>Scale:</strong> 600 million parameters (6× larger than predecessor) - <strong>Training Data:</strong> 4.2 million global time series samples from HLS at 30m resolution - <strong>Architecture:</strong> Temporal transformer with location and temporal embeddings - <strong>Performance:</strong> 75.6% average score on GEO-bench framework (8% improvement over 1.0) - <strong>Availability:</strong> Hugging Face and IBM’s TerraTorch toolkit</p>
<p><strong>Applications Demonstrated:</strong> - <strong>Flood Mapping:</strong> Valencia, Spain floods (October 2024) using Sentinel-1 + Sentinel-2 - <strong>Burn Scar Detection:</strong> Wildfire impact assessment - <strong>Cloud Gap Reconstruction:</strong> Filling missing data in cloudy imagery - <strong>Multi-Temporal Crop Segmentation:</strong> Mapping crop types across United States</p>
<p><strong>Fine-Tuning Workflow:</strong> 1. Load pre-trained Prithvi model 2. Replace classification head for specific task 3. Fine-tune on small labeled dataset (hundreds to thousands of samples) 4. Deploy for inference</p>
<p><strong>Impact:</strong> - Enables users with limited ML expertise to deploy state-of-the-art models - Reduces labeled data requirements by 10-100× - Democratizes access to advanced AI for EO - Foundation for operational systems in resource-constrained settings</p>
<p><strong>Deployment:</strong> - Integrated into IBM’s TerraTorch toolkit for easy fine-tuning - Model zoo with pre-trained variants - Tutorials and example notebooks</p>
</section>
<section id="other-foundation-models" class="level4">
<h4 class="anchored" data-anchor-id="other-foundation-models">Other Foundation Models</h4>
<p><strong>SatMAE:</strong> - Masked autoencoding for satellite imagery - Self-supervised pre-training on unlabeled data - Transfer learning for downstream tasks - Competitive with ImageNet pre-training in few-shot scenarios</p>
<p><strong>SatViT:</strong> - Pre-trained Vision Transformer on 1.3 million satellite-derived RS images - Domain-specific for remote sensing - Improved transfer learning over ImageNet pre-training - Publicly available for EO community</p>
<p><strong>MS-CLIP (IBM, 2024):</strong> - First vision-language model for multi-spectral Sentinel-2 data - Dual encoder architecture adapted from CLIP - Handles 10+ spectral bands (not just RGB) - <strong>Capabilities:</strong> - Zero-shot classification: Classify without task-specific training - Image-text retrieval: “Find images with rice paddies” - Semantic search: Natural language queries over satellite archives</p>
<p><strong>TiMo (2025):</strong> - Spatiotemporal vision transformer foundation model for satellite image time series - Hierarchical gyroscope attention mechanism - Captures evolving multi-scale patterns across time and space - Pre-trained on large temporal satellite datasets</p>
<p><strong>Why Foundation Models Matter:</strong></p>
<ol type="1">
<li><strong>Data Efficiency:</strong> Pre-training on massive unlabeled data, fine-tune with small labeled sets</li>
<li><strong>Generalization:</strong> Learn robust representations applicable across tasks and regions</li>
<li><strong>Democratization:</strong> Lower barrier to entry for EO AI applications</li>
<li><strong>Rapid Deployment:</strong> Quickly adapt to new applications without training from scratch</li>
<li><strong>Transfer Across Domains:</strong> Models pre-trained globally applicable to local Philippine contexts</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Self-Supervised Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Foundation models typically use <strong>self-supervised learning</strong> for pre-training:</p>
<p><strong>Masked Autoencoding (MAE):</strong> - Randomly mask patches of input image - Model learns to reconstruct masked patches - Forces model to learn semantic representations - No labels needed - learns from structure of data itself</p>
<p><strong>Contrastive Learning (MoCo, SimCLR):</strong> - Learn representations by contrasting positive and negative pairs - Augmented views of same image are positive pairs - Different images are negative pairs - Model learns invariance to augmentations</p>
<p><strong>SSL4EO-S12 Dataset:</strong> - Large-scale, global, multimodal corpus from Sentinel-1 and Sentinel-2 - Supports self-supervised pre-training research - Multi-seasonal coverage - Enables research on contrastive learning for remote sensing</p>
</div>
</div>
</section>
</section>
<section id="training-strategies" class="level3">
<h3 class="anchored" data-anchor-id="training-strategies">Training Strategies</h3>
<p><strong>Transfer Learning:</strong> - <strong>Approach:</strong> Pre-train on large dataset (ImageNet, SatViT, Prithvi), fine-tune on task-specific data - <strong>Benefits:</strong> Reduces training time, improves performance with limited data - <strong>Best Practice:</strong> Freeze early layers (generic features), fine-tune later layers (task-specific features) - <strong>Recent Research (2024):</strong> Self-supervised pre-training on RS data offers modest improvements over ImageNet in few-shot settings</p>
<p><strong>Data Augmentation:</strong> - <strong>Rotation and flipping:</strong> Particularly suitable for satellite imagery (no canonical orientation) - <strong>Color jittering:</strong> Simulate atmospheric variations - <strong>Random crops:</strong> Increase spatial diversity - <strong>Mixup and CutMix:</strong> Regularization techniques for classification - <strong>Caution:</strong> Ensure augmentations are realistic for EO (e.g., don’t vertically flip landscapes with clear sky/ground distinction)</p>
<p><strong>Self-Supervised Learning:</strong> - <strong>Contrastive learning:</strong> MoCo, SimCLR for learning representations - <strong>Masked image modeling:</strong> MAE for learning to reconstruct images - <strong>Multi-modal alignment:</strong> CLIP-style vision-language pre-training - <strong>SSL4EO-2024 Summer School:</strong> First summer school on self-supervised learning for EO (July 2024, Copenhagen)</p>
<p><strong>Few-Shot Learning:</strong> - <strong>Motivation:</strong> Limited labeled data, expensive annotation - <strong>Methods:</strong> Metric learning, meta-learning, prototypical networks - <strong>Applications:</strong> Novel land cover classes, rare object detection, new geographic regions - <strong>Example:</strong> Gerry Roxas Foundation deforestation classification achieved 43% accuracy with only 8% training data</p>
<p><strong>Active Learning:</strong> - <strong>Strategy:</strong> Iteratively select most informative samples for labeling - <strong>Process:</strong> Train model → Find uncertain predictions → Label those → Retrain - <strong>Benefits:</strong> Reduced annotation cost (27% improvement in mIoU with only 2% labeled data) - <strong>WeakAL Framework:</strong> Combines active learning and weak supervision, computing &gt;90% of labels automatically while maintaining competitive performance</p>
<p><strong>Best Practices:</strong> - Start with pre-trained weights when available (Prithvi, SatViT, ImageNet) - Use appropriate learning rate schedules (cosine annealing with warm-up) - Apply batch normalization or layer normalization for training stability - Monitor overfitting through validation metrics (gap between train and validation loss) - Implement early stopping and model checkpointing - Use mixed-precision training (FP16) for faster training on modern GPUs</p>
<hr>
</section>
</section>
<section id="part-5-benchmark-datasets-for-training-and-validation" class="level2">
<h2 class="anchored" data-anchor-id="part-5-benchmark-datasets-for-training-and-validation">Part 5: Benchmark Datasets for Training and Validation</h2>
<p><strong>Benchmark datasets enable standardized comparison of algorithms and serve as training resources</strong></p>
<section id="patch-level-classification-datasets" class="level3">
<h3 class="anchored" data-anchor-id="patch-level-classification-datasets">Patch-Level Classification Datasets</h3>
<section id="eurosat" class="level4">
<h4 class="anchored" data-anchor-id="eurosat">EuroSAT</h4>
<p><strong>Specifications:</strong> - <strong>Images:</strong> 27,000 labeled images - <strong>Classes:</strong> 10 land cover types - <strong>Size:</strong> 64×64 pixel patches - <strong>Bands:</strong> 13 (Sentinel-2 multispectral) - <strong>Coverage:</strong> Europe - <strong>Classification Accuracy:</strong> 98.57% achieved with CNNs</p>
<p><strong>Classes:</strong> Annual Crop, Forest, Herbaceous Vegetation, Highway, Industrial Buildings, Pasture, Permanent Crop, Residential Buildings, River, Sea/Lake</p>
<p><strong>Access:</strong> - GitHub: https://github.com/phelber/EuroSAT - TensorFlow Datasets - PyTorch datasets - Commonly used for benchmarking deep learning architectures</p>
</section>
<section id="bigearthnet-v2.0" class="level4">
<h4 class="anchored" data-anchor-id="bigearthnet-v2.0">BigEarthNet v2.0</h4>
<p><strong>Specifications:</strong> - <strong>Patches:</strong> 549,488 paired Sentinel-1 and Sentinel-2 patches - <strong>Size:</strong> 1.2×1.2 km on ground - <strong>Classes:</strong> 19 (CORINE Land Cover nomenclature) - <strong>Type:</strong> Multi-label classification (multiple classes per patch) - <strong>Coverage:</strong> 10 European countries (Austria, Belgium, Finland, Ireland, Kosovo, Lithuania, Luxembourg, Portugal, Serbia, Switzerland)</p>
<p><strong>Key Features:</strong> - Multi-modal (optical + SAR) - Multi-label annotations (real-world complexity) - Large-scale (largest Sentinel dataset)</p>
<p><strong>Access:</strong> - Website: https://bigearth.net/ - TensorFlow Datasets - Papers With Code</p>
<p><strong>Applications:</strong> - Multi-label land cover classification - Multi-modal fusion research - Benchmark for semantic segmentation</p>
</section>
<section id="landcovernet" class="level4">
<h4 class="anchored" data-anchor-id="landcovernet">LandCoverNet</h4>
<p><strong>Specifications:</strong> - Global coverage - Sentinel-2 based - Multi-temporal (annual) - Multiple continents</p>
<p><strong>Applications:</strong> - Global land cover mapping benchmark - Multi-temporal classification - Seasonal analysis and phenology</p>
</section>
</section>
<section id="object-detection-datasets" class="level3">
<h3 class="anchored" data-anchor-id="object-detection-datasets">Object Detection Datasets</h3>
<section id="xview" class="level4">
<h4 class="anchored" data-anchor-id="xview">xView</h4>
<p><strong>Specifications:</strong> - <strong>Objects:</strong> &gt;1 million annotated objects - <strong>Classes:</strong> 60 - <strong>Area:</strong> &gt;1,400 km² - <strong>Resolution:</strong> 0.3m (WorldView-3 satellite) - <strong>Format:</strong> Bounding boxes</p>
<p><strong>Purpose:</strong> - Disaster response applications - Overhead imagery analysis - Object detection benchmarking - Small object detection</p>
<p><strong>Access:</strong> - Website: http://xviewdataset.org/ - Papers With Code - Challenge competitions</p>
</section>
<section id="dota-dataset-for-object-detection-in-aerial-images" class="level4">
<h4 class="anchored" data-anchor-id="dota-dataset-for-object-detection-in-aerial-images">DOTA (Dataset for Object Detection in Aerial Images)</h4>
<p><strong>Specifications:</strong> - <strong>Instances:</strong> 1,793,658 annotated objects - <strong>Categories:</strong> 18 object types - <strong>Images:</strong> 11,268 - <strong>Annotation:</strong> Oriented bounding boxes (OBB) - <strong>Sources:</strong> Google Earth, GF-2 Satellite, aerial platforms</p>
<p><strong>Key Feature:</strong> - <strong>Oriented annotations:</strong> Captures object rotation (important for buildings, ships, aircraft) - Various object orientations and aspect ratios - Multiple sensors and resolutions</p>
<p><strong>Access:</strong> - Website: https://captain-whu.github.io/DOTA/ - Papers With Code - GitHub repositories</p>
</section>
</section>
<section id="semantic-segmentation-datasets" class="level3">
<h3 class="anchored" data-anchor-id="semantic-segmentation-datasets">Semantic Segmentation Datasets</h3>
<section id="openearthmap" class="level4">
<h4 class="anchored" data-anchor-id="openearthmap">OpenEarthMap</h4>
<p><strong>Specifications:</strong> - Global high-resolution land cover mapping benchmark - Multiple continents represented - Semantic segmentation annotations - High-resolution imagery</p>
<p><strong>Purpose:</strong> - Global mapping challenges - Multi-region training and generalization testing - Standardized semantic segmentation evaluation</p>
</section>
<section id="spacenet" class="level4">
<h4 class="anchored" data-anchor-id="spacenet">SpaceNet</h4>
<p><strong>Overview:</strong> Foundation dataset for building footprints and road networks</p>
<p><strong>Versions:</strong> - SpaceNet 1-7: Multiple cities, different tasks - Building footprint extraction - Road network mapping - Flood impact assessment (SpaceNet 8) - Open competition with benchmark results</p>
<p><strong>Applications:</strong> - Building extraction algorithms - Road network detection - Multi-sensor fusion (optical + SAR for SpaceNet 6)</p>
</section>
</section>
<section id="scene-classification-datasets" class="level3">
<h3 class="anchored" data-anchor-id="scene-classification-datasets">Scene Classification Datasets</h3>
<section id="aid-aerial-image-dataset" class="level4">
<h4 class="anchored" data-anchor-id="aid-aerial-image-dataset">AID (Aerial Image Dataset)</h4>
<p><strong>Specifications:</strong> - <strong>Images:</strong> 10,000 - <strong>Categories:</strong> 30 scene categories - <strong>Size:</strong> 600×600 pixels - <strong>Resolution:</strong> 0.5-8m spatial resolution - <strong>Source:</strong> Google Earth imagery</p>
<p><strong>Purpose:</strong> - Scene classification benchmarking - Transfer learning evaluation - Feature extraction research</p>
</section>
<section id="nwpu-resisc45" class="level4">
<h4 class="anchored" data-anchor-id="nwpu-resisc45">NWPU-RESISC45</h4>
<p><strong>Specifications:</strong> - <strong>Categories:</strong> 45 scene types - <strong>Images:</strong> 31,500 (700 per class) - <strong>Size:</strong> 256×256 pixels - <strong>Source:</strong> High-resolution aerial images</p>
<p><strong>Applications:</strong> - Scene recognition - Transfer learning source - Benchmark comparisons</p>
</section>
</section>
<section id="time-series-datasets" class="level3">
<h3 class="anchored" data-anchor-id="time-series-datasets">Time Series Datasets</h3>
<section id="tiselac-time-series-land-cover" class="level4">
<h4 class="anchored" data-anchor-id="tiselac-time-series-land-cover">TiSeLaC (Time Series Land Cover)</h4>
<p><strong>Purpose:</strong> - Multi-temporal classification - Phenology analysis - Temporal pattern learning</p>
<p><strong>Applications:</strong> - Crop type mapping from time series - Vegetation dynamics - Seasonal change detection</p>
</section>
<section id="satellite-image-time-series-sits-datasets" class="level4">
<h4 class="anchored" data-anchor-id="satellite-image-time-series-sits-datasets">Satellite Image Time Series (SITS) Datasets</h4>
<p><strong>Various Sources:</strong> - MODIS time series (daily, 250m-1km) - Sentinel-2 time series (5-day, 10-20m) - Landsat time series (16-day, 30m)</p>
<p><strong>Applications:</strong> - LSTM and temporal attention training - Phenology extraction - Land cover trajectory analysis</p>
</section>
</section>
<section id="philippine-specific-data-resources" class="level3">
<h3 class="anchored" data-anchor-id="philippine-specific-data-resources">Philippine-Specific Data Resources</h3>
<p><strong>Available Operational Data:</strong></p>
<p><strong>PRiSM Products:</strong> - Rice area maps (per season: wet and dry) - Seasonality information (planting dates, growth stages) - Yield estimates - Historical archive since 2014 - Website: https://prism.philrice.gov.ph/</p>
<p><strong>PhilSA Products:</strong> - Flood extent maps from DATOS system - Mangrove extent maps (PhilSA-DENR collaboration) - Land cover maps - Disaster damage assessment outputs - Website: https://philsa.gov.ph/</p>
<p><strong>DOST-ASTI:</strong> - DATOS disaster response maps - Hazard maps (flood, landslide susceptibility) - AI-powered rapid assessments - Website: https://hazardhunter.georisk.gov.ph/map</p>
<p><strong>NAMRIA Geoportal:</strong> - Topographic maps - Land cover basemaps - Administrative boundaries - Digital Elevation Models</p>
<p><strong>Importance of Benchmark Datasets:</strong> 1. <strong>Standardized Evaluation:</strong> Compare algorithms objectively 2. <strong>Training Resources:</strong> Pre-labeled data for model training 3. <strong>Transfer Learning:</strong> Pre-train on large datasets, fine-tune for specific applications 4. <strong>Research Reproducibility:</strong> Enable comparison across studies 5. <strong>Community Building:</strong> Shared resources accelerate progress</p>
<hr>
</section>
</section>
<section id="part-6-data-centric-ai-in-earth-observation" class="level2">
<h2 class="anchored" data-anchor-id="part-6-data-centric-ai-in-earth-observation">Part 6: Data-Centric AI in Earth Observation</h2>
<section id="the-paradigm-shift-2025" class="level3">
<h3 class="anchored" data-anchor-id="the-paradigm-shift-2025">The Paradigm Shift (2025)</h3>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Data &gt; Models
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Old paradigm (Model-Centric AI):</strong> - Focus on developing better algorithms - Keep data fixed, iterate on model architecture - “Our new model achieves 92% accuracy!” - Endless hyperparameter tuning</p>
<p><strong>New paradigm (Data-Centric AI):</strong> - Focus on improving data quality and curation - Keep model fixed (use proven architectures), iterate on data - “Better data improved our model from 85% to 95% accuracy!” - Systematic data improvement</p>
<p><strong>Van der Schaar Lab’s DC-Check Framework:</strong> Argues that reliable ML hinges on characterizing, evaluating, and monitoring training data across the pipeline - not just model complexity.</p>
</div>
</div>
<p><strong>Why the shift?</strong></p>
<ol type="1">
<li><strong>Model architectures have matured:</strong> ResNet, U-Net, LSTM, Transformers are well-established and publicly available</li>
<li><strong>Biggest gains come from data:</strong> Research shows most underperforming models suffer from data issues, not algorithm deficiencies</li>
<li><strong>Real-world deployment:</strong> Data quality determines operational success and trustworthiness</li>
<li><strong>Diminishing returns:</strong> Incremental model improvements yield smaller gains than data improvements</li>
<li><strong>Foundation models:</strong> Pre-trained models (Prithvi, SatViT) reduce need for architecture innovation</li>
</ol>
<p><strong>Data-Centric Principles:</strong></p>
<p>From van der Schaar Lab’s DC-Check framework: - <strong>Characterizing:</strong> Understand training data distribution, coverage, biases - <strong>Evaluating:</strong> Assess data quality, label accuracy, representation - <strong>Monitoring:</strong> Track data drift, performance on subgroups, uncertainty - <strong>Stratification:</strong> Easy/Ambiguous/Hard samples require different treatment - <strong>Data-SUITE:</strong> Suitability, Usefulness, Insufficiency, Thoroughness, Expressiveness checks</p>
<div class="cell" data-fig-width="100%" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TB
    subgraph ModelCentric["MODEL-CENTRIC AI (Old Paradigm)"]
        MC1[Fixed Data] --&gt; MC2[Iterate Models]
        MC2 --&gt; MC3[Tune Hyperparameters]
        MC3 --&gt; MC4[Try New Architectures]
        MC4 --&gt; MC5[85% → 87% → 88%&lt;br/&gt;Diminishing Returns]
    end

    subgraph DataCentric["DATA-CENTRIC AI (2025 Paradigm)"]
        DC1[Proven Architecture&lt;br/&gt;ResNet, U-Net, ViT] --&gt; DC2[Improve Data Quality]
        DC2 --&gt; DC3[Increase Data Quantity]
        DC3 --&gt; DC4[Enhance Data Diversity]
        DC4 --&gt; DC5[Refine Annotations]
        DC5 --&gt; DC6[85% → 92% → 95%&lt;br/&gt;Significant Gains]
    end

    subgraph Pillars["FOUR PILLARS OF DATA-CENTRIC AI"]
        P1[1. Quality&lt;br/&gt;Accurate, consistent&lt;br/&gt;Cloud-free&lt;br/&gt;Atmospherically corrected]
        P2[2. Quantity&lt;br/&gt;Sufficient samples&lt;br/&gt;Per class balance&lt;br/&gt;Training data scale]
        P3[3. Diversity&lt;br/&gt;Geographic coverage&lt;br/&gt;Temporal variation&lt;br/&gt;Seasonal representation]
        P4[4. Annotation&lt;br/&gt;Label accuracy&lt;br/&gt;Boundary precision&lt;br/&gt;Class consistency]
    end

    DC2 --&gt; P1
    DC3 --&gt; P2
    DC4 --&gt; P3
    DC5 --&gt; P4

    subgraph DCCheck["DC-CHECK FRAMEWORK"]
        DCC1[Characterize&lt;br/&gt;Data distribution&lt;br/&gt;Coverage, biases]
        DCC2[Evaluate&lt;br/&gt;Label quality&lt;br/&gt;Representation]
        DCC3[Monitor&lt;br/&gt;Data drift&lt;br/&gt;Performance tracking]
    end

    P1 --&gt; DCC1
    P2 --&gt; DCC1
    P3 --&gt; DCC2
    P4 --&gt; DCC2
    DCC1 --&gt; DCC3
    DCC2 --&gt; DCC3

    DCC3 --&gt; Result[Robust,&lt;br/&gt;Operational&lt;br/&gt;Models]

    style ModelCentric fill:#ffe6e6,stroke:#cc0044,stroke-width:2px
    style DataCentric fill:#e6ffe6,stroke:#00aa44,stroke-width:3px
    style Pillars fill:#e6f3ff,stroke:#0066cc,stroke-width:2px
    style DCCheck fill:#fff4e6,stroke:#ff8800,stroke-width:2px
    style Result fill:#ccffcc,stroke:#00aa44,stroke-width:3px,color:#000
</pre>
</div>
<p></p><figcaption> Data-Centric AI Framework for Earth Observation</figcaption> </figure><p></p>
</div>
</div>
</div>
</section>
<section id="pillar-1-data-quality" class="level3">
<h3 class="anchored" data-anchor-id="pillar-1-data-quality">Pillar 1: Data Quality</h3>
<p><strong>High-quality data is accurate, consistent, and properly processed</strong></p>
<p><strong>For satellite imagery:</strong></p>
<p><strong>Quality issues to address:</strong></p>
<ul>
<li><strong>Cloud contamination:</strong> Use Level-2A with SCL cloud masks, aggressive filtering</li>
<li><strong>Atmospheric effects:</strong> Always use atmospherically corrected data (surface reflectance, not TOA)</li>
<li><strong>Sensor artifacts:</strong> Check for striping, banding, saturation, dead pixels</li>
<li><strong>Geometric accuracy:</strong> Ensure sub-pixel registration across time and sensors</li>
<li><strong>Radiometric consistency:</strong> Calibrate across sensors and acquisition times</li>
<li><strong>Temporal alignment:</strong> Match acquisition dates to ground conditions (phenology, seasonal changes)</li>
</ul>
<div class="philippine-context">
<p><strong>Philippine Challenge: Cloud Cover</strong></p>
<p>Philippines has one of highest cloud cover frequencies globally (&gt;60% during monsoon season).</p>
<p><strong>Data quality solutions:</strong> - <strong>Multi-temporal compositing:</strong> Median over 3-6 months to reduce cloud impact - <strong>Multi-sensor fusion:</strong> Combine optical (Sentinel-2) + SAR (Sentinel-1) which penetrates clouds - <strong>Aggressive cloud masking:</strong> Accept fewer images for higher quality (quality &gt; quantity) - <strong>Leverage dry season:</strong> December-May for optical data acquisition - <strong>Deep learning reconstruction:</strong> Prithvi-EO-2.0 demonstrated cloud gap reconstruction - <strong>Temporal interpolation:</strong> Fill gaps using adjacent clear observations</p>
<p><strong>DATOS System Approach:</strong> - Prioritize Sentinel-1 SAR during typhoon season (cloud-independent) - Rapid processing (10-20 minutes) for disaster response - Multi-temporal composites for flood extent mapping - Integration with pre-event optical data for context</p>
</div>
<p><strong>For training labels:</strong></p>
<p><strong>Quality issues:</strong></p>
<ul>
<li><strong>Positional error:</strong> GPS drift (±5-10m common), georeferencing mismatch</li>
<li><strong>Temporal mismatch:</strong> 2018 labels with 2020 imagery (land cover changes)</li>
<li><strong>Class ambiguity:</strong> Unclear definitions (shrub vs.&nbsp;sparse forest? informal settlement vs.&nbsp;slum?)</li>
<li><strong>Mixed pixels:</strong> Polygon boundaries include multiple classes (especially at coarse resolutions)</li>
<li><strong>Labeling inconsistency:</strong> Different interpreters apply different criteria</li>
<li><strong>Edge effects:</strong> Boundaries between classes often have high uncertainty</li>
<li><strong>Scale mismatch:</strong> Labels created at different resolution than imagery</li>
</ul>
<p><strong>Best practices:</strong></p>
<ol type="1">
<li><strong>Clear class definitions:</strong> Document what each class includes/excludes with examples</li>
<li><strong>Consistent methodology:</strong> Same interpreter(s), same time of year, same reference imagery</li>
<li><strong>Quality control:</strong> Multiple reviewers, consensus protocols, inter-annotator agreement metrics</li>
<li><strong>Temporal alignment:</strong> Labels contemporary with imagery (within months for dynamic classes)</li>
<li><strong>Positional accuracy:</strong> Use high-resolution reference imagery (VHR, Google Earth)</li>
<li><strong>Buffer boundaries:</strong> Consider excluding mixed pixels at class boundaries from training</li>
<li><strong>Metadata:</strong> Record labeling conditions, interpreter, date, confidence level</li>
<li><strong>Iterative refinement:</strong> Use model predictions to identify and correct label errors</li>
</ol>
<p><strong>Training Data Errors Impact:</strong></p>
<p>Research shows training data errors cause substantial errors in final predictions. Example scenarios: - Mislabeled rice paddies → Model confuses rice with other crops - Temporal mismatch → Model learns outdated patterns - Positional errors → Model learns from wrong pixels - Inconsistent labels → Model learns noise rather than signal</p>
</section>
<section id="pillar-2-data-quantity" class="level3">
<h3 class="anchored" data-anchor-id="pillar-2-data-quantity">Pillar 2: Data Quantity</h3>
<p><strong>More data (usually) improves performance, but quality matters more!</strong></p>
<p><strong>How much data do you need?</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 37%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Typical Requirements</th>
<th>With Transfer Learning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Random Forest</td>
<td>100s - 1000s samples per class</td>
<td>Same</td>
</tr>
<tr class="even">
<td>SVM</td>
<td>100s - 1000s samples</td>
<td>Same</td>
</tr>
<tr class="odd">
<td>Simple CNN</td>
<td>1000s - 10,000s samples</td>
<td>100s - 1000s</td>
</tr>
<tr class="even">
<td>Deep CNN (ResNet, U-Net)</td>
<td>10,000s - 100,000s samples</td>
<td>1000s - 10,000s</td>
</tr>
<tr class="odd">
<td>Vision Transformer</td>
<td>100,000s - millions</td>
<td>10,000s - 100,000s</td>
</tr>
<tr class="even">
<td>Foundation Models (pre-training)</td>
<td>Millions - billions</td>
<td>N/A (already pre-trained)</td>
</tr>
<tr class="odd">
<td>Foundation Models (fine-tuning)</td>
<td>N/A</td>
<td>100s - 1000s</td>
</tr>
</tbody>
</table>
<p><strong>Strategies when labeled data is limited:</strong></p>
<p><strong>1. Data Augmentation</strong> - <strong>Geometric:</strong> Rotation, flipping, cropping, scaling, translation - <strong>Photometric:</strong> Brightness, contrast, saturation adjustments - <strong>Noise addition:</strong> Gaussian noise, salt-and-pepper - <strong>Spectral:</strong> Band dropout, mixup between spectral signatures - <strong>Caution:</strong> Ensure augmentations are realistic for EO (e.g., don’t flip images with clear up/down orientation)</p>
<p><strong>2. Transfer Learning</strong> - Use model pre-trained on large dataset (ImageNet, SatMAE, Prithvi) - Fine-tune on your small dataset - Leverages learned features from similar tasks - <strong>Reduces data requirements by 10-100×</strong> - Philippine poverty mapping example: 14.1% improvement using transfer learning</p>
<p><strong>3. Active Learning</strong> - <strong>Process:</strong> Iteratively train model → find uncertain predictions → label those → retrain - Efficiently focuses labeling effort where it matters most - Research shows 27% improvement in mIoU with only 2% labeled data - Prioritize samples near decision boundaries</p>
<p><strong>4. Few-Shot Learning</strong> - <strong>Methods:</strong> Metric learning, meta-learning, prototypical networks - Learn from very few examples per class - Gerry Roxas Foundation deforestation: 43% accuracy with only 8% training data - Useful for rare classes or novel geographic regions</p>
<p><strong>5. Weak Supervision</strong> - Leverage noisy or incomplete labels - <strong>WeakAL framework:</strong> Combines active learning and weak supervision - Computes &gt;90% of labels automatically while maintaining competitive performance - Trade-off: Lower individual label quality, but much larger quantity</p>
<p><strong>6. Synthetic Data</strong> - Generate training data via simulation or GANs - Example: Simulated SAR scenes for flood detection - Useful when real data is dangerous/expensive to collect - Caution: Domain gap between synthetic and real data</p>
<p><strong>7. Self-Supervised Pre-training</strong> - Pre-train on unlabeled data (masked autoencoding, contrastive learning) - Fine-tune on small labeled dataset - Foundation models (Prithvi) exemplify this approach - <strong>SSL4EO-S12:</strong> Large-scale dataset for self-supervised learning</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>2024 Research: Data Efficiency
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Findings from “Data-Centric Machine Learning for Earth Observation” (arXiv 2024):</strong></p>
<ul>
<li>Some EO tasks reach optimal accuracy with <strong>&lt;20% of temporal instances</strong></li>
<li><strong>Single band</strong> from single sensor can be sufficient for specific tasks</li>
<li><strong>Implication:</strong> Smart data selection &gt; brute force data collection</li>
<li>Feature selection and dimensionality reduction crucial</li>
<li>Use PCA, tree-based feature importance, or domain knowledge to identify essential features</li>
</ul>
<p><strong>Takeaway:</strong> Focus on acquiring diverse, high-quality samples rather than maximizing quantity indiscriminately.</p>
</div>
</div>
<div class="philippine-context">
<p><strong>Philippine Solution: ALaM Project (DOST-ASTI)</strong></p>
<p><strong>Automated Labeling Machine (ALaM)</strong> addresses annotation bottleneck:</p>
<p><strong>Approach:</strong> - <strong>Automated labeling:</strong> ML models generate initial labels - <strong>Crowdsourcing:</strong> Distributed verification and correction - <strong>Human-in-the-loop quality control:</strong> Expert review of uncertain labels - <strong>Active learning integration:</strong> Prioritize samples for human review</p>
<p><strong>Benefits:</strong> - Significantly reduces labeling time and cost - Scales to national coverage - Integration with DIMER model repository for continuous improvement - Democratizes access to labeled training data</p>
<p><strong>Integration with SkAI-Pinas:</strong> - Part of national AI framework - Addresses gap between abundant remote sensing data and sustainable AI pipelines - Supports operational systems like DATOS and PRiSM</p>
</div>
</section>
<section id="pillar-3-data-diversity" class="level3">
<h3 class="anchored" data-anchor-id="pillar-3-data-diversity">Pillar 3: Data Diversity</h3>
<p><strong>Representative data covers the full range of scenarios the model will encounter</strong></p>
<p>Models trained on narrow data distributions fail when deployed in diverse real-world conditions. Diversity ensures robustness and generalization.</p>
<p><strong>Dimensions of diversity:</strong></p>
<p><strong>1. Geographic diversity</strong> - Different regions (Luzon, Visayas, Mindanao) - Different ecosystems (lowland rainforest, montane cloud forest, mangrove, coral reef) - Different climate zones (Type I-IV Philippine climate classification) - Urban, peri-urban, rural contexts - Different topography (flat, hilly, mountainous)</p>
<p><strong>2. Temporal diversity</strong> - Different seasons (wet season: June-Nov, dry season: Dec-May) - Different years (inter-annual variability, El Niño vs.&nbsp;La Niña) - Different phenological stages (rice: planting, vegetative, reproductive, maturity) - Different times of day (for SAR: morning vs.&nbsp;evening passes) - Historical baselines and recent conditions</p>
<p><strong>3. Class diversity</strong> - Multiple examples per class capturing intra-class variability - Edge cases and rare types (e.g., burned forest, flooded agriculture) - Transitional zones (forest-agriculture boundary, urban-rural fringe) - Different sub-types (e.g., rice varieties, mangrove species, building materials)</p>
<p><strong>4. Sensor diversity</strong> - Different satellites (Sentinel-2A, 2B, 2C) - Different atmospheric conditions (clear, hazy, dusty) - Different viewing angles (SAR: ascending vs.&nbsp;descending) - Different processing baselines (if applicable) - Multi-sensor when relevant (optical + SAR)</p>
<p><strong>5. Socioeconomic diversity</strong> - Different development contexts (high-density urban, informal settlements, rural villages) - Different agricultural practices (mechanized, traditional, mixed) - Different infrastructure quality (paved roads, dirt tracks)</p>
<p><strong>Example: Urban classification</strong></p>
<p><strong>Poor diversity:</strong> All training samples from Metro Manila CBD (Central Business District)</p>
<p><strong>Result:</strong> Model fails on: - Small provincial towns (different building density, height, materials) - Informal settlements (different patterns, materials, roof types) - Peri-urban areas (mixed land cover, agriculture near buildings) - Historical centers (older building styles)</p>
<p><strong>Good diversity:</strong> Samples from: - <strong>Large cities:</strong> Manila, Cebu, Davao (high-density, modern buildings) - <strong>Medium towns:</strong> Baguio, Iloilo, Cagayan de Oro (mixed density) - <strong>Small municipalities:</strong> Various provinces - <strong>Different building materials:</strong> Concrete, metal roofing, nipa huts, wood - <strong>Different periods:</strong> Capture urban growth and change - <strong>Informal settlements:</strong> Slums, squatter areas - <strong>Peri-urban:</strong> Transition zones</p>
<p><strong>Result:</strong> Model generalizes well across Philippines</p>
<p><strong>Validation of Diversity:</strong></p>
<p>Test model performance on stratified subsets: - Per-region accuracy (does it work in all islands?) - Per-season accuracy (dry vs.&nbsp;wet season) - Per-class accuracy (all classes represented equally well?) - Cross-region generalization (train on Luzon, test on Mindanao)</p>
</section>
<section id="pillar-4-annotation-strategy" class="level3">
<h3 class="anchored" data-anchor-id="pillar-4-annotation-strategy">Pillar 4: Annotation Strategy</h3>
<p><strong>How you label data profoundly impacts model performance</strong></p>
<p>Annotation is often the most expensive and time-consuming part of ML workflow. Strategic annotation maximizes value.</p>
<p><strong>Annotation approaches:</strong></p>
<ol type="1">
<li><strong>Point sampling:</strong> Fast, but limited context, suitable for classification</li>
<li><strong>Polygon delineation:</strong> More information, more time-consuming, required for semantic segmentation</li>
<li><strong>Pixel-level labeling:</strong> Maximum detail, most expensive, essential for precise segmentation</li>
<li><strong>Image-level labels:</strong> Easiest, suitable for scene classification, limited spatial information</li>
<li><strong>Bounding boxes:</strong> For object detection, faster than pixel-level masks</li>
</ol>
<p><strong>Best practices:</strong></p>
<p><strong>1. Expert involvement</strong> - Use domain experts for complex classes (forest types, crop stages, mangrove species) - Train labelers thoroughly on class definitions with examples - Regular calibration sessions to maintain consistency - Document difficult cases and edge cases</p>
<p><strong>2. Quality over quantity</strong> - <strong>500 high-quality labels &gt; 5000 noisy labels</strong> - Invest in review and correction processes - Document difficult cases and ambiguous examples - Use confidence scores to flag uncertain labels</p>
<p><strong>3. Class balance</strong> - Ensure adequate representation of minority classes - Stratified sampling by class (not just random) - Consider class weights in training if imbalanced - Oversampling rare classes or undersampling common classes - <strong>Imbalanced classes:</strong> Major challenge in EO (e.g., rare disasters, rare land cover types)</p>
<p><strong>4. Consensus protocols</strong> - Multiple labelers per sample (especially for ambiguous cases) - Majority vote or adjudication for disagreements - Measure inter-annotator agreement (Cohen’s Kappa, Krippendorff’s Alpha) - Establish minimum agreement threshold (e.g., 80%)</p>
<p><strong>5. Iterative refinement</strong> - Use model predictions to find label errors (disagreement between model and label) - Retrain after improving labels (data-centric iteration) - Focus effort on low-confidence predictions - <strong>Model-in-the-loop labeling:</strong> Model suggests labels, humans verify</p>
<p><strong>6. Annotation tools and platforms</strong> - Use efficient labeling tools (LabelMe, CVAT, Label Studio, Labelbox) - For EO: Tools supporting geospatial formats (GeoTIFF, shapefiles) - Integration with cloud platforms (Google Earth Engine, QGIS) - Export to ML-ready formats</p>
<p><strong>7. Crowdsourcing considerations</strong> - Clear instructions and examples - Quality control through redundancy and expert review - Gamification to maintain engagement - Examples: Humanitarian OpenStreetMap Team (HOT OSM) for disaster mapping</p>
<p><strong>EO-Specific Annotation Challenges:</strong></p>
<p>From Kili Technology’s Earth Observation Data Labeling Guide: - <strong>Sensor diversity:</strong> Different spectral bands, resolutions, formats - <strong>Massive data volumes:</strong> Petabyte-scale archives (“four Vs”: Volume, Velocity, Variety, Veracity) - <strong>Domain expertise requirements:</strong> Complex classes require specialized knowledge - <strong>Weak labeling approaches:</strong> Leverage noisy labels, distant supervision - <strong>Active learning integration:</strong> Prioritize informative samples - <strong>Stakeholder-friendly tooling:</strong> Tools accessible to non-ML experts</p>
<div class="philippine-context">
<p><strong>Philippine Annotation Ecosystem:</strong></p>
<p><strong>ALaM (Automated Labeling Machine - DOST-ASTI):</strong> - Combines automated labeling with crowdsourcing - Human-in-the-loop quality control - Integration with DIMER model repository - Reduces labeling time and cost significantly - <strong>Workflow:</strong> Automated labels → Crowdsourced verification → Expert review → Training data</p>
<p><strong>DATOS (DOST-ASTI):</strong> - Rapid disaster mapping (10-20 minute response) - On-the-fly labeling during disaster response - Iterative refinement based on ground validation - Integration with LGU feedback</p>
<p><strong>Academic Partnerships:</strong> - University of the Philippines - remote sensing courses with labeling components - PhilRice - rice field delineation and crop stage labeling - DENR - forest and mangrove mapping with expert foresters</p>
<p><strong>International Support:</strong> - Humanitarian OpenStreetMap Team (HOT OSM) for disaster mapping - CoPhil training programs on labeling best practices - European Copernicus expertise transfer</p>
</div>
</section>
<section id="examples-data-centric-success-stories" class="level3">
<h3 class="anchored" data-anchor-id="examples-data-centric-success-stories">2025 Examples: Data-Centric Success Stories</h3>
<section id="nasa-ibm-geospatial-foundation-model-prithvi" class="level4">
<h4 class="anchored" data-anchor-id="nasa-ibm-geospatial-foundation-model-prithvi">NASA-IBM Geospatial Foundation Model (Prithvi)</h4>
<p><strong>Open-source model trained on massive HLS dataset (Harmonized Landsat-Sentinel-2)</strong></p>
<p><strong>Data-centric approach:</strong> - <strong>Scale:</strong> Millions of satellite images from HLS (30m resolution, global coverage) - <strong>Self-supervised pre-training:</strong> Masked autoencoding (no labels needed) - <strong>Data quality:</strong> HLS provides analysis-ready data (atmospheric correction, BRDF normalization, co-registration) - <strong>Fine-tuned for specific tasks:</strong> With small labeled datasets (100s-1000s samples)</p>
<p><strong>Result:</strong> - State-of-the-art performance on multiple EO tasks (flood mapping, burn scar detection, crop segmentation) - <strong>Reduces labeled data requirements by 10-100×</strong> - Democratizes access to powerful EO AI - Foundation for operational systems worldwide</p>
<p><strong>Key Insight:</strong> Investment in massive, high-quality pre-training data enables downstream applications with minimal task-specific labels.</p>
</section>
<section id="esa-φsat-2-on-board-ai-launched-2024" class="level4">
<h4 class="anchored" data-anchor-id="esa-φsat-2-on-board-ai-launched-2024">ESA Φsat-2 On-Board AI (Launched 2024)</h4>
<p><strong>22cm CubeSat with on-board AI processing</strong></p>
<p><strong>Data-centric innovation:</strong> - Processes imagery directly on satellite - <strong>Data quality selection happens in space!</strong> - Only transmits actionable information (not raw data) - Cloud filtering: Only clear, usable images sent to Earth - Reduces bandwidth requirements by orders of magnitude - Enables real-time event detection (fires, ships, clouds)</p>
<p><strong>Rationale:</strong> With 1,052 active EO satellites generating thousands of terabytes daily, traditional radio frequency communication cannot relay this volume. On-board AI filters data at source.</p>
<p><strong>Implication:</strong> Data quality and relevance prioritized over quantity. Shift from “collect everything” to “collect intelligently.”</p>
</section>
<section id="earthdaily-constellation" class="level4">
<h4 class="anchored" data-anchor-id="earthdaily-constellation">EarthDaily Constellation</h4>
<p><strong>10-satellite constellation for daily global coverage at 5-10m resolution</strong></p>
<p><strong>Focus on AI-ready data:</strong> - <strong>Scientific-grade calibration:</strong> Rigorous radiometric accuracy - <strong>Consistent, reliable acquisitions:</strong> Predictable revisit times - <strong>Optimized spectral bands for ML:</strong> Bands selected based on ML feature importance - <strong>Emphasis on data quality for algorithm performance:</strong> Analysis-ready data products</p>
<p><strong>Philosophy:</strong> Data quality and consistency are first-class design criteria, not afterthoughts. Build satellites around AI needs.</p>
</section>
<section id="weakal-framework-active-learning-weak-supervision" class="level4">
<h4 class="anchored" data-anchor-id="weakal-framework-active-learning-weak-supervision">WeakAL Framework (Active Learning + Weak Supervision)</h4>
<p><strong>Research from remote sensing ML community</strong></p>
<p><strong>Approach:</strong> - Combines active learning (select informative samples) with weak supervision (leverage noisy labels) - Computes <strong>&gt;90% of labels automatically</strong> while maintaining competitive performance - Human effort focused on most uncertain/informative samples</p>
<p><strong>Results:</strong> - 27% improvement in mIoU with only 2% manually labeled data - Demonstrates data-efficient learning - Practical for large-scale operational mapping</p>
<p><strong>Key Insight:</strong> Strategic data selection and semi-automated labeling can achieve strong performance with minimal human effort.</p>
<hr>
</section>
</section>
</section>
<section id="part-7-explainable-ai-xai-for-earth-observation" class="level2">
<h2 class="anchored" data-anchor-id="part-7-explainable-ai-xai-for-earth-observation">Part 7: Explainable AI (XAI) for Earth Observation</h2>
<section id="why-xai-matters-in-eo" class="level3">
<h3 class="anchored" data-anchor-id="why-xai-matters-in-eo">Why XAI Matters in EO</h3>
<p><strong>The Problem:</strong></p>
<p>Deep learning models are often <strong>“black boxes”</strong> - they produce accurate predictions, but we don’t understand why. For operational EO systems, this creates challenges:</p>
<ol type="1">
<li><strong>Scientific Insights:</strong> Can’t extract physical understanding from model decisions</li>
<li><strong>Bias Detection:</strong> Can’t identify if model relies on spurious correlations (e.g., cloud shadows, artifacts)</li>
<li><strong>Trust and Adoption:</strong> Stakeholders reluctant to use models they don’t understand</li>
<li><strong>Debugging:</strong> Difficult to diagnose errors and improve models</li>
<li><strong>Regulatory/Policy:</strong> Some applications require explainability (e.g., disaster fund allocation)</li>
</ol>
<p><strong>Recent Efforts (2023-2025):</strong></p>
<p>Despite significant advances in deep learning for remote sensing, <strong>lack of explainability remains a major criticism</strong>. The community is increasingly exploring Explainable AI techniques:</p>
<ul>
<li>Increasingly intensive exploration of XAI methods for EO</li>
<li>Integration of attention visualization in transformer architectures</li>
<li>Saliency maps and feature attribution techniques</li>
<li>Trade-off studies: accuracy vs.&nbsp;interpretability</li>
</ul>
</section>
<section id="xai-methods-for-eo" class="level3">
<h3 class="anchored" data-anchor-id="xai-methods-for-eo">XAI Methods for EO</h3>
<p><strong>Gradient-Based Methods:</strong></p>
<p><strong>Grad-CAM (Gradient-weighted Class Activation Mapping):</strong> - <strong>Process:</strong> Compute gradients of target class with respect to final convolutional layer - <strong>Output:</strong> Heatmap highlighting regions important for prediction - <strong>Advantages:</strong> Most interpretable method, computationally efficient, works with any CNN - <strong>Applications:</strong> Visualize which parts of satellite image model focuses on (e.g., “model detects water by focusing on blue spectral signature”)</p>
<p><strong>Guided Backpropagation:</strong> - Visualizes pixels contributing to prediction - Sharper visualizations than Grad-CAM - Highlights fine-grained features</p>
<p><strong>Integrated Gradients:</strong> - Accumulates gradients along path from baseline to input - More robust attributions than simple gradients - Satisfies desirable axioms (sensitivity, implementation invariance)</p>
<p><strong>Perturbation-Based Methods:</strong></p>
<p><strong>Occlusion:</strong> - <strong>Process:</strong> Block image regions and observe prediction change - <strong>Output:</strong> Sensitivity map showing which regions are critical - <strong>Advantages:</strong> High interpretability, intuitive - <strong>Disadvantages:</strong> Computationally expensive (must test many occlusions)</p>
<p><strong>LIME (Local Interpretable Model-agnostic Explanations):</strong> - <strong>Process:</strong> Train simple, interpretable model (e.g., linear) to approximate complex model locally - <strong>Output:</strong> Feature importances for specific prediction - <strong>Advantages:</strong> Model-agnostic, interpretable - <strong>Disadvantages:</strong> Expensive computation, local rather than global explanation</p>
<p><strong>Model-Based Methods:</strong></p>
<p><strong>SHAP (SHapley Additive exPlanations):</strong> - <strong>Process:</strong> Game theory approach - compute contribution of each feature - <strong>Output:</strong> Feature importance values for prediction - <strong>Advantages:</strong> Theoretically grounded, consistent - <strong>Applications:</strong> Explain which spectral bands, indices, or temporal features drive predictions</p>
<p><strong>Attention Visualization (for Transformers):</strong> - <strong>Process:</strong> Visualize attention weights from self-attention mechanism - <strong>Output:</strong> Heatmap showing which patches/regions model attends to - <strong>Advantages:</strong> Built into architecture, interpretable - <strong>Applications:</strong> Vision Transformers (ViT), UNetFormer - see which spatial regions model focuses on</p>
<p><strong>Feature Importance (for Tree-Based Models):</strong> - Random Forest, XGBoost provide feature importance scores - <strong>Output:</strong> Ranking of features by contribution to predictions - <strong>Advantages:</strong> Simple, intuitive, built-in - <strong>Applications:</strong> Understand which spectral bands, indices, temporal features are most informative</p>
</section>
<section id="applications-in-eo" class="level3">
<h3 class="anchored" data-anchor-id="applications-in-eo">Applications in EO</h3>
<p><strong>1. Understanding Model Decisions:</strong> - Visualize which spectral bands contribute most (e.g., does model rely on SWIR for burn detection?) - Identify spatial patterns model focuses on (e.g., texture vs.&nbsp;spectral signature) - Discover unexpected correlations (e.g., model using cloud shadows instead of actual land cover)</p>
<p><strong>2. Discovering Scientific Insights:</strong> - Identify which vegetation indices are most predictive for crop types - Understand temporal patterns in multi-date imagery (which dates are critical for classification?) - Extract biophysical relationships learned by model</p>
<p><strong>3. Detecting and Mitigating Biases:</strong> - Identify if model relies on artifacts (e.g., sensor striping, JPEG compression) - Detect geographic biases (model works in training region, fails elsewhere due to spurious features) - Ensure model uses physically meaningful features</p>
<p><strong>4. Building Trust with Stakeholders:</strong> - Demonstrate to policymakers that model decisions are reasonable - Show LGUs which features drive disaster risk predictions - Explain to farmers why certain fields are flagged for attention</p>
<p><strong>5. Debugging and Improving Models:</strong> - Identify when model makes errors (e.g., confuses rice with water due to flooding) - Guide data collection (which features need more training samples?) - Inform feature engineering (which derived features would help?)</p>
</section>
<section id="challenges-and-trade-offs" class="level3">
<h3 class="anchored" data-anchor-id="challenges-and-trade-offs">Challenges and Trade-Offs</h3>
<p><strong>Accuracy vs.&nbsp;Interpretability:</strong> - Simple models (decision trees, linear regression) are interpretable but less accurate - Complex models (deep CNNs, transformers) are more accurate but less interpretable - <strong>Trade-off:</strong> Choose based on application criticality and stakeholder needs</p>
<p><strong>Computational Cost:</strong> - Post-hoc explanation methods (LIME, occlusion) can be expensive - Gradient-based methods (Grad-CAM) are fast - Consider explanation cost for operational systems</p>
<p><strong>Faithfulness:</strong> - Do explanations truly reflect model’s reasoning, or are they misleading? - Saliency maps can be noisy or highlight irrelevant features - Validation: Compare explanations against domain knowledge</p>
<p><strong>Global vs.&nbsp;Local:</strong> - Local explanations (single prediction) may not generalize - Global explanations (entire model behavior) are harder to compute and interpret - Need both perspectives for complete understanding</p>
</section>
<section id="best-practices-for-xai-in-eo" class="level3">
<h3 class="anchored" data-anchor-id="best-practices-for-xai-in-eo">Best Practices for XAI in EO</h3>
<ol type="1">
<li><strong>Use Multiple Methods:</strong> Different XAI methods can reveal complementary insights</li>
<li><strong>Validate Explanations:</strong> Check against domain knowledge, physical understanding</li>
<li><strong>Integrate into Workflow:</strong> Make XAI routine part of model development, not afterthought</li>
<li><strong>Communicate Effectively:</strong> Visualize explanations clearly for stakeholders (heatmaps, feature importance plots)</li>
<li><strong>Document Limitations:</strong> Be transparent about what explanations can and cannot tell us</li>
<li><strong>Balance Complexity:</strong> For operational systems, consider interpretable models when accuracy difference is small</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>XAI Resources for EO
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Tools:</strong> - <strong>Captum (PyTorch):</strong> Library for model interpretability (Grad-CAM, Integrated Gradients, SHAP) - <strong>SHAP Library:</strong> SHapley Additive exPlanations for Python - <strong>Grad-CAM Implementations:</strong> Available for TensorFlow/Keras and PyTorch - <strong>Attention Visualization:</strong> Built into transformer implementations (HuggingFace Transformers)</p>
<p><strong>Research:</strong> - “Explainable AI for Earth Observation: A Review” (ongoing research area) - SSL4EO-2024 Summer School included XAI sessions - Growing number of papers combining EO and XAI at IGARSS, ISPRS, ML4Earth conferences</p>
</div>
</div>
<hr>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Session 2 Summary
</div>
</div>
<div class="callout-body-container callout-body">
<section id="core-concepts" class="level3">
<h3 class="anchored" data-anchor-id="core-concepts">Core Concepts</h3>
<ol type="1">
<li><strong>AI/ML learns patterns from data</strong> rather than explicit programming - enables automated analysis of massive satellite archives</li>
<li><strong>The EO workflow</strong> spans problem definition → data acquisition → preprocessing → features → training → validation → deployment</li>
<li><strong>Supervised learning</strong> (classification &amp; regression) is dominant for EO because we need specific outputs; unsupervised (clustering) useful for exploration</li>
</ol>
</section>
<section id="deep-learning-architectures" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-architectures">Deep Learning Architectures</h3>
<ol start="4" type="1">
<li><strong>CNNs</strong> are foundation of EO image analysis - automatic feature extraction, spatial awareness, hierarchical learning</li>
<li><strong>U-Net</strong> excels at semantic segmentation with encoder-decoder + skip connections (e.g., Benguet deforestation: 99.73% accuracy)</li>
<li><strong>Vision Transformers</strong> capture global context and long-range dependencies via self-attention (SatViT, MS-CLIP for multi-spectral data)</li>
<li><strong>LSTMs/RNNs</strong> model temporal patterns in time series (PRiSM rice monitoring, crop yield prediction: R² &gt; 0.93)</li>
<li><strong>Object Detection</strong> (YOLO, Faster R-CNN) localize objects with bounding boxes (buildings, ships, vehicles)</li>
<li><strong>Foundation Models</strong> (Prithvi-EO-2.0: 600M parameters) enable fine-tuning with 10-100× less labeled data</li>
</ol>
</section>
<section id="advanced-techniques" class="level3">
<h3 class="anchored" data-anchor-id="advanced-techniques">Advanced Techniques</h3>
<ol start="10" type="1">
<li><strong>Multi-modal fusion</strong> combines optical + SAR for all-weather monitoring (critical for Philippine monsoon season)</li>
<li><strong>Transfer learning</strong> dramatically reduces data requirements - pre-train on large dataset, fine-tune on small task-specific dataset</li>
<li><strong>Self-supervised learning</strong> pre-trains on unlabeled data via masked autoencoding (Prithvi) or contrastive learning</li>
</ol>
</section>
<section id="benchmark-datasets" class="level3">
<h3 class="anchored" data-anchor-id="benchmark-datasets">Benchmark Datasets</h3>
<ol start="13" type="1">
<li><strong>EuroSAT</strong> (27,000 images, 10 classes, 98.57% accuracy), <strong>BigEarthNet</strong> (549,488 patches, multi-modal), <strong>xView</strong> (&gt;1M objects, 60 classes)</li>
<li>Benchmarks enable standardized evaluation, provide training resources, support transfer learning</li>
</ol>
</section>
<section id="data-centric-ai-2025-paradigm" class="level3">
<h3 class="anchored" data-anchor-id="data-centric-ai-2025-paradigm">Data-Centric AI (2025 Paradigm)</h3>
<ol start="15" type="1">
<li><strong>Data quality &gt; model complexity:</strong> Improving data from 85% → 95% accuracy beats endless model tuning</li>
<li><strong>Four Pillars:</strong> Quality (accurate, consistent, properly processed), Quantity (sufficient samples, augmentation), Diversity (geographic, temporal, class, sensor), Annotation (strategic, high-quality labeling)</li>
<li><strong>Philippine Solutions:</strong> DOST-ASTI ALaM (Automated Labeling Machine), DIMER model repository, active learning</li>
</ol>
</section>
<section id="explainable-ai" class="level3">
<h3 class="anchored" data-anchor-id="explainable-ai">Explainable AI</h3>
<ol start="18" type="1">
<li><strong>XAI crucial for operational systems:</strong> Builds trust, enables debugging, extracts scientific insights, detects biases</li>
<li><strong>Methods:</strong> Grad-CAM (heatmaps), SHAP (feature importance), Attention visualization (transformers)</li>
</ol>
</section>
<section id="philippine-operational-context" class="level3">
<h3 class="anchored" data-anchor-id="philippine-operational-context">Philippine Operational Context</h3>
<ol start="20" type="1">
<li><strong>DATOS (DOST-ASTI):</strong> 10-20 minute AI-powered flood mapping from Sentinel-1 SAR</li>
<li><strong>PRiSM (PhilRice-IRRI):</strong> Operational since 2014, all-weather rice monitoring combining SAR + optical</li>
<li><strong>PhilSA-DENR:</strong> Nationwide mangrove mapping with U-Net (99.73% accuracy)</li>
<li><strong>CoPhil Data Centre (2025):</strong> Local, high-bandwidth access to Sentinel data, cloud-native distribution</li>
<li><strong>Leverage existing infrastructure:</strong> DIMER, AIPI, ALaM, CoPhil to operationalize AI/ML workflows</li>
</ol>
<p><strong>Next steps:</strong> Hands-on Python for geospatial data (Session 3) and Google Earth Engine (Session 4) to put these concepts into practice!</p>
</section>
</div>
</div>
<hr>
</section>
<section id="discussion-questions" class="level2">
<h2 class="anchored" data-anchor-id="discussion-questions">Discussion Questions</h2>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Reflect &amp; Discuss
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>What EO problem in your work</strong> could benefit from ML? Is it classification, regression, segmentation, or object detection? Which architecture would you choose?</p></li>
<li><p><strong>Data quality in Philippine context:</strong> How do you address cloud cover, temporal dynamics, and atmospheric effects in your satellite data?</p></li>
<li><p><strong>Foundation models:</strong> How could Prithvi-EO-2.0 or other pre-trained models reduce barriers for your organization? What Philippine-specific fine-tuning would be needed?</p></li>
<li><p><strong>Multi-modal fusion:</strong> When would you combine Sentinel-2 optical with Sentinel-1 SAR? What are practical challenges?</p></li>
<li><p><strong>Data-centric approach:</strong> What are biggest data quality issues you face? How could ALaM or active learning help?</p></li>
<li><p><strong>Benchmark datasets:</strong> Which international datasets could you use for pre-training? How to ensure models generalize to Philippines?</p></li>
<li><p><strong>Explainable AI:</strong> For your application, why would explainability matter? Which XAI method would you use?</p></li>
<li><p><strong>DIMER and AIPI platforms:</strong> How might these reduce barriers to deploying ML in your organization? What models would you contribute or use?</p></li>
<li><p><strong>Temporal modeling:</strong> For what applications would LSTM or temporal attention be valuable? What data would you need?</p></li>
<li><p><strong>CoPhil opportunities:</strong> How can you leverage the upcoming Data Centre and training programs? What collaborations would be valuable?</p></li>
</ol>
</div>
</div>
<hr>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further Reading</h2>
<section id="foundational-concepts" class="level3">
<h3 class="anchored" data-anchor-id="foundational-concepts">Foundational Concepts</h3>
<ul>
<li><a href="https://appliedsciences.nasa.gov/get-involved/training/english/arset-fundamentals-machine-learning-earth-science">NASA ARSET: Fundamentals of Machine Learning for Earth Science</a></li>
<li><a href="https://arxiv.org/abs/2312.05327">Data-Centric AI: Better, Not Just More</a></li>
<li><a href="https://www.vanderschaar-lab.com/dc-check/what-is-data-centric-ai/">Van der Schaar Lab: What is Data-Centric AI?</a></li>
</ul>
</section>
<section id="deep-learning-architectures-1" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-architectures-1">Deep Learning Architectures</h3>
<ul>
<li><a href="https://www.deeplearningbook.org/">Deep Learning Book (Goodfellow et al.)</a> - Free online</li>
<li><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning (Nielsen)</a> - Interactive tutorial</li>
<li><a href="https://github.com/satellite-image-deep-learning/techniques">Satellite Image Deep Learning Techniques</a> - Comprehensive GitHub repository</li>
</ul>
</section>
<section id="deep-learning-for-eo" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-for-eo">Deep Learning for EO</h3>
<ul>
<li><a href="https://www.mdpi.com/2072-4292/12/15/2495">Deep Learning for Land Use and Land Cover Classification</a> - 2020 review</li>
<li><a href="https://www.tandfonline.com/doi/full/10.1080/17538947.2024.2328827">Deep Learning for Remote Sensing Image Segmentation</a> - 2024 review</li>
<li><a href="https://www.mdpi.com/2072-4292/12/10/1667">Object Detection and Image Segmentation with Deep Learning on EO Data</a></li>
</ul>
</section>
<section id="foundation-models" class="level3">
<h3 class="anchored" data-anchor-id="foundation-models">Foundation Models</h3>
<ul>
<li><a href="https://huggingface.co/ibm-nasa-geospatial">IBM-NASA Prithvi Models on Hugging Face</a></li>
<li><a href="https://arxiv.org/abs/2412.02732">Prithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model</a></li>
<li><a href="https://research.ibm.com/blog/prithvi2-geospatial">IBM Research: Prithvi-EO-2.0 Blog</a></li>
</ul>
</section>
<section id="self-supervised-learning-1" class="level3">
<h3 class="anchored" data-anchor-id="self-supervised-learning-1">Self-Supervised Learning</h3>
<ul>
<li><a href="https://langnico.github.io/posts/SSL4EO-2024-review/">SSL4EO-2024 Summer School Review</a></li>
<li><a href="https://arxiv.org/abs/2405.20462">Multi-Label Guided Soft Contrastive Learning for EO</a></li>
</ul>
</section>
<section id="data-centric-ai" class="level3">
<h3 class="anchored" data-anchor-id="data-centric-ai">Data-Centric AI</h3>
<ul>
<li><a href="https://arxiv.org/html/2408.11384v1">Data-Centric Machine Learning for Earth Observation</a></li>
<li><a href="https://kili-technology.com/data-labeling/earth-observation-data-labeling-guide">Kili Technology: Earth Observation Data Labeling Guide</a></li>
</ul>
</section>
<section id="explainable-ai-1" class="level3">
<h3 class="anchored" data-anchor-id="explainable-ai-1">Explainable AI</h3>
<ul>
<li><a href="https://captum.ai/">Captum: Model Interpretability for PyTorch</a></li>
<li><a href="https://shap.readthedocs.io/">SHAP Library Documentation</a></li>
</ul>
</section>
<section id="eo-specific-ml" class="level3">
<h3 class="anchored" data-anchor-id="eo-specific-ml">EO-Specific ML</h3>
<ul>
<li><a href="https://eo-college.org/courses/introduction-to-machine-learning-for-earth-observation/">EO College: Introduction to Machine Learning for Earth Observation</a></li>
<li><a href="https://ml4earth.de/">ML4Earth Resources</a></li>
<li><a href="https://www.climatechange.ai/subject_areas/earth_observation_monitoring">Climate Change AI: Earth Observation &amp; Monitoring</a></li>
<li><a href="https://www.mdpi.com/2072-4292/15/16/4112">A Review of Practical AI for Remote Sensing in Earth Sciences</a> - 2023</li>
</ul>
</section>
<section id="benchmark-datasets-1" class="level3">
<h3 class="anchored" data-anchor-id="benchmark-datasets-1">Benchmark Datasets</h3>
<ul>
<li><a href="https://github.com/phelber/EuroSAT">EuroSAT GitHub</a></li>
<li><a href="https://bigearth.net/">BigEarthNet Website</a></li>
<li><a href="http://xviewdataset.org/">xView Dataset</a></li>
<li><a href="https://captain-whu.github.io/DOTA/">DOTA: Dataset for Object Detection in Aerial Images</a></li>
</ul>
</section>
<section id="philippine-ai-initiatives" class="level3">
<h3 class="anchored" data-anchor-id="philippine-ai-initiatives">Philippine AI Initiatives</h3>
<ul>
<li><a href="https://asti.dost.gov.ph/projects/datos">DOST-ASTI: Remote Sensing and Data Science (DATOS) Help Desk</a></li>
<li><a href="https://www.pna.gov.ph/articles/1136226">Philippine News Agency: DOST AI R&amp;D Projects</a> - SkAI-Pinas, DIMER, AIPI</li>
<li><a href="https://prism.philrice.gov.ph/">PRiSM: Philippine Rice Information System</a></li>
<li><a href="https://philsa.gov.ph/">PhilSA: Philippine Space Agency</a></li>
<li><a href="https://copphil.philsa.gov.ph/">CoPhil Centre</a></li>
</ul>
</section>
<section id="recent-advances" class="level3">
<h3 class="anchored" data-anchor-id="recent-advances">Recent Advances</h3>
<ul>
<li><a href="https://arxiv.org/abs/2305.08413">Artificial Intelligence to Advance Earth Observation: A Review</a> - 2023</li>
<li><a href="https://arxiv.org/html/2501.12030v1">Advancing Earth Observation with AI</a> - 2025</li>
<li><a href="https://ai4eo.eu/">ESA AI for Earth Observation</a></li>
<li><a href="https://github.com/acgeospatial/awesome-earthobservation-code">Awesome Earth Observation Code</a></li>
</ul>
<hr>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="cophil-training-v1.0" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../day1/sessions/session1.html" class="pagination-link" aria-label="Session 1: Copernicus Sentinel Data Deep Dive &amp; Philippine EO Ecosystem">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Session 1: Copernicus Sentinel Data Deep Dive &amp; Philippine EO Ecosystem</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../day1/sessions/session3.html" class="pagination-link" aria-label="Session 3: Hands-on Python for Geospatial Data">
        <span class="nav-page-text">Session 3: Hands-on Python for Geospatial Data</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>CoPhil EO AI/ML Training Programme</p>
</div>   
    <div class="nav-footer-center">
<p>Funded by the European Union - Global Gateway Initiative</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:skotsopoulos@neuralio.ai">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://philsa.gov.ph">
      <i class="bi bi-globe" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>