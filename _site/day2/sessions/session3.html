<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="CoPhil Advanced Training Program">
<meta name="dcterms.date" content="2025-10-19">

<title>Session 3: Introduction to Deep Learning and CNNs â€“ CoPhil EO AI/ML Training</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../day2/sessions/session4.html" rel="next">
<link href="../../day2/sessions/session2.html" rel="prev">
<link href="../../images/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c9822816d3895e59fda95a6fa7545fef.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-3775014fae9fc394bbda1d6ff89dd45e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-509191933"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-509191933', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles/custom.css">
<link rel="stylesheet" href="../../styles/phase2-enhancements.css">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CoPhil EO AI/ML Training</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-training-days" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Training Days</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-training-days">    
        <li>
    <a class="dropdown-item" href="../../day1/index.html">
 <span class="dropdown-text">Day 1: EO Data &amp; Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day2/index.html">
 <span class="dropdown-text">Day 2: Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day3/index.html">
 <span class="dropdown-text">Day 3: Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day4/index.html">
 <span class="dropdown-text">Day 4: Advanced Topics</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="../../resources/setup.html">
 <span class="dropdown-text">Setup Guide</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/philippine-eo.html">
 <span class="dropdown-text">Philippine EO Links</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/cheatsheets.html">
 <span class="dropdown-text">Cheat Sheets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/faq.html">
 <span class="dropdown-text">FAQ</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/glossary.html">
 <span class="dropdown-text">Glossary</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../resources/downloads.html"> <i class="bi bi-download" role="img">
</i> 
<span class="menu-text">Materials</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day2/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day2/sessions/session3.html">Session 3: Introduction to Deep Learning and CNNs</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Day 2: Machine Learning for Earth Observation</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Sessions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/sessions/session1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 1: Supervised Classification with Random Forest</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/sessions/session2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 2: Advanced Palawan Land Cover Lab</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/sessions/session3.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Session 3: Introduction to Deep Learning and CNNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/sessions/session4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: CNN Hands-on Lab</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Notebooks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session1_theory_notebook_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 1 Theory: Understanding Random Forest for Earth Observation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session1_hands_on_lab_student.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 1 Hands-on Lab: Palawan Land Cover Classification with Random Forest</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session2_extended_lab_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 2: Advanced Palawan Land Cover Classification Lab</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session3_theory_interactive.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 3: Deep Learning &amp; CNN Theory - Interactive Notebook</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session4_cnn_classification_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: CNN Hands-On Lab - EuroSAT Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session4_transfer_learning_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4 Part B: Transfer Learning with ResNet50</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#session-3-introduction-to-deep-learning-and-cnns" id="toc-session-3-introduction-to-deep-learning-and-cnns" class="nav-link active" data-scroll-target="#session-3-introduction-to-deep-learning-and-cnns">Session 3: Introduction to Deep Learning and CNNs</a>
  <ul class="collapse">
  <li><a href="#neural-networks-and-convolutional-architectures-for-earth-observation" id="toc-neural-networks-and-convolutional-architectures-for-earth-observation" class="nav-link" data-scroll-target="#neural-networks-and-convolutional-architectures-for-earth-observation">Neural Networks and Convolutional Architectures for Earth Observation</a></li>
  </ul></li>
  <li><a href="#session-overview" id="toc-session-overview" class="nav-link" data-scroll-target="#session-overview">Session Overview</a></li>
  <li><a href="#presentation-slides" id="toc-presentation-slides" class="nav-link" data-scroll-target="#presentation-slides">Presentation Slides</a></li>
  <li><a href="#what-youll-learn" id="toc-what-youll-learn" class="nav-link" data-scroll-target="#what-youll-learn">What Youâ€™ll Learn</a></li>
  <li><a href="#session-structure" id="toc-session-structure" class="nav-link" data-scroll-target="#session-structure">Session Structure</a>
  <ul class="collapse">
  <li><a href="#part-a-from-machine-learning-to-deep-learning-15-minutes" id="toc-part-a-from-machine-learning-to-deep-learning-15-minutes" class="nav-link" data-scroll-target="#part-a-from-machine-learning-to-deep-learning-15-minutes">Part A: From Machine Learning to Deep Learning (15 minutes)</a></li>
  <li><a href="#part-b-neural-network-fundamentals-25-minutes" id="toc-part-b-neural-network-fundamentals-25-minutes" class="nav-link" data-scroll-target="#part-b-neural-network-fundamentals-25-minutes">Part B: Neural Network Fundamentals (25 minutes)</a></li>
  <li><a href="#part-c-convolutional-neural-networks-30-minutes" id="toc-part-c-convolutional-neural-networks-30-minutes" class="nav-link" data-scroll-target="#part-c-convolutional-neural-networks-30-minutes">Part C: Convolutional Neural Networks (30 minutes)</a></li>
  <li><a href="#part-d-cnns-for-earth-observation-25-minutes" id="toc-part-d-cnns-for-earth-observation-25-minutes" class="nav-link" data-scroll-target="#part-d-cnns-for-earth-observation-25-minutes">Part D: CNNs for Earth Observation (25 minutes)</a></li>
  <li><a href="#part-e-practical-considerations-for-eo-deep-learning-15-minutes" id="toc-part-e-practical-considerations-for-eo-deep-learning-15-minutes" class="nav-link" data-scroll-target="#part-e-practical-considerations-for-eo-deep-learning-15-minutes">Part E: Practical Considerations for EO Deep Learning (15 minutes)</a></li>
  </ul></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts">Key Concepts</a>
  <ul class="collapse">
  <li><a href="#automatic-feature-learning" id="toc-automatic-feature-learning" class="nav-link" data-scroll-target="#automatic-feature-learning">Automatic Feature Learning</a></li>
  <li><a href="#receptive-field" id="toc-receptive-field" class="nav-link" data-scroll-target="#receptive-field">Receptive Field</a></li>
  <li><a href="#translation-invariance" id="toc-translation-invariance" class="nav-link" data-scroll-target="#translation-invariance">Translation Invariance</a></li>
  <li><a href="#gradient-descent-and-backpropagation" id="toc-gradient-descent-and-backpropagation" class="nav-link" data-scroll-target="#gradient-descent-and-backpropagation">Gradient Descent and Backpropagation</a></li>
  </ul></li>
  <li><a href="#interactive-demonstrations" id="toc-interactive-demonstrations" class="nav-link" data-scroll-target="#interactive-demonstrations">Interactive Demonstrations</a>
  <ul class="collapse">
  <li><a href="#demo-1-perceptron-playground" id="toc-demo-1-perceptron-playground" class="nav-link" data-scroll-target="#demo-1-perceptron-playground">Demo 1: Perceptron Playground</a></li>
  <li><a href="#demo-2-activation-function-gallery" id="toc-demo-2-activation-function-gallery" class="nav-link" data-scroll-target="#demo-2-activation-function-gallery">Demo 2: Activation Function Gallery</a></li>
  <li><a href="#demo-3-manual-convolution-on-sentinel-2" id="toc-demo-3-manual-convolution-on-sentinel-2" class="nav-link" data-scroll-target="#demo-3-manual-convolution-on-sentinel-2">Demo 3: Manual Convolution on Sentinel-2</a></li>
  <li><a href="#demo-4-pooling-demonstration" id="toc-demo-4-pooling-demonstration" class="nav-link" data-scroll-target="#demo-4-pooling-demonstration">Demo 4: Pooling Demonstration</a></li>
  <li><a href="#demo-5-architecture-exploration" id="toc-demo-5-architecture-exploration" class="nav-link" data-scroll-target="#demo-5-architecture-exploration">Demo 5: Architecture Exploration</a></li>
  </ul></li>
  <li><a href="#philippine-eo-applications" id="toc-philippine-eo-applications" class="nav-link" data-scroll-target="#philippine-eo-applications">Philippine EO Applications</a>
  <ul class="collapse">
  <li><a href="#philsa-space-dashboard" id="toc-philsa-space-dashboard" class="nav-link" data-scroll-target="#philsa-space-dashboard">PhilSA Space+ Dashboard</a></li>
  <li><a href="#denr-forest-monitoring" id="toc-denr-forest-monitoring" class="nav-link" data-scroll-target="#denr-forest-monitoring">DENR Forest Monitoring</a></li>
  <li><a href="#lgu-applications-session-4-focus" id="toc-lgu-applications-session-4-focus" class="nav-link" data-scroll-target="#lgu-applications-session-4-focus">LGU Applications (Session 4 Focus)</a></li>
  </ul></li>
  <li><a href="#expected-outcomes" id="toc-expected-outcomes" class="nav-link" data-scroll-target="#expected-outcomes">Expected Outcomes</a>
  <ul class="collapse">
  <li><a href="#conceptual-understanding" id="toc-conceptual-understanding" class="nav-link" data-scroll-target="#conceptual-understanding">Conceptual Understanding</a></li>
  <li><a href="#technical-skills" id="toc-technical-skills" class="nav-link" data-scroll-target="#technical-skills">Technical Skills</a></li>
  <li><a href="#practical-readiness-for-session-4" id="toc-practical-readiness-for-session-4" class="nav-link" data-scroll-target="#practical-readiness-for-session-4">Practical Readiness for Session 4</a></li>
  </ul></li>
  <li><a href="#hands-on-notebooks" id="toc-hands-on-notebooks" class="nav-link" data-scroll-target="#hands-on-notebooks">Hands-On Notebooks</a>
  <ul class="collapse">
  <li><a href="#access-the-interactive-materials" id="toc-access-the-interactive-materials" class="nav-link" data-scroll-target="#access-the-interactive-materials">Access the Interactive Materials</a></li>
  <li><a href="#supporting-documentation" id="toc-supporting-documentation" class="nav-link" data-scroll-target="#supporting-documentation">Supporting Documentation</a></li>
  </ul></li>
  <li><a href="#troubleshooting" id="toc-troubleshooting" class="nav-link" data-scroll-target="#troubleshooting">Troubleshooting</a>
  <ul class="collapse">
  <li><a href="#common-conceptual-questions" id="toc-common-conceptual-questions" class="nav-link" data-scroll-target="#common-conceptual-questions">Common Conceptual Questions</a></li>
  <li><a href="#technical-issues" id="toc-technical-issues" class="nav-link" data-scroll-target="#technical-issues">Technical Issues</a></li>
  <li><a href="#getting-help" id="toc-getting-help" class="nav-link" data-scroll-target="#getting-help">Getting Help</a></li>
  </ul></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources">Additional Resources</a>
  <ul class="collapse">
  <li><a href="#foundational-learning" id="toc-foundational-learning" class="nav-link" data-scroll-target="#foundational-learning">Foundational Learning</a></li>
  <li><a href="#earth-observation-deep-learning" id="toc-earth-observation-deep-learning" class="nav-link" data-scroll-target="#earth-observation-deep-learning">Earth Observation Deep Learning</a></li>
  <li><a href="#philippine-context" id="toc-philippine-context" class="nav-link" data-scroll-target="#philippine-context">Philippine Context</a></li>
  </ul></li>
  <li><a href="#assessment" id="toc-assessment" class="nav-link" data-scroll-target="#assessment">Assessment</a>
  <ul class="collapse">
  <li><a href="#formative-assessment-during-session" id="toc-formative-assessment-during-session" class="nav-link" data-scroll-target="#formative-assessment-during-session">Formative Assessment (During Session)</a></li>
  <li><a href="#summative-assessment-end-of-session" id="toc-summative-assessment-end-of-session" class="nav-link" data-scroll-target="#summative-assessment-end-of-session">Summative Assessment (End of Session)</a></li>
  </ul></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next Steps</a>
  <ul class="collapse">
  <li><a href="#recommended-pre-work-for-session-4" id="toc-recommended-pre-work-for-session-4" class="nav-link" data-scroll-target="#recommended-pre-work-for-session-4">Recommended Pre-Work for Session 4</a></li>
  <li><a href="#extended-learning-paths" id="toc-extended-learning-paths" class="nav-link" data-scroll-target="#extended-learning-paths">Extended Learning Paths</a></li>
  </ul></li>
  <li><a href="#quick-links" id="toc-quick-links" class="nav-link" data-scroll-target="#quick-links">Quick Links</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day2/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day2/sessions/session3.html">Session 3: Introduction to Deep Learning and CNNs</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Session 3: Introduction to Deep Learning and CNNs</h1>
<p class="subtitle lead">Neural Networks and Convolutional Architectures for Earth Observation</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Instructor</div>
    <div class="quarto-title-meta-contents">
             <p>CoPhil Advanced Training Program </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Date</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 19, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<nav class="breadcrumb" aria-label="Breadcrumb">
<a href="../../index.html">Home</a> <span class="breadcrumb-separator" aria-hidden="true">â€º</span> <a href="../index.html">Day 2</a> <span class="breadcrumb-separator" aria-hidden="true">â€º</span> <span class="breadcrumb-current">Session 3</span>
</nav>
<section id="session-3-introduction-to-deep-learning-and-cnns" class="level1 hero">
<h1>Session 3: Introduction to Deep Learning and CNNs</h1>
<section id="neural-networks-and-convolutional-architectures-for-earth-observation" class="level3">
<h3 class="anchored" data-anchor-id="neural-networks-and-convolutional-architectures-for-earth-observation">Neural Networks and Convolutional Architectures for Earth Observation</h3>
<p>Transitioning from feature engineering to feature learning</p>
</section>
</section>
<section id="session-overview" class="level2">
<h2 class="anchored" data-anchor-id="session-overview">Session Overview</h2>
<p><strong>Duration:</strong> 2.5 hours | <strong>Type:</strong> Theory + Interactive Demonstrations | <strong>Difficulty:</strong> Intermediate</p>
<hr>
<p>This pivotal session bridges traditional machine learning (Sessions 1-2) and modern deep learning approaches. Youâ€™ll understand the fundamental shift from manual feature engineering to automatic feature learning through neural networks, with specific focus on Convolutional Neural Networks (CNNs) for Earth observation applications.</p>
</section>
<section id="presentation-slides" class="level2">
<h2 class="anchored" data-anchor-id="presentation-slides">Presentation Slides</h2>
<iframe src="../presentations/session3_deep_learning.html" width="100%" height="600" style="border: 1px solid #ccc; border-radius: 4px;">
</iframe>
<hr>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Prerequisites
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>âœ“ Complete Sessions 1-2 (Random Forest classification)</li>
<li>âœ“ Understanding of classification concepts (accuracy, confusion matrix)</li>
<li>âœ“ Basic Python and NumPy familiarity</li>
<li>âœ“ Colab environment with GPU runtime enabled</li>
<li>âœ“ Conceptual understanding of matrix operations</li>
</ul>
</div>
</div>
<hr>
</section>
<section id="what-youll-learn" class="level2">
<h2 class="anchored" data-anchor-id="what-youll-learn">What Youâ€™ll Learn</h2>
<p>After completing this session, you will be able to:</p>
<ol type="1">
<li><strong>Understand the ML â†’ DL Transition</strong>
<ul>
<li>Recognize when to use traditional ML vs deep learning</li>
<li>Explain the automatic feature learning paradigm</li>
<li>Identify computational requirements and trade-offs</li>
<li>Understand data requirements for deep learning success</li>
</ul></li>
<li><strong>Master Neural Network Fundamentals</strong>
<ul>
<li>Build simple perceptrons from scratch using NumPy</li>
<li>Implement activation functions (ReLU, sigmoid, softmax)</li>
<li>Understand forward propagation and backpropagation</li>
<li>Visualize decision boundaries and learning dynamics</li>
</ul></li>
<li><strong>Comprehend Convolutional Neural Networks</strong>
<ul>
<li>Explain convolution operations and their purpose</li>
<li>Understand pooling, padding, and stride concepts</li>
<li>Visualize filter responses on satellite imagery</li>
<li>Compare CNN architectures (LeNet, VGG, ResNet, U-Net)</li>
</ul></li>
<li><strong>Apply CNNs to Earth Observation Tasks</strong>
<ul>
<li>Identify appropriate CNN architectures for EO problems</li>
<li>Understand scene classification vs semantic segmentation</li>
<li>Recognize object detection and change detection approaches</li>
<li>Connect CNN capabilities to Philippine EO applications</li>
</ul></li>
<li><strong>Navigate Practical Considerations</strong>
<ul>
<li>Address data-centric AI principles for EO</li>
<li>Handle limited training data scenarios</li>
<li>Understand transfer learning and pre-trained models</li>
<li>Recognize computational constraints and optimization strategies</li>
</ul></li>
</ol>
<hr>
</section>
<section id="session-structure" class="level2">
<h2 class="anchored" data-anchor-id="session-structure">Session Structure</h2>
<section id="part-a-from-machine-learning-to-deep-learning-15-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-a-from-machine-learning-to-deep-learning-15-minutes">Part A: From Machine Learning to Deep Learning (15 minutes)</h3>
<p>Understanding the paradigm shift from manual to automatic feature learning.</p>
<div class="feature-grid">
<div class="feature-card">
<p><strong>ğŸ”§ Traditional ML (Sessions 1-2)</strong></p>
<p><strong>What you did:</strong> - Manually calculated NDVI, NDWI, NDBI - Engineered GLCM texture features - Extracted temporal statistics - Combined features thoughtfully</p>
<p><strong>Pros:</strong> Interpretable, works with small datasets <strong>Cons:</strong> Requires domain expertise, limited by imagination</p>
</div>
<div class="feature-card">
<p><strong>ğŸ§  Deep Learning (Sessions 3-4)</strong></p>
<p><strong>What CNNs do:</strong> - Learn features automatically from raw pixels - Discover hidden patterns humans miss - Build hierarchical representations - Optimize end-to-end</p>
<p><strong>Pros:</strong> No feature engineering, state-of-the-art accuracy <strong>Cons:</strong> Needs large datasets, computationally intensive</p>
</div>
<div class="feature-card">
<p><strong>âš–ï¸ When to Use Which?</strong></p>
<p><strong>Use Random Forest when:</strong> - Limited training data (&lt;1000 samples) - Need interpretability (DENR reports) - Have domain features (indices) - Fast prototyping needed</p>
<p><strong>Use CNNs when:</strong> - Large labeled datasets (&gt;10,000 samples) - Complex spatial patterns - Maximum accuracy required - GPU resources available</p>
</div>
<div class="feature-card">
<p><strong>ğŸ‡µğŸ‡­ Philippine EO Context</strong></p>
<p><strong>PhilSA Applications:</strong> - Scene classification (land cover) - Cloud detection (Sentinel-2) - Building footprint extraction - Flood extent mapping</p>
<p><strong>Why CNNs?</strong> Handle complex tropical landscapes, monsoon cloud patterns, informal settlements</p>
</div>
</div>
<p><strong>Key Insight:</strong> <em>â€œIn Sessions 1-2, you manually engineered features. CNNs will learn these features automaticallyâ€”and discover new ones you never imagined!â€</em></p>
</section>
<section id="part-b-neural-network-fundamentals-25-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-b-neural-network-fundamentals-25-minutes">Part B: Neural Network Fundamentals (25 minutes)</h3>
<p>Building intuition from the ground up using interactive Jupyter notebooks.</p>
<section id="b.1-the-perceptron---simplest-neural-unit" class="level4">
<h4 class="anchored" data-anchor-id="b.1-the-perceptron---simplest-neural-unit">B.1: The Perceptron - Simplest Neural Unit</h4>
<p>Understanding the building block of all neural networks.</p>
<p><strong>Mathematical Foundation:</strong> <span class="math display">\[
y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
\]</span></p>
<p>Where: - <span class="math inline">\(x_i\)</span>: Input features (e.g., pixel values, NDVI) - <span class="math inline">\(w_i\)</span>: Learned weights - <span class="math inline">\(b\)</span>: Bias term - <span class="math inline">\(f\)</span>: Activation function - <span class="math inline">\(y\)</span>: Output prediction</p>
<p><strong>Hands-On:</strong> Build a perceptron from scratch to classify â€œWater vs Non-Waterâ€ using NDWI.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple perceptron implementation</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceptron:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.random.randn(input_dim)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        linear_output <span class="op">=</span> np.dot(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.activation(linear_output)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> activation(<span class="va">self</span>, z):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> z <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span>  <span class="co"># Step function</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="b.2-activation-functions---adding-non-linearity" class="level4">
<h4 class="anchored" data-anchor-id="b.2-activation-functions---adding-non-linearity">B.2: Activation Functions - Adding Non-Linearity</h4>
<p>Why neural networks need activation functions to solve complex problems.</p>
<div class="feature-grid">
<div class="feature-card">
<p><strong>Sigmoid</strong> <span class="math display">\[\sigma(z) = \frac{1}{1 + e^{-z}}\]</span></p>
<p><strong>Range:</strong> (0, 1) <strong>Use:</strong> Binary classification output <strong>EO Example:</strong> Cloud probability</p>
</div>
<div class="feature-card">
<p><strong>ReLU (Rectified Linear Unit)</strong> <span class="math display">\[\text{ReLU}(z) = \max(0, z)\]</span></p>
<p><strong>Range:</strong> [0, âˆ) <strong>Use:</strong> Hidden layers (most popular) <strong>Why:</strong> Fast, sparse activation</p>
</div>
<div class="feature-card">
<p><strong>Softmax</strong> <span class="math display">\[\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]</span></p>
<p><strong>Range:</strong> (0, 1), sums to 1 <strong>Use:</strong> Multi-class classification <strong>EO Example:</strong> Land cover classes</p>
</div>
<div class="feature-card">
<p><strong>Tanh</strong> <span class="math display">\[\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\]</span></p>
<p><strong>Range:</strong> (-1, 1) <strong>Use:</strong> When centered data needed <strong>Note:</strong> Less common than ReLU</p>
</div>
</div>
<p><strong>Interactive Demo:</strong> Visualize how each activation function transforms Sentinel-2 spectral values.</p>
</section>
<section id="b.3-multi-layer-networks---learning-complex-patterns" class="level4">
<h4 class="anchored" data-anchor-id="b.3-multi-layer-networks---learning-complex-patterns">B.3: Multi-Layer Networks - Learning Complex Patterns</h4>
<p>Stacking layers to learn hierarchical representations.</p>
<p><strong>Architecture:</strong></p>
<pre><code>Input Layer (e.g., Sentinel-2 bands)
    â†“
Hidden Layer 1 (64 neurons, ReLU)
    â†“
Hidden Layer 2 (32 neurons, ReLU)
    â†“
Output Layer (8 classes, Softmax)</code></pre>
<p><strong>What Each Layer Learns:</strong> - <strong>Layer 1:</strong> Low-level features (edges, textures) - <strong>Layer 2:</strong> Mid-level features (shapes, patterns) - <strong>Layer 3:</strong> High-level features (objects, scenes) - <strong>Output:</strong> Class probabilities</p>
<p><strong>Hands-On:</strong> Train a 2-layer network to classify Palawan land cover using spectral features (from Session 2).</p>
</section>
<section id="b.4-training-process---learning-from-data" class="level4">
<h4 class="anchored" data-anchor-id="b.4-training-process---learning-from-data">B.4: Training Process - Learning from Data</h4>
<p>Understanding gradient descent and backpropagation intuitively.</p>
<p><strong>Training Loop:</strong> 1. <strong>Forward Pass:</strong> Input â†’ Hidden â†’ Output â†’ Prediction 2. <strong>Calculate Loss:</strong> Compare prediction to true label 3. <strong>Backward Pass:</strong> Compute gradients (how much each weight contributed to error) 4. <strong>Update Weights:</strong> <span class="math inline">\(w_{new} = w_{old} - \alpha \cdot \frac{\partial L}{\partial w}\)</span> 5. <strong>Repeat:</strong> Until loss converges</p>
<p><strong>Key Hyperparameters:</strong> - <strong>Learning Rate (<span class="math inline">\(\alpha\)</span>):</strong> Step size for weight updates (0.001 - 0.1) - <strong>Batch Size:</strong> Number of samples per gradient update (32, 64, 128) - <strong>Epochs:</strong> Complete passes through training data (10-100)</p>
<p><strong>Interactive Exploration:</strong> Experiment with learning rates to see overfitting vs underfitting.</p>
</section>
</section>
<section id="part-c-convolutional-neural-networks-30-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-c-convolutional-neural-networks-30-minutes">Part C: Convolutional Neural Networks (30 minutes)</h3>
<p>Deep dive into the architecture that revolutionized computer vision and Earth observation.</p>
<section id="c.1-why-cnns-for-images" class="level4">
<h4 class="anchored" data-anchor-id="c.1-why-cnns-for-images">C.1: Why CNNs for Images?</h4>
<p><strong>Problem with Regular Neural Networks:</strong> - A 10m Sentinel-2 image chip (256Ã—256Ã—10 bands) = 655,360 parameters just for first layer! - No spatial awareness (treats nearby pixels same as distant ones) - Computationally infeasible</p>
<p><strong>CNN Solutions:</strong> - <strong>Local Connectivity:</strong> Each neuron connects to small spatial region - <strong>Parameter Sharing:</strong> Same filter applied across entire image - <strong>Translation Invariance:</strong> Detect features anywhere in image</p>
<p><strong>Result:</strong> Millions fewer parameters, spatially-aware learning!</p>
</section>
<section id="c.2-convolution-operation---the-heart-of-cnns" class="level4">
<h4 class="anchored" data-anchor-id="c.2-convolution-operation---the-heart-of-cnns">C.2: Convolution Operation - The Heart of CNNs</h4>
<p>Understanding how convolutions extract features from satellite imagery.</p>
<p><strong>Mathematical Definition:</strong> <span class="math display">\[
(I * K)(i,j) = \sum_{m}\sum_{n} I(i+m, j+n) \cdot K(m,n)
\]</span></p>
<p>Where: - <span class="math inline">\(I\)</span>: Input image (e.g., Sentinel-2 NIR band) - <span class="math inline">\(K\)</span>: Filter/kernel (e.g., 3Ã—3 edge detector) - <span class="math inline">\(*\)</span>: Convolution operator</p>
<p><strong>Visual Example:</strong></p>
<pre><code>Sentinel-2 NIR Image (5Ã—5)    Edge Detection Filter (3Ã—3)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 120 115 118 122 â”‚           â”‚ -1  -1  -1 â”‚
â”‚ 118 245 242 125 â”‚     *     â”‚  0   0   0 â”‚
â”‚ 119 248 244 121 â”‚           â”‚  1   1   1 â”‚
â”‚ 121 116 119 123 â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
    Feature Map (detects water edges)</code></pre>
<p><strong>Hands-On:</strong> - Apply manual convolutions to Sentinel-2 patches - Visualize classic filters (edge detection, blur, sharpen) - See how filters respond to forests, water, urban areas</p>
</section>
<section id="c.3-cnn-building-blocks" class="level4">
<h4 class="anchored" data-anchor-id="c.3-cnn-building-blocks">C.3: CNN Building Blocks</h4>
<p><strong>1. Convolutional Layer</strong> - Applies multiple filters to input - Each filter learns to detect different feature - Output: Feature maps (activations)</p>
<p><strong>Parameters:</strong> - <code>filters</code>: Number of filters (32, 64, 128â€¦) - <code>kernel_size</code>: Filter dimensions (3Ã—3, 5Ã—5) - <code>stride</code>: Step size for filter movement (usually 1) - <code>padding</code>: Border handling (â€˜sameâ€™ or â€˜validâ€™)</p>
<p><strong>2. Pooling Layer</strong> - Reduces spatial dimensions (downsampling) - Provides translation invariance - Most common: Max pooling</p>
<p><strong>Max Pooling (2Ã—2):</strong></p>
<pre><code>Input (4Ã—4)                Output (2Ã—2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ 1  3  2  4 â”‚            â”‚ 3  4 â”‚
â”‚ 2  3  1  2 â”‚    â†’       â”‚ 7  9 â”‚
â”‚ 5  7  8  9 â”‚            â””â”€â”€â”€â”€â”€â”€â”˜
â”‚ 1  2  3  4 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Takes maximum in each 2Ã—2 window</code></pre>
<p><strong>3. Fully Connected Layer</strong> - Traditional neural network layer - Connects all features to output classes - Usually at end of network</p>
<p><strong>4. Dropout Layer</strong> - Randomly deactivates neurons during training - Prevents overfitting - Common rate: 0.3-0.5</p>
</section>
<section id="c.4-classic-cnn-architectures" class="level4">
<h4 class="anchored" data-anchor-id="c.4-classic-cnn-architectures">C.4: Classic CNN Architectures</h4>
<p>Understanding architectures used in EO applications.</p>
<div class="feature-grid">
<div class="feature-card">
<p><strong>LeNet-5 (1998)</strong></p>
<p><strong>Structure:</strong> - Conv â†’ Pool â†’ Conv â†’ Pool â†’ FC - 60K parameters - Original: Handwritten digits</p>
<p><strong>EO Use:</strong> - Simple scene classification - Educational examples - Quick prototypes</p>
</div>
<div class="feature-card">
<p><strong>VGG-16 (2014)</strong></p>
<p><strong>Structure:</strong> - 13 Conv layers + 3 FC - 138M parameters - Small 3Ã—3 filters stacked</p>
<p><strong>EO Use:</strong> - Scene classification - Pre-trained on ImageNet - Transfer learning baseline</p>
</div>
<div class="feature-card">
<p><strong>ResNet-50 (2015)</strong></p>
<p><strong>Innovation:</strong> - Skip connections (residual blocks) - Solves vanishing gradient - 50 layers deep</p>
<p><strong>EO Use:</strong> - High-accuracy classification - Feature extraction - PhilSA scene classifier</p>
</div>
<div class="feature-card">
<p><strong>U-Net (2015)</strong></p>
<p><strong>Innovation:</strong> - Encoder-decoder architecture - Skip connections preserve detail - Outputs same-size segmentation</p>
<p><strong>EO Use:</strong> - Semantic segmentation - Flood mapping - Building extraction - <strong>Session 4 focus!</strong></p>
</div>
</div>
<p><strong>Interactive Visualization:</strong> Explore how different architectures process Sentinel-2 imagery.</p>
</section>
</section>
<section id="part-d-cnns-for-earth-observation-25-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-d-cnns-for-earth-observation-25-minutes">Part D: CNNs for Earth Observation (25 minutes)</h3>
<p>Connecting CNN capabilities to Philippine EO operational needs.</p>
<section id="d.1-scene-classification" class="level4">
<h4 class="anchored" data-anchor-id="d.1-scene-classification">D.1: Scene Classification</h4>
<p><strong>Task:</strong> Assign single label to entire image patch</p>
<p><strong>Architecture:</strong> ResNet, VGG, EfficientNet (classification head)</p>
<p><strong>Philippine Applications:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Application</th>
<th>Classes</th>
<th>Dataset</th>
<th>Stakeholder</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Land Cover</strong></td>
<td>Forest, Urban, Agriculture, Water, Bare</td>
<td>PhilSA Sentinel-2</td>
<td>DENR, LGUs</td>
</tr>
<tr class="even">
<td><strong>Cloud Detection</strong></td>
<td>Clear, Thin Cloud, Thick Cloud, Shadow</td>
<td>Sentinel-2 Level-1C</td>
<td>PhilSA (preprocessing)</td>
</tr>
<tr class="odd">
<td><strong>Rice Field Stage</strong></td>
<td>Land Prep, Transplanting, Vegetative, Harvest</td>
<td>PlanetScope + field data</td>
<td>DA, PhilRice</td>
</tr>
<tr class="even">
<td><strong>Disaster Assessment</strong></td>
<td>Damaged, Undamaged, Debris</td>
<td>Drones + Sentinel-2</td>
<td>NDRRMC, PAGASA</td>
</tr>
</tbody>
</table>
<p><strong>Data Requirements:</strong> - Typical: 1,000-10,000 labeled image chips per class - PhilSA strategy: Start with 500/class, augment with rotations/flips</p>
<p><strong>Hands-On (Session 4):</strong> Build ResNet-based classifier for Palawan land cover</p>
</section>
<section id="d.2-semantic-segmentation" class="level4">
<h4 class="anchored" data-anchor-id="d.2-semantic-segmentation">D.2: Semantic Segmentation</h4>
<p><strong>Task:</strong> Classify every pixel in image (pixel-wise labels)</p>
<p><strong>Architecture:</strong> U-Net, DeepLabv3+, SegNet</p>
<p><strong>Philippine Applications:</strong></p>
<div class="feature-grid">
<div class="feature-card">
<p><strong>ğŸŒŠ Flood Mapping</strong></p>
<p><strong>Challenge:</strong> Rapid post-disaster assessment</p>
<p><strong>CNN Solution:</strong> - Input: Sentinel-1 SAR (pre + post event) - Output: Flooded/Non-flooded pixel map - Architecture: U-Net</p>
<p><strong>PhilSA Use Case:</strong> Pampanga flood 2023 - 6-hour processing time (vs 2 days manual)</p>
</div>
<div class="feature-card">
<p><strong>ğŸ˜ï¸ Informal Settlements</strong></p>
<p><strong>Challenge:</strong> Map slums for disaster planning</p>
<p><strong>CNN Solution:</strong> - Input: High-res imagery (PlanetScope, drones) - Output: Building footprints - Architecture: U-Net + post-processing</p>
<p><strong>NEDA Application:</strong> Metro Manila vulnerability assessment</p>
</div>
<div class="feature-card">
<p><strong>ğŸŒ³ Forest Degradation</strong></p>
<p><strong>Challenge:</strong> Detect selective logging</p>
<p><strong>CNN Solution:</strong> - Input: Multi-temporal Sentinel-2 - Output: Degraded forest pixels - Architecture: U-Net + LSTM</p>
<p><strong>DENR Use:</strong> Protected area monitoring</p>
</div>
<div class="feature-card">
<p><strong>â›ï¸ Mining Activity</strong></p>
<p><strong>Challenge:</strong> Illegal mining detection</p>
<p><strong>CNN Solution:</strong> - Input: Sentinel-2 + Sentinel-1 - Output: Mining site polygons - Post-process: Vectorize</p>
<p><strong>MGB Application:</strong> Permit compliance checking</p>
</div>
</div>
<p><strong>Data Requirements:</strong> - Annotation intensive: Need pixel-level labels - Typical: 100-500 labeled images (256Ã—256 chips) - Tools: QGIS, LabelMe, CVAT</p>
<p><strong>Hands-On (Session 4):</strong> Implement U-Net for flood mapping in Central Luzon</p>
</section>
<section id="d.3-object-detection" class="level4">
<h4 class="anchored" data-anchor-id="d.3-object-detection">D.3: Object Detection</h4>
<p><strong>Task:</strong> Find and localize objects with bounding boxes</p>
<p><strong>Architecture:</strong> Faster R-CNN, YOLO, RetinaNet</p>
<p><strong>Philippine Applications:</strong> - <strong>Ship Detection:</strong> Illegal fishing monitoring (Sentinel-1) - <strong>Building Detection:</strong> Infrastructure mapping (high-res) - <strong>Tree Counting:</strong> Forest inventory (drone imagery) - <strong>Vehicle Detection:</strong> Traffic monitoring (PlanetScope)</p>
<p><strong>Data Format:</strong> Bounding boxes + class labels (COCO, PASCAL VOC formats)</p>
<p><strong>Computational Note:</strong> More complex than classification, requires anchor boxes and region proposals</p>
</section>
<section id="d.4-change-detection" class="level4">
<h4 class="anchored" data-anchor-id="d.4-change-detection">D.4: Change Detection</h4>
<p><strong>Task:</strong> Identify what changed between two time points</p>
<p><strong>CNN Approaches:</strong></p>
<ol type="1">
<li><strong>Siamese Networks:</strong> Compare two images with shared weights</li>
<li><strong>Early Fusion:</strong> Stack temporal images as input channels</li>
<li><strong>Late Fusion:</strong> Separate encoders + change decoder</li>
</ol>
<p><strong>Philippine Applications:</strong> - Deforestation (Palawan, Mindanao) - Urban expansion (Metro Manila, Cebu) - Post-disaster damage (typhoon impacts) - Agricultural change (conversion detection)</p>
<p><strong>Challenge:</strong> Need paired labeled change data (before + after + change mask)</p>
</section>
</section>
<section id="part-e-practical-considerations-for-eo-deep-learning-15-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-e-practical-considerations-for-eo-deep-learning-15-minutes">Part E: Practical Considerations for EO Deep Learning (15 minutes)</h3>
<p>Real-world challenges and solutions for Philippine EO practitioners.</p>
<section id="e.1-the-data-challenge" class="level4">
<h4 class="anchored" data-anchor-id="e.1-the-data-challenge">E.1: The Data Challenge</h4>
<p><strong>How Much Data Do You Need?</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 36%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Model Complexity</th>
<th>Typical Requirement</th>
<th>Philippine Reality</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple CNN (5 layers)</td>
<td>5,000-10,000 samples</td>
<td>âœ“ Achievable</td>
</tr>
<tr class="even">
<td>ResNet-50 (from scratch)</td>
<td>100,000+ samples</td>
<td>âœ— Rarely available</td>
</tr>
<tr class="odd">
<td>ResNet-50 (fine-tuned)</td>
<td>1,000-5,000 samples</td>
<td>âœ“ Achievable with augmentation</td>
</tr>
<tr class="even">
<td>U-Net (segmentation)</td>
<td>100-500 images</td>
<td>âœ“ Achievable but labor-intensive</td>
</tr>
</tbody>
</table>
<p><strong>Data-Centric AI Principles:</strong> 1. <strong>Quality &gt; Quantity:</strong> 500 clean labels &gt;&gt; 5,000 noisy labels 2. <strong>Representative Sampling:</strong> Cover all Philippine ecosystems (lowland, upland, coastal) 3. <strong>Class Balance:</strong> Equal samples per class (or weighted loss) 4. <strong>Validation Split:</strong> Hold out 20% for unbiased evaluation</p>
<p><strong>Philippine Data Sources:</strong> - PhilSA Space+ Data Dashboard (satellite imagery) - NAMRIA Geoportal (reference maps) - DOST-ASTI DATOS (disaster imagery) - LiDAR Portal (elevation + canopy) - Field campaigns (GPS + photos)</p>
</section>
<section id="e.2-transfer-learning---training-on-limited-data" class="level4">
<h4 class="anchored" data-anchor-id="e.2-transfer-learning---training-on-limited-data">E.2: Transfer Learning - Training on Limited Data</h4>
<p><strong>Strategy:</strong> Start with model pre-trained on large dataset (ImageNet), fine-tune on Philippine data</p>
<p><strong>Benefits:</strong> - Need 10Ã— less data - Train 5Ã— faster - Better accuracy with small datasets</p>
<p><strong>Implementation:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained ResNet</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> ResNet50(weights<span class="op">=</span><span class="st">'imagenet'</span>, include_top<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Freeze early layers (keep learned features)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> base_model.layers[:<span class="dv">100</span>]:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    layer.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add custom classification head</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> GlobalAveragePooling2D()(base_model.output)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> Dense(<span class="dv">8</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x)  <span class="co"># 8 Palawan classes</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>base_model.<span class="bu">input</span>, outputs<span class="op">=</span>output)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>When to Use:</strong> - Limited training data (&lt;5,000 samples) - Similar task to pre-training (natural images) - Need quick results</p>
<p><strong>Caution:</strong> ImageNet has RGB images. Sentinel-2 has 10+ bands. Adaptation strategies needed (Session 4).</p>
</section>
<section id="e.3-data-augmentation---artificially-expanding-training-data" class="level4">
<h4 class="anchored" data-anchor-id="e.3-data-augmentation---artificially-expanding-training-data">E.3: Data Augmentation - Artificially Expanding Training Data</h4>
<p><strong>Geometric Transformations:</strong> - <strong>Rotation:</strong> 90Â°, 180Â°, 270Â° (satellites view from any angle) - <strong>Horizontal/Vertical Flip:</strong> Valid for overhead imagery - <strong>Zoom/Scale:</strong> Simulate different resolutions - <strong>Translation:</strong> Small shifts</p>
<p><strong>Spectral Augmentations:</strong> - <strong>Brightness/Contrast:</strong> Simulate atmospheric conditions - <strong>Gaussian Noise:</strong> Simulate sensor noise - <strong>Band Dropout:</strong> Improve robustness</p>
<p><strong>Philippine Context:</strong> - âœ“ Use rotation/flips for land cover (no preferential orientation) - âœ— Avoid rotation for infrastructure (roads have direction) - âœ“ Augment brightness for cloud variations</p>
<p><strong>Implementation (Session 4):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>augmentation <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    rotation_range<span class="op">=</span><span class="dv">90</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    horizontal_flip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    vertical_flip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    brightness_range<span class="op">=</span>[<span class="fl">0.8</span>, <span class="fl">1.2</span>],</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    zoom_range<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="e.4-computational-requirements" class="level4">
<h4 class="anchored" data-anchor-id="e.4-computational-requirements">E.4: Computational Requirements</h4>
<p><strong>Training Resources:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Training Time*</th>
<th>GPU Memory</th>
<th>Cost (Colab Pro)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple CNN</td>
<td>30 min</td>
<td>4 GB</td>
<td>Free tier OK</td>
</tr>
<tr class="even">
<td>ResNet-50 (fine-tune)</td>
<td>2-4 hours</td>
<td>8 GB</td>
<td>Free tier OK</td>
</tr>
<tr class="odd">
<td>U-Net (segmentation)</td>
<td>4-8 hours</td>
<td>12 GB</td>
<td>Pro needed</td>
</tr>
<tr class="even">
<td>ResNet-50 (from scratch)</td>
<td>24+ hours</td>
<td>16 GB</td>
<td>Pro+ needed</td>
</tr>
</tbody>
</table>
<p>*1,000 training images, 50 epochs, V100 GPU</p>
<p><strong>Philippine Context:</strong> - PhilSA uses on-premise GPU servers (8Ã— NVIDIA A100) - Universities: Limited GPU access (submit jobs) - Practitioners: Google Colab Pro ($10/month) recommended</p>
<p><strong>Optimization Strategies (Session 4):</strong> - Use mixed precision training (FP16) - Reduce batch size if memory limited - Train on smaller image chips (128Ã—128 instead of 256Ã—256) - Use gradient checkpointing for large models</p>
</section>
<section id="e.5-model-interpretability---understanding-cnn-decisions" class="level4">
<h4 class="anchored" data-anchor-id="e.5-model-interpretability---understanding-cnn-decisions">E.5: Model Interpretability - Understanding CNN Decisions</h4>
<p><strong>Why It Matters:</strong> - DENR reports need explanations (â€œWhy was this classified as deforested?â€) - Debugging poor performance - Building stakeholder trust</p>
<p><strong>Techniques (Session 4):</strong> 1. <strong>Activation Visualization:</strong> See what filters learned 2. <strong>Saliency Maps:</strong> Which pixels influenced decision? 3. <strong>Class Activation Maps (CAM):</strong> Highlight relevant regions 4. <strong>Filter Visualization:</strong> What patterns do filters detect?</p>
<p><strong>Philippine Application Example:</strong> - <strong>Question:</strong> Why did model classify mangroves as agriculture? - <strong>CAM Analysis:</strong> Model focused on water proximity, not canopy structure - <strong>Solution:</strong> Add texture features or more mangrove training samples</p>
<hr>
</section>
</section>
</section>
<section id="key-concepts" class="level2">
<h2 class="anchored" data-anchor-id="key-concepts">Key Concepts</h2>
<section id="automatic-feature-learning" class="level3">
<h3 class="anchored" data-anchor-id="automatic-feature-learning">Automatic Feature Learning</h3>
<p><strong>What is it?</strong> CNNs learn optimal features directly from raw pixel data, eliminating manual feature engineering.</p>
<p><strong>How it works:</strong> - <strong>Layer 1:</strong> Learns edges (horizontal, vertical, diagonal) - like Sobel filters you created manually - <strong>Layer 2:</strong> Combines edges into textures and simple shapes - <strong>Layer 3:</strong> Combines shapes into complex patterns (canopy structure, urban grid) - <strong>Layer 4+:</strong> High-level semantic features (forest type, settlement pattern)</p>
<p><strong>Comparison to Session 2:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 50%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Random Forest (Session 2)</th>
<th>CNN (Session 3-4)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Features</strong></td>
<td>Manual (NDVI, GLCM)</td>
<td>Learned automatically</td>
</tr>
<tr class="even">
<td><strong>Spatial Context</strong></td>
<td>Limited (within feature)</td>
<td>Fully exploited (receptive field)</td>
</tr>
<tr class="odd">
<td><strong>Data Needed</strong></td>
<td>500-1,000 samples</td>
<td>5,000-10,000 samples</td>
</tr>
<tr class="even">
<td><strong>Training Time</strong></td>
<td>Minutes</td>
<td>Hours</td>
</tr>
<tr class="odd">
<td><strong>Accuracy</strong></td>
<td>80-85% typical</td>
<td>90-95% possible</td>
</tr>
</tbody>
</table>
</section>
<section id="receptive-field" class="level3">
<h3 class="anchored" data-anchor-id="receptive-field">Receptive Field</h3>
<p><strong>Definition:</strong> The region of input image that influences a particular neuronâ€™s activation.</p>
<p><strong>Example:</strong> - A neuron in Layer 1 sees a 3Ã—3 pixel region (30m Ã— 30m for Sentinel-2) - A neuron in Layer 3 sees a 15Ã—15 pixel region (150m Ã— 150m) - Output neuron â€œseesâ€ entire image chip</p>
<p><strong>Why it matters:</strong> - Small objects (buildings): Need fewer layers - Large objects (agricultural fields): Need deeper networks - Contextual classification: Large receptive field captures neighborhood</p>
<p><strong>Philippine Example:</strong> Classifying a pixel as â€œmangroveâ€ requires seeing water proximity (large receptive field) AND canopy texture (small receptive field). Multi-scale processing essential!</p>
</section>
<section id="translation-invariance" class="level3">
<h3 class="anchored" data-anchor-id="translation-invariance">Translation Invariance</h3>
<p><strong>What is it?</strong> CNN can recognize patterns regardless of position in image.</p>
<p><strong>How achieved:</strong> 1. <strong>Parameter sharing:</strong> Same filter applied everywhere 2. <strong>Pooling:</strong> Abstracts exact position</p>
<p><strong>EO Benefit:</strong> Forest is forest whether in top-left or bottom-right of image. Train once, apply anywhere in Philippines!</p>
<p><strong>Contrast with Position:</strong> For some tasks, position DOES matter (e.g., urban always near coasts in Philippines). Advanced architectures can encode position.</p>
</section>
<section id="gradient-descent-and-backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent-and-backpropagation">Gradient Descent and Backpropagation</h3>
<p><strong>Intuitive Explanation:</strong></p>
<p>Imagine hiking down a foggy mountain (error surface) to reach valley (minimum loss): - <strong>Gradient:</strong> Direction of steepest descent - <strong>Learning rate:</strong> Step size - <strong>Backpropagation:</strong> Efficiently calculates gradients for all weights</p>
<p><strong>Training Process:</strong> 1. Forward pass: Image â†’ Predictions 2. Calculate loss: How wrong were predictions? 3. Backward pass: Compute âˆ‚Loss/âˆ‚Weight for every parameter 4. Update weights: Move â€œdownhillâ€ toward better performance</p>
<p><strong>Common Issues:</strong> - <strong>Learning rate too high:</strong> Jump over minimum (unstable) - <strong>Learning rate too low:</strong> Painfully slow convergence - <strong>Local minima:</strong> Stuck in suboptimal solution (less common with large networks)</p>
<p><strong>Session 4:</strong> Implement Adam optimizer (adaptive learning rates)</p>
<hr>
</section>
</section>
<section id="interactive-demonstrations" class="level2">
<h2 class="anchored" data-anchor-id="interactive-demonstrations">Interactive Demonstrations</h2>
<section id="demo-1-perceptron-playground" class="level3">
<h3 class="anchored" data-anchor-id="demo-1-perceptron-playground">Demo 1: Perceptron Playground</h3>
<p><strong>Objective:</strong> Build intuition for how perceptrons learn decision boundaries</p>
<p><strong>Activity:</strong> 1. Load 2D dataset (NDVI vs NDWI for water classification) 2. Initialize random weights 3. Visualize decision boundary 4. Update weights iteratively 5. Watch boundary align with data</p>
<p><strong>Notebook:</strong> <code>session3_theory_STUDENT.ipynb</code> (Part 1)</p>
<p><strong>Expected Outcome:</strong> Understand that neural networks find separating hyperplanes through gradient descent.</p>
</section>
<section id="demo-2-activation-function-gallery" class="level3">
<h3 class="anchored" data-anchor-id="demo-2-activation-function-gallery">Demo 2: Activation Function Gallery</h3>
<p><strong>Objective:</strong> Visualize how different activation functions transform data</p>
<p><strong>Activity:</strong> 1. Plot sigmoid, ReLU, tanh, Leaky ReLU 2. Apply to Sentinel-2 reflectance values 3. Compare output distributions 4. See why ReLU is most popular</p>
<p><strong>Notebook:</strong> <code>session3_theory_STUDENT.ipynb</code> (Part 2)</p>
<p><strong>Key Insight:</strong> ReLU is simple, fast, and sparse (many zeros = efficient).</p>
</section>
<section id="demo-3-manual-convolution-on-sentinel-2" class="level3">
<h3 class="anchored" data-anchor-id="demo-3-manual-convolution-on-sentinel-2">Demo 3: Manual Convolution on Sentinel-2</h3>
<p><strong>Objective:</strong> Understand convolution as a sliding filter operation</p>
<p><strong>Activity:</strong> 1. Load Sentinel-2 NIR band (Palawan forest patch) 2. Define edge detection filter (3Ã—3 Sobel) 3. Manually compute convolution (NumPy) 4. Visualize feature map 5. Try different filters (blur, sharpen, Gaussian)</p>
<p><strong>Notebook:</strong> <code>session3_cnn_operations_STUDENT.ipynb</code> (Part 1)</p>
<p><strong>Aha Moment:</strong> â€œEdge detection filter highlights forest boundariesâ€”exactly what CNN learns automatically!â€</p>
</section>
<section id="demo-4-pooling-demonstration" class="level3">
<h3 class="anchored" data-anchor-id="demo-4-pooling-demonstration">Demo 4: Pooling Demonstration</h3>
<p><strong>Objective:</strong> Understand downsampling and translation invariance</p>
<p><strong>Activity:</strong> 1. Load Sentinel-2 image chip (256Ã—256) 2. Apply max pooling (2Ã—2, stride 2) 3. Compare original vs pooled (128Ã—128) 4. Shift image by 1 pixel, repeat 5. See that pooled output is nearly identical (translation invariance)</p>
<p><strong>Notebook:</strong> <code>session3_cnn_operations_STUDENT.ipynb</code> (Part 3)</p>
<p><strong>Takeaway:</strong> Pooling reduces dimensionality while preserving important features.</p>
</section>
<section id="demo-5-architecture-exploration" class="level3">
<h3 class="anchored" data-anchor-id="demo-5-architecture-exploration">Demo 5: Architecture Exploration</h3>
<p><strong>Objective:</strong> Compare CNN architectures visually</p>
<p><strong>Activity:</strong> 1. Visualize LeNet-5, VGG-16, ResNet-50, U-Net architectures 2. Count parameters for each 3. Trace receptive field growth 4. Discuss trade-offs (accuracy vs speed)</p>
<p><strong>Notebook:</strong> <code>session3_cnn_operations_STUDENT.ipynb</code> (Part 4)</p>
<p><strong>Connection to Session 4:</strong> Choose architecture based on task (classification â†’ ResNet, segmentation â†’ U-Net).</p>
<hr>
</section>
</section>
<section id="philippine-eo-applications" class="level2">
<h2 class="anchored" data-anchor-id="philippine-eo-applications">Philippine EO Applications</h2>
<section id="philsa-space-dashboard" class="level3">
<h3 class="anchored" data-anchor-id="philsa-space-dashboard">PhilSA Space+ Dashboard</h3>
<p><strong>Current CNN Applications:</strong></p>
<ol type="1">
<li><strong>Automated Cloud Masking</strong>
<ul>
<li><strong>Model:</strong> U-Net trained on 5,000 Sentinel-2 scenes</li>
<li><strong>Performance:</strong> 95% accuracy, 2 min per scene</li>
<li><strong>Impact:</strong> Enables rapid mosaic generation</li>
</ul></li>
<li><strong>Land Cover Classification (National)</strong>
<ul>
<li><strong>Model:</strong> ResNet-50 fine-tuned on Philippine landscape</li>
<li><strong>Classes:</strong> 10 (following FAO LCCS)</li>
<li><strong>Coverage:</strong> Entire Philippines, quarterly updates</li>
<li><strong>Users:</strong> DENR, DAR, NEDA</li>
</ul></li>
<li><strong>Disaster Rapid Mapping</strong>
<ul>
<li><strong>Flood Detection:</strong> Sentinel-1 + U-Net â†’ 6-hour response</li>
<li><strong>Damage Assessment:</strong> High-res + object detection â†’ building damage maps</li>
<li><strong>Integration:</strong> NDRRMC operations dashboard</li>
</ul></li>
</ol>
</section>
<section id="denr-forest-monitoring" class="level3">
<h3 class="anchored" data-anchor-id="denr-forest-monitoring">DENR Forest Monitoring</h3>
<p><strong>CNN Use Cases:</strong></p>
<ul>
<li><strong>Protected Area Surveillance:</strong> Monthly Sentinel-2 analysis (ResNet classifier)</li>
<li><strong>Illegal Logging Detection:</strong> Change detection CNN on multi-temporal stacks</li>
<li><strong>Biodiversity Hotspot Mapping:</strong> Fine-grained forest type classification</li>
<li><strong>REDD+ MRV:</strong> Automated forest cover change reporting</li>
</ul>
<p><strong>Data Pipeline:</strong></p>
<pre><code>Sentinel-2 (PhilSA) â†’ Preprocessing (cloud mask) â†’
CNN Classification â†’ Change Detection â†’
Alert Generation â†’ Field Validation</code></pre>
</section>
<section id="lgu-applications-session-4-focus" class="level3">
<h3 class="anchored" data-anchor-id="lgu-applications-session-4-focus">LGU Applications (Session 4 Focus)</h3>
<p><strong>Accessible CNN Tools for Local Governments:</strong></p>
<ol type="1">
<li><strong>ASTI SkAI-Pinas:</strong> Pre-trained models for common PH tasks</li>
<li><strong>Google Earth Engine:</strong> CNN inference on cloud platform</li>
<li><strong>Colab Notebooks:</strong> Low-cost GPU training (this training!)</li>
</ol>
<p><strong>Example Workflow (Session 4):</strong> - LGU staff collects 500 training labels (Palawan land cover) - Fine-tunes ResNet-50 using Session 4 notebook - Deploys model for quarterly monitoring - Integrates into local land use planning</p>
<hr>
</section>
</section>
<section id="expected-outcomes" class="level2">
<h2 class="anchored" data-anchor-id="expected-outcomes">Expected Outcomes</h2>
<section id="conceptual-understanding" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-understanding">Conceptual Understanding</h3>
<p>By the end of Session 3, you should be able to:</p>
<p>âœ… <strong>Explain to a colleague</strong> why CNNs are better than Random Forest for complex spatial patterns âœ… <strong>Sketch</strong> a simple CNN architecture and label components (Conv, Pool, FC) âœ… <strong>Describe</strong> what happens during forward and backward propagation âœ… <strong>Identify</strong> appropriate architectures for classification vs segmentation âœ… <strong>Discuss</strong> data requirements and computational constraints</p>
</section>
<section id="technical-skills" class="level3">
<h3 class="anchored" data-anchor-id="technical-skills">Technical Skills</h3>
<p>âœ… <strong>Build</strong> a simple perceptron from scratch using NumPy âœ… <strong>Implement</strong> activation functions and visualize their behavior âœ… <strong>Perform</strong> manual convolution on satellite imagery âœ… <strong>Apply</strong> classic edge detection filters (Sobel, Gaussian) âœ… <strong>Visualize</strong> feature maps and pooling operations</p>
</section>
<section id="practical-readiness-for-session-4" class="level3">
<h3 class="anchored" data-anchor-id="practical-readiness-for-session-4">Practical Readiness for Session 4</h3>
<p>âœ… <strong>Understand</strong> why weâ€™ll use TensorFlow/Keras (vs building from scratch) âœ… <strong>Anticipate</strong> challenges with Sentinel-2 multi-band data âœ… <strong>Recognize</strong> data preparation needs (chips, labels, augmentation) âœ… <strong>Set expectations</strong> for training time and resource requirements</p>
<hr>
</section>
</section>
<section id="hands-on-notebooks" class="level2">
<h2 class="anchored" data-anchor-id="hands-on-notebooks">Hands-On Notebooks</h2>
<section id="access-the-interactive-materials" class="level3">
<h3 class="anchored" data-anchor-id="access-the-interactive-materials">Access the Interactive Materials</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>ğŸ““ Jupyter Notebooks (Theory + Interactive)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Two comprehensive notebooks guide you through neural network and CNN fundamentals:</p>
<p><strong>Notebook 1: Neural Network Theory</strong> <a href="../../day2/notebooks/session3_theory_interactive.html"><code>session3_theory_interactive.ipynb</code></a></p>
<p><strong>Contents:</strong> - Build perceptron from scratch - Implement activation functions - Train 2-layer network on spectral data - Explore learning rate effects - Visualize decision boundaries</p>
<p><strong>Duration:</strong> 45 minutes</p>
<hr>
<p><strong>Notebook 2: CNN Operations</strong> <a href="../../day2/notebooks/session3_theory_interactive.html"><code>session3_theory_interactive.ipynb</code></a></p>
<p><strong>Contents:</strong> - Manual convolution on Sentinel-2 imagery - Edge detection filters (Sobel, Gaussian, Laplacian) - Max pooling demonstration - Architecture comparison (LeNet, VGG, ResNet, U-Net) - Feature map visualization</p>
<p><strong>Duration:</strong> 55 minutes</p>
<hr>
<p><strong>Google Colab:</strong> <a href="https://colab.research.google.com/github/DimitrisKasabalis/EO_trainning/blob/main/course_site/day2/notebooks/session3_theory_interactive.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open Theory Notebook"></a> <a href="https://colab.research.google.com/github/DimitrisKasabalis/EO_trainning/blob/main/course_site/day2/notebooks/session3_theory_interactive.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open CNN Notebook"></a></p>
</div>
</div>
</section>
<section id="supporting-documentation" class="level3">
<h3 class="anchored" data-anchor-id="supporting-documentation">Supporting Documentation</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>ğŸ“š Reference Materials
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>CNN Architectures Deep Dive:</strong> <a href="../../resources/cheatsheets.html"><code>Cheatsheets &amp; References</code></a></p>
<p>Detailed explanations of LeNet-5, VGG-16, ResNet-50, U-Net, and modern variants (EfficientNet, Vision Transformers). Includes architecture diagrams, parameter counts, and EO applications.</p>
<hr>
<p><strong>EO Applications Guide:</strong> <a href="../../resources/glossary.html"><code>Glossary &amp; EO Resources</code></a></p>
<p>Comprehensive guide to CNN applications in Earth observation: - Scene classification examples - Semantic segmentation workflows - Object detection case studies - Change detection methods - Philippine-specific use cases</p>
<hr>
<p><strong>Session Overview:</strong> <a href="../../day2/index.html"><code>Day 2 Overview</code></a></p>
<p>Quick reference with session objectives, structure, and prerequisites.</p>
</div>
</div>
<hr>
</section>
</section>
<section id="troubleshooting" class="level2">
<h2 class="anchored" data-anchor-id="troubleshooting">Troubleshooting</h2>
<section id="common-conceptual-questions" class="level3">
<h3 class="anchored" data-anchor-id="common-conceptual-questions">Common Conceptual Questions</h3>
<p><strong>â€œWhy does CNN need so much more data than Random Forest?â€</strong></p>
<p>Random Forest learns from features YOU engineered (NDVI, GLCM). It needs to learn relationships between ~10-20 features.</p>
<p>CNNs learn from raw pixels (~10,000 per image chip). It needs to learn what features to extract PLUS how to classify. Much harder optimization problem = more data needed.</p>
<p><strong>Mitigation:</strong> Transfer learning (Session 4) dramatically reduces data needs.</p>
<hr>
<p><strong>â€œCan I use CNNs with small datasets (&lt;500 samples)?â€</strong></p>
<p>Technically yes, but results will be poor if training from scratch.</p>
<p><strong>Better approach:</strong> 1. Use pre-trained models (ImageNet weights) 2. Aggressive data augmentation 3. Use simpler architectures (fewer layers) 4. Consider traditional ML if data is truly limited</p>
<hr>
<p><strong>â€œWhy are my manual convolutions so slow in the notebook?â€</strong></p>
<p>NumPy convolutions (for learning) are intentionally simple. Production CNNs use highly optimized libraries (cuDNN) on GPUsâ€”1000Ã— faster!</p>
<p>Session 4 will use TensorFlow/PyTorch with GPU acceleration.</p>
<hr>
<p><strong>â€œHow do I know which architecture to use?â€</strong></p>
<p><strong>Simple decision tree:</strong> - <strong>Classification task</strong> (one label per image) â†’ ResNet, EfficientNet - <strong>Segmentation task</strong> (label every pixel) â†’ U-Net, DeepLabv3+ - <strong>Object detection</strong> (find + localize) â†’ YOLO, Faster R-CNN - <strong>Limited data</strong> â†’ Simpler architecture + transfer learning - <strong>Real-time inference needed</strong> â†’ MobileNet, EfficientNet-Lite</p>
<p>Session 4 will implement ResNet (classification) and U-Net (segmentation).</p>
<hr>
</section>
<section id="technical-issues" class="level3">
<h3 class="anchored" data-anchor-id="technical-issues">Technical Issues</h3>
<p><strong>â€œNotebook cells wonâ€™t execute in Colabâ€</strong></p>
<ol type="1">
<li>Check GPU is enabled: Runtime â†’ Change runtime type â†’ GPU</li>
<li>Restart runtime: Runtime â†’ Restart runtime</li>
<li>Verify installations: <code>!pip list | grep numpy</code></li>
</ol>
<hr>
<p><strong>â€œOut of memory error when running convolutionsâ€</strong></p>
<ul>
<li>Use smaller image chips (128Ã—128 instead of 256Ã—256)</li>
<li>Process one image at a time (donâ€™t load entire dataset)</li>
<li>Restart Colab runtime to clear memory</li>
</ul>
<hr>
<p><strong>â€œVisualizations arenâ€™t displayingâ€</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add to beginning of notebook</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">10</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
<p><strong>â€œCanâ€™t access Sentinel-2 data in notebookâ€</strong></p>
<p>Session 3 uses pre-downloaded Sentinel-2 samples (included in notebook). No GEE authentication needed.</p>
<p>Session 4 will integrate GEE for larger-scale data loading.</p>
<hr>
</section>
<section id="getting-help" class="level3">
<h3 class="anchored" data-anchor-id="getting-help">Getting Help</h3>
<ul>
<li>ğŸ“– <a href="https://developers.google.com/machine-learning/crash-course/image-classification">CNN Tutorial (Google ML Crash Course)</a></li>
<li>ğŸ“– <a href="http://cs231n.stanford.edu/">CS231n Stanford Course</a> - Best CNN educational resource</li>
<li>ğŸ’¬ <a href="mailto:skotsopoulos@neuralio.ai">Instructor support</a> - Questions during lab hours</li>
<li>ğŸ“§ <a href="https://data.philsa.gov.ph/support">PhilSA Data Support</a> - Access issues</li>
</ul>
<hr>
</section>
</section>
<section id="additional-resources" class="level2">
<h2 class="anchored" data-anchor-id="additional-resources">Additional Resources</h2>
<section id="foundational-learning" class="level3">
<h3 class="anchored" data-anchor-id="foundational-learning">Foundational Learning</h3>
<p><strong>Neural Networks:</strong> - <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">3Blue1Brown Neural Network Series</a> - Best intuitive explanation (visual) - <a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning (Free Book)</a> - <a href="https://www.coursera.org/specializations/deep-learning">Andrew Ngâ€™s Deep Learning Specialization</a> (Coursera)</p>
<p><strong>CNNs Specifically:</strong> - <a href="http://cs231n.stanford.edu/schedule.html">Stanford CS231n Lectures</a> - Comprehensive (free) - <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer (Interactive)</a> - Visualize CNNs in browser - <a href="https://distill.pub/">Distill.pub Articles</a> - Beautiful visual explanations</p>
</section>
<section id="earth-observation-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="earth-observation-deep-learning">Earth Observation Deep Learning</h3>
<p><strong>Papers:</strong> - Zhu et al.&nbsp;(2017). â€œDeep Learning in Remote Sensing: A Review.â€ <em>IEEE GRSM</em> - Ma et al.&nbsp;(2019). â€œDeep learning in remote sensing applications: A meta-analysis and review.â€ <em>ISPRS</em> - RuÃŸwurm &amp; KÃ¶rner (2020). â€œSelf-attention for raw optical satellite time series classification.â€ <em>ISPRS</em></p>
<p><strong>Tutorials:</strong> - <a href="https://eo-college.org/resources/deep-learning/">Deep Learning for Earth Observation (ESA)</a> - <a href="https://rastervision.io/">Raster Vision</a> - Framework for geospatial ML - <a href="https://torchgeo.readthedocs.io/">TorchGeo</a> - PyTorch library for geospatial data</p>
<p><strong>Datasets:</strong> - <a href="https://github.com/phelber/EuroSAT">EuroSAT</a> - Sentinel-2 scene classification (10 classes) - <a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html">UC Merced Land Use</a> - High-res classification - <a href="http://deepglobe.org/">DeepGlobe</a> - Segmentation challenges - <a href="https://spacenet.ai/">SpaceNet</a> - Building detection</p>
</section>
<section id="philippine-context" class="level3">
<h3 class="anchored" data-anchor-id="philippine-context">Philippine Context</h3>
<ul>
<li><a href="https://philsa.gov.ph/publications/">PhilSA Research Publications</a> - CNN applications in Philippines</li>
<li><a href="https://skai.dost.gov.ph/docs">ASTI SkAI-Pinas Documentation</a> - Pre-trained PH models</li>
<li><a href="https://dimer.asti.dost.gov.ph/">DIMER Database</a> - Philippine disaster imagery</li>
</ul>
<hr>
</section>
</section>
<section id="assessment" class="level2">
<h2 class="anchored" data-anchor-id="assessment">Assessment</h2>
<section id="formative-assessment-during-session" class="level3">
<h3 class="anchored" data-anchor-id="formative-assessment-during-session">Formative Assessment (During Session)</h3>
<p><strong>Self-Check Questions:</strong></p>
<ol type="1">
<li>âœ“ Can you explain the difference between a perceptron and a multi-layer network?</li>
<li>âœ“ Why does ReLU work better than sigmoid in hidden layers?</li>
<li>âœ“ What does a convolutional filter â€œlearnâ€ during training?</li>
<li>âœ“ How does pooling provide translation invariance?</li>
<li>âœ“ When would you use U-Net instead of ResNet?</li>
</ol>
<p><strong>Interactive Exercises:</strong> - âœ“ Complete all TODO cells in theory notebook - âœ“ Implement manual convolution from scratch - âœ“ Visualize at least 3 different activation functions - âœ“ Compare filter responses on forest vs urban areas</p>
</section>
<section id="summative-assessment-end-of-session" class="level3">
<h3 class="anchored" data-anchor-id="summative-assessment-end-of-session">Summative Assessment (End of Session)</h3>
<p><strong>Knowledge Check (10 questions, multiple choice):</strong> - Neural network components - CNN architecture understanding - Application selection (classification vs segmentation) - Data requirements and constraints</p>
<p><strong>Practical Demonstration:</strong> - Explain CNN decision-making process for a sample image - Sketch appropriate architecture for given EO task - Estimate data requirements for Philippine use case</p>
<p><strong>Readiness for Session 4:</strong> - âœ“ Understand TensorFlow/Keras workflow conceptually - âœ“ Recognize data preparation needs - âœ“ Anticipate training challenges</p>
<hr>
</section>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>After Session 3
</div>
</div>
<div class="callout-body-container callout-body">
<p>You now understand CNN theory and operations! Session 4 puts this knowledge into practice with hands-on implementation.</p>
<p><strong>Session 4: Hands-On CNN Implementation</strong></p>
<p>Youâ€™ll build and train: 1. <strong>ResNet Classifier:</strong> Palawan land cover (8 classes) 2. <strong>U-Net Segmentation:</strong> Flood mapping (Central Luzon)</p>
<p>Using TensorFlow/Keras with real Sentinel-2 data.</p>
<p><a href="../../day2/sessions/session4.html" class="btn btn-primary">Continue to Session 4 â†’</a></p>
</div>
</div>
<section id="recommended-pre-work-for-session-4" class="level3">
<h3 class="anchored" data-anchor-id="recommended-pre-work-for-session-4">Recommended Pre-Work for Session 4</h3>
<p>Before Session 4, please:</p>
<ol type="1">
<li>âœ… <strong>Review notebook exercises</strong> - Ensure you understand convolution and activation functions</li>
<li>âœ… <strong>Read U-Net paper</strong> - <a href="https://arxiv.org/abs/1505.04597">Ronneberger et al.&nbsp;2015</a> (15 min)</li>
<li>âœ… <strong>Check GPU access</strong> - Enable GPU in Colab (Settings â†’ Hardware Accelerator â†’ GPU)</li>
<li>âœ… <strong>Install TensorFlow</strong> - <code>!pip install tensorflow==2.15</code> (will do in Session 4, but test now)</li>
</ol>
</section>
<section id="extended-learning-paths" class="level3">
<h3 class="anchored" data-anchor-id="extended-learning-paths">Extended Learning Paths</h3>
<p><strong>Path 1: Deep Dive into Theory</strong> - Complete CS231n Stanford course - Implement backpropagation from scratch - Study optimization algorithms (Adam, RMSprop)</p>
<p><strong>Path 2: Explore Advanced Architectures</strong> - Attention mechanisms (Transformers for EO) - Vision Transformers (ViT) - Self-supervised learning (SimCLR, MoCo)</p>
<p><strong>Path 3: Philippine EO Applications</strong> - Contribute training data to PhilSA Space+ - Develop CNN model for local area - Publish results in Philippine GIS conference</p>
<hr>
</section>
</section>
<section id="quick-links" class="level2">
<h2 class="anchored" data-anchor-id="quick-links">Quick Links</h2>
<ul>
<li><strong>Part A (MLâ†’DL):</strong> 15 min - Keep conceptual, avoid getting bogged down in math</li>
<li><strong>Part B (NN Fundamentals):</strong> 25 min - Live code perceptron, let students modify</li>
<li><strong>Part C (CNNs):</strong> 30 min - Most critical section, use lots of visuals</li>
<li><strong>Part D (EO Applications):</strong> 25 min - Show real PhilSA examples if possible</li>
<li><strong>Part E (Practical):</strong> 15 min - Set realistic expectations for Session 4</li>
</ul>
<p><strong>Total:</strong> 110 min (2.5 hours with 20 min buffer for questions)</p>
<hr>
<p><strong>Common Student Challenges:</strong></p>
<ol type="1">
<li><p><strong>â€œI donâ€™t understand backpropagationâ€</strong> â†’ Focus on intuition (gradient descent down error surface), not calculus. Session 4 uses libraries that handle this automatically.</p></li>
<li><p><strong>â€œWhy are we doing manual convolutions in NumPy?â€</strong> â†’ Emphasize this is for learning. Session 4 uses optimized libraries (1000Ã— faster). Like learning to drive with manual transmissionâ€”helps understand whatâ€™s happening under the hood.</p></li>
<li><p><strong>â€œWill my laptop be fast enough for Session 4?â€</strong> â†’ No local installation needed! Google Colab provides free GPUs. Sessions 4 notebooks are optimized for Colab free tier.</p></li>
<li><p><strong>â€œCan CNNs use Sentinel-2â€™s 10+ bands?â€</strong> â†’ YES! Unlike ImageNet RGB pre-training. Session 4 shows adaptation strategies. This is a huge advantage of CNNs for EO.</p></li>
</ol>
<hr>
<p><strong>Teaching Tips:</strong></p>
<ul>
<li><strong>Start with Familiar:</strong> Connect to Session 2 Random Forest throughout</li>
<li><strong>Visual Heavy:</strong> Show lots of images/diagrams (CNNs are visual learners!)</li>
<li><strong>Interactive:</strong> Have students modify filter values in real-time</li>
<li><strong>Philippine Examples:</strong> Use Palawan imagery from Session 2 for continuity</li>
<li><strong>Manage Expectations:</strong> Be honest about data/compute requirements</li>
<li><strong>Celebrate Progress:</strong> â€œYou now understand CNNs better than 90% of GIS professionals!â€</li>
</ul>
<hr>
<p><strong>Demonstration Best Practices:</strong></p>
<ol type="1">
<li><strong>Perceptron Demo:</strong> Use simple 2D data first (NDVI vs NDWI), visualize decision boundary updating in real-time</li>
<li><strong>Convolution Demo:</strong> Pick dramatic example (forest edge) where edge detection is obvious</li>
<li><strong>Architecture Comparison:</strong> Show parameter counts to emphasize efficiency gains</li>
<li><strong>Philippine Focus:</strong> Always end each section with â€œHow does this help DENR/PhilSA/LGUs?â€</li>
</ol>
<hr>
<p><strong>Assessment Rubric:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 21%">
<col style="width: 14%">
<col style="width: 19%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Excellent (5)</th>
<th>Good (4)</th>
<th>Adequate (3)</th>
<th>Needs Improvement (2)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Conceptual</strong></td>
<td>Explains CNN advantages with examples</td>
<td>States CNN benefits</td>
<td>Lists CNN components</td>
<td>Confuses CNN with traditional ML</td>
</tr>
<tr class="even">
<td><strong>Technical</strong></td>
<td>Implements convolution from scratch</td>
<td>Completes all notebook exercises</td>
<td>Completes &gt;70% exercises</td>
<td>Struggles with NumPy operations</td>
</tr>
<tr class="odd">
<td><strong>Application</strong></td>
<td>Proposes appropriate architecture for novel task</td>
<td>Identifies correct architecture for standard tasks</td>
<td>Distinguishes classification vs segmentation</td>
<td>Cannot select architecture</td>
</tr>
<tr class="even">
<td><strong>Readiness</strong></td>
<td>Articulates Session 4 workflow</td>
<td>Understands TensorFlow role</td>
<td>Knows Session 4 is hands-on</td>
<td>Unclear on next steps</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>Session 4 Transition:</strong></p>
<p>End with excitement: <em>â€œYouâ€™ve learned the theory. Tomorrow youâ€™ll build a production-ready land cover classifier and flood mapper using TensorFlow. Bring your questions, bring your data ideas, and get ready to train some CNNs!â€</em></p>
<p><strong>Preview Session 4 Outcomes:</strong> - ResNet model achieving &gt;85% accuracy on Palawan - U-Net generating flood maps in 2 minutes - Exportable models for operational use</p>
<hr>
<p><strong>Backup Activities (if ahead of schedule):</strong></p>
<ol type="1">
<li><strong>Live Code Challenge:</strong> â€œBuild a 3-layer network for Palawan classificationâ€ (10 min)</li>
<li><strong>Architecture Design:</strong> â€œSketch CNN for building detection taskâ€ (5 min)</li>
<li><strong>Group Discussion:</strong> â€œWhat EO problem in YOUR organization could use CNNs?â€ (10 min)</li>
</ol>
<hr>
<p><strong>Resource Check (Before Session):</strong></p>
<ul>
<li>âœ“ Test both Colab notebooks execute without errors</li>
<li>âœ“ Verify Sentinel-2 sample data loads correctly</li>
<li>âœ“ Pre-download datasets to Google Drive (backup)</li>
<li>âœ“ Have architecture diagrams ready (slides or drawn)</li>
<li>âœ“ Queue up CNN Explainer website for demos</li>
<li>âœ“ Test video/screen sharing for visualizations :::</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="cophil-training-v1.0" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../day2/sessions/session2.html" class="pagination-link" aria-label="Session 2: Advanced Palawan Land Cover Lab">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Session 2: Advanced Palawan Land Cover Lab</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../day2/sessions/session4.html" class="pagination-link" aria-label="Session 4: CNN Hands-on Lab">
        <span class="nav-page-text">Session 4: CNN Hands-on Lab</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>CoPhil EO AI/ML Training Programme</p>
</div>   
    <div class="nav-footer-center">
<p>Funded by the European Union - Global Gateway Initiative</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:skotsopoulos@neuralio.ai">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://philsa.gov.ph">
      <i class="bi bi-globe" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>