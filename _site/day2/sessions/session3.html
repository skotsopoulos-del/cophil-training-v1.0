<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="CoPhil Advanced Training Program">
<meta name="dcterms.date" content="2025-10-19">

<title>Session 3: Introduction to Deep Learning and CNNs – CoPhil EO AI/ML Training</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../day2/sessions/session4.html" rel="next">
<link href="../../day2/sessions/session2.html" rel="prev">
<link href="../../images/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c9822816d3895e59fda95a6fa7545fef.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-3775014fae9fc394bbda1d6ff89dd45e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-509191933"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-509191933', { 'anonymize_ip': true});
</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles/custom.css">
<link rel="stylesheet" href="../../styles/phase2-enhancements.css">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CoPhil EO AI/ML Training</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-training-days" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Training Days</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-training-days">    
        <li>
    <a class="dropdown-item" href="../../day1/index.html">
 <span class="dropdown-text">Day 1: EO Data &amp; Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day2/index.html">
 <span class="dropdown-text">Day 2: Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day3/index.html">
 <span class="dropdown-text">Day 3: Deep Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../day4/index.html">
 <span class="dropdown-text">Day 4: Advanced Topics</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-resources" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Resources</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-resources">    
        <li>
    <a class="dropdown-item" href="../../resources/setup.html">
 <span class="dropdown-text">Setup Guide</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/philippine-eo.html">
 <span class="dropdown-text">Philippine EO Links</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/cheatsheets.html">
 <span class="dropdown-text">Cheat Sheets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/faq.html">
 <span class="dropdown-text">FAQ</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../resources/glossary.html">
 <span class="dropdown-text">Glossary</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../resources/downloads.html"> <i class="bi bi-download" role="img">
</i> 
<span class="menu-text">Materials</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day2/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day2/sessions/session3.html">Session 3: Introduction to Deep Learning and CNNs</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Day 2: Machine Learning for Earth Observation</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Sessions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/sessions/session1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 1: Supervised Classification with Random Forest</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/sessions/session2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 2: Advanced Palawan Land Cover Lab</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/sessions/session3.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Session 3: Introduction to Deep Learning and CNNs</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/sessions/session4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: CNN Hands-on Lab</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Notebooks</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session1_theory_notebook_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 1 Theory: Understanding Random Forest for Earth Observation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session1_hands_on_lab_student.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 1 Hands-on Lab: Palawan Land Cover Classification with Random Forest</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session2_extended_lab_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 2: Advanced Palawan Land Cover Classification Lab</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session3_theory_interactive.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 3: Deep Learning &amp; CNN Theory - Interactive Notebook</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session4_cnn_classification_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4: CNN Hands-On Lab - EuroSAT Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../day2/notebooks/session4_transfer_learning_STUDENT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Session 4 Part B: Transfer Learning with ResNet50</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">On This Page</h2>
   
  <ul>
  <li><a href="#session-3-introduction-to-deep-learning-and-cnns" id="toc-session-3-introduction-to-deep-learning-and-cnns" class="nav-link active" data-scroll-target="#session-3-introduction-to-deep-learning-and-cnns">Session 3: Introduction to Deep Learning and CNNs</a>
  <ul class="collapse">
  <li><a href="#neural-networks-and-convolutional-architectures-for-earth-observation" id="toc-neural-networks-and-convolutional-architectures-for-earth-observation" class="nav-link" data-scroll-target="#neural-networks-and-convolutional-architectures-for-earth-observation">Neural Networks and Convolutional Architectures for Earth Observation</a></li>
  </ul></li>
  <li><a href="#session-overview" id="toc-session-overview" class="nav-link" data-scroll-target="#session-overview">Session Overview</a></li>
  <li><a href="#presentation-slides" id="toc-presentation-slides" class="nav-link" data-scroll-target="#presentation-slides">Presentation Slides</a></li>
  <li><a href="#what-youll-learn" id="toc-what-youll-learn" class="nav-link" data-scroll-target="#what-youll-learn">What You’ll Learn</a></li>
  <li><a href="#session-structure" id="toc-session-structure" class="nav-link" data-scroll-target="#session-structure">Session Structure</a>
  <ul class="collapse">
  <li><a href="#part-a-from-machine-learning-to-deep-learning-15-minutes" id="toc-part-a-from-machine-learning-to-deep-learning-15-minutes" class="nav-link" data-scroll-target="#part-a-from-machine-learning-to-deep-learning-15-minutes">Part A: From Machine Learning to Deep Learning (15 minutes)</a></li>
  <li><a href="#part-b-neural-network-fundamentals-25-minutes" id="toc-part-b-neural-network-fundamentals-25-minutes" class="nav-link" data-scroll-target="#part-b-neural-network-fundamentals-25-minutes">Part B: Neural Network Fundamentals (25 minutes)</a></li>
  <li><a href="#part-c-convolutional-neural-networks-30-minutes" id="toc-part-c-convolutional-neural-networks-30-minutes" class="nav-link" data-scroll-target="#part-c-convolutional-neural-networks-30-minutes">Part C: Convolutional Neural Networks (30 minutes)</a></li>
  <li><a href="#part-d-cnns-for-earth-observation-25-minutes" id="toc-part-d-cnns-for-earth-observation-25-minutes" class="nav-link" data-scroll-target="#part-d-cnns-for-earth-observation-25-minutes">Part D: CNNs for Earth Observation (25 minutes)</a></li>
  <li><a href="#part-e-practical-considerations-for-eo-deep-learning-15-minutes" id="toc-part-e-practical-considerations-for-eo-deep-learning-15-minutes" class="nav-link" data-scroll-target="#part-e-practical-considerations-for-eo-deep-learning-15-minutes">Part E: Practical Considerations for EO Deep Learning (15 minutes)</a></li>
  </ul></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts">Key Concepts</a>
  <ul class="collapse">
  <li><a href="#automatic-feature-learning" id="toc-automatic-feature-learning" class="nav-link" data-scroll-target="#automatic-feature-learning">Automatic Feature Learning</a></li>
  <li><a href="#receptive-field" id="toc-receptive-field" class="nav-link" data-scroll-target="#receptive-field">Receptive Field</a></li>
  <li><a href="#translation-invariance" id="toc-translation-invariance" class="nav-link" data-scroll-target="#translation-invariance">Translation Invariance</a></li>
  <li><a href="#gradient-descent-and-backpropagation" id="toc-gradient-descent-and-backpropagation" class="nav-link" data-scroll-target="#gradient-descent-and-backpropagation">Gradient Descent and Backpropagation</a></li>
  </ul></li>
  <li><a href="#interactive-demonstrations" id="toc-interactive-demonstrations" class="nav-link" data-scroll-target="#interactive-demonstrations">Interactive Demonstrations</a>
  <ul class="collapse">
  <li><a href="#demo-1-perceptron-playground" id="toc-demo-1-perceptron-playground" class="nav-link" data-scroll-target="#demo-1-perceptron-playground">Demo 1: Perceptron Playground</a></li>
  <li><a href="#demo-2-activation-function-gallery" id="toc-demo-2-activation-function-gallery" class="nav-link" data-scroll-target="#demo-2-activation-function-gallery">Demo 2: Activation Function Gallery</a></li>
  <li><a href="#demo-3-manual-convolution-on-sentinel-2" id="toc-demo-3-manual-convolution-on-sentinel-2" class="nav-link" data-scroll-target="#demo-3-manual-convolution-on-sentinel-2">Demo 3: Manual Convolution on Sentinel-2</a></li>
  <li><a href="#demo-4-pooling-demonstration" id="toc-demo-4-pooling-demonstration" class="nav-link" data-scroll-target="#demo-4-pooling-demonstration">Demo 4: Pooling Demonstration</a></li>
  <li><a href="#demo-5-architecture-exploration" id="toc-demo-5-architecture-exploration" class="nav-link" data-scroll-target="#demo-5-architecture-exploration">Demo 5: Architecture Exploration</a></li>
  </ul></li>
  <li><a href="#philippine-eo-applications" id="toc-philippine-eo-applications" class="nav-link" data-scroll-target="#philippine-eo-applications">Philippine EO Applications</a>
  <ul class="collapse">
  <li><a href="#philsa-space-dashboard" id="toc-philsa-space-dashboard" class="nav-link" data-scroll-target="#philsa-space-dashboard">PhilSA Space+ Dashboard</a></li>
  <li><a href="#denr-forest-monitoring" id="toc-denr-forest-monitoring" class="nav-link" data-scroll-target="#denr-forest-monitoring">DENR Forest Monitoring</a></li>
  <li><a href="#lgu-applications-session-4-focus" id="toc-lgu-applications-session-4-focus" class="nav-link" data-scroll-target="#lgu-applications-session-4-focus">LGU Applications (Session 4 Focus)</a></li>
  </ul></li>
  <li><a href="#expected-outcomes" id="toc-expected-outcomes" class="nav-link" data-scroll-target="#expected-outcomes">Expected Outcomes</a>
  <ul class="collapse">
  <li><a href="#conceptual-understanding" id="toc-conceptual-understanding" class="nav-link" data-scroll-target="#conceptual-understanding">Conceptual Understanding</a></li>
  <li><a href="#technical-skills" id="toc-technical-skills" class="nav-link" data-scroll-target="#technical-skills">Technical Skills</a></li>
  <li><a href="#practical-readiness-for-session-4" id="toc-practical-readiness-for-session-4" class="nav-link" data-scroll-target="#practical-readiness-for-session-4">Practical Readiness for Session 4</a></li>
  </ul></li>
  <li><a href="#hands-on-notebooks" id="toc-hands-on-notebooks" class="nav-link" data-scroll-target="#hands-on-notebooks">Hands-On Notebooks</a>
  <ul class="collapse">
  <li><a href="#access-the-interactive-materials" id="toc-access-the-interactive-materials" class="nav-link" data-scroll-target="#access-the-interactive-materials">Access the Interactive Materials</a></li>
  <li><a href="#supporting-documentation" id="toc-supporting-documentation" class="nav-link" data-scroll-target="#supporting-documentation">Supporting Documentation</a></li>
  </ul></li>
  <li><a href="#troubleshooting" id="toc-troubleshooting" class="nav-link" data-scroll-target="#troubleshooting">Troubleshooting</a>
  <ul class="collapse">
  <li><a href="#common-conceptual-questions" id="toc-common-conceptual-questions" class="nav-link" data-scroll-target="#common-conceptual-questions">Common Conceptual Questions</a></li>
  <li><a href="#technical-issues" id="toc-technical-issues" class="nav-link" data-scroll-target="#technical-issues">Technical Issues</a></li>
  <li><a href="#getting-help" id="toc-getting-help" class="nav-link" data-scroll-target="#getting-help">Getting Help</a></li>
  </ul></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources">Additional Resources</a>
  <ul class="collapse">
  <li><a href="#foundational-learning" id="toc-foundational-learning" class="nav-link" data-scroll-target="#foundational-learning">Foundational Learning</a></li>
  <li><a href="#earth-observation-deep-learning" id="toc-earth-observation-deep-learning" class="nav-link" data-scroll-target="#earth-observation-deep-learning">Earth Observation Deep Learning</a></li>
  <li><a href="#philippine-context" id="toc-philippine-context" class="nav-link" data-scroll-target="#philippine-context">Philippine Context</a></li>
  </ul></li>
  <li><a href="#assessment" id="toc-assessment" class="nav-link" data-scroll-target="#assessment">Assessment</a>
  <ul class="collapse">
  <li><a href="#formative-assessment-during-session" id="toc-formative-assessment-during-session" class="nav-link" data-scroll-target="#formative-assessment-during-session">Formative Assessment (During Session)</a></li>
  <li><a href="#summative-assessment-end-of-session" id="toc-summative-assessment-end-of-session" class="nav-link" data-scroll-target="#summative-assessment-end-of-session">Summative Assessment (End of Session)</a></li>
  </ul></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps">Next Steps</a>
  <ul class="collapse">
  <li><a href="#recommended-pre-work-for-session-4" id="toc-recommended-pre-work-for-session-4" class="nav-link" data-scroll-target="#recommended-pre-work-for-session-4">Recommended Pre-Work for Session 4</a></li>
  <li><a href="#extended-learning-paths" id="toc-extended-learning-paths" class="nav-link" data-scroll-target="#extended-learning-paths">Extended Learning Paths</a></li>
  </ul></li>
  <li><a href="#quick-links" id="toc-quick-links" class="nav-link" data-scroll-target="#quick-links">Quick Links</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../day2/sessions/session1.html">Sessions</a></li><li class="breadcrumb-item"><a href="../../day2/sessions/session3.html">Session 3: Introduction to Deep Learning and CNNs</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Session 3: Introduction to Deep Learning and CNNs</h1>
<p class="subtitle lead">Neural Networks and Convolutional Architectures for Earth Observation</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Instructor</div>
    <div class="quarto-title-meta-contents">
             <p>CoPhil Advanced Training Program </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Date</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 19, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<nav class="breadcrumb" aria-label="Breadcrumb">
<a href="../../index.html">Home</a> <span class="breadcrumb-separator" aria-hidden="true">›</span> <a href="../index.html">Day 2</a> <span class="breadcrumb-separator" aria-hidden="true">›</span> <span class="breadcrumb-current">Session 3</span>
</nav>
<section id="session-3-introduction-to-deep-learning-and-cnns" class="level1 hero">
<h1>Session 3: Introduction to Deep Learning and CNNs</h1>
<section id="neural-networks-and-convolutional-architectures-for-earth-observation" class="level3">
<h3 class="anchored" data-anchor-id="neural-networks-and-convolutional-architectures-for-earth-observation">Neural Networks and Convolutional Architectures for Earth Observation</h3>
<p>Transitioning from feature engineering to feature learning</p>
</section>
</section>
<section id="session-overview" class="level2">
<h2 class="anchored" data-anchor-id="session-overview">Session Overview</h2>
<p><strong>Duration:</strong> 2.5 hours | <strong>Type:</strong> Theory + Interactive Demonstrations | <strong>Difficulty:</strong> Intermediate</p>
<hr>
<p>This pivotal session bridges traditional machine learning (Sessions 1-2) and modern deep learning approaches. You’ll understand the fundamental shift from manual feature engineering to automatic feature learning through neural networks, with specific focus on Convolutional Neural Networks (CNNs) for Earth observation applications.</p>
</section>
<section id="presentation-slides" class="level2">
<h2 class="anchored" data-anchor-id="presentation-slides">Presentation Slides</h2>
<iframe src="../presentations/session3_deep_learning.html" width="100%" height="600" style="border: 1px solid #ccc; border-radius: 4px;">
</iframe>
<hr>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Prerequisites
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>✓ Complete Sessions 1-2 (Random Forest classification)</li>
<li>✓ Understanding of classification concepts (accuracy, confusion matrix)</li>
<li>✓ Basic Python and NumPy familiarity</li>
<li>✓ Colab environment with GPU runtime enabled</li>
<li>✓ Conceptual understanding of matrix operations</li>
</ul>
</div>
</div>
<hr>
</section>
<section id="what-youll-learn" class="level2">
<h2 class="anchored" data-anchor-id="what-youll-learn">What You’ll Learn</h2>
<p>After completing this session, you will be able to:</p>
<ol type="1">
<li><strong>Understand the ML → DL Transition</strong>
<ul>
<li>Recognize when to use traditional ML vs deep learning</li>
<li>Explain the automatic feature learning paradigm</li>
<li>Identify computational requirements and trade-offs</li>
<li>Understand data requirements for deep learning success</li>
</ul></li>
<li><strong>Master Neural Network Fundamentals</strong>
<ul>
<li>Build simple perceptrons from scratch using NumPy</li>
<li>Implement activation functions (ReLU, sigmoid, softmax)</li>
<li>Understand forward propagation and backpropagation</li>
<li>Visualize decision boundaries and learning dynamics</li>
</ul></li>
<li><strong>Comprehend Convolutional Neural Networks</strong>
<ul>
<li>Explain convolution operations and their purpose</li>
<li>Understand pooling, padding, and stride concepts</li>
<li>Visualize filter responses on satellite imagery</li>
<li>Compare CNN architectures (LeNet, VGG, ResNet, U-Net)</li>
</ul></li>
<li><strong>Apply CNNs to Earth Observation Tasks</strong>
<ul>
<li>Identify appropriate CNN architectures for EO problems</li>
<li>Understand scene classification vs semantic segmentation</li>
<li>Recognize object detection and change detection approaches</li>
<li>Connect CNN capabilities to Philippine EO applications</li>
</ul></li>
<li><strong>Navigate Practical Considerations</strong>
<ul>
<li>Address data-centric AI principles for EO</li>
<li>Handle limited training data scenarios</li>
<li>Understand transfer learning and pre-trained models</li>
<li>Recognize computational constraints and optimization strategies</li>
</ul></li>
</ol>
<hr>
</section>
<section id="session-structure" class="level2">
<h2 class="anchored" data-anchor-id="session-structure">Session Structure</h2>
<section id="part-a-from-machine-learning-to-deep-learning-15-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-a-from-machine-learning-to-deep-learning-15-minutes">Part A: From Machine Learning to Deep Learning (15 minutes)</h3>
<p>Understanding the paradigm shift from manual to automatic feature learning.</p>
<div class="feature-grid">
<div class="feature-card">
<p><strong>🔧 Traditional ML (Sessions 1-2)</strong></p>
<p><strong>What you did:</strong> - Manually calculated NDVI, NDWI, NDBI - Engineered GLCM texture features - Extracted temporal statistics - Combined features thoughtfully</p>
<p><strong>Pros:</strong> Interpretable, works with small datasets <strong>Cons:</strong> Requires domain expertise, limited by imagination</p>
</div>
<div class="feature-card">
<p><strong>🧠 Deep Learning (Sessions 3-4)</strong></p>
<p><strong>What CNNs do:</strong> - Learn features automatically from raw pixels - Discover hidden patterns humans miss - Build hierarchical representations - Optimize end-to-end</p>
<p><strong>Pros:</strong> No feature engineering, state-of-the-art accuracy <strong>Cons:</strong> Needs large datasets, computationally intensive</p>
</div>
<div class="feature-card">
<p><strong>⚖️ When to Use Which?</strong></p>
<p><strong>Use Random Forest when:</strong> - Limited training data (&lt;1000 samples) - Need interpretability (DENR reports) - Have domain features (indices) - Fast prototyping needed</p>
<p><strong>Use CNNs when:</strong> - Large labeled datasets (&gt;10,000 samples) - Complex spatial patterns - Maximum accuracy required - GPU resources available</p>
</div>
<div class="feature-card">
<p><strong>🇵🇭 Philippine EO Context</strong></p>
<p><strong>PhilSA Applications:</strong> - Scene classification (land cover) - Cloud detection (Sentinel-2) - Building footprint extraction - Flood extent mapping</p>
<p><strong>Why CNNs?</strong> Handle complex tropical landscapes, monsoon cloud patterns, informal settlements</p>
</div>
</div>
<p><strong>Key Insight:</strong> <em>“In Sessions 1-2, you manually engineered features. CNNs will learn these features automatically—and discover new ones you never imagined!”</em></p>
</section>
<section id="part-b-neural-network-fundamentals-25-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-b-neural-network-fundamentals-25-minutes">Part B: Neural Network Fundamentals (25 minutes)</h3>
<p>Building intuition from the ground up using interactive Jupyter notebooks.</p>
<section id="b.1-the-perceptron---simplest-neural-unit" class="level4">
<h4 class="anchored" data-anchor-id="b.1-the-perceptron---simplest-neural-unit">B.1: The Perceptron - Simplest Neural Unit</h4>
<p>Understanding the building block of all neural networks.</p>
<p><strong>Mathematical Foundation:</strong> <span class="math display">\[
y = f\left(\sum_{i=1}^{n} w_i x_i + b\right)
\]</span></p>
<p>Where: - <span class="math inline">\(x_i\)</span>: Input features (e.g., pixel values, NDVI) - <span class="math inline">\(w_i\)</span>: Learned weights - <span class="math inline">\(b\)</span>: Bias term - <span class="math inline">\(f\)</span>: Activation function - <span class="math inline">\(y\)</span>: Output prediction</p>
<p><strong>Hands-On:</strong> Build a perceptron from scratch to classify “Water vs Non-Water” using NDWI.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple perceptron implementation</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceptron:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.random.randn(input_dim)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        linear_output <span class="op">=</span> np.dot(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.activation(linear_output)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> activation(<span class="va">self</span>, z):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> z <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span>  <span class="co"># Step function</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="b.2-activation-functions---adding-non-linearity" class="level4">
<h4 class="anchored" data-anchor-id="b.2-activation-functions---adding-non-linearity">B.2: Activation Functions - Adding Non-Linearity</h4>
<p>Why neural networks need activation functions to solve complex problems.</p>
<div class="feature-grid">
<div class="feature-card">
<p><strong>Sigmoid</strong> <span class="math display">\[\sigma(z) = \frac{1}{1 + e^{-z}}\]</span></p>
<p><strong>Range:</strong> (0, 1) <strong>Use:</strong> Binary classification output <strong>EO Example:</strong> Cloud probability</p>
</div>
<div class="feature-card">
<p><strong>ReLU (Rectified Linear Unit)</strong> <span class="math display">\[\text{ReLU}(z) = \max(0, z)\]</span></p>
<p><strong>Range:</strong> [0, ∞) <strong>Use:</strong> Hidden layers (most popular) <strong>Why:</strong> Fast, sparse activation</p>
</div>
<div class="feature-card">
<p><strong>Softmax</strong> <span class="math display">\[\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]</span></p>
<p><strong>Range:</strong> (0, 1), sums to 1 <strong>Use:</strong> Multi-class classification <strong>EO Example:</strong> Land cover classes</p>
</div>
<div class="feature-card">
<p><strong>Tanh</strong> <span class="math display">\[\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\]</span></p>
<p><strong>Range:</strong> (-1, 1) <strong>Use:</strong> When centered data needed <strong>Note:</strong> Less common than ReLU</p>
</div>
</div>
<p><strong>Interactive Demo:</strong> Visualize how each activation function transforms Sentinel-2 spectral values.</p>
</section>
<section id="b.3-multi-layer-networks---learning-complex-patterns" class="level4">
<h4 class="anchored" data-anchor-id="b.3-multi-layer-networks---learning-complex-patterns">B.3: Multi-Layer Networks - Learning Complex Patterns</h4>
<p>Stacking layers to learn hierarchical representations.</p>
<p><strong>Architecture:</strong></p>
<pre><code>Input Layer (e.g., Sentinel-2 bands)
    ↓
Hidden Layer 1 (64 neurons, ReLU)
    ↓
Hidden Layer 2 (32 neurons, ReLU)
    ↓
Output Layer (8 classes, Softmax)</code></pre>
<p><strong>What Each Layer Learns:</strong> - <strong>Layer 1:</strong> Low-level features (edges, textures) - <strong>Layer 2:</strong> Mid-level features (shapes, patterns) - <strong>Layer 3:</strong> High-level features (objects, scenes) - <strong>Output:</strong> Class probabilities</p>
<p><strong>Hands-On:</strong> Train a 2-layer network to classify Palawan land cover using spectral features (from Session 2).</p>
</section>
<section id="b.4-training-process---learning-from-data" class="level4">
<h4 class="anchored" data-anchor-id="b.4-training-process---learning-from-data">B.4: Training Process - Learning from Data</h4>
<p>Understanding gradient descent and backpropagation intuitively.</p>
<p><strong>Training Loop:</strong> 1. <strong>Forward Pass:</strong> Input → Hidden → Output → Prediction 2. <strong>Calculate Loss:</strong> Compare prediction to true label 3. <strong>Backward Pass:</strong> Compute gradients (how much each weight contributed to error) 4. <strong>Update Weights:</strong> <span class="math inline">\(w_{new} = w_{old} - \alpha \cdot \frac{\partial L}{\partial w}\)</span> 5. <strong>Repeat:</strong> Until loss converges</p>
<p><strong>Key Hyperparameters:</strong> - <strong>Learning Rate (<span class="math inline">\(\alpha\)</span>):</strong> Step size for weight updates (0.001 - 0.1) - <strong>Batch Size:</strong> Number of samples per gradient update (32, 64, 128) - <strong>Epochs:</strong> Complete passes through training data (10-100)</p>
<p><strong>Interactive Exploration:</strong> Experiment with learning rates to see overfitting vs underfitting.</p>
</section>
</section>
<section id="part-c-convolutional-neural-networks-30-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-c-convolutional-neural-networks-30-minutes">Part C: Convolutional Neural Networks (30 minutes)</h3>
<p>Deep dive into the architecture that revolutionized computer vision and Earth observation.</p>
<section id="c.1-why-cnns-for-images" class="level4">
<h4 class="anchored" data-anchor-id="c.1-why-cnns-for-images">C.1: Why CNNs for Images?</h4>
<p><strong>Problem with Regular Neural Networks:</strong> - A 10m Sentinel-2 image chip (256×256×10 bands) = 655,360 parameters just for first layer! - No spatial awareness (treats nearby pixels same as distant ones) - Computationally infeasible</p>
<p><strong>CNN Solutions:</strong> - <strong>Local Connectivity:</strong> Each neuron connects to small spatial region - <strong>Parameter Sharing:</strong> Same filter applied across entire image - <strong>Translation Invariance:</strong> Detect features anywhere in image</p>
<p><strong>Result:</strong> Millions fewer parameters, spatially-aware learning!</p>
</section>
<section id="c.2-convolution-operation---the-heart-of-cnns" class="level4">
<h4 class="anchored" data-anchor-id="c.2-convolution-operation---the-heart-of-cnns">C.2: Convolution Operation - The Heart of CNNs</h4>
<p>Understanding how convolutions extract features from satellite imagery.</p>
<p><strong>Mathematical Definition:</strong> <span class="math display">\[
(I * K)(i,j) = \sum_{m}\sum_{n} I(i+m, j+n) \cdot K(m,n)
\]</span></p>
<p>Where: - <span class="math inline">\(I\)</span>: Input image (e.g., Sentinel-2 NIR band) - <span class="math inline">\(K\)</span>: Filter/kernel (e.g., 3×3 edge detector) - <span class="math inline">\(*\)</span>: Convolution operator</p>
<p><strong>Visual Example:</strong></p>
<pre><code>Sentinel-2 NIR Image (5×5)    Edge Detection Filter (3×3)
┌─────────────────┐           ┌─────────┐
│ 120 115 118 122 │           │ -1  -1  -1 │
│ 118 245 242 125 │     *     │  0   0   0 │
│ 119 248 244 121 │           │  1   1   1 │
│ 121 116 119 123 │           └─────────┘
└─────────────────┘
                ↓
    Feature Map (detects water edges)</code></pre>
<p><strong>Hands-On:</strong> - Apply manual convolutions to Sentinel-2 patches - Visualize classic filters (edge detection, blur, sharpen) - See how filters respond to forests, water, urban areas</p>
</section>
<section id="c.3-cnn-building-blocks" class="level4">
<h4 class="anchored" data-anchor-id="c.3-cnn-building-blocks">C.3: CNN Building Blocks</h4>
<p><strong>1. Convolutional Layer</strong> - Applies multiple filters to input - Each filter learns to detect different feature - Output: Feature maps (activations)</p>
<p><strong>Parameters:</strong> - <code>filters</code>: Number of filters (32, 64, 128…) - <code>kernel_size</code>: Filter dimensions (3×3, 5×5) - <code>stride</code>: Step size for filter movement (usually 1) - <code>padding</code>: Border handling (‘same’ or ‘valid’)</p>
<p><strong>2. Pooling Layer</strong> - Reduces spatial dimensions (downsampling) - Provides translation invariance - Most common: Max pooling</p>
<p><strong>Max Pooling (2×2):</strong></p>
<pre><code>Input (4×4)                Output (2×2)
┌────────────┐            ┌──────┐
│ 1  3  2  4 │            │ 3  4 │
│ 2  3  1  2 │    →       │ 7  9 │
│ 5  7  8  9 │            └──────┘
│ 1  2  3  4 │
└────────────┘
Takes maximum in each 2×2 window</code></pre>
<p><strong>3. Fully Connected Layer</strong> - Traditional neural network layer - Connects all features to output classes - Usually at end of network</p>
<p><strong>4. Dropout Layer</strong> - Randomly deactivates neurons during training - Prevents overfitting - Common rate: 0.3-0.5</p>
</section>
<section id="c.4-classic-cnn-architectures" class="level4">
<h4 class="anchored" data-anchor-id="c.4-classic-cnn-architectures">C.4: Classic CNN Architectures</h4>
<p>Understanding architectures used in EO applications.</p>
<div class="feature-grid">
<div class="feature-card">
<p><strong>LeNet-5 (1998)</strong></p>
<p><strong>Structure:</strong> - Conv → Pool → Conv → Pool → FC - 60K parameters - Original: Handwritten digits</p>
<p><strong>EO Use:</strong> - Simple scene classification - Educational examples - Quick prototypes</p>
</div>
<div class="feature-card">
<p><strong>VGG-16 (2014)</strong></p>
<p><strong>Structure:</strong> - 13 Conv layers + 3 FC - 138M parameters - Small 3×3 filters stacked</p>
<p><strong>EO Use:</strong> - Scene classification - Pre-trained on ImageNet - Transfer learning baseline</p>
</div>
<div class="feature-card">
<p><strong>ResNet-50 (2015)</strong></p>
<p><strong>Innovation:</strong> - Skip connections (residual blocks) - Solves vanishing gradient - 50 layers deep</p>
<p><strong>EO Use:</strong> - High-accuracy classification - Feature extraction - PhilSA scene classifier</p>
</div>
<div class="feature-card">
<p><strong>U-Net (2015)</strong></p>
<p><strong>Innovation:</strong> - Encoder-decoder architecture - Skip connections preserve detail - Outputs same-size segmentation</p>
<p><strong>EO Use:</strong> - Semantic segmentation - Flood mapping - Building extraction - <strong>Session 4 focus!</strong></p>
</div>
</div>
<p><strong>Interactive Visualization:</strong> Explore how different architectures process Sentinel-2 imagery.</p>
</section>
</section>
<section id="part-d-cnns-for-earth-observation-25-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-d-cnns-for-earth-observation-25-minutes">Part D: CNNs for Earth Observation (25 minutes)</h3>
<p>Connecting CNN capabilities to Philippine EO operational needs.</p>
<section id="d.1-scene-classification" class="level4">
<h4 class="anchored" data-anchor-id="d.1-scene-classification">D.1: Scene Classification</h4>
<p><strong>Task:</strong> Assign single label to entire image patch</p>
<p><strong>Architecture:</strong> ResNet, VGG, EfficientNet (classification head)</p>
<p><strong>Philippine Applications:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th>Application</th>
<th>Classes</th>
<th>Dataset</th>
<th>Stakeholder</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Land Cover</strong></td>
<td>Forest, Urban, Agriculture, Water, Bare</td>
<td>PhilSA Sentinel-2</td>
<td>DENR, LGUs</td>
</tr>
<tr class="even">
<td><strong>Cloud Detection</strong></td>
<td>Clear, Thin Cloud, Thick Cloud, Shadow</td>
<td>Sentinel-2 Level-1C</td>
<td>PhilSA (preprocessing)</td>
</tr>
<tr class="odd">
<td><strong>Rice Field Stage</strong></td>
<td>Land Prep, Transplanting, Vegetative, Harvest</td>
<td>PlanetScope + field data</td>
<td>DA, PhilRice</td>
</tr>
<tr class="even">
<td><strong>Disaster Assessment</strong></td>
<td>Damaged, Undamaged, Debris</td>
<td>Drones + Sentinel-2</td>
<td>NDRRMC, PAGASA</td>
</tr>
</tbody>
</table>
<p><strong>Data Requirements:</strong> - Typical: 1,000-10,000 labeled image chips per class - PhilSA strategy: Start with 500/class, augment with rotations/flips</p>
<p><strong>Hands-On (Session 4):</strong> Build ResNet-based classifier for Palawan land cover</p>
</section>
<section id="d.2-semantic-segmentation" class="level4">
<h4 class="anchored" data-anchor-id="d.2-semantic-segmentation">D.2: Semantic Segmentation</h4>
<p><strong>Task:</strong> Classify every pixel in image (pixel-wise labels)</p>
<p><strong>Architecture:</strong> U-Net, DeepLabv3+, SegNet</p>
<p><strong>Philippine Applications:</strong></p>
<div class="feature-grid">
<div class="feature-card">
<p><strong>🌊 Flood Mapping</strong></p>
<p><strong>Challenge:</strong> Rapid post-disaster assessment</p>
<p><strong>CNN Solution:</strong> - Input: Sentinel-1 SAR (pre + post event) - Output: Flooded/Non-flooded pixel map - Architecture: U-Net</p>
<p><strong>PhilSA Use Case:</strong> Pampanga flood 2023 - 6-hour processing time (vs 2 days manual)</p>
</div>
<div class="feature-card">
<p><strong>🏘️ Informal Settlements</strong></p>
<p><strong>Challenge:</strong> Map slums for disaster planning</p>
<p><strong>CNN Solution:</strong> - Input: High-res imagery (PlanetScope, drones) - Output: Building footprints - Architecture: U-Net + post-processing</p>
<p><strong>NEDA Application:</strong> Metro Manila vulnerability assessment</p>
</div>
<div class="feature-card">
<p><strong>🌳 Forest Degradation</strong></p>
<p><strong>Challenge:</strong> Detect selective logging</p>
<p><strong>CNN Solution:</strong> - Input: Multi-temporal Sentinel-2 - Output: Degraded forest pixels - Architecture: U-Net + LSTM</p>
<p><strong>DENR Use:</strong> Protected area monitoring</p>
</div>
<div class="feature-card">
<p><strong>⛏️ Mining Activity</strong></p>
<p><strong>Challenge:</strong> Illegal mining detection</p>
<p><strong>CNN Solution:</strong> - Input: Sentinel-2 + Sentinel-1 - Output: Mining site polygons - Post-process: Vectorize</p>
<p><strong>MGB Application:</strong> Permit compliance checking</p>
</div>
</div>
<p><strong>Data Requirements:</strong> - Annotation intensive: Need pixel-level labels - Typical: 100-500 labeled images (256×256 chips) - Tools: QGIS, LabelMe, CVAT</p>
<p><strong>Hands-On (Session 4):</strong> Implement U-Net for flood mapping in Central Luzon</p>
</section>
<section id="d.3-object-detection" class="level4">
<h4 class="anchored" data-anchor-id="d.3-object-detection">D.3: Object Detection</h4>
<p><strong>Task:</strong> Find and localize objects with bounding boxes</p>
<p><strong>Architecture:</strong> Faster R-CNN, YOLO, RetinaNet</p>
<p><strong>Philippine Applications:</strong> - <strong>Ship Detection:</strong> Illegal fishing monitoring (Sentinel-1) - <strong>Building Detection:</strong> Infrastructure mapping (high-res) - <strong>Tree Counting:</strong> Forest inventory (drone imagery) - <strong>Vehicle Detection:</strong> Traffic monitoring (PlanetScope)</p>
<p><strong>Data Format:</strong> Bounding boxes + class labels (COCO, PASCAL VOC formats)</p>
<p><strong>Computational Note:</strong> More complex than classification, requires anchor boxes and region proposals</p>
</section>
<section id="d.4-change-detection" class="level4">
<h4 class="anchored" data-anchor-id="d.4-change-detection">D.4: Change Detection</h4>
<p><strong>Task:</strong> Identify what changed between two time points</p>
<p><strong>CNN Approaches:</strong></p>
<ol type="1">
<li><strong>Siamese Networks:</strong> Compare two images with shared weights</li>
<li><strong>Early Fusion:</strong> Stack temporal images as input channels</li>
<li><strong>Late Fusion:</strong> Separate encoders + change decoder</li>
</ol>
<p><strong>Philippine Applications:</strong> - Deforestation (Palawan, Mindanao) - Urban expansion (Metro Manila, Cebu) - Post-disaster damage (typhoon impacts) - Agricultural change (conversion detection)</p>
<p><strong>Challenge:</strong> Need paired labeled change data (before + after + change mask)</p>
</section>
</section>
<section id="part-e-practical-considerations-for-eo-deep-learning-15-minutes" class="level3">
<h3 class="anchored" data-anchor-id="part-e-practical-considerations-for-eo-deep-learning-15-minutes">Part E: Practical Considerations for EO Deep Learning (15 minutes)</h3>
<p>Real-world challenges and solutions for Philippine EO practitioners.</p>
<section id="e.1-the-data-challenge" class="level4">
<h4 class="anchored" data-anchor-id="e.1-the-data-challenge">E.1: The Data Challenge</h4>
<p><strong>How Much Data Do You Need?</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 36%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Model Complexity</th>
<th>Typical Requirement</th>
<th>Philippine Reality</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple CNN (5 layers)</td>
<td>5,000-10,000 samples</td>
<td>✓ Achievable</td>
</tr>
<tr class="even">
<td>ResNet-50 (from scratch)</td>
<td>100,000+ samples</td>
<td>✗ Rarely available</td>
</tr>
<tr class="odd">
<td>ResNet-50 (fine-tuned)</td>
<td>1,000-5,000 samples</td>
<td>✓ Achievable with augmentation</td>
</tr>
<tr class="even">
<td>U-Net (segmentation)</td>
<td>100-500 images</td>
<td>✓ Achievable but labor-intensive</td>
</tr>
</tbody>
</table>
<p><strong>Data-Centric AI Principles:</strong> 1. <strong>Quality &gt; Quantity:</strong> 500 clean labels &gt;&gt; 5,000 noisy labels 2. <strong>Representative Sampling:</strong> Cover all Philippine ecosystems (lowland, upland, coastal) 3. <strong>Class Balance:</strong> Equal samples per class (or weighted loss) 4. <strong>Validation Split:</strong> Hold out 20% for unbiased evaluation</p>
<p><strong>Philippine Data Sources:</strong> - PhilSA Space+ Data Dashboard (satellite imagery) - NAMRIA Geoportal (reference maps) - DOST-ASTI DATOS (disaster imagery) - LiDAR Portal (elevation + canopy) - Field campaigns (GPS + photos)</p>
</section>
<section id="e.2-transfer-learning---training-on-limited-data" class="level4">
<h4 class="anchored" data-anchor-id="e.2-transfer-learning---training-on-limited-data">E.2: Transfer Learning - Training on Limited Data</h4>
<p><strong>Strategy:</strong> Start with model pre-trained on large dataset (ImageNet), fine-tune on Philippine data</p>
<p><strong>Benefits:</strong> - Need 10× less data - Train 5× faster - Better accuracy with small datasets</p>
<p><strong>Implementation:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained ResNet</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> ResNet50(weights<span class="op">=</span><span class="st">'imagenet'</span>, include_top<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Freeze early layers (keep learned features)</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layer <span class="kw">in</span> base_model.layers[:<span class="dv">100</span>]:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    layer.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add custom classification head</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> GlobalAveragePooling2D()(base_model.output)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> Dense(<span class="dv">8</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)(x)  <span class="co"># 8 Palawan classes</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>base_model.<span class="bu">input</span>, outputs<span class="op">=</span>output)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>When to Use:</strong> - Limited training data (&lt;5,000 samples) - Similar task to pre-training (natural images) - Need quick results</p>
<p><strong>Caution:</strong> ImageNet has RGB images. Sentinel-2 has 10+ bands. Adaptation strategies needed (Session 4).</p>
</section>
<section id="e.3-data-augmentation---artificially-expanding-training-data" class="level4">
<h4 class="anchored" data-anchor-id="e.3-data-augmentation---artificially-expanding-training-data">E.3: Data Augmentation - Artificially Expanding Training Data</h4>
<p><strong>Geometric Transformations:</strong> - <strong>Rotation:</strong> 90°, 180°, 270° (satellites view from any angle) - <strong>Horizontal/Vertical Flip:</strong> Valid for overhead imagery - <strong>Zoom/Scale:</strong> Simulate different resolutions - <strong>Translation:</strong> Small shifts</p>
<p><strong>Spectral Augmentations:</strong> - <strong>Brightness/Contrast:</strong> Simulate atmospheric conditions - <strong>Gaussian Noise:</strong> Simulate sensor noise - <strong>Band Dropout:</strong> Improve robustness</p>
<p><strong>Philippine Context:</strong> - ✓ Use rotation/flips for land cover (no preferential orientation) - ✗ Avoid rotation for infrastructure (roads have direction) - ✓ Augment brightness for cloud variations</p>
<p><strong>Implementation (Session 4):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>augmentation <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    rotation_range<span class="op">=</span><span class="dv">90</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    horizontal_flip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    vertical_flip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    brightness_range<span class="op">=</span>[<span class="fl">0.8</span>, <span class="fl">1.2</span>],</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    zoom_range<span class="op">=</span><span class="fl">0.1</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="e.4-computational-requirements" class="level4">
<h4 class="anchored" data-anchor-id="e.4-computational-requirements">E.4: Computational Requirements</h4>
<p><strong>Training Resources:</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Training Time*</th>
<th>GPU Memory</th>
<th>Cost (Colab Pro)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Simple CNN</td>
<td>30 min</td>
<td>4 GB</td>
<td>Free tier OK</td>
</tr>
<tr class="even">
<td>ResNet-50 (fine-tune)</td>
<td>2-4 hours</td>
<td>8 GB</td>
<td>Free tier OK</td>
</tr>
<tr class="odd">
<td>U-Net (segmentation)</td>
<td>4-8 hours</td>
<td>12 GB</td>
<td>Pro needed</td>
</tr>
<tr class="even">
<td>ResNet-50 (from scratch)</td>
<td>24+ hours</td>
<td>16 GB</td>
<td>Pro+ needed</td>
</tr>
</tbody>
</table>
<p>*1,000 training images, 50 epochs, V100 GPU</p>
<p><strong>Philippine Context:</strong> - PhilSA uses on-premise GPU servers (8× NVIDIA A100) - Universities: Limited GPU access (submit jobs) - Practitioners: Google Colab Pro ($10/month) recommended</p>
<p><strong>Optimization Strategies (Session 4):</strong> - Use mixed precision training (FP16) - Reduce batch size if memory limited - Train on smaller image chips (128×128 instead of 256×256) - Use gradient checkpointing for large models</p>
</section>
<section id="e.5-model-interpretability---understanding-cnn-decisions" class="level4">
<h4 class="anchored" data-anchor-id="e.5-model-interpretability---understanding-cnn-decisions">E.5: Model Interpretability - Understanding CNN Decisions</h4>
<p><strong>Why It Matters:</strong> - DENR reports need explanations (“Why was this classified as deforested?”) - Debugging poor performance - Building stakeholder trust</p>
<p><strong>Techniques (Session 4):</strong> 1. <strong>Activation Visualization:</strong> See what filters learned 2. <strong>Saliency Maps:</strong> Which pixels influenced decision? 3. <strong>Class Activation Maps (CAM):</strong> Highlight relevant regions 4. <strong>Filter Visualization:</strong> What patterns do filters detect?</p>
<p><strong>Philippine Application Example:</strong> - <strong>Question:</strong> Why did model classify mangroves as agriculture? - <strong>CAM Analysis:</strong> Model focused on water proximity, not canopy structure - <strong>Solution:</strong> Add texture features or more mangrove training samples</p>
<hr>
</section>
</section>
</section>
<section id="key-concepts" class="level2">
<h2 class="anchored" data-anchor-id="key-concepts">Key Concepts</h2>
<section id="automatic-feature-learning" class="level3">
<h3 class="anchored" data-anchor-id="automatic-feature-learning">Automatic Feature Learning</h3>
<p><strong>What is it?</strong> CNNs learn optimal features directly from raw pixel data, eliminating manual feature engineering.</p>
<p><strong>How it works:</strong> - <strong>Layer 1:</strong> Learns edges (horizontal, vertical, diagonal) - like Sobel filters you created manually - <strong>Layer 2:</strong> Combines edges into textures and simple shapes - <strong>Layer 3:</strong> Combines shapes into complex patterns (canopy structure, urban grid) - <strong>Layer 4+:</strong> High-level semantic features (forest type, settlement pattern)</p>
<p><strong>Comparison to Session 2:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 50%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Random Forest (Session 2)</th>
<th>CNN (Session 3-4)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Features</strong></td>
<td>Manual (NDVI, GLCM)</td>
<td>Learned automatically</td>
</tr>
<tr class="even">
<td><strong>Spatial Context</strong></td>
<td>Limited (within feature)</td>
<td>Fully exploited (receptive field)</td>
</tr>
<tr class="odd">
<td><strong>Data Needed</strong></td>
<td>500-1,000 samples</td>
<td>5,000-10,000 samples</td>
</tr>
<tr class="even">
<td><strong>Training Time</strong></td>
<td>Minutes</td>
<td>Hours</td>
</tr>
<tr class="odd">
<td><strong>Accuracy</strong></td>
<td>80-85% typical</td>
<td>90-95% possible</td>
</tr>
</tbody>
</table>
</section>
<section id="receptive-field" class="level3">
<h3 class="anchored" data-anchor-id="receptive-field">Receptive Field</h3>
<p><strong>Definition:</strong> The region of input image that influences a particular neuron’s activation.</p>
<p><strong>Example:</strong> - A neuron in Layer 1 sees a 3×3 pixel region (30m × 30m for Sentinel-2) - A neuron in Layer 3 sees a 15×15 pixel region (150m × 150m) - Output neuron “sees” entire image chip</p>
<p><strong>Why it matters:</strong> - Small objects (buildings): Need fewer layers - Large objects (agricultural fields): Need deeper networks - Contextual classification: Large receptive field captures neighborhood</p>
<p><strong>Philippine Example:</strong> Classifying a pixel as “mangrove” requires seeing water proximity (large receptive field) AND canopy texture (small receptive field). Multi-scale processing essential!</p>
</section>
<section id="translation-invariance" class="level3">
<h3 class="anchored" data-anchor-id="translation-invariance">Translation Invariance</h3>
<p><strong>What is it?</strong> CNN can recognize patterns regardless of position in image.</p>
<p><strong>How achieved:</strong> 1. <strong>Parameter sharing:</strong> Same filter applied everywhere 2. <strong>Pooling:</strong> Abstracts exact position</p>
<p><strong>EO Benefit:</strong> Forest is forest whether in top-left or bottom-right of image. Train once, apply anywhere in Philippines!</p>
<p><strong>Contrast with Position:</strong> For some tasks, position DOES matter (e.g., urban always near coasts in Philippines). Advanced architectures can encode position.</p>
</section>
<section id="gradient-descent-and-backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent-and-backpropagation">Gradient Descent and Backpropagation</h3>
<p><strong>Intuitive Explanation:</strong></p>
<p>Imagine hiking down a foggy mountain (error surface) to reach valley (minimum loss): - <strong>Gradient:</strong> Direction of steepest descent - <strong>Learning rate:</strong> Step size - <strong>Backpropagation:</strong> Efficiently calculates gradients for all weights</p>
<p><strong>Training Process:</strong> 1. Forward pass: Image → Predictions 2. Calculate loss: How wrong were predictions? 3. Backward pass: Compute ∂Loss/∂Weight for every parameter 4. Update weights: Move “downhill” toward better performance</p>
<p><strong>Common Issues:</strong> - <strong>Learning rate too high:</strong> Jump over minimum (unstable) - <strong>Learning rate too low:</strong> Painfully slow convergence - <strong>Local minima:</strong> Stuck in suboptimal solution (less common with large networks)</p>
<p><strong>Session 4:</strong> Implement Adam optimizer (adaptive learning rates)</p>
<hr>
</section>
</section>
<section id="interactive-demonstrations" class="level2">
<h2 class="anchored" data-anchor-id="interactive-demonstrations">Interactive Demonstrations</h2>
<section id="demo-1-perceptron-playground" class="level3">
<h3 class="anchored" data-anchor-id="demo-1-perceptron-playground">Demo 1: Perceptron Playground</h3>
<p><strong>Objective:</strong> Build intuition for how perceptrons learn decision boundaries</p>
<p><strong>Activity:</strong> 1. Load 2D dataset (NDVI vs NDWI for water classification) 2. Initialize random weights 3. Visualize decision boundary 4. Update weights iteratively 5. Watch boundary align with data</p>
<p><strong>Notebook:</strong> <code>session3_theory_STUDENT.ipynb</code> (Part 1)</p>
<p><strong>Expected Outcome:</strong> Understand that neural networks find separating hyperplanes through gradient descent.</p>
</section>
<section id="demo-2-activation-function-gallery" class="level3">
<h3 class="anchored" data-anchor-id="demo-2-activation-function-gallery">Demo 2: Activation Function Gallery</h3>
<p><strong>Objective:</strong> Visualize how different activation functions transform data</p>
<p><strong>Activity:</strong> 1. Plot sigmoid, ReLU, tanh, Leaky ReLU 2. Apply to Sentinel-2 reflectance values 3. Compare output distributions 4. See why ReLU is most popular</p>
<p><strong>Notebook:</strong> <code>session3_theory_STUDENT.ipynb</code> (Part 2)</p>
<p><strong>Key Insight:</strong> ReLU is simple, fast, and sparse (many zeros = efficient).</p>
</section>
<section id="demo-3-manual-convolution-on-sentinel-2" class="level3">
<h3 class="anchored" data-anchor-id="demo-3-manual-convolution-on-sentinel-2">Demo 3: Manual Convolution on Sentinel-2</h3>
<p><strong>Objective:</strong> Understand convolution as a sliding filter operation</p>
<p><strong>Activity:</strong> 1. Load Sentinel-2 NIR band (Palawan forest patch) 2. Define edge detection filter (3×3 Sobel) 3. Manually compute convolution (NumPy) 4. Visualize feature map 5. Try different filters (blur, sharpen, Gaussian)</p>
<p><strong>Notebook:</strong> <code>session3_cnn_operations_STUDENT.ipynb</code> (Part 1)</p>
<p><strong>Aha Moment:</strong> “Edge detection filter highlights forest boundaries—exactly what CNN learns automatically!”</p>
</section>
<section id="demo-4-pooling-demonstration" class="level3">
<h3 class="anchored" data-anchor-id="demo-4-pooling-demonstration">Demo 4: Pooling Demonstration</h3>
<p><strong>Objective:</strong> Understand downsampling and translation invariance</p>
<p><strong>Activity:</strong> 1. Load Sentinel-2 image chip (256×256) 2. Apply max pooling (2×2, stride 2) 3. Compare original vs pooled (128×128) 4. Shift image by 1 pixel, repeat 5. See that pooled output is nearly identical (translation invariance)</p>
<p><strong>Notebook:</strong> <code>session3_cnn_operations_STUDENT.ipynb</code> (Part 3)</p>
<p><strong>Takeaway:</strong> Pooling reduces dimensionality while preserving important features.</p>
</section>
<section id="demo-5-architecture-exploration" class="level3">
<h3 class="anchored" data-anchor-id="demo-5-architecture-exploration">Demo 5: Architecture Exploration</h3>
<p><strong>Objective:</strong> Compare CNN architectures visually</p>
<p><strong>Activity:</strong> 1. Visualize LeNet-5, VGG-16, ResNet-50, U-Net architectures 2. Count parameters for each 3. Trace receptive field growth 4. Discuss trade-offs (accuracy vs speed)</p>
<p><strong>Notebook:</strong> <code>session3_cnn_operations_STUDENT.ipynb</code> (Part 4)</p>
<p><strong>Connection to Session 4:</strong> Choose architecture based on task (classification → ResNet, segmentation → U-Net).</p>
<hr>
</section>
</section>
<section id="philippine-eo-applications" class="level2">
<h2 class="anchored" data-anchor-id="philippine-eo-applications">Philippine EO Applications</h2>
<section id="philsa-space-dashboard" class="level3">
<h3 class="anchored" data-anchor-id="philsa-space-dashboard">PhilSA Space+ Dashboard</h3>
<p><strong>Current CNN Applications:</strong></p>
<ol type="1">
<li><strong>Automated Cloud Masking</strong>
<ul>
<li><strong>Model:</strong> U-Net trained on 5,000 Sentinel-2 scenes</li>
<li><strong>Performance:</strong> 95% accuracy, 2 min per scene</li>
<li><strong>Impact:</strong> Enables rapid mosaic generation</li>
</ul></li>
<li><strong>Land Cover Classification (National)</strong>
<ul>
<li><strong>Model:</strong> ResNet-50 fine-tuned on Philippine landscape</li>
<li><strong>Classes:</strong> 10 (following FAO LCCS)</li>
<li><strong>Coverage:</strong> Entire Philippines, quarterly updates</li>
<li><strong>Users:</strong> DENR, DAR, NEDA</li>
</ul></li>
<li><strong>Disaster Rapid Mapping</strong>
<ul>
<li><strong>Flood Detection:</strong> Sentinel-1 + U-Net → 6-hour response</li>
<li><strong>Damage Assessment:</strong> High-res + object detection → building damage maps</li>
<li><strong>Integration:</strong> NDRRMC operations dashboard</li>
</ul></li>
</ol>
</section>
<section id="denr-forest-monitoring" class="level3">
<h3 class="anchored" data-anchor-id="denr-forest-monitoring">DENR Forest Monitoring</h3>
<p><strong>CNN Use Cases:</strong></p>
<ul>
<li><strong>Protected Area Surveillance:</strong> Monthly Sentinel-2 analysis (ResNet classifier)</li>
<li><strong>Illegal Logging Detection:</strong> Change detection CNN on multi-temporal stacks</li>
<li><strong>Biodiversity Hotspot Mapping:</strong> Fine-grained forest type classification</li>
<li><strong>REDD+ MRV:</strong> Automated forest cover change reporting</li>
</ul>
<p><strong>Data Pipeline:</strong></p>
<pre><code>Sentinel-2 (PhilSA) → Preprocessing (cloud mask) →
CNN Classification → Change Detection →
Alert Generation → Field Validation</code></pre>
</section>
<section id="lgu-applications-session-4-focus" class="level3">
<h3 class="anchored" data-anchor-id="lgu-applications-session-4-focus">LGU Applications (Session 4 Focus)</h3>
<p><strong>Accessible CNN Tools for Local Governments:</strong></p>
<ol type="1">
<li><strong>ASTI SkAI-Pinas:</strong> Pre-trained models for common PH tasks</li>
<li><strong>Google Earth Engine:</strong> CNN inference on cloud platform</li>
<li><strong>Colab Notebooks:</strong> Low-cost GPU training (this training!)</li>
</ol>
<p><strong>Example Workflow (Session 4):</strong> - LGU staff collects 500 training labels (Palawan land cover) - Fine-tunes ResNet-50 using Session 4 notebook - Deploys model for quarterly monitoring - Integrates into local land use planning</p>
<hr>
</section>
</section>
<section id="expected-outcomes" class="level2">
<h2 class="anchored" data-anchor-id="expected-outcomes">Expected Outcomes</h2>
<section id="conceptual-understanding" class="level3">
<h3 class="anchored" data-anchor-id="conceptual-understanding">Conceptual Understanding</h3>
<p>By the end of Session 3, you should be able to:</p>
<p>✅ <strong>Explain to a colleague</strong> why CNNs are better than Random Forest for complex spatial patterns ✅ <strong>Sketch</strong> a simple CNN architecture and label components (Conv, Pool, FC) ✅ <strong>Describe</strong> what happens during forward and backward propagation ✅ <strong>Identify</strong> appropriate architectures for classification vs segmentation ✅ <strong>Discuss</strong> data requirements and computational constraints</p>
</section>
<section id="technical-skills" class="level3">
<h3 class="anchored" data-anchor-id="technical-skills">Technical Skills</h3>
<p>✅ <strong>Build</strong> a simple perceptron from scratch using NumPy ✅ <strong>Implement</strong> activation functions and visualize their behavior ✅ <strong>Perform</strong> manual convolution on satellite imagery ✅ <strong>Apply</strong> classic edge detection filters (Sobel, Gaussian) ✅ <strong>Visualize</strong> feature maps and pooling operations</p>
</section>
<section id="practical-readiness-for-session-4" class="level3">
<h3 class="anchored" data-anchor-id="practical-readiness-for-session-4">Practical Readiness for Session 4</h3>
<p>✅ <strong>Understand</strong> why we’ll use TensorFlow/Keras (vs building from scratch) ✅ <strong>Anticipate</strong> challenges with Sentinel-2 multi-band data ✅ <strong>Recognize</strong> data preparation needs (chips, labels, augmentation) ✅ <strong>Set expectations</strong> for training time and resource requirements</p>
<hr>
</section>
</section>
<section id="hands-on-notebooks" class="level2">
<h2 class="anchored" data-anchor-id="hands-on-notebooks">Hands-On Notebooks</h2>
<section id="access-the-interactive-materials" class="level3">
<h3 class="anchored" data-anchor-id="access-the-interactive-materials">Access the Interactive Materials</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>📓 Jupyter Notebooks (Theory + Interactive)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Two comprehensive notebooks guide you through neural network and CNN fundamentals:</p>
<p><strong>Notebook 1: Neural Network Theory</strong> <a href="../../day2/notebooks/session3_theory_interactive.html"><code>session3_theory_interactive.ipynb</code></a></p>
<p><strong>Contents:</strong> - Build perceptron from scratch - Implement activation functions - Train 2-layer network on spectral data - Explore learning rate effects - Visualize decision boundaries</p>
<p><strong>Duration:</strong> 45 minutes</p>
<hr>
<p><strong>Notebook 2: CNN Operations</strong> <a href="../../day2/notebooks/session3_theory_interactive.html"><code>session3_theory_interactive.ipynb</code></a></p>
<p><strong>Contents:</strong> - Manual convolution on Sentinel-2 imagery - Edge detection filters (Sobel, Gaussian, Laplacian) - Max pooling demonstration - Architecture comparison (LeNet, VGG, ResNet, U-Net) - Feature map visualization</p>
<p><strong>Duration:</strong> 55 minutes</p>
<hr>
<p><strong>Google Colab:</strong> <a href="https://colab.research.google.com/github/DimitrisKasabalis/EO_trainning/blob/main/course_site/day2/notebooks/session3_theory_interactive.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open Theory Notebook"></a> <a href="https://colab.research.google.com/github/DimitrisKasabalis/EO_trainning/blob/main/course_site/day2/notebooks/session3_theory_interactive.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" class="img-fluid" alt="Open CNN Notebook"></a></p>
</div>
</div>
</section>
<section id="supporting-documentation" class="level3">
<h3 class="anchored" data-anchor-id="supporting-documentation">Supporting Documentation</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>📚 Reference Materials
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>CNN Architectures Deep Dive:</strong> <a href="../../resources/cheatsheets.html"><code>Cheatsheets &amp; References</code></a></p>
<p>Detailed explanations of LeNet-5, VGG-16, ResNet-50, U-Net, and modern variants (EfficientNet, Vision Transformers). Includes architecture diagrams, parameter counts, and EO applications.</p>
<hr>
<p><strong>EO Applications Guide:</strong> <a href="../../resources/glossary.html"><code>Glossary &amp; EO Resources</code></a></p>
<p>Comprehensive guide to CNN applications in Earth observation: - Scene classification examples - Semantic segmentation workflows - Object detection case studies - Change detection methods - Philippine-specific use cases</p>
<hr>
<p><strong>Session Overview:</strong> <a href="../../day2/index.html"><code>Day 2 Overview</code></a></p>
<p>Quick reference with session objectives, structure, and prerequisites.</p>
</div>
</div>
<hr>
</section>
</section>
<section id="troubleshooting" class="level2">
<h2 class="anchored" data-anchor-id="troubleshooting">Troubleshooting</h2>
<section id="common-conceptual-questions" class="level3">
<h3 class="anchored" data-anchor-id="common-conceptual-questions">Common Conceptual Questions</h3>
<p><strong>“Why does CNN need so much more data than Random Forest?”</strong></p>
<p>Random Forest learns from features YOU engineered (NDVI, GLCM). It needs to learn relationships between ~10-20 features.</p>
<p>CNNs learn from raw pixels (~10,000 per image chip). It needs to learn what features to extract PLUS how to classify. Much harder optimization problem = more data needed.</p>
<p><strong>Mitigation:</strong> Transfer learning (Session 4) dramatically reduces data needs.</p>
<hr>
<p><strong>“Can I use CNNs with small datasets (&lt;500 samples)?”</strong></p>
<p>Technically yes, but results will be poor if training from scratch.</p>
<p><strong>Better approach:</strong> 1. Use pre-trained models (ImageNet weights) 2. Aggressive data augmentation 3. Use simpler architectures (fewer layers) 4. Consider traditional ML if data is truly limited</p>
<hr>
<p><strong>“Why are my manual convolutions so slow in the notebook?”</strong></p>
<p>NumPy convolutions (for learning) are intentionally simple. Production CNNs use highly optimized libraries (cuDNN) on GPUs—1000× faster!</p>
<p>Session 4 will use TensorFlow/PyTorch with GPU acceleration.</p>
<hr>
<p><strong>“How do I know which architecture to use?”</strong></p>
<p><strong>Simple decision tree:</strong> - <strong>Classification task</strong> (one label per image) → ResNet, EfficientNet - <strong>Segmentation task</strong> (label every pixel) → U-Net, DeepLabv3+ - <strong>Object detection</strong> (find + localize) → YOLO, Faster R-CNN - <strong>Limited data</strong> → Simpler architecture + transfer learning - <strong>Real-time inference needed</strong> → MobileNet, EfficientNet-Lite</p>
<p>Session 4 will implement ResNet (classification) and U-Net (segmentation).</p>
<hr>
</section>
<section id="technical-issues" class="level3">
<h3 class="anchored" data-anchor-id="technical-issues">Technical Issues</h3>
<p><strong>“Notebook cells won’t execute in Colab”</strong></p>
<ol type="1">
<li>Check GPU is enabled: Runtime → Change runtime type → GPU</li>
<li>Restart runtime: Runtime → Restart runtime</li>
<li>Verify installations: <code>!pip list | grep numpy</code></li>
</ol>
<hr>
<p><strong>“Out of memory error when running convolutions”</strong></p>
<ul>
<li>Use smaller image chips (128×128 instead of 256×256)</li>
<li>Process one image at a time (don’t load entire dataset)</li>
<li>Restart Colab runtime to clear memory</li>
</ul>
<hr>
<p><strong>“Visualizations aren’t displaying”</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add to beginning of notebook</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">10</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<hr>
<p><strong>“Can’t access Sentinel-2 data in notebook”</strong></p>
<p>Session 3 uses pre-downloaded Sentinel-2 samples (included in notebook). No GEE authentication needed.</p>
<p>Session 4 will integrate GEE for larger-scale data loading.</p>
<hr>
</section>
<section id="getting-help" class="level3">
<h3 class="anchored" data-anchor-id="getting-help">Getting Help</h3>
<ul>
<li>📖 <a href="https://developers.google.com/machine-learning/crash-course/image-classification">CNN Tutorial (Google ML Crash Course)</a></li>
<li>📖 <a href="http://cs231n.stanford.edu/">CS231n Stanford Course</a> - Best CNN educational resource</li>
<li>💬 <a href="mailto:skotsopoulos@neuralio.ai">Instructor support</a> - Questions during lab hours</li>
<li>📧 <a href="https://data.philsa.gov.ph/support">PhilSA Data Support</a> - Access issues</li>
</ul>
<hr>
</section>
</section>
<section id="additional-resources" class="level2">
<h2 class="anchored" data-anchor-id="additional-resources">Additional Resources</h2>
<section id="foundational-learning" class="level3">
<h3 class="anchored" data-anchor-id="foundational-learning">Foundational Learning</h3>
<p><strong>Neural Networks:</strong> - <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">3Blue1Brown Neural Network Series</a> - Best intuitive explanation (visual) - <a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning (Free Book)</a> - <a href="https://www.coursera.org/specializations/deep-learning">Andrew Ng’s Deep Learning Specialization</a> (Coursera)</p>
<p><strong>CNNs Specifically:</strong> - <a href="http://cs231n.stanford.edu/schedule.html">Stanford CS231n Lectures</a> - Comprehensive (free) - <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer (Interactive)</a> - Visualize CNNs in browser - <a href="https://distill.pub/">Distill.pub Articles</a> - Beautiful visual explanations</p>
</section>
<section id="earth-observation-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="earth-observation-deep-learning">Earth Observation Deep Learning</h3>
<p><strong>Papers:</strong> - Zhu et al.&nbsp;(2017). “Deep Learning in Remote Sensing: A Review.” <em>IEEE GRSM</em> - Ma et al.&nbsp;(2019). “Deep learning in remote sensing applications: A meta-analysis and review.” <em>ISPRS</em> - Rußwurm &amp; Körner (2020). “Self-attention for raw optical satellite time series classification.” <em>ISPRS</em></p>
<p><strong>Tutorials:</strong> - <a href="https://eo-college.org/resources/deep-learning/">Deep Learning for Earth Observation (ESA)</a> - <a href="https://rastervision.io/">Raster Vision</a> - Framework for geospatial ML - <a href="https://torchgeo.readthedocs.io/">TorchGeo</a> - PyTorch library for geospatial data</p>
<p><strong>Datasets:</strong> - <a href="https://github.com/phelber/EuroSAT">EuroSAT</a> - Sentinel-2 scene classification (10 classes) - <a href="http://weegee.vision.ucmerced.edu/datasets/landuse.html">UC Merced Land Use</a> - High-res classification - <a href="http://deepglobe.org/">DeepGlobe</a> - Segmentation challenges - <a href="https://spacenet.ai/">SpaceNet</a> - Building detection</p>
</section>
<section id="philippine-context" class="level3">
<h3 class="anchored" data-anchor-id="philippine-context">Philippine Context</h3>
<ul>
<li><a href="https://philsa.gov.ph/publications/">PhilSA Research Publications</a> - CNN applications in Philippines</li>
<li><a href="https://skai.dost.gov.ph/docs">ASTI SkAI-Pinas Documentation</a> - Pre-trained PH models</li>
<li><a href="https://dimer.asti.dost.gov.ph/">DIMER Database</a> - Philippine disaster imagery</li>
</ul>
<hr>
</section>
</section>
<section id="assessment" class="level2">
<h2 class="anchored" data-anchor-id="assessment">Assessment</h2>
<section id="formative-assessment-during-session" class="level3">
<h3 class="anchored" data-anchor-id="formative-assessment-during-session">Formative Assessment (During Session)</h3>
<p><strong>Self-Check Questions:</strong></p>
<ol type="1">
<li>✓ Can you explain the difference between a perceptron and a multi-layer network?</li>
<li>✓ Why does ReLU work better than sigmoid in hidden layers?</li>
<li>✓ What does a convolutional filter “learn” during training?</li>
<li>✓ How does pooling provide translation invariance?</li>
<li>✓ When would you use U-Net instead of ResNet?</li>
</ol>
<p><strong>Interactive Exercises:</strong> - ✓ Complete all TODO cells in theory notebook - ✓ Implement manual convolution from scratch - ✓ Visualize at least 3 different activation functions - ✓ Compare filter responses on forest vs urban areas</p>
</section>
<section id="summative-assessment-end-of-session" class="level3">
<h3 class="anchored" data-anchor-id="summative-assessment-end-of-session">Summative Assessment (End of Session)</h3>
<p><strong>Knowledge Check (10 questions, multiple choice):</strong> - Neural network components - CNN architecture understanding - Application selection (classification vs segmentation) - Data requirements and constraints</p>
<p><strong>Practical Demonstration:</strong> - Explain CNN decision-making process for a sample image - Sketch appropriate architecture for given EO task - Estimate data requirements for Philippine use case</p>
<p><strong>Readiness for Session 4:</strong> - ✓ Understand TensorFlow/Keras workflow conceptually - ✓ Recognize data preparation needs - ✓ Anticipate training challenges</p>
<hr>
</section>
</section>
<section id="next-steps" class="level2">
<h2 class="anchored" data-anchor-id="next-steps">Next Steps</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>After Session 3
</div>
</div>
<div class="callout-body-container callout-body">
<p>You now understand CNN theory and operations! Session 4 puts this knowledge into practice with hands-on implementation.</p>
<p><strong>Session 4: Hands-On CNN Implementation</strong></p>
<p>You’ll build and train: 1. <strong>ResNet Classifier:</strong> Palawan land cover (8 classes) 2. <strong>U-Net Segmentation:</strong> Flood mapping (Central Luzon)</p>
<p>Using TensorFlow/Keras with real Sentinel-2 data.</p>
<p><a href="../../day2/sessions/session4.html" class="btn btn-primary">Continue to Session 4 →</a></p>
</div>
</div>
<section id="recommended-pre-work-for-session-4" class="level3">
<h3 class="anchored" data-anchor-id="recommended-pre-work-for-session-4">Recommended Pre-Work for Session 4</h3>
<p>Before Session 4, please:</p>
<ol type="1">
<li>✅ <strong>Review notebook exercises</strong> - Ensure you understand convolution and activation functions</li>
<li>✅ <strong>Read U-Net paper</strong> - <a href="https://arxiv.org/abs/1505.04597">Ronneberger et al.&nbsp;2015</a> (15 min)</li>
<li>✅ <strong>Check GPU access</strong> - Enable GPU in Colab (Settings → Hardware Accelerator → GPU)</li>
<li>✅ <strong>Install TensorFlow</strong> - <code>!pip install tensorflow==2.15</code> (will do in Session 4, but test now)</li>
</ol>
</section>
<section id="extended-learning-paths" class="level3">
<h3 class="anchored" data-anchor-id="extended-learning-paths">Extended Learning Paths</h3>
<p><strong>Path 1: Deep Dive into Theory</strong> - Complete CS231n Stanford course - Implement backpropagation from scratch - Study optimization algorithms (Adam, RMSprop)</p>
<p><strong>Path 2: Explore Advanced Architectures</strong> - Attention mechanisms (Transformers for EO) - Vision Transformers (ViT) - Self-supervised learning (SimCLR, MoCo)</p>
<p><strong>Path 3: Philippine EO Applications</strong> - Contribute training data to PhilSA Space+ - Develop CNN model for local area - Publish results in Philippine GIS conference</p>
<hr>
</section>
</section>
<section id="quick-links" class="level2">
<h2 class="anchored" data-anchor-id="quick-links">Quick Links</h2>
<ul>
<li><strong>Part A (ML→DL):</strong> 15 min - Keep conceptual, avoid getting bogged down in math</li>
<li><strong>Part B (NN Fundamentals):</strong> 25 min - Live code perceptron, let students modify</li>
<li><strong>Part C (CNNs):</strong> 30 min - Most critical section, use lots of visuals</li>
<li><strong>Part D (EO Applications):</strong> 25 min - Show real PhilSA examples if possible</li>
<li><strong>Part E (Practical):</strong> 15 min - Set realistic expectations for Session 4</li>
</ul>
<p><strong>Total:</strong> 110 min (2.5 hours with 20 min buffer for questions)</p>
<hr>
<p><strong>Common Student Challenges:</strong></p>
<ol type="1">
<li><p><strong>“I don’t understand backpropagation”</strong> → Focus on intuition (gradient descent down error surface), not calculus. Session 4 uses libraries that handle this automatically.</p></li>
<li><p><strong>“Why are we doing manual convolutions in NumPy?”</strong> → Emphasize this is for learning. Session 4 uses optimized libraries (1000× faster). Like learning to drive with manual transmission—helps understand what’s happening under the hood.</p></li>
<li><p><strong>“Will my laptop be fast enough for Session 4?”</strong> → No local installation needed! Google Colab provides free GPUs. Sessions 4 notebooks are optimized for Colab free tier.</p></li>
<li><p><strong>“Can CNNs use Sentinel-2’s 10+ bands?”</strong> → YES! Unlike ImageNet RGB pre-training. Session 4 shows adaptation strategies. This is a huge advantage of CNNs for EO.</p></li>
</ol>
<hr>
<p><strong>Teaching Tips:</strong></p>
<ul>
<li><strong>Start with Familiar:</strong> Connect to Session 2 Random Forest throughout</li>
<li><strong>Visual Heavy:</strong> Show lots of images/diagrams (CNNs are visual learners!)</li>
<li><strong>Interactive:</strong> Have students modify filter values in real-time</li>
<li><strong>Philippine Examples:</strong> Use Palawan imagery from Session 2 for continuity</li>
<li><strong>Manage Expectations:</strong> Be honest about data/compute requirements</li>
<li><strong>Celebrate Progress:</strong> “You now understand CNNs better than 90% of GIS professionals!”</li>
</ul>
<hr>
<p><strong>Demonstration Best Practices:</strong></p>
<ol type="1">
<li><strong>Perceptron Demo:</strong> Use simple 2D data first (NDVI vs NDWI), visualize decision boundary updating in real-time</li>
<li><strong>Convolution Demo:</strong> Pick dramatic example (forest edge) where edge detection is obvious</li>
<li><strong>Architecture Comparison:</strong> Show parameter counts to emphasize efficiency gains</li>
<li><strong>Philippine Focus:</strong> Always end each section with “How does this help DENR/PhilSA/LGUs?”</li>
</ol>
<hr>
<p><strong>Assessment Rubric:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 21%">
<col style="width: 14%">
<col style="width: 19%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Excellent (5)</th>
<th>Good (4)</th>
<th>Adequate (3)</th>
<th>Needs Improvement (2)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Conceptual</strong></td>
<td>Explains CNN advantages with examples</td>
<td>States CNN benefits</td>
<td>Lists CNN components</td>
<td>Confuses CNN with traditional ML</td>
</tr>
<tr class="even">
<td><strong>Technical</strong></td>
<td>Implements convolution from scratch</td>
<td>Completes all notebook exercises</td>
<td>Completes &gt;70% exercises</td>
<td>Struggles with NumPy operations</td>
</tr>
<tr class="odd">
<td><strong>Application</strong></td>
<td>Proposes appropriate architecture for novel task</td>
<td>Identifies correct architecture for standard tasks</td>
<td>Distinguishes classification vs segmentation</td>
<td>Cannot select architecture</td>
</tr>
<tr class="even">
<td><strong>Readiness</strong></td>
<td>Articulates Session 4 workflow</td>
<td>Understands TensorFlow role</td>
<td>Knows Session 4 is hands-on</td>
<td>Unclear on next steps</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>Session 4 Transition:</strong></p>
<p>End with excitement: <em>“You’ve learned the theory. Tomorrow you’ll build a production-ready land cover classifier and flood mapper using TensorFlow. Bring your questions, bring your data ideas, and get ready to train some CNNs!”</em></p>
<p><strong>Preview Session 4 Outcomes:</strong> - ResNet model achieving &gt;85% accuracy on Palawan - U-Net generating flood maps in 2 minutes - Exportable models for operational use</p>
<hr>
<p><strong>Backup Activities (if ahead of schedule):</strong></p>
<ol type="1">
<li><strong>Live Code Challenge:</strong> “Build a 3-layer network for Palawan classification” (10 min)</li>
<li><strong>Architecture Design:</strong> “Sketch CNN for building detection task” (5 min)</li>
<li><strong>Group Discussion:</strong> “What EO problem in YOUR organization could use CNNs?” (10 min)</li>
</ol>
<hr>
<p><strong>Resource Check (Before Session):</strong></p>
<ul>
<li>✓ Test both Colab notebooks execute without errors</li>
<li>✓ Verify Sentinel-2 sample data loads correctly</li>
<li>✓ Pre-download datasets to Google Drive (backup)</li>
<li>✓ Have architecture diagrams ready (slides or drawn)</li>
<li>✓ Queue up CNN Explainer website for demos</li>
<li>✓ Test video/screen sharing for visualizations :::</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<script src="https://utteranc.es/client.js" repo="cophil-training-v1.0" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../day2/sessions/session2.html" class="pagination-link" aria-label="Session 2: Advanced Palawan Land Cover Lab">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Session 2: Advanced Palawan Land Cover Lab</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../day2/sessions/session4.html" class="pagination-link" aria-label="Session 4: CNN Hands-on Lab">
        <span class="nav-page-text">Session 4: CNN Hands-on Lab</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>CoPhil EO AI/ML Training Programme</p>
</div>   
    <div class="nav-footer-center">
<p>Funded by the European Union - Global Gateway Initiative</p>
</div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:skotsopoulos@neuralio.ai">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://philsa.gov.ph">
      <i class="bi bi-globe" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>