[
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#course-introduction",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#course-introduction",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Course Introduction",
    "text": "Course Introduction\n\n\n4-Day Advanced Training\n\nAI/ML for Earth Observation\nPhilippine EO Professionals\nFocus: DRR, CCA, NRM\nOnline format\n\n\nToday’s Goals\n\nUnderstand Copernicus data\nExplore Philippine EO ecosystem\nLearn AI/ML fundamentals\nHands-on Python and GEE\n\n\n\nWelcome participants to the 4-day advanced training. Emphasize that this is part of the EU-Philippines partnership and will provide practical skills for disaster risk reduction, climate change adaptation, and natural resource management."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#eu-global-gateway-initiative",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#eu-global-gateway-initiative",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "EU Global Gateway Initiative",
    "text": "EU Global Gateway Initiative\n\n\n\nEU-Philippines space cooperation flagship\nBuilding strong partnerships\nSmart, clean, secure digital links\nStrengthening health, education, research systems globally\n\n\n\n\n\nThe Global Gateway strategy represents the EU’s commitment to building partnerships that boost smart, clean and secure infrastructure globally. CoPhil is a unique flagship initiative within this framework."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#cophil-programme-overview",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#cophil-programme-overview",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "CoPhil Programme Overview",
    "text": "CoPhil Programme Overview\n\n\nMission\nSupport Philippine Space Agency (PhilSA) and DOST to improve use of Earth Observation data for:\n\nDisaster Risk Reduction (DRR)\nClimate Change Adaptation (CCA)\nNatural Resource Management (NRM)\n\n\nKey Outputs\n\nCopernicus Mirror Site\nDigital Space Campus\nCapacity building\nPilot services\n\n\n\nCoPhil is an EU-funded Technical Assistance programme positioning the Philippines as a pioneer in the EU’s international cooperation on Copernicus."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#philsa-dost-partnership",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#philsa-dost-partnership",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "PhilSA & DOST Partnership",
    "text": "PhilSA & DOST Partnership\n\n\nPhilippine Space Agency\n\n\nEstablished 2019\nCentral civilian space agency\nSpace+ Data Dashboard\nCo-chair of CoPhil\n\n\nDepartment of Science and Technology\n\n\nASTI AI initiatives\nSkAI-Pinas program\nNational AI investments\nCo-chair of CoPhil\n\n\n\nBoth PhilSA and DOST are co-chairs of the CoPhil programme, demonstrating strong national commitment to building EO and AI capacity."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#session-1-roadmap",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#session-1-roadmap",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Session 1 Roadmap",
    "text": "Session 1 Roadmap\n\nCopernicus Programme Overview\nSentinel-1 Mission (SAR)\nSentinel-2 Mission (Optical)\nData Access Methods\nPhilippine EO Ecosystem\nCoPhil Infrastructure\n\nDuration: 2 hours\n\nThis session provides the foundation for understanding where EO data comes from and how Philippine agencies complement Copernicus data."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#what-is-copernicus",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#what-is-copernicus",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "What is Copernicus?",
    "text": "What is Copernicus?\n\n\nEurope’s Eyes on Earth\n\nEU flagship Earth Observation program\nFamily of Sentinel satellites\nFree and open data policy\nOperational since 2014\n\n\n\n\n\n“Looking at our planet and its environment for the benefit of all European citizens”\n\n\nCopernicus is the European Union’s Earth Observation program, providing free and open satellite data globally. It comprises a fleet of Sentinel satellites designed for environmental monitoring."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#copernicus-programme-architecture",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#copernicus-programme-architecture",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Copernicus Programme Architecture",
    "text": "Copernicus Programme Architecture\n\nCopernicus Programme Structure showing Space Component, Services, and End Users\nThis diagram shows the complete Copernicus architecture: the space component with Sentinel missions, in-situ data sources, the six core services (CAMS, CMEMS, Land, Climate, Emergency, Security), and the various end-user categories."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#the-sentinel-family",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#the-sentinel-family",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "The Sentinel Family",
    "text": "The Sentinel Family\n\n\nSentinel-1 (SAR)\n\nC-band radar imaging\nAll-weather, day/night\n1A operational, 1C launched Dec 2024\n6-day repeat (dual constellation)\n\nSentinel-2 (Optical)\n\nMultispectral imaging\n13 spectral bands\n2A, 2B, 2C operational Jan 2025\n5-day repeat (three satellites)\n\n\nSentinel-3 (Ocean/Land)\n\nOcean and land monitoring\nSea surface temperature\nOcean color, vegetation\n\nSentinel-5P (Atmosphere)\n\nAir quality monitoring\nAtmospheric composition\n\n\n\nTiming: 4 minutes\nKey Points: - Six Sentinel missions operational (1, 2, 3, 5P, 6) with 4, 7-12 planned - 2025 Update: Sentinel-1C launched December 2024, restoring 6-day global repeat - 2025 Update: Sentinel-2C operational January 2025, creating 3-satellite constellation with 5-day repeat - Today we focus on Sentinel-1 and Sentinel-2 - most relevant for Philippine DRR/CCA/NRM - Sentinel-3 important for marine/coastal applications - Sentinel-5P monitors air pollution (useful for Manila air quality)\nTransition: “These missions generate petabytes of free data. Let’s see how this data is used…”"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#copernicus-applications",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#copernicus-applications",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Copernicus Applications",
    "text": "Copernicus Applications\n\n\n\nEmergency Management\n\nFlood mapping\nFire detection\nDisaster response\n\n\nClimate & Environment\n\nDeforestation monitoring\nAgricultural monitoring\nWater quality assessment\n\n\n\nCopernicus supports applications in emergency management, health care, agriculture, and environment, with crucial impact on society, climate, and economy."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-overview",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-overview",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Overview",
    "text": "Sentinel-1 Overview\n\n\nMission Configuration (2025)\n\nSensor: C-band Synthetic Aperture Radar\nSatellites: 1A, 1C (operational 2025)\nOrbit: Polar sun-synchronous\nAll-weather, day/night capability\n\n\n\n\n\nKey Advantage: Penetrates clouds and works at night\n\n\nSentinel-1 is a radar imaging mission with C-band SAR enabling all-weather, day-and-night observations unaffected by clouds - crucial in tropical regions like the Philippines."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-technical-specifications",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-technical-specifications",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Technical Specifications",
    "text": "Sentinel-1 Technical Specifications\n\n\n\nParameter\nValue\n\n\n\n\nSensor Type\nC-band SAR (5.405 GHz)\n\n\nRevisit Time\n6-12 days (constellation)\n\n\nSwath Width\n250 km (IW mode)\n\n\nSpatial Resolution\n5m × 20m (IW mode)\n\n\nPolarization\nVV + VH or HH + HV\n\n\nOrbit\n693 km altitude\n\n\n\n\nSentinel-1 operates in Interferometric Wide (IW) mode with ~5m by 20m spatial resolution. With two satellites, the revisit cycle is 6 days globally. Note: Sentinel-1B became inoperative in 2022; Sentinel-1C launched in 2024 to restore 6-day revisit."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sar-how-it-works",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sar-how-it-works",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "SAR: How It Works",
    "text": "SAR: How It Works\n\n\nSatellite sends microwave pulses to Earth\nSignal reflects from surface\nSensor measures backscatter intensity\nDifferent surfaces = different backscatter\n\n\nSAR satellites send microwave signals and measure the backscatter. Water appears dark (low backscatter), urban areas appear bright (strong backscatter), and vegetation shows moderate backscatter."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-polarization",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-polarization",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Polarization",
    "text": "Sentinel-1 Polarization\n\n\nWhat is Polarization?\n\nOrientation of radar wave\nVV: Vertical send/receive\nVH: Vertical send/Horizontal receive\nHH: Horizontal send/receive\n\n\nApplications\n\nVV: Good for water/flood mapping\nVH: Sensitive to volume scattering (vegetation)\nVV/VH Ratio: Discriminates surface types\n\n\n\n\nSentinel-1 IW mode over land typically provides VV and VH polarizations. Different polarizations are sensitive to different surface characteristics."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#backscatter-characteristics-by-target",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#backscatter-characteristics-by-target",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Backscatter Characteristics by Target",
    "text": "Backscatter Characteristics by Target\n\n\n\nSurface Type\nBackscatter\nAppearance\nReason\n\n\n\n\nWater (smooth)\nVery Low\nDark/Black\nSpecular reflection\n\n\nUrban/Buildings\nVery High\nBright White\nCorner reflectors\n\n\nForest/Vegetation\nMedium-High\nGray\nVolume scattering\n\n\nAgricultural Fields\nMedium\nLight Gray\nSurface roughness\n\n\nBare Soil (dry)\nLow-Medium\nDark Gray\nSmooth surface\n\n\nBare Soil (wet)\nMedium\nMedium Gray\nIncreased dielectric\n\n\n\n\nKey Insight: Water appears dark, structures appear bright - the basis for flood mapping!\n\n\nUnderstanding backscatter is crucial for interpreting SAR imagery. Smooth surfaces (water) reflect signals away = dark. Rough surfaces and structures reflect signals back = bright."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-imaging-modes",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-imaging-modes",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Imaging Modes",
    "text": "Sentinel-1 Imaging Modes\n\n\nInterferometric Wide Swath (IW) - 250 km swath - 5m × 20m resolution - Default over land - Philippine standard mode\nExtra Wide Swath (EW) - 400 km swath - 20m × 40m resolution - Maritime/polar regions\n\nStrip Map (SM) - 80 km swath - 5m × 5m resolution - Emergency response - High detail needed\nWave (WV) - Ocean waves - Not used for land\n\n\nInterferometric Wide (IW) mode is used over land and provides the best balance of resolution and coverage. All Philippine acquisitions use IW mode."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-data-products",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-data-products",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Data Products",
    "text": "Sentinel-1 Data Products\n\n\nLevel-1 GRD (Ground Range Detected)\n\nMulti-looked (reduced speckle)\nProjected to ground range\nMost commonly used\nFaster to process\nSmaller file size\nApplications:\n\nChange detection\nClassification\nFlood mapping\nShip detection\n\n\n\nLevel-1 SLC (Single Look Complex)\n\nPreserves phase information\nComplex-valued pixels\nRequired for InSAR\nLarger files\nApplications:\n\nGround deformation\nInterferometry\nCoherence analysis\nSubsidence monitoring\n\n\n\n\nFor most applications including flood mapping and land cover, GRD products are sufficient. SLC products are needed for advanced interferometric applications like volcano monitoring."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#grd-vs-slc---which-to-choose",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#grd-vs-slc---which-to-choose",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "GRD vs SLC - Which to Choose?",
    "text": "GRD vs SLC - Which to Choose?\n\n\n\nFactor\nGRD\nSLC\n\n\n\n\nUse Case\nMost applications\nInterferometry only\n\n\nProcessing\nReady to use\nComplex processing\n\n\nFile Size\n~1 GB\n~4 GB\n\n\nSpeckle\nReduced\nFull speckle\n\n\nPhase\nNot preserved\nPreserved\n\n\nTypical User\nMost analysts\nAdvanced specialists\n\n\n\n\nFor this training and most Philippine applications: Use GRD\n\n\nUnless you specifically need interferometry (earthquake/volcano deformation), always use GRD products. They’re easier to work with and sufficient for 90% of applications."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-pre-processing-under-the-hood",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-pre-processing-under-the-hood",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Pre-Processing “Under the Hood”",
    "text": "Sentinel-1 Pre-Processing “Under the Hood”\n\n\n\n\n\n\nWhat Happens Before You See SAR Data?\n\n\nFor this training, we use pre-processed Sentinel-1 GRD data. Here’s what happens “under the hood”:\n\n\n\nS1 Processing Pipeline:\n\nGRD Download → Raw ground-range detected amplitude\nRadiometric Calibration → Convert to backscatter coefficient (σ⁰)\nTerrain Correction (RTC) → Remove topographic distortions using DEM\nSpeckle Filtering → Reduce SAR noise (Lee, Refined Lee, or Gamma-MAP filters)\nConversion to dB → γ⁰ (gamma-naught) in decibels for visual interpretation\nTiling/Clipping → Extract area of interest\n\n\nFor Day 3 flood mapping labs: We provide analysis-ready patches with these steps already applied\n\n\nP0 IMPROVEMENT APPLIED: Explicit S1 pre-processing assumptions before U-Net flood lab (Day 3).\nKey Teaching Points: - Participants need to understand what “analysis-ready” means - RTC removes terrain effects - critical in mountainous Philippines - Gamma-naught (γ⁰) in dB is standard for land applications - Speckle filtering is essential but can blur edges - These steps are computationally intensive - we pre-process for efficiency\nTiming: 3 minutes\nTransition: “Now let’s look at Sentinel-2 optical data…”"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-applications",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-applications",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Applications",
    "text": "Sentinel-1 Applications\n\n\nFlood Mapping\n\n\n\nCopernicus Sentinel-1 Flood Monitoring\n\n\n\nWater appears dark in SAR\nWorks through clouds\nNear real-time monitoring\n\n\nDeformation Monitoring\n\n\n\nSentinel-1C Interferogram of Northern Chile\n\n\n\nInSAR technique\nMillimeter precision\nVolcano and earthquake monitoring\n\n\n\nSentinel-1’s ability to penetrate clouds makes it invaluable for flood mapping during typhoons. The left image shows actual flood monitoring using Sentinel-1 data. The right image shows an interferogram from Sentinel-1C demonstrating ground deformation detection with millimeter precision in northern Chile."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#philippine-example-flood-monitoring",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#philippine-example-flood-monitoring",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Philippine Example: Flood Monitoring",
    "text": "Philippine Example: Flood Monitoring\n\n\n\nNovember 2020: Typhoon Ulysses\n\nExtensive flooding in Luzon\nSentinel-1 detected flood extent\nRapid mapping capability\n\n\nKey Benefits\n\nNo cloud interference\nQuick response time\nUsed for rapid damage assessment\nSupported emergency response\n\n\n\nDuring Typhoon Ulysses, Sentinel-1 provided crucial flood extent mapping when optical satellites couldn’t see through clouds. This data supported PAGASA and NDRRMC response efforts."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-overview",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-overview",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-2 Overview",
    "text": "Sentinel-2 Overview\n\n\nKey Specifications:\n\n13 spectral bands (visible, NIR, SWIR)\n10m to 60m spatial resolution\n290 km swath width\n5-day revisit (three satellites operational 2025)\nL1C & L2A processing levels\n\n\nPhilippine Example: Mayon Volcano\n\n\n\n\n\nSentinel-2 monitoring of 2018 eruption\n\nFalse-color composite highlighting lava flows\n10m resolution captures detail\n5-day revisit enables continuous monitoring\n\n\n\nThis Sentinel-2 false-color image of Mayon Volcano from 2018 shows recent lava flows. With Sentinel-2C operational in 2025, the three-satellite constellation provides 5-day global revisit."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-spectral-bands",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-spectral-bands",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-2 Spectral Bands",
    "text": "Sentinel-2 Spectral Bands\n\n\n\n\n\n\n\n\n\n\n\nBand\nName\nWavelength (nm)\nResolution\nPurpose\n\n\n\n\nB1\nCoastal Aerosol\n443\n60m\nAerosol correction, water color\n\n\nB2\nBlue\n490\n10m\nWater bodies, atmospheric\n\n\nB3\nGreen\n560\n10m\nVegetation health\n\n\nB4\nRed\n665\n10m\nVegetation discrimination\n\n\nB5\nRed Edge 1\n705\n20m\nVegetation stress detection\n\n\nB6\nRed Edge 2\n740\n20m\nVegetation classification\n\n\nB7\nRed Edge 3\n783\n20m\nVegetation stress, chlorophyll\n\n\nB8\nNIR\n842\n10m\nBiomass, water bodies\n\n\nB8A\nNIR Narrow\n865\n20m\nAtmospheric correction\n\n\nB9\nWater Vapor\n945\n60m\nAtmospheric correction\n\n\nB10\nSWIR Cirrus\n1375\n60m\nCirrus cloud detection\n\n\nB11\nSWIR 1\n1610\n20m\nMoisture content, fire\n\n\nB12\nSWIR 2\n2190\n20m\nMoisture, geology, soil\n\n\n\n\n\nAll 13 Sentinel-2 bands: Four 10m bands (B2-B4, B8) for detailed mapping, six 20m bands including the unique Red Edge trio (B5-B7) for vegetation analysis, and three 60m atmospheric bands (B1, B9, B10) for corrections. The Red Edge bands are Sentinel-2’s signature capability."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#red-edge-bands---sentinel-2s-special-capability",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#red-edge-bands---sentinel-2s-special-capability",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Red Edge Bands - Sentinel-2’s Special Capability",
    "text": "Red Edge Bands - Sentinel-2’s Special Capability\n\n\nWhat is Red Edge?\n\nTransition zone between red and NIR (700-780nm)\nThree dedicated bands (B5, B6, B7)\nSensitive to chlorophyll content\nUnique to Sentinel-2 among free satellites\n\nApplications:\n\nEarly vegetation stress detection\nCrop health monitoring\nForest disease identification\nPre-harvest yield estimation\n\n\nPhilippine Use Cases:\n\nRice crop assessment - Monitor crop health and nitrogen status during panicle initiation in Nueva Ecija rice paddies\nCoconut disease detection - Identify stem bleeding disease and pest infestations through spectral signatures\nMangrove health monitoring - Track vegetation stress and recovery in Palawan mangrove forests using red edge indices\n\n\nRed edge bands detect stress weeks before visible bands show changes\n\n\n\nRed Edge bands are Sentinel-2’s superpower. They detect vegetation stress weeks before visible bands show changes - crucial for precision agriculture and forest health monitoring."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-data-products",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-data-products",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-2 Data Products",
    "text": "Sentinel-2 Data Products\n\n\nLevel-1C (L1C)\n\nTop-of-Atmosphere reflectance\nRadiometrically corrected\nGeometrically refined\nNo atmospheric correction\nUse: If you need raw data for custom processing\n\n\nLevel-2A (L2A)\n\nBottom-of-Atmosphere (surface) reflectance\nAtmospherically corrected\nAnalysis-ready\nScene Classification Layer included\nUse: For most applications - RECOMMENDED\n\n\n\nAlways use Level-2A when available - it’s analysis-ready!\n\n\nL2A data is atmospherically corrected and ready for analysis. Unless you have a specific reason to use L1C, always choose L2A products."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-band-combinations",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-band-combinations",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-2 Band Combinations",
    "text": "Sentinel-2 Band Combinations\n\n\nTrue Color (Natural) - RGB: B4-B3-B2 (Red-Green-Blue) - Looks like a photograph - Good for visual inspection\nFalse Color Infrared - RGB: B8-B4-B3 (NIR-Red-Green) - Vegetation appears RED - Classic for vegetation assessment\n\nSWIR Composite (Agriculture) - RGB: B11-B8-B2 - Highlights crop moisture - Soil moisture visible\nSWIR-NIR-Red (Burn/Fire) - RGB: B12-B8-B4 - Active fires appear BRIGHT - Burn scars dark purple\n\n\nDifferent band combinations highlight different features. False Color (NIR-Red-Green) is the most common for vegetation mapping - healthy vegetation appears bright red."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#complementary-capabilities",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#complementary-capabilities",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Complementary Capabilities",
    "text": "Complementary Capabilities\n\n\n\n\n\n\n\n\nAspect\nSentinel-1\nSentinel-2\n\n\n\n\nSensor Type\nRadar (SAR)\nOptical (MSI)\n\n\nWeather\nAll-weather\nCloud-affected\n\n\nTime\nDay & night\nDaytime only\n\n\nResolution\n5m × 20m\n10m / 20m / 60m\n\n\nRevisit\n6-12 days\n5 days\n\n\nBands\nPolarizations (2)\nSpectral bands (13)\n\n\nBest For\nWater, structure, moisture\nVegetation, land cover, color\n\n\n\n\n\n\n\n\n\nS1 Flood Mapping Tip\n\n\nVV polarization shows dark water (low backscatter from smooth surfaces)\nBest practice: Compare pre-event vs post-event delta for reliability\nSynergy: Pair S1 (flood extent through clouds) with S2 (vegetation damage when clear) for complete impact assessment\n\n\n\n\nSentinel-1 and Sentinel-2 are complementary. SAR penetrates clouds and works at night, while optical provides rich spectral information about surface properties.\nMICRO-EDIT APPLIED: Added S1 vs S2 flood tip to set up Day 3 logic - VV dark water, pre/post comparison, and S1+S2 synergy."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#synergistic-use-cases",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#synergistic-use-cases",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Synergistic Use Cases",
    "text": "Synergistic Use Cases\n\n\nFlood Mapping\n\nS1: Detect water extent (through clouds)\nS2: Assess damage to vegetation/crops (when clear)\nCombined: Complete flood impact assessment\n\n\nForest Monitoring\n\nS1: Detect structural changes, biomass\nS2: Identify tree species, health\nCombined: Comprehensive forest mapping\n\n\n\n\nUsing both missions together provides more complete information. After a typhoon, S1 maps floods under clouds while S2 assesses vegetation damage where it’s clear."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#minute-break",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#minute-break",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "☕ 5-Minute Break",
    "text": "☕ 5-Minute Break\n\nStretch Break\nStand up • Grab water • Back in 5 minutes\n\n\nTiming: 5 minutes\nInstructor Actions: - Announce break clearly - Mention return time - Leave slides/browser open for questions - Be available for quick questions during break\nWhen Resuming: - Wait 1-2 minutes for everyone to return - Quick recap: “We’ve covered Sentinel-1 SAR and Sentinel-2 optical. Now we’ll explore the Philippine EO ecosystem.”"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#platform-choices-for-this-training",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#platform-choices-for-this-training",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Platform Choices for This Training",
    "text": "Platform Choices for This Training\n\n\n\n\n\n\nWhich Platform for Which Task?\n\n\nUnderstanding where to work is crucial - don’t try to train deep models on GEE or download 100GB in Colab!\n\n\n\n\n\n\n\n\n\n\n\n\nTask\nPlatform\nWhy\nLimitations\n\n\n\n\nData Prep & Exploration\nGoogle Earth Engine\nPetabyte catalog, no download, cloud composites\nExport limit 32 MB (tile large areas), no deep learning training\n\n\nML Training (RF, shallow)\nGEE or Colab\nRF works in GEE; small data in Colab\nGEE memory limits; Colab free tier quotas\n\n\nDeep Learning (CNN, U-Net)\nLocal GPU / Colab Pro\nRequires PyTorch/TensorFlow\nColab free = limited GPU time; large models need local resources\n\n\nLarge-Scale Processing\nCoPhil Mirror Site / COARE\n400TB local data, HPC resources\nRequires account; learning curve for APIs\n\n\nQuick Viz & Download\nCopernicus Browser\nInteractive, fast previews\nManual selection; bulk downloads tedious\n\n\n\n\nQuotas/Pitfalls to know: - GEE: Memory errors with large computations (tile exports!) - Colab Free: GPU disconnects after inactivity; limited sessions/day - CoPhil/Digital Space Campus: Hosts this training’s materials + local data access\n\n\nP1 IMPROVEMENT APPLIED: Explicit platform choices table prevents common mistakes.\nKey Teaching Points: - GEE is NOT for training U-Nets or downloading GBs - Colab free tier has GPU quotas - manage expectations - CoPhil infrastructure enables local processing - Choose the right tool for the right job\nTiming: 3 minutes\nTransition: “Let’s look at these platforms in detail…”"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#copernicus-data-space-ecosystem-2025",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#copernicus-data-space-ecosystem-2025",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Copernicus Data Space Ecosystem (2025)",
    "text": "Copernicus Data Space Ecosystem (2025)\n\n\n\nNew Platform (2023+)\n\nReplaced SciHub\nModern interface\nAPI access\nFree registration\n\n\nFeatures\n\nSearch by location/date\nPreview before download\nDirect download\nBulk processing\n\n\nURL: https://dataspace.copernicus.eu\n\nThe Copernicus Data Space Ecosystem is the new portal replacing the legacy SciHub. All Sentinel data are free and open."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentiboard-dashboard-october-2025",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentiboard-dashboard-october-2025",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "SentiBoard Dashboard (October 2025)",
    "text": "SentiBoard Dashboard (October 2025)\n\n\nReal-time mission status\nData availability insights\nAcquisition plans\nQuality metrics\nInteractive dashboard\n\n\nSentiBoard is a new feature (October 2025) providing real-time insights into Copernicus Sentinel missions and data availability."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#google-earth-engine",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#google-earth-engine",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Google Earth Engine",
    "text": "Google Earth Engine\n\n\nPlanetary-Scale Platform\n\nPetabyte-scale data catalog\nAll Sentinel-1 & Sentinel-2 data\nCloud-based processing\nFree for research/education\nNo download needed!\n\n\n\n\n\nWe’ll use GEE extensively in this training\n\nURL: https://earthengine.google.com\n\nGoogle Earth Engine provides ready access to all Sentinel data with cloud-based processing. We’ll use GEE extensively for data access and preprocessing."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#alternative-data-sources",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#alternative-data-sources",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Alternative Data Sources",
    "text": "Alternative Data Sources\n\n\nAlaska Satellite Facility (ASF)\n\nSentinel-1 specialist\nUser-friendly interface\nPreprocessing tools\nhttps://asf.alaska.edu\n\n\nAWS Open Data\n\nSentinel-2 on AWS\nCloud-optimized\nPay for compute only\nProgrammatic access\n\n\n\nNow Available: CoPhil Mirror Site in the Philippines! (Operational 2024-2025)\n\n\nMICRO-EDIT APPLIED: Mirror Site is NOW operational (not “coming soon”).\nMultiple platforms provide Sentinel data access. The CoPhil Mirror Site provides local data access in the Philippines for faster downloads - critical for reducing international bandwidth bottlenecks.\nKey Point: 400TB capacity, focused on PH scenes, free for government/academia/researchers"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#overview-of-philippine-eo-landscape",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#overview-of-philippine-eo-landscape",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Overview of Philippine EO Landscape",
    "text": "Overview of Philippine EO Landscape\n\n\nPhilSA: Space data and operations\nNAMRIA: National mapping and geospatial data\nDOST-ASTI: AI and remote sensing R&D\nPAGASA: Climate and weather data\n\n\nBeyond European satellites, the Philippines has its own geospatial data platforms and agencies that provide crucial complementary data and support."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#philsa-national-space-agency",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#philsa-national-space-agency",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "PhilSA: National Space Agency",
    "text": "PhilSA: National Space Agency\n\n\nEstablished: August 2019\nMandate:\n\nCentral civilian space agency\nPromote space data use\nBuild national capacity\nSupport DRR, CCA, NRM\nCo-chair CoPhil programme\n\n\n\n\nWebsite: https://philsa.gov.ph\n\nPhilSA is the central civilian space agency established in 2019. As co-chair of CoPhil, PhilSA is leading national efforts to leverage Copernicus data."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#siyasat-data-portal",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#siyasat-data-portal",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "SIYASAT Data Portal",
    "text": "SIYASAT Data Portal\n\n\n\nPurpose (2025)\n\nSecure data archive operational\nVisualization system\nData distribution\nMaritime & terrestrial monitoring\n\n\nData Types\n\nNovaSAR-1 radar imagery\nAIS ship tracking data\nProcessed products\nAnalysis-ready data\n\n\n\nTiming: 3 minutes\nKey Points: - SIYASAT (Secure Interactive Yield-Assessment & SAR Analytics Tools) is PhilSA’s operational data portal - 2025 Status: Fully operational with NovaSAR-1 data access - Provides secure archive for Philippine government agencies - Critical for maritime security (illegal fishing, territorial monitoring) - Registration required - priority for government agencies\nTransition: “Beyond PhilSA, DOST-ASTI is leading AI innovation for EO applications…”"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#space-data-dashboard",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#space-data-dashboard",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Space+ Data Dashboard",
    "text": "Space+ Data Dashboard\n\n\nUser-friendly web portal\nBrowse satellite imagery\nVisualization tools\nDownload datasets\nNo programming required\nOpen to government, researchers, public\n\n\nThe Space+ Data Dashboard democratizes access to satellite data, making it available to local governments, researchers, and even students without requiring programming skills."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#philsa-2025-initiatives",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#philsa-2025-initiatives",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "PhilSA 2025 Initiatives",
    "text": "PhilSA 2025 Initiatives\n\n\nSpace Business Innovation Challenge\n\nEmpowers Filipino innovators\nFree satellite data access\nBuild solutions for local needs\nEarth observation focus\nWeather & environmental data\n\n\nTraining Programs\n\nDownstream data utilization\nPractical applications\nCapacity building nationwide\nPartnership with DOST\n\n\n\nPhilSA’s 2025 initiatives focus on empowering innovators and building capacity across the Philippines to use satellite data effectively."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#coare-infrastructure",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#coare-infrastructure",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "COARE Infrastructure",
    "text": "COARE Infrastructure\n\n\nComputing and Archiving Research Environment\n\nHigh-performance computing\nData archiving capabilities\nScience cloud facilities\nSupports data-intensive research\nEnables AI/ML workflows\n\n\n\n\n\nCOARE provides the computational infrastructure needed for processing large satellite datasets and running AI/ML models at scale."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#namria-national-mapping-authority",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#namria-national-mapping-authority",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "NAMRIA: National Mapping Authority",
    "text": "NAMRIA: National Mapping Authority\n\n\nNational Mapping and Resource Information Authority\nRole:\n\nOfficial mapping agency\nAuthoritative geospatial data\nTopographic maps\nHazard maps\nLand cover datasets\n\n\n\n\nWebsite: https://www.geoportal.gov.ph\n\nNAMRIA is the national mapping agency responsible for topographic maps, hydrographic data, and authoritative geospatial datasets."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#namria-geoportal",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#namria-geoportal",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "NAMRIA Geoportal",
    "text": "NAMRIA Geoportal\n\nOne-Stop Shop for Philippine Geospatial Data\n\nNational basemaps (1:50,000 scale)\nAdministrative boundaries\nTopographic maps\nThematic maps\nDownloadable shapefiles and rasters\n\n\nThe NAMRIA Geoportal is the go-to source for official Philippine spatial data including boundaries, topographic maps, and land cover data."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#land-cover-mapping-project",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#land-cover-mapping-project",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Land Cover Mapping Project",
    "text": "Land Cover Mapping Project\n\n\n\nLatest: 2020 National Land Cover\nClasses:\n\nForest types\nAgriculture\nBuilt-up areas\nWater bodies\nWetlands\nBarren land\n\n\nData Formats:\n\nShapefile\nGeoTIFF\nCSV\nGeoJSON\nKML\nPNG\n\n\nPortal: https://land-cover-mapping-project-namria.hub.arcgis.com\n\nNAMRIA’s land cover datasets are valuable for validation and comparison with satellite-derived classifications. The 2020 national land cover map is available in multiple formats."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#hazardhunterph-portal",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#hazardhunterph-portal",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "HazardHunterPH Portal",
    "text": "HazardHunterPH Portal\n\nComprehensive Hazard Assessment Platform\n\n\nHazard Types:\n\nEarthquake-induced hazards\nActive fault lines\nTsunami susceptibility\nLiquefaction zones\nLandslide hazards\n\n\nApplications:\n\nDisaster risk assessment\nLand use planning\nInfrastructure siting\nEmergency preparedness\n\n\nURL: https://hazardhunter.georisk.gov.ph/map\n\nHazardHunterPH provides critical hazard data for DRR planning. These maps can be overlaid with satellite data for vulnerability assessments."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#how-namria-complements-sentinel-data",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#how-namria-complements-sentinel-data",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "How NAMRIA Complements Sentinel Data",
    "text": "How NAMRIA Complements Sentinel Data\n\n\nSentinel Imagery Provides:\n\nCurrent conditions\nFrequent updates\nLarge-area coverage\nMulti-temporal analysis\n\n\nNAMRIA Data Provides:\n\nGround truth for validation\nTraining labels for ML models\nHistorical baselines\nOfficial classifications\nHazard context\n\n\n\n\n\n\n\n\nMICRO-EDIT: Using NAMRIA/Space+ as Label Sources\n\n\nFor Day 2 Palawan RF lab: 1. Download NAMRIA land cover shapefile (authoritative classes) 2. Overlay on Sentinel-2 imagery 3. Extract training points per class (forest, agriculture, water, etc.) 4. Train Random Forest classifier 5. Validate predictions against NAMRIA hold-out samples\nSpace+ Dashboard also provides admin boundaries and infrastructure layers for context in all visualizations.\n\n\n\n\nExample: Use Sentinel-2 to map current land cover, validate against NAMRIA’s official 2020 map, detect changes\n\n\nMICRO-EDIT APPLIED: Explicit link between NAMRIA/Space+ as label sources & validation layers, not just context.\nNAMRIA data provides essential ground truth and context for satellite-based analysis. Official boundaries and hazard maps enhance satellite data interpretation.\nShow one example overlay: NAMRIA training polygons (land cover classes) overlaid on S2 RGB composite."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#dost-asti-overview",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#dost-asti-overview",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "DOST-ASTI Overview",
    "text": "DOST-ASTI Overview\n\n\nAdvanced Science and Technology Institute\n\nLead agency for EO and AI R&D\nRemote sensing expertise\nMachine learning development\nNational AI infrastructure\nP2.6 billion investment until 2028\n\n\n\n\nWebsite: https://asti.dost.gov.ph\n\nDOST-ASTI leads several projects at the intersection of remote sensing, AI, and big data, with significant national investment of P2.6 billion through 2028."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#datos-remote-sensing-help-desk",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#datos-remote-sensing-help-desk",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "DATOS: Remote Sensing Help Desk",
    "text": "DATOS: Remote Sensing Help Desk\n\n\nRemote Sensing and Data Science Help Desk\nRapid analytics during disasters\nFlood mapping from satellite imagery\nDamage assessment\nCrop mapping (rice, sugarcane)\nRoad network detection\nSupports emergency response agencies\n\n\nDuring past typhoons, DATOS used satellite images to produce flood maps and give them to disaster response agencies within hours. They also worked on crop mapping and infrastructure detection using AI."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#skai-pinas-programme",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#skai-pinas-programme",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "SkAI-Pinas Programme",
    "text": "SkAI-Pinas Programme\n\n\n\nPhilippine Sky AI Program (2021-2028)\n\nFlagship AI R&D programme\nPart of P2.6B DOST AI investment\nDemocratize AI across Philippines\nRemote sensing & big data focus\n\n\nImpact (2025)\n\n300+ institutions supported\nUniversities & colleges\nSMEs & research teams\nLocal government units\n\n\n\nTiming: 3 minutes\nKey Points: - 2025 Update: SkAI-Pinas is part of DOST’s P2.6 billion AI investment (2024-2028) - Focuses on democratizing AI for EO applications - Provides infrastructure, training, and support - Successfully engaged 300+ institutions nationwide - Critical for building national AI capacity\nExamples: - Universities using SkAI for flood mapping research - LGUs leveraging AI for disaster preparedness - SMEs building EO-based agricultural solutions\nTransition: “SkAI provides the platform. DIMER provides the models. PANDA brings it all together…”"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#dimer-ai-model-hub",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#dimer-ai-model-hub",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "DIMER: AI Model Hub",
    "text": "DIMER: AI Model Hub\n\n\n\nDemocratized Intelligent Model Exchange Repository\n\nDigital “model store”\nPre-trained AI models\nReady-to-use\nFilipino-specific challenges\n\n\nAvailable Models\n\nLandslide detection\nTraffic surveys\nCrop monitoring\nLand cover classification\nFlood detection\n\n\n\nDIMER lowers barriers to AI by enabling end-users to reuse optimized AI models. If you need a model for land cover or flood detection, check DIMER first before building from scratch."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#aipi-ai-processing-interface",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#aipi-ai-processing-interface",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "AIPI: AI Processing Interface",
    "text": "AIPI: AI Processing Interface\n\n\nPurpose\n\nStreamline large-scale remote sensing tasks\nReduce computational barriers\nRun AI models on ASTI servers\nProcess hundreds of images efficiently\n\n\n\n\n\nExample: Apply an AI model to 100 Sentinel-2 images over entire region without your laptop\n\n\nAIPI allows users to run large computations on ASTI’s servers without heavy local processing. This is especially valuable when processing many satellite images with AI models."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#alam-automated-labeling-machine",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#alam-automated-labeling-machine",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "ALaM: Automated Labeling Machine",
    "text": "ALaM: Automated Labeling Machine\n\n\nChallenge\nCreating labeled training data is:\n\nTime-consuming\nExpensive\nRequires expertise\nMajor bottleneck for AI\n\n\nALaM Solution\n\nAutomate labeling process\nCrowdsourcing capabilities\nExpert validation\nBuild training datasets\n\n\n\nResult: Faster creation of high-quality training data for Filipino contexts\n\n\nCreating labeled data is a big challenge in EO AI. ALaM tries to address this through automation and crowdsourcing, making it easier to build training datasets for Philippine applications."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#how-dost-asti-tools-work-together",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#how-dost-asti-tools-work-together",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "How DOST-ASTI Tools Work Together",
    "text": "How DOST-ASTI Tools Work Together\n\n\nALaM creates training data\nTrain models and share via DIMER\nProcess large datasets with AIPI\nDeploy for operational use via SkAI-Pinas\nSupport disaster response through DATOS\n\n\nThese initiatives work together to create a complete ecosystem from data labeling through model development to operational deployment."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#pagasa-weather-climate-data",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#pagasa-weather-climate-data",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "PAGASA: Weather & Climate Data",
    "text": "PAGASA: Weather & Climate Data\n\n\nPhilippine Atmospheric, Geophysical and Astronomical Services Administration\nData Types:\n\nHistorical rainfall\nTemperature records\nTyphoon tracks\nClimate forecasts\nWeather observations\n\n\n\n\n\nIntegration: Combine with satellite data for climate analysis\n\n\nPAGASA provides meteorological and climate data that can be integrated with satellite observations for comprehensive analysis. Rainfall data + flood maps, for example."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#synergy-satellite-ground-data",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#synergy-satellite-ground-data",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Synergy: Satellite + Ground Data",
    "text": "Synergy: Satellite + Ground Data\n\n\nSatellite Data (Sentinel)\n\nSpatial coverage\nConsistent acquisition\nMultiple variables\nTime series\n\n\nGround Data (Philippine agencies)\n\nPoint validation\nGround truth\nMeteorological context\nLocal expertise\n\n\n\nCombined = More robust analysis and higher confidence\n\n\nThe Philippines is developing a rich EO ecosystem. The goal of CoPhil is to ensure you can leverage both international (Sentinel) and national data together effectively."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#cophil-mirror-site",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#cophil-mirror-site",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "CoPhil Mirror Site",
    "text": "CoPhil Mirror Site\n\n\nPhilippines-based Copernicus data repository\nLocal mirror of Sentinel data\nFocus on Philippine region\nFaster access (no international bandwidth)\nReliable availability\nOperational by 2025\nHosted by PhilSA with CloudFerro support\n\n\nThe CoPhil Mirror Site will locally store Sentinel data focused on the Philippines, enabling much faster downloads than from European servers."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#digital-space-campus",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#digital-space-campus",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Digital Space Campus",
    "text": "Digital Space Campus\n\n\n\nPurpose\n\nOnline learning portal\nTraining materials repository\nSelf-paced learning\nCommunity of practice\nKnowledge sharing\n\n\nContent\n\nCourse presentations\nJupyter notebooks\nDatasets\nGuides & tutorials\nForum discussions\n\n\n\nAll materials from this 4-day training will be available on the Digital Space Campus for future reference and for colleagues who couldn’t attend."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#building-a-sustainable-eo-ecosystem",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#building-a-sustainable-eo-ecosystem",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Building a Sustainable EO Ecosystem",
    "text": "Building a Sustainable EO Ecosystem\n\nData Access: Mirror Site + Data Space Ecosystem\nProcessing: COARE + AIPI + Google Earth Engine\nModels: DIMER repository\nTraining: Digital Space Campus\nOperations: DATOS + Agency integration\nCommunity: SkAI-Pinas network\n\n\nResult: Complete infrastructure for operational EO AI/ML**\n\n\nCoPhil is building sustainable infrastructure ensuring you can continue working with Sentinel data and AI/ML tools after this training, without starting from scratch."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#key-takeaways",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#key-takeaways",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nCopernicus provides free, high-quality satellite data globally\nSentinel-1 (SAR) works day/night, all-weather - essential for tropics\nSentinel-2 (optical) provides rich spectral information - 5 day revisit\nMultiple access methods available (Data Space, GEE, Mirror Site)\nPhilippine agencies provide complementary data and expertise\nCoPhil infrastructure supports sustainable capacity building\n\n\nYou now understand the full picture from data sources through national infrastructure. This foundation supports all subsequent AI/ML work."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#how-it-all-connects",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#how-it-all-connects",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "How It All Connects",
    "text": "How It All Connects\n\n\nSatellite data + Philippine ground truth + AI models + processing infrastructure = Operational applications for DRR, CCA, and NRM"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#example-workflow-flood-mapping",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#example-workflow-flood-mapping",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Example Workflow: Flood Mapping",
    "text": "Example Workflow: Flood Mapping\n\nAcquire Sentinel-1 SAR data (GEE or Mirror Site)\nValidate with PAGASA rainfall data\nProcess using AI model from DIMER\nScale processing via AIPI\nCombine with NAMRIA hazard maps\nDeliver to NDRRMC via DATOS\n\n\nThis is what integrated EO capacity looks like!\n\n\nA real-world workflow leverages multiple data sources, AI tools, and institutional partnerships. This is exactly what CoPhil aims to enable."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#operational-cautions",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#operational-cautions",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Operational Cautions",
    "text": "Operational Cautions\n\n\n\n\n\n\n“Don’t Do This” - Common Pitfalls\n\n\nQuality & Governance additions to prevent common mistakes:\n\n\n\nModel Applicability: - ❌ Don’t apply a Palawan RF land cover model to Mindanao without re-sampling - Why: Different climate, vegetation types, seasonality - Do: Collect local training samples from Mindanao - ❌ Don’t use un-calibrated SAR (digital numbers only) - Why: Meaningless for quantitative analysis - Do: Always calibrate to σ⁰ or γ⁰ in dB\nProcessing Assumptions: - ❌ Don’t skip terrain correction (RTC) for SAR in mountainous areas - Why: Topographic distortions create false changes - Do: Apply RTC using SRTM or better DEM - ❌ Don’t mix Sentinel-2 processing baselines without harmonization - Why: DN offset of 1000 after Jan 2022 breaks time series - Do: Use HARMONIZED collections in GEE\nEvaluation: - ❌ Don’t train and test on the same tile/province - Why: Inflated accuracy, poor generalization - Do: Geographic hold-out (different tiles/provinces for test)\n\nP2 IMPROVEMENT APPLIED: Operational cautions slide with “don’t do this” examples.\nThese are real mistakes participants WILL make without explicit warnings. One slide can save hours of debugging and poor results."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#pre-flight-checklist",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#pre-flight-checklist",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Pre-Flight Checklist",
    "text": "Pre-Flight Checklist\n\n\n\n\n\n\nP1 IMPROVEMENT: Before Day 1 Hands-On Sessions\n\n\nSend this checklist 1 week before training:\n\n\n\n✅ Accounts & Access: - [ ] Google Earth Engine account enabled (signup.earthengine.google.com) - [ ] Google Drive with ≥5 GB free space - [ ] Google Colab tested (login with same Google account) - [ ] CoPhil Infrastructure registration (application.infra.copphil.philsa.gov.ph)\n✅ Software & Data: - [ ] Downloaded sample vector/raster bundle (link provided via email) - [ ] Confirmed zip file extracts correctly - [ ] Python 3.8+ installed (if working locally) - [ ] Jupyter notebook tested (if working locally)\n✅ Troubleshooting Contacts: - [ ] Have training support email/chat details - [ ] Know how to access Digital Space Campus materials\n\nIf any issues: Contact training organizers BEFORE Day 1 to resolve access problems!\n\n\nP1 IMPROVEMENT APPLIED: Pre-flight checklist (send 1 week before).\nAction Item for Organizers: - Create downloadable “sample bundle” (small GeoPackage + COG) - Set up support email/chat channel - Send checklist email 7 days before training - Follow up 2 days before to confirm readiness"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#whats-next-in-day-1",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#whats-next-in-day-1",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "What’s Next in Day 1?",
    "text": "What’s Next in Day 1?\n\n\nSession 2 (Next)\n\nAI/ML fundamentals\nSupervised vs unsupervised learning\nNeural networks basics\nData-centric AI\n→ 5-min Concept Check (3 questions)\n\n\nSessions 3 & 4\n\nHands-on Python (GeoPandas, Rasterio)\n\nPre-run pip installs to save time\n\nGoogle Earth Engine tutorial\n\nStart from ready script, modify filters only\n\nAccess real Sentinel data\nSCL cloud/shadow masking (not QA60)\n\n\n\nNow that you understand where data comes from, we’ll learn how AI/ML can extract information from these data, then get hands-on with actual satellite imagery.\nP1 IMPROVEMENTS NOTED: - Concept check after Session 2 - Pre-run installations in Session 3 - Ready GEE script in Session 4 (modify filters live, don’t type from scratch)"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#session-summary",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#session-summary",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Session Summary",
    "text": "Session Summary\nWhat We Covered:\n✅ Copernicus Programme & free/open data policy\n✅ Sentinel-1 (SAR) - all-weather monitoring\n✅ Sentinel-2 (Optical) - 13 bands, 10m resolution\n✅ 2025 Updates: 1C & 2C operational\n✅ Philippine EO infrastructure (PhilSA, NAMRIA, DOST-ASTI)\n✅ Data access platforms (SIYASAT, Geoportal, SkAI-Pinas)\n\nTiming: 2 minutes\nKey Takeaways: - You now understand WHERE EO data comes from - You know WHAT satellites provide what data - You know HOW to access data (both EU and PH platforms) - Ready for Session 2: Learning WHAT to do with this data using AI/ML"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#qa",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#qa",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Q&A",
    "text": "Q&A\n\n\nCopernicus & Sentinel\n\nMission specifications?\nData products & formats?\nAccess methods?\nProcessing levels?\n\n\nPhilippine EO Ecosystem\n\nAgency roles & mandates?\nData access procedures?\nIntegration with Copernicus?\nP2.6B AI investment details?\n\n\n\nTiming: 3-5 minutes for Q&A\nFacilitation Tips: - Encourage questions from different agency participants - Share knowledge across government, academic, private sectors - If technical question about access, note for Session 4 (GEE hands-on) - Keep responses brief - detailed hands-on coming in Sessions 3-4\nCommon Questions & Answers: - “How do I sign up for GEE?” → Will cover in Session 4 - “Which satellite should I use?” → Depends on application (Day 2 topic) - “Is SIYASAT public?” → Registration required, priority for gov agencies - “Can I combine Sentinel-1 and -2?” → Yes! Synergistic use cases discussed today"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#core-concepts-of-aiml-for-earth-observation",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#core-concepts-of-aiml-for-earth-observation",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Core Concepts of AI/ML for Earth Observation",
    "text": "Core Concepts of AI/ML for Earth Observation\nComing up after break:\n\nWhat is AI/ML and why for EO?\nThe EO AI/ML workflow\nSupervised vs unsupervised learning\nIntroduction to deep learning\nData-centric AI approaches\n\n\nSee you in Session 2! 🚀"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#contact-resources",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#contact-resources",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Contact & Resources",
    "text": "Contact & Resources\nEuropean Platforms:\nCopernicus Data Space: https://dataspace.copernicus.eu\nCoPhil Programme: https://www.cophil.eu\nPhilippine Platforms:\nPhilSA: https://philsa.gov.ph\nNAMRIA Geoportal: https://www.geoportal.gov.ph\nDOST-ASTI: https://asti.dost.gov.ph\n\nSession 1 Complete!\nTotal timing: ~120 minutes including breaks and Q&A\nBefore Session 2: - Short bio break (5-10 minutes) - Check that all participants are ready - Ensure presentation materials for Session 2 are loaded"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#session-objectives",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#session-objectives",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Session Objectives",
    "text": "Session Objectives\n\nUnderstand what AI/ML means in Earth Observation context\nLearn the end-to-end workflow for ML projects\nDistinguish supervised vs unsupervised learning\nGrasp deep learning and neural network basics\nExplore 2025 AI innovations (foundation models, data-centric AI)\n\n\nDuration: 2 hours\n\n\nThis session covers fundamental AI/ML concepts tailored to EO applications. Mostly conceptual, but essential foundation for hands-on work in Sessions 3-4 and throughout Day 2-4."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#session-roadmap",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#session-roadmap",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Session Roadmap",
    "text": "Session Roadmap\n\n\n\nTime\nTopic\nDuration\n\n\n\n\n00-10 min\nWhat is AI/ML?\n10 min\n\n\n10-35 min\nEO Workflow & Data Pipeline\n25 min\n\n\n35-60 min\nSupervised vs Unsupervised Learning\n25 min\n\n\n60-65 min\n☕ Break\n5 min\n\n\n65-90 min\nDeep Learning & Neural Networks\n25 min\n\n\n90-110 min\nData-Centric AI & 2025 Updates\n20 min\n\n\n110-120 min\nQ&A & Summary\n10 min\n\n\n\n\nTiming: 2 minutes\nThis session is more conceptual than Session 1. Focus on building intuition and mental models. Hands-on practice comes in Sessions 3-4 and Days 2-4."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-aiml-for-earth-observation",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-aiml-for-earth-observation",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Why AI/ML for Earth Observation?",
    "text": "Why AI/ML for Earth Observation?\n\n\nTraditional Approach\n\nManual interpretation\nRule-based classification\nSimple thresholds\nTime-consuming\nHard to scale\n\n\nAI/ML Approach\n\nAutomated pattern recognition\nLearn from examples\nComplex decision boundaries\nFast processing\nScalable to large areas\n\n\n\nML can process years of satellite data in hours!\n\n\nMachine learning allows us to automatically recognize patterns in satellite imagery without hard-coding rules for every scenario. This is transformative for large-area, time-series analysis."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#defining-the-terms",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#defining-the-terms",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Defining the Terms",
    "text": "Defining the Terms\n\nAI, ML, and Deep Learning Hierarchy with EO Applications\nArtificial Intelligence (AI): Broad field of making machines “smart”\nMachine Learning (ML): Subset of AI where algorithms learn from data\nDeep Learning (DL): Subset of ML using neural networks with many layers\n\n\nAI is the broadest term. Machine Learning is a subset where computer algorithms learn patterns from data without being explicitly programmed. Deep Learning uses neural networks. This comprehensive diagram shows the nested hierarchy with specific algorithms at each level (Random Forest, CNNs, RNNs, etc.) and their Earth Observation applications like land cover classification and time series analysis."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#machine-learning-in-simple-terms",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#machine-learning-in-simple-terms",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Machine Learning in Simple Terms",
    "text": "Machine Learning in Simple Terms\n\n\nTraditional Programming\nRules + Data → Results\n\nProgrammer writes explicit rules\nFixed logic\nHard to handle complexity\n\n\nMachine Learning\nData + Results → Rules\n\nAlgorithm learns rules from examples\nAdaptive\nHandles complex patterns\n\n\n\nIn traditional programming, we tell computers what to do step-by-step. In ML, we show examples and the algorithm figures out the pattern."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#ml-in-earth-observation-context",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#ml-in-earth-observation-context",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "ML in Earth Observation Context",
    "text": "ML in Earth Observation Context\n\n\nExample: Forest vs Non-Forest\nTraditional:\nIF NDVI &gt; 0.6 THEN Forest\nELSE Non-Forest\nSimple, but breaks easily\n\nMachine Learning:\n\nShow 1000 examples of forest pixels\nShow 1000 examples of non-forest\nAlgorithm learns complex patterns\nWorks in diverse conditions\n\n\n\n\nA simple NDVI threshold might work in one region but fail in another. ML can learn the nuanced patterns that distinguish forest from non-forest across different conditions."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#end-to-end-ml-workflow",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#end-to-end-ml-workflow",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "End-to-End ML Workflow",
    "text": "End-to-End ML Workflow\n\n\nThis is the typical workflow for any ML project in Earth Observation. Understanding these steps is crucial for successful implementation."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-1-problem-definition",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-1-problem-definition",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 1: Problem Definition",
    "text": "Step 1: Problem Definition\n\n\nKey Questions\n\nWhat exactly are we trying to achieve?\nWhat decisions will this support?\nWhat level of accuracy is needed?\nWhat resources are available?\n\n\nEO Examples\n\nMap rice paddy extent\nDetect flooded areas after typhoon\nClassify land cover types\nEstimate crop yield\nMonitor deforestation\n\n\n\nClear problem definition = 50% of success\n\n\nBeing clear on the question helps design the solution. “We want to classify land cover in Palawan” is much more actionable than “We want to use AI.”"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-2-data-acquisition",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-2-data-acquisition",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 2: Data Acquisition",
    "text": "Step 2: Data Acquisition\n\n\nSatellite Imagery\n\nSentinel-1/2 (covered in Session 1!)\nLandsat\nPlanet\nHigh-resolution commercial\nMultiple dates/seasons\n\n\nGround Truth / Labels\n\nField surveys\nGPS points\nExisting maps\nPhoto interpretation\nExpert knowledge\n\n\n\nChallenge: Getting quality labels is often hardest part\n\n\nData acquisition includes both satellite images and the ground truth labels needed to train supervised models. The quality and quantity of labels directly impact model performance."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-3-data-preprocessing",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-3-data-preprocessing",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 3: Data Preprocessing",
    "text": "Step 3: Data Preprocessing\nFor Satellite Imagery:\n\nAtmospheric correction (use Level-2A!)\nCloud masking\nGeometric correction\nRadiometric calibration\nCo-registration (multiple sensors)\nTemporal compositing\n\n\n“Garbage In, Garbage Out” - preprocessing matters!\n\n\nWell-prepared input data is crucial. Even the best model will fail if fed cloudy, misaligned, or uncorrected images."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#preprocessing-example",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#preprocessing-example",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Preprocessing Example",
    "text": "Preprocessing Example\n\nCloud Removal Before and After Comparison\n\nBefore Preprocessing: - Clouds present - Atmospheric haze - Different acquisition dates\n\nAfter Preprocessing: - Clouds masked - Atmospherically corrected - Temporal composite created\n\n\nPreprocessing transforms raw satellite data into analysis-ready products. This side-by-side comparison shows the dramatic improvement from cloud masking and creating temporal composites - essential steps before any ML analysis."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-4-feature-engineering",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-4-feature-engineering",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 4: Feature Engineering",
    "text": "Step 4: Feature Engineering\n\n\nWhat are Features?\n\nInput variables for the model\nDerived from raw data\nInformative for the task\n\n\nEO Features\n\nSpectral bands (Blue, Red, NIR, etc.)\nSpectral indices (NDVI, NDWI)\nTexture measures\nTemporal statistics\nTopography (elevation, slope)\n\n\n\nDeep Learning: Often learns features automatically!\n\n\nFor traditional ML like Random Forest, we engineer features. For deep learning, the network learns features automatically from raw pixels."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#common-eo-features",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#common-eo-features",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Common EO Features",
    "text": "Common EO Features\n\n\n\n\n\n\n\n\nFeature Type\nExamples\nWhat They Capture\n\n\n\n\nSpectral Bands\nB2, B3, B4, B8\nReflectance at different wavelengths\n\n\nVegetation Indices\nNDVI, EVI, SAVI\nVegetation health, density\n\n\nWater Indices\nNDWI, MNDWI\nWater presence, moisture\n\n\nTexture\nGLCM variance, entropy\nSpatial patterns\n\n\nTemporal\nMean, std over time\nPhenology, seasonality\n\n\nTopographic\nElevation, slope, aspect\nTerrain characteristics\n\n\n\n\nDifferent features highlight different aspects of the landscape. Vegetation indices emphasize green biomass, water indices highlight water bodies, etc."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-5-model-selection-training",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-5-model-selection-training",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 5: Model Selection & Training",
    "text": "Step 5: Model Selection & Training\n\n\nModel Selection\nChoose based on:\n\nProblem type (classification vs regression)\nData size\nInterpretability needs\nComputational resources\n\n\nCommon EO Models\n\nRandom Forest\nSupport Vector Machines\nConvolutional Neural Networks\nU-Net (segmentation)\nRecurrent networks (time series)\n\n\n\nModel choice depends on your specific problem, available data, and resources. We’ll cover Random Forest on Day 2 and CNNs on Day 3."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#training-process",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#training-process",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Training Process",
    "text": "Training Process\n\n\nSplit data: Training set (70-80%) & Validation set (20-30%)\nFeed training data to model\nModel learns patterns by adjusting internal parameters\nValidate on unseen validation data\nIterate: Adjust model or data if needed\n\n\nTraining involves feeding labeled examples to the model. The model adjusts its internal parameters to minimize errors on the training data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-6-validation-evaluation",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-6-validation-evaluation",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 6: Validation & Evaluation",
    "text": "Step 6: Validation & Evaluation\n\n\nWhy Validate?\n\nEnsure model generalizes\nDetect overfitting\nCompare different models\nBuild confidence\n\n\nEvaluation Metrics\n\nOverall Accuracy\nConfusion Matrix\nPrecision & Recall\nF1-Score\nKappa coefficient\n\n\n\nUse independent test data - never validate on training data!\n\n\nRigorous validation using held-out data ensures the model works on new, unseen examples - not just memorizing training data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#confusion-matrix-example",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#confusion-matrix-example",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Confusion Matrix Example",
    "text": "Confusion Matrix Example\n\n\n\nWhat it shows:\n\nTrue Positives (correct predictions)\nFalse Positives (type I error)\nFalse Negatives (type II error)\nTrue Negatives\n\n\nDerived Metrics:\n\nPrecision = TP / (TP + FP)\nRecall = TP / (TP + FN)\nAccuracy = (TP + TN) / Total\n\n\n\nThe confusion matrix shows where your model is making mistakes. This helps identify which classes are being confused."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-7-deployment",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-7-deployment",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 7: Deployment",
    "text": "Step 7: Deployment\n\n\nDeployment Options\n\nGenerate full maps\nNear real-time monitoring\nOperational pipelines\nDecision support systems\nWeb applications\n\n\nConsiderations\n\nModel retraining schedule\nComputational requirements\nUser interface\nData updates\nMaintenance plan\n\n\n\nIf the model is satisfactory, deploy it for operational use. This might mean generating maps for entire regions or setting up automatic processing of new satellite images."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#workflow-is-iterative",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#workflow-is-iterative",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Workflow is Iterative",
    "text": "Workflow is Iterative\n\n\nPoor validation? → Go back to data acquisition or model selection\nNew data available? → Retrain model\nRequirements change? → Redefine problem\nContinuous improvement is key\n\n\nReal projects are iterative. You often loop back: if validation is poor, you might need more data, different features, or a different model."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#main-ml-paradigms",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#main-ml-paradigms",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Main ML Paradigms",
    "text": "Main ML Paradigms\n\n\nSupervised Learning (most common in EO)\nUnsupervised Learning (exploratory analysis)\nSemi-supervised Learning (combines both)\nReinforcement Learning (less common in EO)\n\n\nWe’ll focus on supervised and unsupervised learning as these are most relevant for Earth Observation applications."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-is-supervised-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-is-supervised-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "What is Supervised Learning?",
    "text": "What is Supervised Learning?\n\n\nDefinition\n\nLearning from labeled data\nKnown input-output pairs\nModel learns mapping from inputs to outputs\nLike learning with an answer key\n\n\n\n\n\nRequires ground truth labels for training\n\n\nSupervised learning is the most common in EO. The algorithm is given examples with known outcomes (labels) and learns to predict labels for new, unseen data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#two-types-of-supervised-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#two-types-of-supervised-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Two Types of Supervised Learning",
    "text": "Two Types of Supervised Learning\n\n\nClassification\n\nPredict categorical labels\nDiscrete classes\nExample outputs: “Forest”, “Water”, “Urban”\n\n\n\nRegression\n\nPredict continuous values\nNumeric outputs\nExample outputs: 25.3 tons/hectare, 15.2°C\n\n\n\n\nClassification assigns data to categories. Regression predicts numeric values. Both require labeled training data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#classification-examples-in-eo",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#classification-examples-in-eo",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Classification Examples in EO",
    "text": "Classification Examples in EO\n\n\nLand Cover Classification\n\n\nForest, agriculture, urban, water\nPixel-wise or object-based\nMulti-class problem\n\n\nCrop Type Mapping\n\n\nRice, corn, sugarcane\nSeasonal patterns important\nSupports agricultural planning\n\n\n\nLand cover classification is the classic EO supervised learning task. Each pixel or region is assigned to a class like forest, water, or urban."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#regression-examples-in-eo",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#regression-examples-in-eo",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Regression Examples in EO",
    "text": "Regression Examples in EO\n\n\nBiomass Estimation\n\n\nPredict tons of biomass per hectare\nImportant for carbon accounting\nUses SAR and optical data\n\n\nCrop Yield Prediction\n\n\nPredict tons per hectare\nSeasonal NDVI time series\nSupports food security planning\n\n\n\nRegression tasks predict continuous values like biomass density, crop yield, soil moisture, or sea surface temperature from satellite data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#common-supervised-algorithms",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#common-supervised-algorithms",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Common Supervised Algorithms",
    "text": "Common Supervised Algorithms\n\n\n\n\n\n\n\n\nAlgorithm\nStrengths\nEO Applications\n\n\n\n\nRandom Forest\nHandles high dimensions, robust\nLand cover, crop classification\n\n\nSVM\nEffective in high dimensions\nBinary classification, change detection\n\n\nNeural Networks\nLearns complex patterns\nImage classification, segmentation\n\n\nDecision Trees\nInterpretable\nQuick classifications\n\n\nk-NN\nSimple, non-parametric\nLocal classifications\n\n\n\n\nDifferent algorithms have different strengths. Random Forest is popular in EO for its robustness and ability to handle many features."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#supervised-learning-requirements",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#supervised-learning-requirements",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Supervised Learning Requirements",
    "text": "Supervised Learning Requirements\nEssential:\n\nTraining data with known labels\nRepresentative samples covering all classes\nSufficient quantity (varies by algorithm)\nQuality labels (accurate, consistent)\nIndependent validation data\n\n\nChallenge: Getting quality labels is often the bottleneck!\n\n\nSupervised learning needs ground truth. For land cover, this might be field surveys, GPS points, or careful photo interpretation. Quality matters more than quantity!"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-is-unsupervised-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-is-unsupervised-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "What is Unsupervised Learning?",
    "text": "What is Unsupervised Learning?\n\n\nDefinition\n\nLearning from unlabeled data\nNo known outputs\nDiscover hidden patterns\nLike sorting without instructions\n\n\n\n\n\nUseful for exploratory analysis and finding structure\n\n\nUnsupervised learning finds patterns or groupings inherent in the data without being told what to look for."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#clustering-main-unsupervised-technique",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#clustering-main-unsupervised-technique",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Clustering: Main Unsupervised Technique",
    "text": "Clustering: Main Unsupervised Technique\n\n\nGroup similar pixels based on spectral characteristics\nAlgorithm decides number of clusters (or you specify)\nAnalyst interprets what each cluster means\nExample: “Cluster 3 looks like water, Cluster 7 looks like forest”\n\n\nK-means clustering is a common unsupervised method. It groups pixels with similar reflectance, but you have to interpret what those groups mean."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#unsupervised-eo-applications",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#unsupervised-eo-applications",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Unsupervised EO Applications",
    "text": "Unsupervised EO Applications\n\n\nChange Detection\n\nCluster “before” and “after” images\nIdentify changed areas\nNo labels needed\n\nAnomaly Detection\n\nFind unusual pixels\nPotential forest disturbance\nData quality issues\n\n\nInitial Exploration\n\nQuick overview of spectral classes\nInform supervised approach\nGenerate training samples\n\nDimensionality Reduction\n\nPCA, t-SNE\nVisualize high-dimensional data\nFeature extraction\n\n\n\nUnsupervised methods are useful for quick initial analysis or when you don’t have ground truth labels. Results need interpretation though."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#supervised-vs-unsupervised",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#supervised-vs-unsupervised",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Supervised vs Unsupervised",
    "text": "Supervised vs Unsupervised\n\n\n\n\n\n\n\n\nAspect\nSupervised\nUnsupervised\n\n\n\n\nLabels\nRequired\nNot needed\n\n\nAccuracy\nGenerally higher\nLower, needs interpretation\n\n\nUse Case\nPrecise classification\nExploration, pattern discovery\n\n\nEffort\nHigh (collecting labels)\nLow (no labels)\n\n\nOutput\nPredefined classes\nDiscovered clusters\n\n\nControl\nHigh (you define classes)\nLow (algorithm decides groups)\n\n\n\n\nSupervised methods generally yield more accurate results when good training data is available. Unsupervised is useful when labels are unavailable or for exploratory work."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#which-to-choose",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#which-to-choose",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Which to Choose?",
    "text": "Which to Choose?\n\n\nUse Supervised When:\n\nYou have ground truth labels\nNeed specific classes\nAccuracy is critical\nOperational application\n\n\nUse Unsupervised When:\n\nNo labels available\nExploratory analysis\nDiscovering unknown patterns\nQuick initial assessment\n\n\n\nIn practice: Often combine both approaches!\n\n\nMost operational EO applications use supervised learning because accuracy and specific class definitions are important. Unsupervised helps with initial exploration."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#minute-break",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#minute-break",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "☕ 5-Minute Break",
    "text": "☕ 5-Minute Break\n\nStretch Break\nStand up • Grab water • Back in 5 minutes\n\n\nTiming: 5 minutes\nInstructor Actions: - Announce break - Mention we’ll dive into deep learning next - Be available for quick questions\nWhen Resuming: - Quick recap: “We’ve covered ML basics, workflow, supervised/unsupervised. Now: deep learning!”"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-is-deep-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-is-deep-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "What is Deep Learning?",
    "text": "What is Deep Learning?\n\n\nDeep Learning = Neural Networks with Many Layers\n\nSubset of machine learning\n“Deep” refers to multiple layers\nAutomatically learns features\nExcels at image analysis\nData-hungry\n\n\n\n\n\nDeep learning is essentially about neural networks with many layers (dozens or even hundreds). These “deep” networks can capture very complex relationships."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#neural-networks-building-blocks",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#neural-networks-building-blocks",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Neural Networks: Building Blocks",
    "text": "Neural Networks: Building Blocks\n\nArtificial Neuron:\n\nTakes multiple inputs\nMultiplies each by a weight\nAdds a bias\nApplies activation function\nProduces output\n\n\nA single neuron is like a logistic regression unit. It takes inputs, applies weights, and uses an activation function to produce an output."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#neural-network-architecture",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#neural-network-architecture",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Neural Network Architecture",
    "text": "Neural Network Architecture\n\n\n\nLayers:\n\nInput Layer: Receives data (e.g., pixel values)\nHidden Layers: Process and transform\nOutput Layer: Final prediction\n\n\nConnections:\n\nEach neuron connects to next layer\nWeights on connections\nInformation flows forward\n\n\n\nNeurons are organized into layers. The input layer receives data (pixel values), hidden layers progressively extract features, and the output layer makes predictions."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#key-concepts",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#key-concepts",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Key Concepts",
    "text": "Key Concepts\nActivation Functions\n\nIntroduce non-linearity\nCommon: ReLU, Sigmoid, Tanh\nAllow network to learn complex patterns\n\nWeights and Biases\n\nParameters the network learns\nMillions of parameters in deep networks\nAdjusted during training\n\nForward Propagation\n\nData flows input → output\nGenerate prediction\n\n\nActivation functions are crucial - they allow neural networks to learn non-linear relationships. Without them, the network would just be linear regression!"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#how-neural-networks-learn",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#how-neural-networks-learn",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "How Neural Networks Learn",
    "text": "How Neural Networks Learn\n\n\nForward pass: Input data, get prediction\nCalculate loss: How wrong is the prediction?\nBackpropagation: Calculate gradients\nUpdate weights: Adjust to reduce error\nRepeat: Thousands of times (epochs)\n\n\nTraining adjusts weights to minimize error. This happens through backpropagation - computing gradients and updating weights in the direction that reduces loss."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#loss-functions",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#loss-functions",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Loss Functions",
    "text": "Loss Functions\n\n\nClassification\nCross-Entropy Loss\n\nMeasures classification error\nHigher penalty for confident wrong predictions\nStandard for multi-class problems\n\n\nRegression\nMean Squared Error\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\n\nMeasures prediction error\nSquared difference from true value\n\n\n\nLoss functions quantify “how bad” predictions are. The training process tries to minimize this loss by adjusting weights."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#optimizers",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#optimizers",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Optimizers",
    "text": "Optimizers\n\n\nStochastic Gradient Descent (SGD)\n\nBasic optimizer\nUpdates weights based on gradients\nLearning rate controls step size\n\n\nAdam Optimizer\n\nAdaptive learning rates\nFaster convergence\nMost popular for deep learning\nGenerally works well\n\n\n\nYou don’t need to implement these - frameworks do it for you!\n\n\nOptimizers determine how weights are updated. Adam is the most popular because it adapts learning rates automatically and generally converges faster than SGD."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#convolutional-neural-networks-cnns",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#convolutional-neural-networks-cnns",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Convolutional Neural Networks (CNNs)",
    "text": "Convolutional Neural Networks (CNNs)\n\nSpecialized for images:\n\nConvolutional layers: Detect spatial patterns\nPooling layers: Reduce dimensionality\nFully connected layers: Final classification\nAutomatically learn features (edges, textures, objects)\n\n\nCNNs are neural networks specialized for grid data like images. They use convolutional layers to automatically extract spatial features."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#how-cnns-process-images",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#how-cnns-process-images",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "How CNNs Process Images",
    "text": "How CNNs Process Images\n\nHierarchical Feature Learning:\n\nEarly layers: Detect edges, simple patterns\nMiddle layers: Detect textures, parts\nLater layers: Detect objects, scenes\nNo manual feature engineering needed!\n\n\nCNNs learn increasingly complex features at each layer. Early layers detect edges, later layers detect whole objects. This happens automatically during training!"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#cnns-in-earth-observation",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#cnns-in-earth-observation",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "CNNs in Earth Observation",
    "text": "CNNs in Earth Observation\n\n\nApplications:\n\nImage classification\nObject detection (ships, buildings)\nSemantic segmentation (pixel-wise)\nChange detection\nSuper-resolution\n\n\nAdvantages:\n\nLearn features automatically\nHandle spatial context\nState-of-the-art performance\nTransfer learning possible\n\n\n\n\nCNNs have achieved state-of-the-art results in many EO tasks. They can learn to recognize complex patterns without manual feature engineering."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#popular-cnn-architectures-for-eo",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#popular-cnn-architectures-for-eo",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Popular CNN Architectures for EO",
    "text": "Popular CNN Architectures for EO\n\n\n\n\n\n\n\n\n\nArchitecture\nYear\nKey Innovation\nEO Use Cases\n\n\n\n\nResNet\n2015\nResidual connections\nClassification, backbone for detection\n\n\nU-Net\n2015\nSkip connections\nSemantic segmentation, flood mapping\n\n\nEfficientNet\n2019\nCompound scaling\nEfficient classification, mobile deployment\n\n\nDeepLabv3+\n2018\nAtrous convolution\nLand cover segmentation\n\n\nYOLOv8\n2023\nReal-time detection\nObject detection, ship/vehicle counting\n\n\n\n\nResNet and EfficientNet are most popular backbones for EO\n\n\nThese are proven architectures widely used in EO. ResNet-50 is often the starting point for transfer learning. U-Net dominates semantic segmentation tasks."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#resnet-residual-networks",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#resnet-residual-networks",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "ResNet: Residual Networks",
    "text": "ResNet: Residual Networks\n\n\n\nKey Innovation: Skip Connections\n\nAllows training very deep networks (50, 101, 152 layers)\nSolves vanishing gradient problem\nIdentity mapping preserves information\n\nCommon Variants: - ResNet-50 (25M parameters) - ResNet-101 (44M parameters) - ResNet-152 (60M parameters)\n\nEO Applications:\n\nPre-trained on ImageNet\nFine-tune for EO tasks\nBackbone for object detection\nTransfer learning baseline\n\nPerformance: - Top-5 error: 3.57% (ImageNet) - Works well with 10k+ images\n\n\nResNet revolutionized deep learning by enabling training of very deep networks. Skip connections allow gradients to flow directly through the network."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#u-net-for-semantic-segmentation",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#u-net-for-semantic-segmentation",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "U-Net for Semantic Segmentation",
    "text": "U-Net for Semantic Segmentation\n\nArchitecture: - Encoder (contracting path): Captures context - Decoder (expanding path): Enables precise localization - Skip connections: Combine low & high-level features\nWhy Dominant in EO: - Works with small datasets (hundreds of images) - Precise pixel-wise predictions - Perfect for segmentation tasks\n\nEO Applications: Flood mapping, land cover, building footprints, crop fields\n\n\nU-Net is THE architecture for semantic segmentation in EO. Originally designed for biomedical image segmentation, it’s now standard for pixel-wise classification tasks."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#deep-learning-frameworks",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#deep-learning-frameworks",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Deep Learning Frameworks",
    "text": "Deep Learning Frameworks\n\n\nTensorFlow / Keras\n\n\nGoogle’s framework\nHigh-level Keras API\nProduction-ready\nLarge ecosystem\n\n\nPyTorch\n\n\nFacebook’s framework\nPythonic and intuitive\nPopular in research\nFlexible\n\n\n\nWe’ll use TensorFlow/Keras in this training\n\n\nYou don’t implement backpropagation yourself - frameworks like TensorFlow and PyTorch handle the math. You just define the architecture and provide data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#deep-learning-considerations",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#deep-learning-considerations",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Deep Learning Considerations",
    "text": "Deep Learning Considerations\n\n\nAdvantages:\n\nAutomatic feature learning\nState-of-the-art accuracy\nHandles complex patterns\nScales to big data\n\n\nChallenges:\n\nRequires lots of training data\nComputationally intensive (need GPUs)\nLess interpretable (“black box”)\nHarder to debug\n\n\n\nStart simple (Random Forest), move to DL when you have data and compute\n\n\nDeep learning is powerful but data-hungry and computationally expensive. For many EO tasks, simpler models like Random Forest work well with less data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-benchmark-datasets-matter",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-benchmark-datasets-matter",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Why Benchmark Datasets Matter",
    "text": "Why Benchmark Datasets Matter\n\nStandardized Evaluation - Compare algorithms objectively\nTraining Resources - Pre-labeled data for model training\nTransfer Learning - Pre-train on large datasets, fine-tune locally\nResearch Reproducibility - Enable comparison across studies\nCommunity Building - Shared resources accelerate progress\n\n\nYou don’t need to label everything from scratch!\n\n\nBenchmark datasets are crucial for EO ML. They provide labeled training data and enable fair comparison of methods across research groups worldwide."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#eurosat-land-cover-classification",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#eurosat-land-cover-classification",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "EuroSAT: Land Cover Classification",
    "text": "EuroSAT: Land Cover Classification\n\n\nSpecifications: - Images: 27,000 labeled patches - Classes: 10 land cover types - Size: 64×64 pixels - Bands: All 13 Sentinel-2 bands - Source: European cities\n10 Classes: Annual Crop • Forest • Herbaceous Vegetation • Highway • Industrial • Pasture • Permanent Crop • Residential • River • Sea/Lake\n\n\nAchievement: 98.57% accuracy with CNNs\nWhy Popular: - Sentinel-2 based - Balanced classes - Easy to use\n\n\nEuroSAT is one of the most popular benchmarks for EO classification. Based on Sentinel-2, making it highly relevant for operational applications. Great starting point for CNN experiments."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#bigearthnet-large-scale-multi-label",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#bigearthnet-large-scale-multi-label",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "BigEarthNet: Large-Scale Multi-Label",
    "text": "BigEarthNet: Large-Scale Multi-Label\n\n\nMassive Scale: - Images: 590,326 Sentinel-2 patches - Coverage: 10 European countries - Labels: 43 land cover classes - Multi-label: Multiple classes per image - Multi-modal: Optical + SAR version\nReal-World Complexity: - Forest + Water - Urban + Agricultural - Reflects actual landscapes\n\n\nWhy Different:\nUnlike EuroSAT (single label), BigEarthNet has multiple overlapping classes - more realistic!\nAccess: - bigearth.net - TensorFlow Datasets - Papers With Code\n\n\nBigEarthNet’s multi-label nature makes it more challenging but also more realistic. Essential for semantic segmentation research and testing advanced architectures."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#xview-object-detection-benchmark",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#xview-object-detection-benchmark",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "xView: Object Detection Benchmark",
    "text": "xView: Object Detection Benchmark\n\n\nSpecifications: - Objects: &gt;1 million annotated - Classes: 60 object types - Resolution: 0.3m (WorldView-3) - Area: &gt;1,400 km² - Annotations: Bounding boxes\nObject Categories: - Buildings & infrastructure - Vehicles (cars, trucks, aircraft) - Ships & maritime - Storage tanks - Construction equipment\n\n\nCreated for disaster response\nApplications: - YOLO training - Faster R-CNN - Small object detection - Infrastructure mapping\n\n\nxView is THE benchmark for object detection in satellite imagery. Created for disaster response applications, now widely used for testing detection algorithms like YOLO and Faster R-CNN."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#philippine-data-resources",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#philippine-data-resources",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Philippine Data Resources",
    "text": "Philippine Data Resources\n\n\nPRiSM (PhilRice) - Rice area maps (wet/dry season) - Planting dates & growth stages - Yield estimates - Since 2014 - https://prism.philrice.gov.ph/\nPhilSA Products - Flood extent maps (DATOS) - Mangrove extent mapping - Land cover classifications - Disaster damage assessments\n\nDOST-ASTI Outputs - DATOS rapid flood mapping - Hazard susceptibility maps - AI-powered damage assessment - hazardhunter.georisk.gov.ph\nNAMRIA Geoportal - National land cover (2020) - Topographic basemaps - Administrative boundaries - Digital Elevation Models - www.geoportal.gov.ph\n\n\nUse these as training/validation data - don’t start from scratch!\n\n\nPhilippine agencies have produced operational EO products that can serve as training or validation data for your ML models. Leverage existing work!"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#paradigm-shift-model-centric-vs-data-centric",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#paradigm-shift-model-centric-vs-data-centric",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Paradigm Shift: Model-Centric vs Data-Centric",
    "text": "Paradigm Shift: Model-Centric vs Data-Centric\n\n\n\nModel-Centric (Traditional)\n\nFocus on improving algorithms\nKeep data fixed\nTry different models\nTune hyperparameters\n\n\nData-Centric (Modern)\n\nFocus on improving data\nKeep model fixed\nClean and augment data\nBetter annotations\n\n\n\nA lot of early ML progress focused on model algorithms. Data-Centric AI, popularized by Andrew Ng, advocates that improving your data often yields bigger gains."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-data-centric-matters-for-eo",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-data-centric-matters-for-eo",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Why Data-Centric Matters for EO",
    "text": "Why Data-Centric Matters for EO\nEO-Specific Data Challenges:\n\nCloud contamination\nAtmospheric effects\nSensor artifacts and noise\nLabel uncertainty\nGeographic variability\nTemporal dynamics\nClass imbalance\n\n\n“Better data beats a cleverer model” in most cases\n\n\nIn EO, the “food” you feed your AI matters more than fancy model tweaks. Cloudy images, mislabeled points, or biased samples can derail any model."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#research-data-efficiency",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#research-data-efficiency",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "2025 Research: Data Efficiency",
    "text": "2025 Research: Data Efficiency\n\nKey Finding (ArXiv 2025):\n\nSome EO datasets reach optimal accuracy with &lt;20% of temporal instances\nSingle band from single modality can be sufficient\nData efficiency crucial for operational systems\nQuality over quantity\n\n\nRecent research shows you don’t always need all available data. Smart selection of temporal instances and bands can achieve similar accuracy with much less data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#four-pillars-of-data-centric-ai",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#four-pillars-of-data-centric-ai",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Four Pillars of Data-Centric AI",
    "text": "Four Pillars of Data-Centric AI\n\n\n1. Data Quality\n\n\nCloud/shadow removal\nAtmospheric correction\nSensor calibration\nGeometric accuracy\n\n\n2. Data Quantity\n\n\nSufficient training samples\nBalanced classes\nData augmentation\nTransfer learning"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#four-pillars-continued",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#four-pillars-continued",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Four Pillars (Continued)",
    "text": "Four Pillars (Continued)\n\n\n3. Data Diversity\n\n\nMultiple seasons\nDifferent regions\nVarious conditions\nClass variations\n\n\n4. Label Quality\n\n\nClear definitions\nConsistent protocols\nExpert validation\nAccurate geolocation\n\n\n\nThese four aspects - quality, quantity, diversity, and labels - determine model success more than architectural choices."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-quality-in-eo",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-quality-in-eo",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Data Quality in EO",
    "text": "Data Quality in EO\n\n\nCommon Issues:\n\nClouds and shadows\nHaze and aerosols\nSensor artifacts (striping, banding)\nGeometric misalignment\nRadiometric inconsistencies\nMixed pixels at boundaries\n\n\nSolutions:\n\nUse Level-2A products\nRigorous cloud masking\nQuality flag filtering\nMulti-temporal compositing\nValidation checks\nDocument preprocessing\n\n\n\nSatellite data can be noisy. Cloud masking, using atmospherically corrected products, and careful preprocessing are essential."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#quality-example-cloud-masking",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#quality-example-cloud-masking",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Quality Example: Cloud Masking",
    "text": "Quality Example: Cloud Masking\n\n\nWithout Cloud Masking\n\n\nClouds misclassified\nShadows cause errors\nPoor model performance\n\n\nWith Proper Masking\n\n\nClean training data\nAccurate classifications\nBetter generalization\n\n\n\nOne cloudy image can ruin your training data!\n\n\nEven a few cloudy training samples can teach the model wrong patterns. Rigorous cloud masking is non-negotiable."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#how-much-data-do-you-need",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#how-much-data-do-you-need",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "How Much Data Do You Need?",
    "text": "How Much Data Do You Need?\nDepends on:\n\nModel complexity (DL needs more)\nProblem difficulty\nClass separability\nAvailable features\n\nGeneral Guidelines:\n\nTraditional ML: 100s to 1000s of samples per class\nDeep Learning: 1000s to 10,000s per class\nTransfer Learning: Can work with 100s per class\n\n\nDeep learning is data-hungry. Random Forest can work with smaller datasets. Transfer learning (starting from pre-trained models) reduces data requirements."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-augmentation",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-augmentation",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\nTechniques:\n\nRotation (90°, 180°, 270°)\nFlipping (horizontal, vertical)\nBrightness/contrast adjustment\nAdding noise\nElastic deformations\n\n\nResult: 10x more training samples from existing data!\n\n\nData augmentation synthetically increases dataset size by creating modified versions of existing samples. This helps models generalize better."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#transfer-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#transfer-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\n\n\nConcept:\n\nStart with model pre-trained on large dataset\nFine-tune on your specific task\nRequires much less data\n\n\nEO Applications:\n\nUse ImageNet pre-trained models\nNASA-IBM Geospatial Foundation Model\nDomain-specific pre-training\n\n\n\nTransfer learning leverages knowledge learned on large datasets and adapts it to your specific problem with much less data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-diversity-matters",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-diversity-matters",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Why Diversity Matters",
    "text": "Why Diversity Matters\n\n\nProblem: Biased Training\n\n\nAll samples from one season\nOne geographic region only\nSimilar conditions\nResult: Model fails elsewhere\n\n\nSolution: Diverse Training\n\n\nMultiple seasons\nDifferent regions\nVarious conditions\nResult: Model generalizes\n\n\n\nModels trained on narrow datasets often fail when deployed in different conditions. Diversity in training data leads to better generalization."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#sources-of-diversity-needed",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#sources-of-diversity-needed",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Sources of Diversity Needed",
    "text": "Sources of Diversity Needed\nTemporal Diversity:\n\nDifferent seasons (wet/dry)\nMultiple years\nPhenological stages\n\nGeographic Diversity:\n\nDifferent regions\nVarious elevations\nCoastal vs inland\n\nAtmospheric Diversity:\n\nClear vs hazy days\nDifferent solar angles\nSeasonal lighting\n\nClass Diversity:\n\nVariations within classes\nEdge cases\nTransitional zones\n\n\nFor robust models, ensure your training data covers the range of conditions the model will encounter in operational use."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#example-urban-classification",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#example-urban-classification",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Example: Urban Classification",
    "text": "Example: Urban Classification\n\n\nPoor Diversity\n\nOnly Metro Manila samples\nOnly concrete roofs\nOnly high-density areas\nFails in other cities\n\n\nGood Diversity\n\nLarge cities, small towns\nVarious roof materials (concrete, metal, nipa)\nDifferent architectural styles\nDifferent densities\nWorks across Philippines\n\n\n\nIf training only on Metro Manila, the model might not recognize small towns or rural settlements with different characteristics."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#label-quality-is-critical",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#label-quality-is-critical",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Label Quality is Critical",
    "text": "Label Quality is Critical\n\n\nCommon Label Issues:\n\nMislabeled samples\nPositional errors (GPS drift)\nTemporal mismatch (old labels, new image)\nAmbiguous classes\nInconsistent definitions\nMixed pixels\n\n\nImpact:\n\nModel learns wrong patterns\nContradictory signals\nPoor generalization\nLow confidence predictions\nWasted compute\n\n\n\nOne bad label can corrupt model learning!\n\n\nGround truth labels might have errors - GPS inaccuracy, outdated information, or human mistakes. These errors propagate to model predictions."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#label-quality-best-practices",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#label-quality-best-practices",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Label Quality Best Practices",
    "text": "Label Quality Best Practices\n1. Clear Class Definitions\n\nWrite explicit criteria\nProvide examples\nDefine edge cases\nDocument ambiguities\n\n2. Consistent Protocols\n\nStandard operating procedures\nSame interpretation rules\nCalibration sessions\nRegular training for labelers\n\n3. Multiple Annotators\n\nIndependent labeling\nCompare for consistency\nResolve disagreements\nBuild consensus labels\n\n4. Expert Validation\n\nDomain experts review samples\nRandom quality checks\nIterative improvement\n\n\nInvest time in defining classes clearly and training labelers. Consistency matters more than speed."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#label-quality-example",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#label-quality-example",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Label Quality Example",
    "text": "Label Quality Example\n\n\nPoor Labels\n\n\n“Forest” defined inconsistently\nMixed with shrubland\nTemporal mismatch\nPositional errors\n\n\nHigh-Quality Labels\n\n\nClear forest definition\nCareful boundary delineation\nImage-label temporal match\nValidated position\n\n\n\nHigh-quality labels are worth the effort. A smaller dataset with accurate labels often outperforms a larger dataset with noisy labels."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#alam-project-addressing-labels",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#alam-project-addressing-labels",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "ALaM Project: Addressing Labels",
    "text": "ALaM Project: Addressing Labels\n\nDOST-ASTI’s Automated Labeling Machine\n\nAutomates labeling process\nCrowdsourcing capabilities\nExpert validation workflow\nAddresses EO’s biggest bottleneck\n\n\nRemember from Session 1: DOST-ASTI’s ALaM project specifically addresses the label quality and quantity challenge through automation and crowdsourcing."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-centric-workflow",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-centric-workflow",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Data-Centric Workflow",
    "text": "Data-Centric Workflow\nBefore Training:\n\nAudit your data: Visualize samples, check distributions\nClean aggressively: Remove clouds, fix labels, filter outliers\nBalance classes: Address imbalances through sampling or augmentation\nDocument everything: Track data sources, preprocessing, versions\n\nDuring Training:\n\nAnalyze errors: Which samples does model get wrong?\nIdentify patterns: Are errors systematic? (e.g., all in one region)\nFix data: Add more diverse samples, improve labels\nIterate: Retrain with better data\n\n\nData-centric approach means continuously improving data quality based on model feedback. Look at errors to understand what data you’re missing."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-quality-checklist",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-quality-checklist",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Data Quality Checklist",
    "text": "Data Quality Checklist\n\nAtmospherically corrected (Level-2A)?\nClouds and shadows masked?\nGeometric alignment verified?\nTemporal consistency checked?\nLabel accuracy validated?\nClasses clearly defined?\nTraining data balanced?\nGeographic diversity ensured?\nSeasonal coverage adequate?\nEdge cases included?\nQuality flags documented?\n\n\nUse this checklist before training any model. Addressing data issues upfront saves time and improves results."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#case-study-better-data-better-results",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#case-study-better-data-better-results",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Case Study: Better Data = Better Results",
    "text": "Case Study: Better Data = Better Results\n\n\nScenario:\nCoral reef mapping project\nInitial Results:\n\n70% accuracy\nFails in turbid water\nConfuses reef with sand\n\nProblem Identified:\nAll training data from clear water\n\nData-Centric Solution:\n\nAdd turbid water samples\nInclude reef-sand transition zones\nMore diverse depths\nImprove label precision\n\nNew Results:\n\n90% accuracy\nWorks in turbid water\nBetter boundary detection\n\n\n\n10x improvement from better data, same model!\n\n\nReal example of how data improvements had bigger impact than model tuning. The data was the key, not the algorithm."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#foundation-models-for-eo",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#foundation-models-for-eo",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Foundation Models for EO",
    "text": "Foundation Models for EO\n\nWhat are Foundation Models?\n\nLarge models pre-trained on massive EO datasets\nLearn general representations\nFine-tune for specific tasks\nDramatically reduce labeled data needs\n\nExamples (2025):\n\nGoogle AlphaEarth Foundations (DeepMind, 2025) - 1.4 trillion embeddings/year in GEE\nNASA-IBM Geospatial Foundation Model (open-source, Aug 2024)\nPrithvi (IBM/NASA/ESA collaboration)\nClay Foundation Model (open-source)\nPlanet Labs + Anthropic Claude integration\n\n\nTiming: 4 minutes\nKey Points: - 2025 Update: Foundation models are THE major innovation in EO AI - Google AlphaEarth Foundations: Virtual satellite model, 10x10m resolution, integrates Sentinel-1/2 + Landsat + radar, available in Earth Engine, 16x less storage than other AI systems - NASA-IBM model released August 2024 as open-source - Trained on massive Sentinel-2 datasets (1 billion parameters) - Can be fine-tuned with just hundreds of labeled samples (vs thousands before) - Philippine Application: Use foundation models to jumpstart projects with limited labeled data - AlphaEarth embeddings already in GEE!\nExample: “Instead of manually labeling 10,000 images for rice mapping, use AlphaEarth embeddings in GEE or fine-tune Prithvi with just 500 samples and achieve similar accuracy”"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#on-board-ai-processing",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#on-board-ai-processing",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "On-Board AI Processing",
    "text": "On-Board AI Processing\n\n\n\nESA Φsat-2 (Launched 2024)\n\n22×10×33 cm CubeSat\nOnboard AI computer (Intel Myriad X VPU)\nReal-time cloud detection\nProcess before downlink\nSaves bandwidth\n\n\nSatellogic Edge Computing\n\n“AI First” satellites\nOnboard GPUs\nReal-time processing\nImmediate insights\nShip/object detection\n\n\n\nTiming: 3 minutes\nKey Points: - 2025 Update: On-board AI is operational on multiple satellites - ESA’s Φsat-2 launched 2024 with Intel AI chip - Processes images on-orbit before transmitting - Use case: Only download cloud-free portions, save 90% bandwidth - Future: Real-time disaster detection from space\nPhilippine Relevance: “Imagine typhoon damage detected and reported automatically from orbit within minutes, not hours”"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#self-supervised-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#self-supervised-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Self-Supervised Learning",
    "text": "Self-Supervised Learning\n\n\nConcept:\n\nLearn from unlabeled data\nDefine pretext tasks (e.g., predict missing patches)\nModel learns useful representations\nFine-tune with small labeled dataset\n\nWhy Important for EO:\n\nAbundance of unlabeled satellite imagery\nHigh cost of labeling\nImproves transferability\n\n\n\n\n\nSelf-supervised learning is particularly relevant for EO due to abundance of unlabeled imagery. Models learn useful features without expensive labeling."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#explainable-ai-xai",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#explainable-ai-xai",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Explainable AI (XAI)",
    "text": "Explainable AI (XAI)\n\nWhy XAI Matters:\n\nUnderstand model decisions\nBuild trust in AI systems\nDebug and improve models\nRegulatory compliance\n\nMethods:\n\nSHAP: Feature importance\nLIME: Local explanations\nGrad-CAM: Visual attention maps\nSaliency Maps: What pixels matter?\n\n\nAs AI systems make important decisions (disaster response, resource allocation), understanding why they make those decisions becomes crucial."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-we-covered",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-we-covered",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "What We Covered",
    "text": "What We Covered\n\nAI/ML Basics: What it is and why it’s powerful for EO\nML Workflow: 7-step process from problem to deployment\nSupervised Learning: Classification and regression with labeled data\nUnsupervised Learning: Clustering and pattern discovery\nDeep Learning: Neural networks and CNNs for images\nData-Centric AI: Quality, quantity, diversity, labels\n2025 Trends: Foundation models, on-board AI, XAI\n\n\nWe’ve covered the fundamental concepts you need to understand before diving into hands-on implementation."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#key-takeaways",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#key-takeaways",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n1. Focus on Data First\n\nQuality beats quantity\nDiversity enables generalization\nGood labels are gold\n\n2. Start Simple\n\nTry traditional ML before deep learning\nRandom Forest is often enough\nAdd complexity only when needed\n\n3. Iterate Continuously\n\nAnalyze errors\nImprove data\nRetrain models\nDeployment is not the end\n\n\nThese principles will serve you well throughout your AI/ML journey. Data quality and iterative improvement are more important than fancy algorithms."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#practical-advice",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#practical-advice",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Practical Advice",
    "text": "Practical Advice\nFor Your Projects:\n\nDefine the problem clearly before collecting data\nInvest in high-quality training data - it’s worth it\nValidate rigorously on independent data\nDocument everything (data sources, preprocessing, model versions)\nStart with baselines (simple models, existing methods)\nIterate based on errors - let failures guide improvements\nConsider operational constraints early\n\n\nThese practical tips come from real-world experience. Following them will save you time and frustration."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#the-data-centric-mindset",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#the-data-centric-mindset",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "The Data-Centric Mindset",
    "text": "The Data-Centric Mindset\n\n\nWhen model performs poorly, ask:\n\nIs my data clean?\nAre labels accurate?\nIs training data representative?\nDo I have enough diversity?\nAre there systematic biases?\n\n\nBefore trying:\n\nMore complex model\nMore epochs\nDifferent hyperparameters\nNew architecture\n\nCheck your data first!\n\n\n“Better data beats a cleverer model” - Andrew Ng\n\n\nAdopt a data-centric mindset. When models underperform, investigate data issues before blaming the algorithm."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#connection-to-sessions-3-4",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#connection-to-sessions-3-4",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Connection to Sessions 3 & 4",
    "text": "Connection to Sessions 3 & 4\n\n\nSession 3: Python Basics\n\nLoad and explore data\nGeoPandas (vector)\nRasterio (raster)\nFoundation for all ML work\n\n\nSession 4: Google Earth Engine\n\nAccess Sentinel data at scale\nCloud masking (data quality!)\nTemporal compositing\nExport for ML workflows\n\n\n\nEverything builds on these concepts!\n\n\nThe hands-on sessions this afternoon put these concepts into practice. You’ll actually work with data and see these principles in action."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#looking-ahead-days-2-4",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#looking-ahead-days-2-4",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Looking Ahead: Days 2-4",
    "text": "Looking Ahead: Days 2-4\n\n\nDay 2:\n\nRandom Forest classification\nLand cover mapping\nCNN basics\nTensorFlow/Keras intro\n\n\nDays 3-4:\n\nU-Net for segmentation\nFlood mapping (DRR focus)\nTime series with LSTMs\nFoundation models\nExplainable AI\n\n\n\nOver the next three days, we’ll implement these concepts in real EO applications for DRR, CCA, and NRM."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#resources-for-continued-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#resources-for-continued-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Resources for Continued Learning",
    "text": "Resources for Continued Learning\nOnline Courses:\n\nNASA ARSET: ML for Earth Science\nEO College: Introduction to ML for EO\nCoursera: Machine Learning (Andrew Ng)\nFast.ai: Practical Deep Learning\n\nPapers & Tutorials:\n\n“Data-Centric ML for Earth Observation” (ArXiv 2025)\nGoogle Earth Engine tutorials\nTensorFlow Earth Observation tutorials\n\nCommunities:\n\nSkAI-Pinas network\nDigital Space Campus (CoPhil)\nDIMER model repository\n\n\nThese resources will support your continued learning after the training. The Digital Space Campus will have all our materials for reference."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#session-summary",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#session-summary",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Session Summary",
    "text": "Session Summary\nWhat We Covered:\n✅ AI/ML/DL definitions and relationships\n✅ End-to-end ML workflow for EO\n✅ Supervised learning (classification, regression)\n✅ Unsupervised learning (clustering)\n✅ Deep learning & CNNs for satellite imagery\n✅ Data-centric AI philosophy\n✅ 2025 innovations: Foundation models, on-board AI\n\nTiming: 2 minutes\nYou now have conceptual foundation for all ML work in this course. Sessions 3-4 today and Days 2-4 will implement these concepts."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#qa",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#qa",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Q&A",
    "text": "Q&A\n\n\nAI/ML Concepts\n\nSupervised vs unsupervised?\nWhen to use deep learning?\nFoundation models for my use case?\n\n\nPractical Questions\n\nData quality challenges?\nLabel collection strategies?\nComputing requirements?\n\n\n\nTiming: 5-8 minutes for Q&A\nCommon Questions: - “Do I need a GPU?” → Not for Random Forest, yes for deep learning - “How many labels do I need?” → Depends: 100s with foundation models, 1000s for CNN from scratch - “Which algorithm should I use?” → Start simple (RF), then deep learning if needed - “Can I use foundation models for Philippines?” → Yes! They’re global and open-source"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#hands-on-python-for-geospatial-data",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#hands-on-python-for-geospatial-data",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Hands-on Python for Geospatial Data",
    "text": "Hands-on Python for Geospatial Data\nComing up after 15-minute break:\n\nGoogle Colab environment setup\nGeoPandas for vector data\nRasterio for raster data\nWork with Philippine boundaries\nLoad and visualize Sentinel-2 imagery\nCalculate NDVI\n\n\nGet ready to code! 💻"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#resources",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#resources",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Resources",
    "text": "Resources\nFoundation Models:\nNASA-IBM Geospatial: https://huggingface.co/ibm-nasa-geospatial\nPrithvi: https://github.com/NASA-IMPACT/Prithvi\nClay: https://clay-foundation.github.io\nLearning:\nNASA ARSET: https://appliedsciences.nasa.gov/arset\nEO College: https://eo-college.org\nSkAI-Pinas: https://asti.dost.gov.ph/skai-pinas\n\nSession 2 Complete!\n15-minute break before Session 3. Make sure participants have: - Google Colab access working - GEE account registration started (will finalize in Session 4)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#session-overview",
    "href": "day1/presentations/03_session3_python_geospatial.html#session-overview",
    "title": "Python for Geospatial Data Analysis",
    "section": "Session Overview",
    "text": "Session Overview\nPython for Geospatial Data Analysis\n\nHands-on introduction to working with vector and raster data using Python\n\n\nFormat: Brief conceptual intro + Extended hands-on coding\n\n\nDuration: 2 hours (15-20 min presentation + 100 min hands-on)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#learning-objectives",
    "href": "day1/presentations/03_session3_python_geospatial.html#learning-objectives",
    "title": "Python for Geospatial Data Analysis",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you will be able to:\n\nSet up and use Google Colaboratory for geospatial analysis\nLoad, explore, and visualize vector data with GeoPandas\nRead, process, and visualize raster data with Rasterio\nPerform basic geospatial operations (clipping, reprojecting, cropping)\nPrepare data for AI/ML workflows"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#session-roadmap",
    "href": "day1/presentations/03_session3_python_geospatial.html#session-roadmap",
    "title": "Python for Geospatial Data Analysis",
    "section": "Session Roadmap",
    "text": "Session Roadmap\n\n\n\nTime\nTopic\nDuration\n\n\n\n\n00-15 min\nSetup & Python Basics Recap\n15 min\n\n\n15-55 min\nGeoPandas for Vector Data (HANDS-ON)\n40 min\n\n\n55-60 min\n☕ Break\n5 min\n\n\n60-110 min\nRasterio for Raster Data (HANDS-ON)\n50 min\n\n\n110-120 min\nSummary & Next Steps\n10 min\n\n\n\n\nTiming: 2 minutes\nKey Point: This is a HANDS-ON session. Participants code along in their notebooks."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#notebook-access",
    "href": "day1/presentations/03_session3_python_geospatial.html#notebook-access",
    "title": "Python for Geospatial Data Analysis",
    "section": "Notebook Access",
    "text": "Notebook Access\n📓 Google Colab Notebook:\nDay1_Session3_Python_Geospatial_Data.ipynb\n\nOpen link from course materials\nSave a copy to your Drive\nRun first cell to install packages\nFollow along as we code together\n\n\nTiming: 3 minutes\nInstructor Actions: - Share notebook link in chat - Wait for participants to open and save copy - Verify everyone can see the notebook - Explain Colab basics (cells, shift+enter)\nTroubleshooting: - “Can’t access?” → Check Google account login - “Packages fail?” → Restart runtime"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#todays-focus",
    "href": "day1/presentations/03_session3_python_geospatial.html#todays-focus",
    "title": "Python for Geospatial Data Analysis",
    "section": "Today’s Focus",
    "text": "Today’s Focus\n\n\nVector Data:\n\nAdministrative boundaries\nPoints of interest\nRoads, rivers\nTraining sample polygons\nUsing GeoPandas\n\n\nRaster Data:\n\nSatellite imagery\nDigital elevation models\nLand cover maps\nAI model outputs\nUsing Rasterio\n\n\n\nIntegration: Combining vector and raster for complete EO workflows"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#the-python-advantage",
    "href": "day1/presentations/03_session3_python_geospatial.html#the-python-advantage",
    "title": "Python for Geospatial Data Analysis",
    "section": "The Python Advantage",
    "text": "The Python Advantage\nWhy Python is the Leading Language for EO:\n\nRich Ecosystem\n\nHundreds of specialized libraries\nActive development and community\n\nEasy to Learn\n\nClear syntax, readable code\nGentle learning curve\n\nPowerful Integration\n\nConnects data sources, processing, ML\nSingle environment for complete workflows\n\nFree and Open Source\n\nNo licensing costs\nTransparent and reproducible"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#python-geospatial-ecosystem",
    "href": "day1/presentations/03_session3_python_geospatial.html#python-geospatial-ecosystem",
    "title": "Python for Geospatial Data Analysis",
    "section": "Python Geospatial Ecosystem",
    "text": "Python Geospatial Ecosystem\n\nComplete Python Earth Observation Ecosystem organized by function\nThis diagram shows the complete Python ecosystem for Earth Observation, organized by function: Data Access (Earth Engine, Sentinel Hub), Geospatial Processing (GeoPandas, Rasterio), Data Science (NumPy, Pandas), Machine Learning (Scikit-learn, TensorFlow, PyTorch), and Visualization tools. Notice how data flows from access through processing to analysis and visualization."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#integration-capabilities",
    "href": "day1/presentations/03_session3_python_geospatial.html#integration-capabilities",
    "title": "Python for Geospatial Data Analysis",
    "section": "Integration Capabilities",
    "text": "Integration Capabilities\nPython Connects Everything:\n\n# Example workflow\nimport geopandas as gpd\nimport rasterio\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load vector training data\ntraining = gpd.read_file('samples.geojson')\n\n# Load satellite raster\nwith rasterio.open('sentinel2.tif') as src:\n    image = src.read()\n\n# Extract features and train model\nX, y = extract_features(image, training)\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\n# Predict on full image\nprediction = model.predict(image)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#community-and-resources",
    "href": "day1/presentations/03_session3_python_geospatial.html#community-and-resources",
    "title": "Python for Geospatial Data Analysis",
    "section": "Community and Resources",
    "text": "Community and Resources\nVibrant Python Geospatial Community:\nDocumentation:\n\nComprehensive guides for all libraries\nTutorials and examples\nAPI references\n\nCommunity Support:\n\nStack Overflow\nGitHub discussions\nGIS Stack Exchange\nDedicated forums\n\nLearning Resources:\n\nFree courses (Coursera, Udemy)\nBooks (Automating GIS Processes)\nBlogs and tutorials\nConference workshops"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#why-colab-for-this-training",
    "href": "day1/presentations/03_session3_python_geospatial.html#why-colab-for-this-training",
    "title": "Python for Geospatial Data Analysis",
    "section": "Why Colab for This Training?",
    "text": "Why Colab for This Training?\nAdvantages for Learning:\n\nNo Setup Hassles\n\nWorks immediately\nNo environment configuration\nConsistent for all participants\n\nAccessible Anywhere\n\nJust need a browser\nWorks on any computer\nEven tablets\n\nPowerful Resources\n\nFree GPU for deep learning\n12+ GB RAM\nSufficient for all exercises\n\nEasy Sharing\n\nShare notebooks instantly\nCollaborative editing\nCoPhil materials readily accessible"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#colab-interface-overview",
    "href": "day1/presentations/03_session3_python_geospatial.html#colab-interface-overview",
    "title": "Python for Geospatial Data Analysis",
    "section": "Colab Interface Overview",
    "text": "Colab Interface Overview\n\n\nMain Components:\n\n\nMenu Bar\n\nFile, Edit, View, Insert, Runtime\n\n\n\n\n\nToolbar\n\nPlay button to run cells\nAdd code/text cells\n\n\n\n\n\nNotebook Area\n\nCode cells (executable)\nText cells (Markdown)\n\n\n\n\n\nSidebar\n\nTable of contents\nFiles browser\nCode snippets\n\n\n\n\n\n\n\nColab Interface"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#running-code-in-colab",
    "href": "day1/presentations/03_session3_python_geospatial.html#running-code-in-colab",
    "title": "Python for Geospatial Data Analysis",
    "section": "Running Code in Colab",
    "text": "Running Code in Colab\nTwo Ways to Execute Cells:\n\n1. Click the Play Button\n\nLeft side of each code cell\nRuns that specific cell\n\n\n\n2. Keyboard Shortcuts\n\nShift + Enter: Run cell and move to next\nCtrl + Enter: Run cell, stay on current\nCtrl + M then A: Add cell above\nCtrl + M then B: Add cell below\n\n\n\nOutput Appears Below Cell:\nText, plots, tables, errors all display inline."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#google-drive-integration",
    "href": "day1/presentations/03_session3_python_geospatial.html#google-drive-integration",
    "title": "Python for Geospatial Data Analysis",
    "section": "Google Drive Integration",
    "text": "Google Drive Integration\nMounting Your Google Drive:\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\nBenefits:\n\nPersistent storage (Colab resets)\nUpload/download data\nSave outputs\nShare files between notebooks\n\n\n\nAccess Your Files:\n# Your Drive files appear at:\n# /content/drive/MyDrive/\n\n# Example:\nimport geopandas as gpd\ngdf = gpd.read_file('/content/drive/MyDrive/data/boundaries.shp')"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#installing-additional-packages",
    "href": "day1/presentations/03_session3_python_geospatial.html#installing-additional-packages",
    "title": "Python for Geospatial Data Analysis",
    "section": "Installing Additional Packages",
    "text": "Installing Additional Packages\nMost Common Libraries Pre-Installed:\nNumPy, Pandas, Matplotlib, Scikit-learn\n\nFor Geospatial Libraries:\n# GeoPandas (usually pre-installed, but check version)\n!pip install geopandas\n\n# Rasterio\n!pip install rasterio\n\n# Other useful libraries\n!pip install earthengine-api\n!pip install folium\n\n\nNote: Packages need reinstalling each session (Colab resets runtime)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#what-is-geopandas",
    "href": "day1/presentations/03_session3_python_geospatial.html#what-is-geopandas",
    "title": "Python for Geospatial Data Analysis",
    "section": "What is GeoPandas?",
    "text": "What is GeoPandas?\n\n\nPandas + Geometry = GeoPandas\n\nDefinition:\nExtension of Pandas for working with geospatial vector data\n\n\nKey Concept:\nLike a spreadsheet/table where one column contains geometries (points, lines, polygons)\n\n\nBuilt On:\n\nPandas - Data manipulation\nShapely - Geometric operations\nFiona - File I/O\nPyProj - Coordinate systems\n\n\n\n\nGeoPandas\nPandas + Geometry"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#the-geodataframe-concept",
    "href": "day1/presentations/03_session3_python_geospatial.html#the-geodataframe-concept",
    "title": "Python for Geospatial Data Analysis",
    "section": "The GeoDataFrame Concept",
    "text": "The GeoDataFrame Concept\nSimilar to Pandas DataFrame:\n\n\nRegular DataFrame:\n\n\n\nName\nPopulation\nArea\n\n\n\n\nManila\n1.78M\n42.88\n\n\nCebu\n0.92M\n315\n\n\nDavao\n1.63M\n2444\n\n\n\n\nGeoDataFrame:\n\n\n\nName\nPopulation\nArea\ngeometry\n\n\n\n\nManila\n1.78M\n42.88\nPOLYGON(…)\n\n\nCebu\n0.92M\n315\nPOLYGON(…)\n\n\nDavao\n1.63M\n2444\nPOLYGON(…)\n\n\n\n\n\nSpecial “geometry” Column:\nContains spatial information (coordinates defining shapes)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#common-vector-data-operations",
    "href": "day1/presentations/03_session3_python_geospatial.html#common-vector-data-operations",
    "title": "Python for Geospatial Data Analysis",
    "section": "Common Vector Data Operations",
    "text": "Common Vector Data Operations\nWhat You Can Do with GeoPandas:\n\nRead/Write\n\nShapefiles, GeoJSON, GeoPackage, PostGIS\n\nExplore\n\nView attributes, examine geometries\n\nVisualize\n\nQuick plotting, interactive maps\n\nGeoprocessing\n\nBuffer, intersection, union, clip\n\nSpatial Joins\n\nCombine datasets based on location\n\nCoordinate Transformations\n\nReproject to different CRS"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#geopandas-code-example",
    "href": "day1/presentations/03_session3_python_geospatial.html#geopandas-code-example",
    "title": "Python for Geospatial Data Analysis",
    "section": "GeoPandas Code Example",
    "text": "GeoPandas Code Example\nLoading and Visualizing:\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Read Philippine provinces shapefile\nprovinces = gpd.read_file('philippines_provinces.shp')\n\n# Examine data\nprint(provinces.head())\nprint(provinces.crs)  # Check coordinate system\n\n# Simple plot\nprovinces.plot(figsize=(10, 10),\n               edgecolor='black',\n               facecolor='lightblue')\nplt.title('Philippine Provinces')\nplt.show()"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#visualization-capabilities",
    "href": "day1/presentations/03_session3_python_geospatial.html#visualization-capabilities",
    "title": "Python for Geospatial Data Analysis",
    "section": "Visualization Capabilities",
    "text": "Visualization Capabilities\nGeoPandas Plotting:\n\n# Color by attribute\nprovinces.plot(column='population',\n               cmap='YlOrRd',\n               legend=True,\n               figsize=(12, 10))\nplt.title('Population by Province')\n\n# Add basemap (with contextily)\nimport contextily as ctx\nprovinces_web_mercator = provinces.to_crs(epsg=3857)\nax = provinces_web_mercator.plot(figsize=(15, 15),\n                                   alpha=0.5)\nctx.add_basemap(ax)\n\n\nGeoPandas integrates seamlessly with Matplotlib for static plots and can work with Folium or Plotly for interactive maps. For training, we’ll focus on quick visualization for QA and exploration."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#philippine-coordinate-reference-systems",
    "href": "day1/presentations/03_session3_python_geospatial.html#philippine-coordinate-reference-systems",
    "title": "Python for Geospatial Data Analysis",
    "section": "Philippine Coordinate Reference Systems",
    "text": "Philippine Coordinate Reference Systems\nCommon CRS for Philippines:\n\n\n\nEPSG\nName\nType\nUnits\nUse Case\n\n\n\n\n4326\nWGS84\nGeographic\nDegrees\nWeb maps, lat/lon\n\n\n32651\nUTM Zone 51N\nProjected\nMeters\nWestern PH, Manila\n\n\n32652\nUTM Zone 52N\nProjected\nMeters\nEastern PH, Mindanao\n\n\n3123\nPRS92 Zone III\nProjected\nMeters\nNational standard\n\n\n\n\nRule: Use geographic (4326) for storage, projected (UTM) for analysis\n\n\nPhilippines spans two UTM zones. Zone 51N covers Manila, Palawan, western areas. Zone 52N covers Mindanao and eastern regions. Always reproject to UTM for area/distance calculations!"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#philippine-utm-zones",
    "href": "day1/presentations/03_session3_python_geospatial.html#philippine-utm-zones",
    "title": "Python for Geospatial Data Analysis",
    "section": "Philippine UTM Zones",
    "text": "Philippine UTM Zones\n\n\nUTM Zone 51N (EPSG:32651)\nCoverage: - Metro Manila - Palawan - Western Luzon - Western Visayas\nMost common for: - Urban analysis - Palawan studies - Manila projects\n\nUTM Zone 52N (EPSG:32652)\nCoverage: - Mindanao - Eastern Visayas - Bicol Region - Eastern Luzon\nMost common for: - Mindanao analysis - Disaster mapping - Agricultural studies\n\n\nCode Example:\n# Reproject to UTM 51N for area calculation\ngdf_utm = gdf.to_crs(epsg=32651)\ngdf_utm['area_km2'] = gdf_utm.geometry.area / 1_000_000\n\n\nThe UTM zone boundary runs roughly through the middle of the Philippines. For national-scale work, pick one zone and reproject everything to it, or use PRS92."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#philippine-geospatial-data-sources",
    "href": "day1/presentations/03_session3_python_geospatial.html#philippine-geospatial-data-sources",
    "title": "Python for Geospatial Data Analysis",
    "section": "Philippine Geospatial Data Sources",
    "text": "Philippine Geospatial Data Sources\n\n\nNAMRIA Geoportal - Administrative boundaries - Topographic maps - Land cover 2020 - DEMs - www.geoportal.gov.ph\nPhilSA - Satellite imagery - EO products - philsa.gov.ph\n\nPSA - Census boundaries - Barangay data - psa.gov.ph\nOpenStreetMap - Roads, buildings - extract.bbbike.org\nNatural Earth - Country boundaries - naturalearthdata.com\n\n\nAll work with GeoPandas - just gpd.read_file()!\n\n\nNAMRIA is the official source for government work. OSM is community-maintained but very detailed for urban areas. PhilSA provides satellite-derived products."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#what-is-rasterio",
    "href": "day1/presentations/03_session3_python_geospatial.html#what-is-rasterio",
    "title": "Python for Geospatial Data Analysis",
    "section": "What is Rasterio?",
    "text": "What is Rasterio?\n\n\nPython Wrapper for GDAL\n\nDefinition:\nClean, idiomatic Python library for reading and writing geospatial raster data\n\n\nWhy Not Use GDAL Directly?\n\nGDAL Python bindings are cumbersome\nRasterio is more Pythonic\nBetter integration with NumPy\nCleaner syntax\n\n\n\nWorks With:\nAll formats GDAL supports - GeoTIFF, COG, NetCDF, HDF, etc.\n\n\n\nRasterio\nPython Wrapper for GDAL"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#raster-data-structure",
    "href": "day1/presentations/03_session3_python_geospatial.html#raster-data-structure",
    "title": "Python for Geospatial Data Analysis",
    "section": "Raster Data Structure",
    "text": "Raster Data Structure\nHow Rasterio Represents Imagery:\n\n3D NumPy Array:\n(bands, rows, columns)\n\n\nExample: Sentinel-2 10m Bands:\n# 4 bands (Blue, Green, Red, NIR)\n# 1098 rows (10980 m / 10 m)\n# 1098 columns (10980 m / 10 m)\n# Shape: (4, 1098, 1098)\n\narray[0, :, :]  # Band 1 (Blue)\narray[1, :, :]  # Band 2 (Green)\narray[2, :, :]  # Band 3 (Red)\narray[3, :, :]  # Band 4 (NIR)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#reading-raster-data-with-rasterio",
    "href": "day1/presentations/03_session3_python_geospatial.html#reading-raster-data-with-rasterio",
    "title": "Python for Geospatial Data Analysis",
    "section": "Reading Raster Data with Rasterio",
    "text": "Reading Raster Data with Rasterio\nBasic Workflow:\n\nimport rasterio\nimport numpy as np\n\n# Open raster file\nwith rasterio.open('sentinel2_10m.tif') as src:\n    # Read all bands\n    data = src.read()\n\n    # Read specific band\n    red_band = src.read(3)\n\n    # Get metadata\n    print(f\"CRS: {src.crs}\")\n    print(f\"Transform: {src.transform}\")\n    print(f\"Width: {src.width}, Height: {src.height}\")\n    print(f\"Bounds: {src.bounds}\")\n    print(f\"Number of bands: {src.count}\")\n\n\nThe ‘with’ statement ensures the file is properly closed after reading. This is good practice and prevents file locking issues."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#array-operations",
    "href": "day1/presentations/03_session3_python_geospatial.html#array-operations",
    "title": "Python for Geospatial Data Analysis",
    "section": "Array Operations",
    "text": "Array Operations\nRasterio + NumPy = Powerful Processing\n\n# Calculate NDVI\nwith rasterio.open('sentinel2_10m.tif') as src:\n    red = src.read(3).astype(float)\n    nir = src.read(4).astype(float)\n\n# NDVI formula\nndvi = (nir - red) / (nir + red + 1e-8)  # Small value prevents division by zero\n\n# Apply threshold\nvegetation_mask = ndvi &gt; 0.3\n\n# Calculate statistics\nprint(f\"Mean NDVI: {np.mean(ndvi):.3f}\")\nprint(f\"Vegetation pixels: {np.sum(vegetation_mask)}\")"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#common-spectral-indices",
    "href": "day1/presentations/03_session3_python_geospatial.html#common-spectral-indices",
    "title": "Python for Geospatial Data Analysis",
    "section": "Common Spectral Indices",
    "text": "Common Spectral Indices\nKey Indices for EO Analysis:\n\n\n\n\n\n\n\n\n\nIndex\nFormula\nPurpose\nRange\n\n\n\n\nNDVI\n(NIR - Red) / (NIR + Red)\nVegetation health\n-1 to +1\n\n\nEVI\n2.5 × (NIR - Red) / (NIR + 6×Red - 7.5×Blue + 1)\nEnhanced vegetation\n-1 to +1\n\n\nNDWI\n(Green - NIR) / (Green + NIR)\nWater bodies\n-1 to +1\n\n\nNDBI\n(SWIR - NIR) / (SWIR + NIR)\nBuilt-up areas\n-1 to +1\n\n\n\n\nSentinel-2 Bands: Blue (B2), Green (B3), Red (B4), NIR (B8), SWIR (B11, B12)\n\n\nThese indices are fundamental for EO analysis. NDVI is most common for vegetation. EVI better in high biomass areas (tropical forests). NDWI for flood mapping."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#philippine-application-rice-monitoring",
    "href": "day1/presentations/03_session3_python_geospatial.html#philippine-application-rice-monitoring",
    "title": "Python for Geospatial Data Analysis",
    "section": "Philippine Application: Rice Monitoring",
    "text": "Philippine Application: Rice Monitoring\nUsing NDVI for Philippine Rice Paddies:\n# Calculate NDVI for Central Luzon rice area\nwith rasterio.open('sentinel2_central_luzon.tif') as src:\n    red = src.read(4).astype(float)   # Band 4\n    nir = src.read(8).astype(float)   # Band 8\n\n# NDVI calculation\nndvi = (nir - red) / (nir + red + 1e-8)\n\n# Classify vegetation health\nbare_soil = ndvi &lt; 0.2      # Recently planted / fallow\ngrowing = (ndvi &gt;= 0.2) & (ndvi &lt; 0.5)  # Early growth\nmature = (ndvi &gt;= 0.5) & (ndvi &lt; 0.8)   # Peak biomass\nvery_dense = ndvi &gt;= 0.8    # Maximum vegetation\n\n# Calculate rice area statistics\npixel_area = 100  # 10m x 10m = 100 m²\nmature_rice_area_ha = np.sum(mature) * pixel_area / 10000\n\nprint(f\"Mature rice area: {mature_rice_area_ha:.2f} hectares\")\n\nThis workflow is used by PRiSM for operational rice monitoring across the Philippines. NDVI time series tracks crop phenology from planting to harvest."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#visualization-with-rasterio",
    "href": "day1/presentations/03_session3_python_geospatial.html#visualization-with-rasterio",
    "title": "Python for Geospatial Data Analysis",
    "section": "Visualization with Rasterio",
    "text": "Visualization with Rasterio\nDisplaying Satellite Imagery:\n\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\n\n# Open and display\nwith rasterio.open('sentinel2_10m.tif') as src:\n    # Show true color composite (RGB)\n    show((src, [3, 2, 1]), title='True Color')\n\n    # Show false color composite (NIR, Red, Green)\n    show((src, [4, 3, 2]), title='False Color (NIR-R-G)')\n\n# Or read and plot with matplotlib\nwith rasterio.open('sentinel2_10m.tif') as src:\n    data = src.read([3, 2, 1])  # RGB\n    # Scale to 0-255 for display\n    data_scaled = np.clip(data / 3000, 0, 1)\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(np.moveaxis(data_scaled, 0, -1))\n    plt.title('Sentinel-2 True Color')\n    plt.axis('off')\n    plt.show()"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#what-well-build-today",
    "href": "day1/presentations/03_session3_python_geospatial.html#what-well-build-today",
    "title": "Python for Geospatial Data Analysis",
    "section": "What We’ll Build Today",
    "text": "What We’ll Build Today\nNotebook 1: Vector Data with GeoPandas\n\nLoad Philippine administrative boundaries\nExplore and visualize provinces\nFilter specific regions (e.g., Central Luzon)\nCalculate area and basic statistics\nSpatial operations (buffer, clip)\nCreate training sample polygons"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#what-well-build-today-1",
    "href": "day1/presentations/03_session3_python_geospatial.html#what-well-build-today-1",
    "title": "Python for Geospatial Data Analysis",
    "section": "What We’ll Build Today",
    "text": "What We’ll Build Today\nNotebook 2: Raster Data with Rasterio\n\nLoad Sentinel-2 image subset\nExamine metadata and properties\nVisualize true and false color composites\nCalculate vegetation indices (NDVI, EVI)\nCrop to area of interest\nExtract pixel values at point locations\nSave processed outputs"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#integrating-vector-and-raster",
    "href": "day1/presentations/03_session3_python_geospatial.html#integrating-vector-and-raster",
    "title": "Python for Geospatial Data Analysis",
    "section": "Integrating Vector and Raster",
    "text": "Integrating Vector and Raster\nCombining Both Data Types:\n\nimport geopandas as gpd\nimport rasterio\nfrom rasterio.mask import mask\n\n# Load vector boundary\naoi = gpd.read_file('study_area.geojson')\n\n# Load raster\nwith rasterio.open('sentinel2.tif') as src:\n    # Clip raster to vector boundary\n    clipped_data, clipped_transform = mask(\n        src,\n        aoi.geometry,\n        crop=True\n    )\n\n    # Update metadata for output\n    out_meta = src.meta.copy()\n    out_meta.update({\n        \"height\": clipped_data.shape[1],\n        \"width\": clipped_data.shape[2],\n        \"transform\": clipped_transform\n    })"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#preparing-for-ml-workflows",
    "href": "day1/presentations/03_session3_python_geospatial.html#preparing-for-ml-workflows",
    "title": "Python for Geospatial Data Analysis",
    "section": "Preparing for ML Workflows",
    "text": "Preparing for ML Workflows\nWhat You’ll Learn:\n\nExtract Training Data\n\nSample raster values at polygon locations\nCreate feature matrix (X) and labels (y)\n\nSpatial Data Splits\n\nAvoid spatial autocorrelation in train/test\n\nData Formatting\n\nStructure for Scikit-learn, TensorFlow, PyTorch\n\nQuality Control\n\nCheck for NaN values, outliers\nValidate spatial alignment"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#link-to-colab-notebooks",
    "href": "day1/presentations/03_session3_python_geospatial.html#link-to-colab-notebooks",
    "title": "Python for Geospatial Data Analysis",
    "section": "Link to Colab Notebooks",
    "text": "Link to Colab Notebooks\n\nAccess Today’s Notebooks:\n\n\nNotebook 1: Vector Data\n[Link will be provided in chat]\n\n\nNotebook 2: Raster Data\n[Link will be provided in chat]\n\n\nMake a Copy:\nFile → Save a copy in Drive (so you can edit and experiment)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#tips-for-success",
    "href": "day1/presentations/03_session3_python_geospatial.html#tips-for-success",
    "title": "Python for Geospatial Data Analysis",
    "section": "Tips for Success",
    "text": "Tips for Success\nAs We Work Through Notebooks:\n\nRun Cells Sequentially\n\nTop to bottom order matters\nEach cell may depend on previous\n\nRead the Comments\n\nExplanations included in code\nLearn the “why” not just “how”\n\nExperiment\n\nModify parameters\nTry different visualizations\nBreak things and learn\n\nAsk Questions\n\nUse chat or raise hand\nNo question is too basic\n\nTake Notes\n\nUseful patterns and code snippets\nErrors and solutions"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#python-geospatial-ecosystem-1",
    "href": "day1/presentations/03_session3_python_geospatial.html#python-geospatial-ecosystem-1",
    "title": "Python for Geospatial Data Analysis",
    "section": "Python Geospatial Ecosystem",
    "text": "Python Geospatial Ecosystem\nGeoPandas:\n\nDataFrame + geometry column\nVector data operations\nEasy visualization\nIntegration with Pandas\n\nRasterio:\n\nClean GDAL wrapper\nNumPy array representation\nComprehensive metadata handling\nEfficient I/O\n\nIntegration:\n\nBoth work together seamlessly\nComplete EO workflows in Python\nFoundation for ML/AI applications"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#why-these-skills-matter",
    "href": "day1/presentations/03_session3_python_geospatial.html#why-these-skills-matter",
    "title": "Python for Geospatial Data Analysis",
    "section": "Why These Skills Matter",
    "text": "Why These Skills Matter\nFor AI/ML in Earth Observation:\n\nData Preparation\n\nLoading and preprocessing is 80% of work\nQuality in → Quality out\n\nFeature Engineering\n\nCalculate indices, textures, derivatives\nNumPy operations on raster arrays\n\nTraining Data Creation\n\nSample raster at polygon locations\nExtract features for supervised learning\n\nModel Deployment\n\nApply trained models to new imagery\nGenerate prediction maps\n\nValidation and QA\n\nCompare predictions to ground truth\nCalculate accuracy metrics"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#building-blocks-for-this-week",
    "href": "day1/presentations/03_session3_python_geospatial.html#building-blocks-for-this-week",
    "title": "Python for Geospatial Data Analysis",
    "section": "Building Blocks for This Week",
    "text": "Building Blocks for This Week\nToday’s Skills Enable:\nDay 2:\n\nRandom Forest land cover classification\nPreparing training data for ML\n\nDay 3:\n\nDeep learning data pipelines\nU-Net flood mapping inputs\n\nDay 4:\n\nTime series data preparation\nLSTM input formatting\n\n\nMastering these fundamentals now will make everything else smoother."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#minute-break",
    "href": "day1/presentations/03_session3_python_geospatial.html#minute-break",
    "title": "Python for Geospatial Data Analysis",
    "section": "☕ 5-Minute Break",
    "text": "☕ 5-Minute Break\n\nStretch Break\nStand up • Grab water • Back in 5 minutes\n\n\nTiming: 5 minutes\nAfter Break: Continue with Rasterio section (50 minutes of hands-on coding)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#transition-to-hands-on",
    "href": "day1/presentations/03_session3_python_geospatial.html#transition-to-hands-on",
    "title": "Python for Geospatial Data Analysis",
    "section": "Transition to Hands-On",
    "text": "Transition to Hands-On\n\nOpen Your Notebooks\n\n\nWe’ll start with:\nVector Data Analysis using GeoPandas\n\n\nRemember:\n\nMake a copy of the notebook\nMount your Google Drive\nRun cells in order\nAsk questions anytime"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#support-during-hands-on",
    "href": "day1/presentations/03_session3_python_geospatial.html#support-during-hands-on",
    "title": "Python for Geospatial Data Analysis",
    "section": "Support During Hands-On",
    "text": "Support During Hands-On\nInstructors Available:\n\nMain instructor demonstrating\nTeaching assistants in chat\nScreen sharing for debugging\n\nPacing:\n\nWe’ll work through together\nPause points for questions\nExtra exercises for fast finishers\n\nGoal:\nEveryone completes core exercises, understands concepts, ready for GEE"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#session-summary",
    "href": "day1/presentations/03_session3_python_geospatial.html#session-summary",
    "title": "Python for Geospatial Data Analysis",
    "section": "Session Summary",
    "text": "Session Summary\nWhat You’ve Learned:\n✅ Google Colab setup for geospatial work\n✅ GeoPandas for vector data (load, visualize, analyze)\n✅ Rasterio for raster data (read, process, visualize)\n✅ Coordinate systems and projections\n✅ Basic geospatial operations (clip, reproject, crop)\n✅ Data preparation for ML workflows\n\nTiming: 3 minutes\nYou now have foundational Python geospatial skills. Session 4 builds on this with Google Earth Engine."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#qa",
    "href": "day1/presentations/03_session3_python_geospatial.html#qa",
    "title": "Python for Geospatial Data Analysis",
    "section": "Q&A",
    "text": "Q&A\nCommon Questions:\n\n\n\nGeoPandas vs Shapely?\nWhen to use Rasterio vs GDAL?\nCRS issues and solutions?\nMemory errors with large files?\n\n\n\nBest practices for file paths?\nNoData values handling?\nVisualization tips?\nIntegration with ML pipelines?\n\n\n\nCommon Answers: - GeoPandas uses Shapely under the hood - Rasterio = Pythonic GDAL wrapper - Always check CRS before operations - Use chunking/windowing for large rasters"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#google-earth-engine",
    "href": "day1/presentations/03_session3_python_geospatial.html#google-earth-engine",
    "title": "Python for Geospatial Data Analysis",
    "section": "Google Earth Engine",
    "text": "Google Earth Engine\nComing up:\n\nCloud-based EO data processing\nAccess to entire Sentinel archive\nPlanetary-scale analysis\nPython API (geemap)\nCloud masking & compositing\nExport workflows\n\n\nGet ready for GEE! 🌍\n\nEveryone completes core exercises with understanding, not just copying code."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#final-session-of-day-1",
    "href": "day1/presentations/04_session4_google_earth_engine.html#final-session-of-day-1",
    "title": "Introduction to Google Earth Engine",
    "section": "Final Session of Day 1!",
    "text": "Final Session of Day 1!\n\nGoogle Earth Engine\nPlanetary-scale geospatial analysis in the cloud\n\n\nDuration: 2 hours (Hands-on with Python API)\n\n\nThis session introduces Google Earth Engine using Python exclusively (no JavaScript Code Editor). Participants will use geemap library for Python-based GEE access."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#learning-objectives",
    "href": "day1/presentations/04_session4_google_earth_engine.html#learning-objectives",
    "title": "Introduction to Google Earth Engine",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you will be able to:\n\nUnderstand what GEE is and why it’s powerful\nAuthenticate and initialize GEE Python API\nAccess Sentinel-1 and Sentinel-2 imagery\nFilter image collections (spatial, temporal, property)\nApply cloud masking to Sentinel-2\nCreate temporal composites (median, mean)\nCalculate spectral indices (NDVI, NDWI)\nVisualize results with geemap\nExport data for further analysis"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#session-roadmap",
    "href": "day1/presentations/04_session4_google_earth_engine.html#session-roadmap",
    "title": "Introduction to Google Earth Engine",
    "section": "Session Roadmap",
    "text": "Session Roadmap\n\n\n\nTime\nTopic\nDuration\n\n\n\n\n00-15 min\nGEE Overview & Authentication\n15 min\n\n\n15-55 min\nCore Concepts & Sentinel Access (HANDS-ON)\n40 min\n\n\n55-60 min\n☕ Break\n5 min\n\n\n60-110 min\nProcessing & Visualization (HANDS-ON)\n50 min\n\n\n110-120 min\nExport & Summary\n10 min\n\n\n\n\nTiming: 2 minutes\nLike Session 3, this is heavily hands-on. Most time spent coding in notebooks following instructor demonstration."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#what-is-google-earth-engine",
    "href": "day1/presentations/04_session4_google_earth_engine.html#what-is-google-earth-engine",
    "title": "Introduction to Google Earth Engine",
    "section": "What is Google Earth Engine?",
    "text": "What is Google Earth Engine?\n\n\nCloud-Based Platform for Geospatial Analysis\n\nMassive data catalog (petabytes)\nPowerful compute (Google’s infrastructure)\nFree for research & education\nNo download needed\nProcess at scale\n\n\n\n\n\n“Planetary-scale geospatial analysis”\n\n\nTiming: 3 minutes\nKey Points: - GEE hosts 40+ years of satellite imagery - Entire Landsat, Sentinel, MODIS archives - Processing happens on Google’s servers, not your laptop - Analyze entire countries in minutes - Free tier sufficient for most research/education"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#gee-architecture-and-workflow",
    "href": "day1/presentations/04_session4_google_earth_engine.html#gee-architecture-and-workflow",
    "title": "Introduction to Google Earth Engine",
    "section": "GEE Architecture and Workflow",
    "text": "GEE Architecture and Workflow\n\nGoogle Earth Engine complete architecture showing User Interface, Cloud Processing, Data Catalog, and Outputs\nThis comprehensive diagram shows GEE’s architecture: multiple user interfaces (Code Editor, Python API, Apps), the massive data catalog (70+ PB including Landsat, Sentinel, MODIS), distributed processing engine, common operations (filtering, compositing, indices, classification), and various output options (maps, exports, charts, training data)."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#why-gee-for-this-training",
    "href": "day1/presentations/04_session4_google_earth_engine.html#why-gee-for-this-training",
    "title": "Introduction to Google Earth Engine",
    "section": "Why GEE for This Training?",
    "text": "Why GEE for This Training?\nAddresses Key Challenges:\n\n❌ Traditional: Download 100s of GB of Sentinel data\n✅ GEE: Access entire archive without downloading\n❌ Traditional: Need powerful computer for processing\n✅ GEE: Google’s infrastructure does the work\n❌ Traditional: Complex cloud masking & preprocessing\n✅ GEE: Built-in algorithms & analysis-ready data\n❌ Traditional: Time-series analysis is painful\n✅ GEE: Designed for temporal analysis\n\n\nPerfect for Philippine-scale analysis!"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#gee-data-catalog",
    "href": "day1/presentations/04_session4_google_earth_engine.html#gee-data-catalog",
    "title": "Introduction to Google Earth Engine",
    "section": "GEE Data Catalog",
    "text": "GEE Data Catalog\nDatasets Available:\n\n\nSatellite Imagery:\n\nSentinel-1, 2, 3, 5P\nLandsat (entire archive!)\nMODIS\nPlanet, SkySat (some)\nMany more…\n\n\nGeophysical:\n\nClimate data\nElevation (SRTM, ASTER)\nWeather data\nPopulation datasets\nLand cover products\n\n\nBrowse: https://developers.google.com/earth-engine/datasets\n\nGEE hosts 800+ public datasets. All preprocessed and analysis-ready. Focus today on Sentinel-1 and Sentinel-2."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#python-api-vs-javascript-code-editor",
    "href": "day1/presentations/04_session4_google_earth_engine.html#python-api-vs-javascript-code-editor",
    "title": "Introduction to Google Earth Engine",
    "section": "Python API vs JavaScript Code Editor",
    "text": "Python API vs JavaScript Code Editor\n\n\nJavaScript Code Editor\n\nWeb-based IDE\nInteractive visualization\nQuick prototyping\nBuilt-in examples\n\n\nPython API (Our Focus)\n\nJupyter notebooks\nIntegration with ML libraries\nFamiliar Python ecosystem\ngeemap package for visualization\n\n\n\nToday: Python-only approach using geemap\n\n\nWhy Python for this training: - Integrates with ML workflows (scikit-learn, TensorFlow, PyTorch) - Familiar to data scientists - geemap provides all visualization capabilities - Better for reproducible research"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#geemap-package",
    "href": "day1/presentations/04_session4_google_earth_engine.html#geemap-package",
    "title": "Introduction to Google Earth Engine",
    "section": "geemap Package",
    "text": "geemap Package\n\nPython package for interactive GEE mapping\n\nBuilt on ipyleaflet\nInteractive map visualization\nLayer controls\nInspector tool\nSplit-panel comparison\nExport functionality\nMakes Python GEE as easy as Code Editor\n\n\ngeemap by Dr. Qiusheng Wu. Makes GEE accessible from Jupyter. We’ll use it extensively today."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#sign-up-for-gee",
    "href": "day1/presentations/04_session4_google_earth_engine.html#sign-up-for-gee",
    "title": "Introduction to Google Earth Engine",
    "section": "Sign Up for GEE",
    "text": "Sign Up for GEE\n\n\n\n\n\n\nBefore We Code\n\n\nYou need a Google Earth Engine account!\nSign up: https://earthengine.google.com/signup\n\n\n\nSteps:\n\nVisit signup page\nUse Gmail account\nSelect “Research/Education”\nWait for approval (usually instant)\n\n\nAlready have account? Great! Let’s authenticate.\n\n\nTiming: 2 minutes\nCheck: Ask participants if everyone has GEE access approved. If not, they can follow along and authenticate later."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#authentication-process",
    "href": "day1/presentations/04_session4_google_earth_engine.html#authentication-process",
    "title": "Introduction to Google Earth Engine",
    "section": "Authentication Process",
    "text": "Authentication Process\n83d Open Notebook: Day1_Session4_Google_Earth_Engine.ipynb\nAuthentication Code:\nimport ee\nimport geemap\n\n# Authenticate (first time only)\nee.Authenticate()\n\n# Initialize\nee.Initialize()\n\nprint(\"GEE Initialized Successfully!\")\n\nTiming: 5 minutes - participants authenticate\nSteps: 1. Run ee.Authenticate() - opens browser tab 2. Sign in with Google account 3. Allow Earth Engine access 4. Copy token back to notebook 5. Run ee.Initialize() 6. Confirm success message\nTroubleshooting: - “Authentication failed” → Check GEE account approved - “Module not found” → Install geemap: pip install geemap"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#key-gee-objects",
    "href": "day1/presentations/04_session4_google_earth_engine.html#key-gee-objects",
    "title": "Introduction to Google Earth Engine",
    "section": "Key GEE Objects",
    "text": "Key GEE Objects\n\n\nee.Image\n\nSingle raster image\nMultiple bands\nProperties (metadata)\n\nee.ImageCollection\n\nStack of images\nTime series\nFilter and reduce\n\n\nee.Geometry\n\nPoints, lines, polygons\nDefine areas of interest\n\nee.Feature / FeatureCollection\n\nVector data with attributes\nShapefiles, GeoJSON\n\n\n\nEverything is server-side! Code describes operations, execution happens on Google’s servers."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#server-side-vs-client-side",
    "href": "day1/presentations/04_session4_google_earth_engine.html#server-side-vs-client-side",
    "title": "Introduction to Google Earth Engine",
    "section": "Server-Side vs Client-Side",
    "text": "Server-Side vs Client-Side\n\n\n**Server-Side (ee.*):**\n# Runs on Google servers\nimage = ee.Image('COPERNICUS/S2/...')\nndvi = image.normalizedDifference(['B8', 'B4'])\nmean_ndvi = ndvi.reduceRegion(\n    reducer=ee.Reducer.mean(),\n    geometry=aoi,\n    scale=10\n)\nFast, scalable\n\nClient-Side (Python):\n# Runs on your computer\nresult = mean_ndvi.getInfo()\nprint(result)  # Downloads result\n\n# Visualization\nMap = geemap.Map()\nMap.addLayer(ndvi)\nMap  # Display\nFor viewing results\n\n\nKey concept: Build server-side computation, then download only final result. Never download raw petabytes!"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#filtering",
    "href": "day1/presentations/04_session4_google_earth_engine.html#filtering",
    "title": "Introduction to Google Earth Engine",
    "section": "Filtering",
    "text": "Filtering\nThree main filter types:\n1. Spatial (filterBounds):\naoi = ee.Geometry.Rectangle([120.5, 14.5, 121.0, 15.0])  # Metro Manila\nimages = collection.filterBounds(aoi)\n2. Temporal (filterDate):\nimages = collection.filterDate('2024-01-01', '2024-12-31')\n3. Property (filter):\n# Cloud cover &lt; 20%\nimages = collection.filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\nChain filters together!"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#reducers",
    "href": "day1/presentations/04_session4_google_earth_engine.html#reducers",
    "title": "Introduction to Google Earth Engine",
    "section": "Reducers",
    "text": "Reducers\nAggregate data across space or time:\n\n\nTemporal Reduction:\n# Median composite\nmedian = collection.median()\n\n# Mean\nmean = collection.mean()\n\n# Max NDVI\nmax_ndvi = collection.max()\n\nSpatial Reduction:\n# Mean value in region\nmean_val = image.reduceRegion(\n    reducer=ee.Reducer.mean(),\n    geometry=aoi,\n    scale=10\n)\n\n\nMost common: Median composite to remove clouds"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#accessing-sentinel-2",
    "href": "day1/presentations/04_session4_google_earth_engine.html#accessing-sentinel-2",
    "title": "Introduction to Google Earth Engine",
    "section": "Accessing Sentinel-2",
    "text": "Accessing Sentinel-2\n83dLive Coding Exercise 1\n# Define area of interest (Palawan)\naoi = ee.Geometry.Rectangle([118.0, 8.0, 120.5, 11.5])\n\n# Load Sentinel-2 collection\ns2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\\\n    .filterBounds(aoi) \\\\\n    .filterDate('2024-01-01', '2024-12-31') \\\\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\n# Print collection info\nprint('Number of images:', s2.size().getInfo())\n\n# Get first image\nfirst_image = s2.first()\nprint('Bands:', first_image.bandNames().getInfo())\n\nLive coding - participants follow along\nKey Teaching Points: - S2_SR_HARMONIZED is Surface Reflectance (L2A) - Always filter by cloud cover - Use filterBounds before filterDate (more efficient)"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#visualizing-sentinel-2",
    "href": "day1/presentations/04_session4_google_earth_engine.html#visualizing-sentinel-2",
    "title": "Introduction to Google Earth Engine",
    "section": "Visualizing Sentinel-2",
    "text": "Visualizing Sentinel-2\n83dLive Coding Exercise 2\n# Create map\nMap = geemap.Map(center=[9.5, 118.5], zoom=8)\n\n# Visualization parameters - True Color\nvis_params_rgb = {\n    'bands': ['B4', 'B3', 'B2'],\n    'min': 0,\n    'max': 3000,\n    'gamma': 1.4\n}\n\n# Add layer\nMap.addLayer(first_image, vis_params_rgb, 'Sentinel-2 True Color')\nMap\n\nKey points: - Create interactive map - Add Sentinel-2 layer - Zoom, pan, inspect - Explain band selection for RGB"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#false-color-composite",
    "href": "day1/presentations/04_session4_google_earth_engine.html#false-color-composite",
    "title": "Introduction to Google Earth Engine",
    "section": "False Color Composite",
    "text": "False Color Composite\n83dLive Coding Exercise 3\n# False color (vegetation = red)\nvis_params_false = {\n    'bands': ['B8', 'B4', 'B3'],  # NIR, Red, Green\n    'min': 0,\n    'max': 3000\n}\n\nMap.addLayer(first_image, vis_params_false, 'False Color')\n\nVegetation appears bright red!\n\n\nFalse color makes vegetation stand out. Useful for forest monitoring, agriculture."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#accessing-sentinel-1",
    "href": "day1/presentations/04_session4_google_earth_engine.html#accessing-sentinel-1",
    "title": "Introduction to Google Earth Engine",
    "section": "Accessing Sentinel-1",
    "text": "Accessing Sentinel-1\n83dLive Coding Exercise 4\n# Load Sentinel-1 collection\ns1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\\\n    .filterBounds(aoi) \\\\\n    .filterDate('2024-01-01', '2024-12-31') \\\\\n    .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\\\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\\\n    .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n\n# Get median composite\ns1_median = s1.select('VV').median()\n\n# Visualize\nvis_params_s1 = {'min': -25, 'max': 0}\nMap.addLayer(s1_median, vis_params_s1, 'Sentinel-1 VV')\n\nSAR visualization: Dark = smooth (water), Bright = rough (urban, forest)"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#minute-break",
    "href": "day1/presentations/04_session4_google_earth_engine.html#minute-break",
    "title": "Introduction to Google Earth Engine",
    "section": "☕ 5-Minute Break",
    "text": "☕ 5-Minute Break\n\nStretch Break\nStand up • Grab water • Back in 5 minutes\n\n\nAfter break: Cloud masking, indices, compositing, export"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#cloud-masking",
    "href": "day1/presentations/04_session4_google_earth_engine.html#cloud-masking",
    "title": "Introduction to Google Earth Engine",
    "section": "Cloud Masking",
    "text": "Cloud Masking\n83dLive Coding Exercise 5\ndef maskS2clouds(image):\n    \"\"\"Mask clouds using QA60 band\"\"\"\n    qa = image.select('QA60')\n    \n    # Bits 10 and 11 are clouds and cirrus\n    cloudBitMask = 1 &lt;&lt; 10\n    cirrusBitMask = 1 &lt;&lt; 11\n    \n    # Both flags should be zero (clear)\n    mask = qa.bitwiseAnd(cloudBitMask).eq(0) \\\\\n        .And(qa.bitwiseAnd(cirrusBitMask).eq(0))\n    \n    return image.updateMask(mask)\n\n# Apply to collection\ns2_masked = s2.map(maskS2clouds)\n\n# Create cloud-free composite\ncomposite = s2_masked.median()\n\nMap.addLayer(composite, vis_params_rgb, 'Cloud-Free Composite')\n\nKey concept: QA60 band contains cloud information. Mask clouds before compositing for best results."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#understanding-bitwise-operations",
    "href": "day1/presentations/04_session4_google_earth_engine.html#understanding-bitwise-operations",
    "title": "Introduction to Google Earth Engine",
    "section": "Understanding Bitwise Operations",
    "text": "Understanding Bitwise Operations\nHow QA60 Band Stores Cloud Information:\n\n\nQA60 value = 1024 (binary: 10000000000)\n                            ↑\n                         Bit 10 set → Cloud present\n\nBit mask operation:\ncloud_bit_mask = 1 &lt;&lt; 10  # Shift 1 left by 10 = 1024\nqa.bitwiseAnd(cloud_bit_mask)  # Extract bit 10\n\nWhy Bitwise?\n\nEfficient storage (multiple flags in one band)\nBit 10 = Opaque clouds\nBit 11 = Cirrus clouds\nCan check multiple conditions\n\n\n\nQA60 Bit Flags:\n\n\n\nBit\nFlag\n\n\n\n\n10\nOpaque clouds\n\n\n11\nCirrus clouds\n\n\n\nExample Values:\n\n0 = Clear (00000000000)\n1024 = Clouds (10000000000)\n2048 = Cirrus (100000000000)\n3072 = Both (110000000000)\n\n\n\nBitwise operations allow efficient checking of multiple flags stored in a single band. This is common in satellite QA bands."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#advanced-cloud-masking-scl-band",
    "href": "day1/presentations/04_session4_google_earth_engine.html#advanced-cloud-masking-scl-band",
    "title": "Introduction to Google Earth Engine",
    "section": "Advanced Cloud Masking: SCL Band",
    "text": "Advanced Cloud Masking: SCL Band\nScene Classification Layer (SCL) - More Detailed Classification:\ndef mask_s2_clouds_scl(image):\n    \"\"\"Advanced cloud masking using SCL band\"\"\"\n    scl = image.select('SCL')\n\n    # SCL Classification Values:\n    # 3 = Cloud shadows\n    # 4 = Vegetation\n    # 5 = Bare soil\n    # 6 = Water\n    # 8 = Cloud medium probability\n    # 9 = Cloud high probability\n    # 10 = Thin cirrus\n    # 11 = Snow/ice\n\n    # Keep only clear land/water pixels\n    mask = scl.eq(4).Or(scl.eq(5)).Or(scl.eq(6))\n\n    return image.updateMask(mask).divide(10000)\n\nSCL vs QA60: SCL provides more granular classification but requires loading additional band\n\n\nSCL band is available in Sentinel-2 L2A products. Provides detailed scene classification including shadows, vegetation, water, clouds, cirrus, and snow."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#calculating-ndvi",
    "href": "day1/presentations/04_session4_google_earth_engine.html#calculating-ndvi",
    "title": "Introduction to Google Earth Engine",
    "section": "Calculating NDVI",
    "text": "Calculating NDVI\n83dLive Coding Exercise 6\n# Calculate NDVI\nndvi = composite.normalizedDifference(['B8', 'B4']).rename('NDVI')\n\n# Visualization parameters\nndvi_vis = {\n    'min': -0.2,\n    'max': 0.8,\n    'palette': ['brown', 'yellow', 'green', 'darkgreen']\n}\n\nMap.addLayer(ndvi, ndvi_vis, 'NDVI')\n\nDark green = healthy vegetation\n\n\nNDVI = (NIR - Red) / (NIR + Red)\nRanges from -1 to +1: - Negative: Water - 0-0.2: Bare soil - 0.2-0.5: Sparse vegetation - 0.5-0.9: Dense vegetation"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#other-indices",
    "href": "day1/presentations/04_session4_google_earth_engine.html#other-indices",
    "title": "Introduction to Google Earth Engine",
    "section": "Other Indices",
    "text": "Other Indices\n83dLive Coding Exercise 7\n# NDWI (water)\nndwi = composite.normalizedDifference(['B3', 'B8']).rename('NDWI')\n\n# NDBI (built-up)\nndbi = composite.normalizedDifference(['B11', 'B8']).rename('NDBI')\n\n# Add to map\nMap.addLayer(ndwi, {'min': -0.5, 'max': 0.5, 'palette': ['white', 'blue']}, 'NDWI')\nMap.addLayer(ndbi, {'min': -0.5, 'max': 0.5, 'palette': ['green', 'gray']}, 'NDBI')\n\n\nNDWI highlights water bodies\nNDBI highlights urban areas\nMany more indices available for different applications"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#temporal-compositing",
    "href": "day1/presentations/04_session4_google_earth_engine.html#temporal-compositing",
    "title": "Introduction to Google Earth Engine",
    "section": "Temporal Compositing",
    "text": "Temporal Compositing\nCompare different time periods:\n# Dry season (Jan-Mar)\ndry = s2_masked.filterDate('2024-01-01', '2024-03-31').median()\n\n# Wet season (Jul-Sep)\nwet = s2_masked.filterDate('2024-07-01', '2024-09-30').median()\n\n# Calculate NDVI for both\nndvi_dry = dry.normalizedDifference(['B8', 'B4'])\nndvi_wet = wet.normalizedDifference(['B8', 'B4'])\n\n# Difference\nndvi_change = ndvi_wet.subtract(ndvi_dry)\n\nMap.addLayer(ndvi_change, {'min': -0.5, 'max': 0.5, \n                            'palette': ['red', 'white', 'green']}, \n             'NDVI Change')\n\nGreen = vegetation increase, Red = vegetation decrease"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#composite-methods-comparison",
    "href": "day1/presentations/04_session4_google_earth_engine.html#composite-methods-comparison",
    "title": "Introduction to Google Earth Engine",
    "section": "Composite Methods Comparison",
    "text": "Composite Methods Comparison\nDifferent ways to create composites:\n\n\n1. Median Composite\ncomposite = collection.median()\n\nMost common\nReduces outliers\nGood for cloud removal\n\n\n2. Mean Composite\ncomposite = collection.mean()\n\nAverage of all values\nSmooth results\nCan blur features\n\n\n3. Greenest Pixel\ndef add_ndvi(img):\n    ndvi = img.normalizedDifference(['B8','B4'])\n    return img.addBands(ndvi.rename('NDVI'))\n\ncomposite = collection.map(add_ndvi).qualityMosaic('NDVI')\n\nMaximum NDVI pixel\nBest vegetation condition\nIdeal for crop mapping\n\n\n\nGreenest pixel composite selects the pixel with highest NDVI at each location across the time series. Perfect for agricultural applications."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#greenest-pixel-composite-example",
    "href": "day1/presentations/04_session4_google_earth_engine.html#greenest-pixel-composite-example",
    "title": "Introduction to Google Earth Engine",
    "section": "Greenest Pixel Composite Example",
    "text": "Greenest Pixel Composite Example\nPhilippine Rice Monitoring Application:\n# Define Central Luzon rice area\nrice_aoi = ee.Geometry.Rectangle([120.5, 15.0, 121.5, 16.0])\n\n# Load Sentinel-2 for growing season\ns2_rice = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(rice_aoi)\n    .filterDate('2024-06-01', '2024-10-31')  # Main rice season\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .map(maskS2clouds))\n\n# Add NDVI band to each image\ndef add_ndvi_band(image):\n    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n    return image.addBands(ndvi)\n\ns2_with_ndvi = s2_rice.map(add_ndvi_band)\n\n# Create greenest pixel composite\ngreenest_composite = s2_with_ndvi.qualityMosaic('NDVI')\n\n# Visualize\nMap.addLayer(greenest_composite, vis_params_rgb, 'Greenest Pixel - Rice Season')\n\nResult: Captures peak rice biomass across entire growing season\n\n\nGreenest pixel composite is particularly useful for rice monitoring in the Philippines. It captures the peak vegetation condition for each pixel, showing maximum crop development."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#time-series-analysis",
    "href": "day1/presentations/04_session4_google_earth_engine.html#time-series-analysis",
    "title": "Introduction to Google Earth Engine",
    "section": "Time Series Analysis",
    "text": "Time Series Analysis\nExtract time series at a point:\n# Define point (Manila)\npoint = ee.Geometry.Point([121.0, 14.6])\n\n# Function to add date and NDVI\ndef addNDVI(image):\n    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n    return image.addBands(ndvi).set('date', image.date().format('YYYY-MM-dd'))\n\n# Add NDVI to collection\ns2_ndvi = s2_masked.map(addNDVI)\n\n# Extract time series\nts = s2_ndvi.select('NDVI').getRegion(point, 10).getInfo()\n\n# Convert to pandas DataFrame\nimport pandas as pd\ndf = pd.DataFrame(ts[1:], columns=ts[0])\nprint(df.head())\n\nTime series analysis powerful for monitoring changes over time. Can track vegetation seasonality, crop growth, etc."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#philippine-example-rice-monitoring",
    "href": "day1/presentations/04_session4_google_earth_engine.html#philippine-example-rice-monitoring",
    "title": "Introduction to Google Earth Engine",
    "section": "Philippine Example: Rice Monitoring",
    "text": "Philippine Example: Rice Monitoring\n83dLive Coding Exercise 8 - Complete Workflow\n# Rice growing area (Central Luzon)\nrice_aoi = ee.Geometry.Rectangle([120.5, 15.0, 121.0, 15.5])\n\n# One year of data\nrice_s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\\\n    .filterBounds(rice_aoi) \\\\\n    .filterDate('2024-01-01', '2024-12-31') \\\\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30)) \\\\\n    .map(maskS2clouds)\n\n# Monthly composites\ndef monthlyComposite(month):\n    start = ee.Date.fromYMD(2024, month, 1)\n    end = start.advance(1, 'month')\n    return rice_s2.filterDate(start, end).median() \\\\\n        .set('month', month)\n\n# Create 12 monthly NDVI composites\nmonths = ee.List.sequence(1, 12)\nmonthly_ndvi = ee.ImageCollection(months.map(monthlyComposite)) \\\\\n    .map(lambda img: img.normalizedDifference(['B8', 'B4']))\n\n# Visualize (add to map, create chart, etc.)\n\nComplete workflow: AOI, filtering, cloud masking, compositing, index calculation. This pattern applies to many EO applications."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#exporting-data",
    "href": "day1/presentations/04_session4_google_earth_engine.html#exporting-data",
    "title": "Introduction to Google Earth Engine",
    "section": "Exporting Data",
    "text": "Exporting Data\nExport to Google Drive:\n# Export image\ntask = ee.batch.Export.image.toDrive(\n    image=composite,\n    description='Palawan_S2_Composite',\n    folder='GEE_Exports',\n    region=aoi,\n    scale=10,\n    crs='EPSG:4326',\n    maxPixels=1e9\n)\n\n# Start task\ntask.start()\n\n# Check status\nprint('Task Status:', task.status())\n\nFind exported file in Google Drive!\n\n\nExports run in background. Can take minutes to hours depending on size. Check task manager for progress."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#export-options",
    "href": "day1/presentations/04_session4_google_earth_engine.html#export-options",
    "title": "Introduction to Google Earth Engine",
    "section": "Export Options",
    "text": "Export Options\n\n\nExport Types:\n\ntoDrive() - Google Drive\ntoAsset() - GEE Asset (reuse in GEE)\ntoCloudStorage() - Google Cloud Storage\n\nData Types:\n\nImage (raster)\nTable (vector)\nVideo (time series animation)\n\n\nBest Practices:\n\nSet appropriate scale (resolution)\nDefine region (don’t export globally!)\nUse maxPixels wisely\nCheck crs matches your needs\nMonitor tasks in Code Editor"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#integration-with-ml-workflows",
    "href": "day1/presentations/04_session4_google_earth_engine.html#integration-with-ml-workflows",
    "title": "Introduction to Google Earth Engine",
    "section": "Integration with ML Workflows",
    "text": "Integration with ML Workflows\nGEE → Python ML Pipeline:\n# 1. Process in GEE (fast, scalable)\ncomposite = s2_masked.median()\nndvi = composite.normalizedDifference(['B8', 'B4'])\n\n# 2. Sample training data\ntraining = ndvi.sampleRegions(\n    collection=training_polygons,\n    scale=10\n)\n\n# 3. Export to Drive\nee.batch.Export.table.toDrive(\n    collection=training,\n    description='training_data',\n    fileFormat='CSV'\n).start()\n\n# 4. Download and use in scikit-learn/TensorFlow (Day 2!)\n\nGEE perfect for preprocessing. Then export for ML training. We’ll do this extensively in Days 2-4."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#geemap-advanced-features",
    "href": "day1/presentations/04_session4_google_earth_engine.html#geemap-advanced-features",
    "title": "Introduction to Google Earth Engine",
    "section": "geemap Advanced Features",
    "text": "geemap Advanced Features\nSplit-panel comparison:\nleft_layer = geemap.ee_tile_layer(dry, vis_params, 'Dry Season')\nright_layer = geemap.ee_tile_layer(wet, vis_params, 'Wet Season')\n\nMap = geemap.Map()\nMap.split_map(left_layer, right_layer)\nMap\nTime slider:\nMap.add_time_slider(monthly_ndvi, vis_params, date_format='YYYY-MM')\nInteractive charting, legends, colorbars, and more!\n\ngeemap has many advanced features. Explore documentation for more."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#case-study-1-typhoon-impact-assessment",
    "href": "day1/presentations/04_session4_google_earth_engine.html#case-study-1-typhoon-impact-assessment",
    "title": "Introduction to Google Earth Engine",
    "section": "Case Study 1: Typhoon Impact Assessment",
    "text": "Case Study 1: Typhoon Impact Assessment\nScenario: Assess vegetation damage from Typhoon Odette (Rai) - December 2021\n\n\n# Define affected region (Bohol & Cebu)\nvisayas_aoi = ee.Geometry.Rectangle([123.5, 9.5, 125.0, 11.0])\n\n# Pre-typhoon (November 2021)\npre_typhoon = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(visayas_aoi)\n    .filterDate('2021-11-01', '2021-11-30')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .map(maskS2clouds)\n    .median())\n\n# Post-typhoon (January 2022)\npost_typhoon = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(visayas_aoi)\n    .filterDate('2022-01-15', '2022-02-15')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .map(maskS2clouds)\n    .median())\n\n# Calculate NDVI change\nndvi_pre = pre_typhoon.normalizedDifference(['B8', 'B4'])\nndvi_post = post_typhoon.normalizedDifference(['B8', 'B4'])\nndvi_damage = ndvi_post.subtract(ndvi_pre)\n\nMap.addLayer(ndvi_damage,\n    {'min': -0.5, 'max': 0.1, 'palette': ['red', 'yellow', 'white']},\n    'Vegetation Damage')\n\nAnalysis:\n\nRed areas = severe damage\nYellow = moderate damage\nCoastal coconut plantations heavily affected\nRapid assessment for disaster response\n\nOutput: Damage map for NDRRMC\n\n\nTyphoon Odette (international name Rai) was one of the strongest typhoons to hit the Philippines in 2021. GEE enabled rapid damage assessment."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#case-study-2-manila-bay-water-quality",
    "href": "day1/presentations/04_session4_google_earth_engine.html#case-study-2-manila-bay-water-quality",
    "title": "Introduction to Google Earth Engine",
    "section": "Case Study 2: Manila Bay Water Quality",
    "text": "Case Study 2: Manila Bay Water Quality\nScenario: Monitor turbidity and suspended sediment in Manila Bay\n# Define Manila Bay AOI\nmanila_bay = ee.Geometry.Polygon([\n    [[120.7, 14.4], [120.95, 14.4], [121.0, 14.65],\n     [120.75, 14.75], [120.7, 14.4]]\n])\n\n# Load Sentinel-2 (dry season 2024)\ns2_manila = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(manila_bay)\n    .filterDate('2024-02-01', '2024-04-30')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n    .map(maskS2clouds)\n    .median())\n\n# Calculate Turbidity Index (Red/Green ratio)\nturbidity = s2_manila.select('B4').divide(s2_manila.select('B3'))\n\nMap.addLayer(turbidity,\n    {'min': 0.5, 'max': 2.0, 'palette': ['blue', 'cyan', 'yellow', 'red']},\n    'Manila Bay Turbidity')\n\nApplication: Monitor rehabilitation progress, identify pollution sources\n\n\nManila Bay rehabilitation is a major government initiative. Satellite monitoring provides objective, regular assessment of water quality across the entire bay."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#case-study-3-rice-paddy-phenology-sentinel-1",
    "href": "day1/presentations/04_session4_google_earth_engine.html#case-study-3-rice-paddy-phenology-sentinel-1",
    "title": "Introduction to Google Earth Engine",
    "section": "Case Study 3: Rice Paddy Phenology (Sentinel-1)",
    "text": "Case Study 3: Rice Paddy Phenology (Sentinel-1)\nScenario: Track rice growth stages using SAR in Central Luzon\n# Define rice area (Nueva Ecija)\nrice_region = ee.Geometry.Rectangle([120.8, 15.3, 121.3, 15.8])\n\n# Load Sentinel-1 time series (wet season 2024)\ns1_rice = (ee.ImageCollection('COPERNICUS/S1_GRD')\n    .filterBounds(rice_region)\n    .filterDate('2024-06-01', '2024-11-30')\n    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n    .select('VH'))  # VH sensitive to rice canopy\n\n# Create time series chart\nchart = geemap.image_series_by_region(\n    s1_rice, rice_region, reducer='mean',\n    scale=100, x_property='system:time_start'\n)\nchart\n\nPhenology Pattern:\n\nLow VH = flooding/transplanting\nRising VH = vegetative growth\nPeak VH = heading/flowering\nDeclining VH = maturity/harvest\n\n\n\nSentinel-1 SAR penetrates clouds, making it ideal for monitoring rice in the rainy season. VH backscatter correlates with rice biomass and growth stage."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#case-study-4-mangrove-monitoring-in-palawan",
    "href": "day1/presentations/04_session4_google_earth_engine.html#case-study-4-mangrove-monitoring-in-palawan",
    "title": "Introduction to Google Earth Engine",
    "section": "Case Study 4: Mangrove Monitoring in Palawan",
    "text": "Case Study 4: Mangrove Monitoring in Palawan\nScenario: Map and monitor mangrove forest extent in Puerto Princesa\n# Define Palawan coastal area\npalawan_coast = ee.Geometry.Rectangle([118.7, 9.5, 119.0, 10.0])\n\n# Load recent Sentinel-2\ns2_mangrove = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(palawan_coast)\n    .filterDate('2024-01-01', '2024-12-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n    .map(maskS2clouds)\n    .median())\n\n# Mangrove index: NDVI + NDWI combination\nndvi = s2_mangrove.normalizedDifference(['B8', 'B4'])\nndwi = s2_mangrove.normalizedDifference(['B3', 'B8'])\n\n# Simple mangrove classifier\nmangrove_mask = ndvi.gt(0.3).And(ndwi.gt(-0.1))\n\nMap.addLayer(mangrove_mask.selfMask(),\n    {'palette': ['green']},\n    'Potential Mangrove Areas')\n\n# Calculate area\nmangrove_area = mangrove_mask.multiply(ee.Image.pixelArea()).reduceRegion(\n    reducer=ee.Reducer.sum(),\n    geometry=palawan_coast,\n    scale=10,\n    maxPixels=1e9\n)\n\nprint('Mangrove area (hectares):',\n      ee.Number(mangrove_area.get('nd')).divide(10000).getInfo())\n\nMangroves are critical coastal ecosystems in the Philippines. GEE enables monitoring changes over time for conservation planning."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#philippine-applications-summary",
    "href": "day1/presentations/04_session4_google_earth_engine.html#philippine-applications-summary",
    "title": "Introduction to Google Earth Engine",
    "section": "Philippine Applications Summary",
    "text": "Philippine Applications Summary\nWhat GEE Enables for Philippines:\nDisaster Response: - Flood mapping during typhoons - Damage assessment - Recovery monitoring\nAgricultural Monitoring: - Rice area mapping (PRiSM program) - Crop health assessment - Yield prediction\nEnvironmental Management: - Forest cover change - Mangrove monitoring - Water quality assessment\nUrban Planning: - Land cover mapping - Urban expansion tracking - Infrastructure development\n\nAll at national scale, updated regularly, cloud-free!\n\n\nGEE’s planetary-scale capabilities make it ideal for Philippines-wide monitoring. Free access democratizes satellite data analysis for all agencies and researchers."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#session-summary",
    "href": "day1/presentations/04_session4_google_earth_engine.html#session-summary",
    "title": "Introduction to Google Earth Engine",
    "section": "Session Summary",
    "text": "Session Summary\nWhat You’ve Learned:\n✅ GEE platform & Python API authentication ✅ Core concepts: Image, ImageCollection, filtering, reducing ✅ Accessing Sentinel-1 and Sentinel-2 data ✅ Cloud masking (QA60 bitwise operations & SCL band) ✅ Calculating spectral indices (NDVI, NDWI, NDBI) ✅ Temporal compositing (median, mean, greenest pixel) ✅ Time series analysis and multi-temporal comparison ✅ Visualization with geemap ✅ Exporting data for ML workflows ✅ Philippine case studies (typhoon, water quality, rice, mangroves)\n\nTiming: 3 minutes\nYou now have GEE skills to access and process satellite data at scale. Perfect foundation for Days 2-4 ML work."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#qa",
    "href": "day1/presentations/04_session4_google_earth_engine.html#qa",
    "title": "Introduction to Google Earth Engine",
    "section": "Q&A",
    "text": "Q&A\nCommon Questions:\n\n\n\nGEE free tier limits?\nJavaScript vs Python trade-offs?\nHow to handle large exports?\nBest practices for efficiency?\n\n\n\nWorking with Landsat data?\nCustom algorithms in GEE?\nIntegration with QGIS?\nWhere to learn more?\n\n\n\nCommon Answers: - Free tier: 250GB cloud storage, generous compute - Python better for ML integration - Export in tiles if too large - Filter early, compute late - Landsat similar to Sentinel-2 - Custom: Use .map() with functions - QGIS: Use Earth Engine plugin"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#resources",
    "href": "day1/presentations/04_session4_google_earth_engine.html#resources",
    "title": "Introduction to Google Earth Engine",
    "section": "Resources",
    "text": "Resources\nOfficial Documentation:\nhttps://developers.google.com/earth-engine\nPython API:\nhttps://geemap.org\nTutorials:\nhttps://developers.google.com/earth-engine/tutorials\nCommunity:\nhttps://groups.google.com/forum/#!forum/google-earth-engine-developers\nAwesome GEE:\nhttps://github.com/giswqs/Awesome-GEE"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#amazing-progress-today",
    "href": "day1/presentations/04_session4_google_earth_engine.html#amazing-progress-today",
    "title": "Introduction to Google Earth Engine",
    "section": "Amazing Progress Today!",
    "text": "Amazing Progress Today!\nYou’ve mastered:\n\n✅ Copernicus & Philippine EO ecosystem\n✅ AI/ML fundamentals for EO\n✅ Python geospatial libraries (GeoPandas, Rasterio)\n✅ Google Earth Engine Python API\n\n\nTomorrow: Apply these skills to real ML problems!"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#day-2-preview",
    "href": "day1/presentations/04_session4_google_earth_engine.html#day-2-preview",
    "title": "Introduction to Google Earth Engine",
    "section": "Day 2 Preview",
    "text": "Day 2 Preview\nMachine Learning for Earth Observation\n\n\nMorning: - Random Forest classification - Training data preparation - Model evaluation - Palawan land cover mapping\n\nAfternoon: - Deep learning introduction - CNN for imagery - Transfer learning - Building damage assessment\n\n\nSee you tomorrow! 🚀"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#excellent-work-today",
    "href": "day1/presentations/04_session4_google_earth_engine.html#excellent-work-today",
    "title": "Introduction to Google Earth Engine",
    "section": "Excellent Work Today!",
    "text": "Excellent Work Today!\n\nRest well.\nTomorrow we build AI models!\n\n\nDay 1 Complete!\nEnsure participants: - Save their notebooks - Review exercises if needed - Come prepared for hands-on ML tomorrow\nTotal training time Day 1: ~8 hours (with breaks)"
  }
]