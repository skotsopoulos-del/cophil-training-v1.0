[
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#course-introduction",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#course-introduction",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Course Introduction",
    "text": "Course Introduction\n\n\n4-Day Advanced Training\n\nAI/ML for Earth Observation\nPhilippine EO Professionals\nFocus: DRR, CCA, NRM\nOnline format\n\n\nToday’s Goals\n\nUnderstand Copernicus data\nExplore Philippine EO ecosystem\nLearn AI/ML fundamentals\nHands-on Python and GEE\n\n\n\nWelcome participants to the 4-day advanced training. Emphasize that this is part of the EU-Philippines partnership and will provide practical skills for disaster risk reduction, climate change adaptation, and natural resource management."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#eu-global-gateway-initiative",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#eu-global-gateway-initiative",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "EU Global Gateway Initiative",
    "text": "EU Global Gateway Initiative\n\n\n\nEU-Philippines space cooperation flagship\nBuilding strong partnerships\nSmart, clean, secure digital links\nStrengthening health, education, research systems globally\n\n\n\n\n\nThe Global Gateway strategy represents the EU’s commitment to building partnerships that boost smart, clean and secure infrastructure globally. CoPhil is a unique flagship initiative within this framework."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#cophil-programme-overview",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#cophil-programme-overview",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "CoPhil Programme Overview",
    "text": "CoPhil Programme Overview\n\n\nMission\nSupport Philippine Space Agency (PhilSA) and DOST to improve use of Earth Observation data for:\n\nDisaster Risk Reduction (DRR)\nClimate Change Adaptation (CCA)\nNatural Resource Management (NRM)\n\n\nKey Outputs\n\nCopernicus Mirror Site\nDigital Space Campus\nCapacity building\nPilot services\n\n\n\nCoPhil is an EU-funded Technical Assistance programme positioning the Philippines as a pioneer in the EU’s international cooperation on Copernicus."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#philsa-dost-partnership",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#philsa-dost-partnership",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "PhilSA & DOST Partnership",
    "text": "PhilSA & DOST Partnership\n\n\nPhilippine Space Agency\n\n\nEstablished 2019\nCentral civilian space agency\nSpace+ Data Dashboard\nCo-chair of CoPhil\n\n\nDepartment of Science and Technology\n\n\nASTI AI initiatives\nSkAI-Pinas program\nNational AI investments\nCo-chair of CoPhil\n\n\n\nBoth PhilSA and DOST are co-chairs of the CoPhil programme, demonstrating strong national commitment to building EO and AI capacity."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#session-1-roadmap",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#session-1-roadmap",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Session 1 Roadmap",
    "text": "Session 1 Roadmap\n\nCopernicus Programme Overview\nSentinel-1 Mission (SAR)\nSentinel-2 Mission (Optical)\nData Access Methods\nPhilippine EO Ecosystem\nCoPhil Infrastructure\n\nDuration: 2 hours\n\nThis session provides the foundation for understanding where EO data comes from and how Philippine agencies complement Copernicus data."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#what-is-copernicus",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#what-is-copernicus",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "What is Copernicus?",
    "text": "What is Copernicus?\n\n\nEurope’s Eyes on Earth\n\nEU flagship Earth Observation program\nFamily of Sentinel satellites\nFree and open data policy\nOperational since 2014\n\n\n\n\n\n“Looking at our planet and its environment for the benefit of all European citizens”\n\n\nCopernicus is the European Union’s Earth Observation program, providing free and open satellite data globally. It comprises a fleet of Sentinel satellites designed for environmental monitoring."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#copernicus-programme-architecture",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#copernicus-programme-architecture",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Copernicus Programme Architecture",
    "text": "Copernicus Programme Architecture\n\nCopernicus Programme Structure showing Space Component, Services, and End Users\nThis diagram shows the complete Copernicus architecture: the space component with Sentinel missions, in-situ data sources, the six core services (CAMS, CMEMS, Land, Climate, Emergency, Security), and the various end-user categories."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#the-sentinel-family",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#the-sentinel-family",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "The Sentinel Family",
    "text": "The Sentinel Family\n\n\nSentinel-1 (SAR)\n\nC-band radar imaging\nAll-weather, day/night\n1A operational, 1C launched Dec 2024\n6-day repeat (dual constellation)\n\nSentinel-2 (Optical)\n\nMultispectral imaging\n13 spectral bands\n2A, 2B, 2C operational Jan 2025\n5-day repeat (three satellites)\n\n\nSentinel-3 (Ocean/Land)\n\nOcean and land monitoring\nSea surface temperature\nOcean color, vegetation\n\nSentinel-5P (Atmosphere)\n\nAir quality monitoring\nAtmospheric composition\n\n\n\nTiming: 4 minutes\nKey Points: - Six Sentinel missions operational (1, 2, 3, 5P, 6) with 4, 7-12 planned - 2025 Update: Sentinel-1C launched December 2024, restoring 6-day global repeat - 2025 Update: Sentinel-2C operational January 2025, creating 3-satellite constellation with 5-day repeat - Today we focus on Sentinel-1 and Sentinel-2 - most relevant for Philippine DRR/CCA/NRM - Sentinel-3 important for marine/coastal applications - Sentinel-5P monitors air pollution (useful for Manila air quality)\nTransition: “These missions generate petabytes of free data. Let’s see how this data is used…”"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#copernicus-applications",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#copernicus-applications",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Copernicus Applications",
    "text": "Copernicus Applications\n\n\n\nEmergency Management\n\nFlood mapping\nFire detection\nDisaster response\n\n\nClimate & Environment\n\nDeforestation monitoring\nAgricultural monitoring\nWater quality assessment\n\n\n\nCopernicus supports applications in emergency management, health care, agriculture, and environment, with crucial impact on society, climate, and economy."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-overview",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-overview",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Overview",
    "text": "Sentinel-1 Overview\n\n\nMission Configuration (2025)\n\nSensor: C-band Synthetic Aperture Radar\nSatellites: 1A, 1C (operational 2025)\nOrbit: Polar sun-synchronous\nAll-weather, day/night capability\n\n\n\n\n\nKey Advantage: Penetrates clouds and works at night\n\n\nSentinel-1 is a radar imaging mission with C-band SAR enabling all-weather, day-and-night observations unaffected by clouds - crucial in tropical regions like the Philippines."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-technical-specifications",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-technical-specifications",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Technical Specifications",
    "text": "Sentinel-1 Technical Specifications\n\n\n\nParameter\nValue\n\n\n\n\nSensor Type\nC-band SAR (5.405 GHz)\n\n\nRevisit Time\n6-12 days (constellation)\n\n\nSwath Width\n250 km (IW mode)\n\n\nSpatial Resolution\n5m × 20m (IW mode)\n\n\nPolarization\nVV + VH or HH + HV\n\n\nOrbit\n693 km altitude\n\n\n\n\nSentinel-1 operates in Interferometric Wide (IW) mode with ~5m by 20m spatial resolution. With two satellites, the revisit cycle is 6 days globally. Note: Sentinel-1B became inoperative in 2022; Sentinel-1C launched in 2024 to restore 6-day revisit."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sar-how-it-works",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sar-how-it-works",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "SAR: How It Works",
    "text": "SAR: How It Works\n\n\nSatellite sends microwave pulses to Earth\nSignal reflects from surface\nSensor measures backscatter intensity\nDifferent surfaces = different backscatter\n\n\nSAR satellites send microwave signals and measure the backscatter. Water appears dark (low backscatter), urban areas appear bright (strong backscatter), and vegetation shows moderate backscatter."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-polarization",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-polarization",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Polarization",
    "text": "Sentinel-1 Polarization\n\n\nWhat is Polarization?\n\nOrientation of radar wave\nVV: Vertical send/receive\nVH: Vertical send/Horizontal receive\nHH: Horizontal send/receive\n\n\nApplications\n\nVV: Good for water/flood mapping\nVH: Sensitive to volume scattering (vegetation)\nVV/VH Ratio: Discriminates surface types\n\n\n\n\nSentinel-1 IW mode over land typically provides VV and VH polarizations. Different polarizations are sensitive to different surface characteristics."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#backscatter-characteristics-by-target",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#backscatter-characteristics-by-target",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Backscatter Characteristics by Target",
    "text": "Backscatter Characteristics by Target\n\n\n\nSurface Type\nBackscatter\nAppearance\nReason\n\n\n\n\nWater (smooth)\nVery Low\nDark/Black\nSpecular reflection\n\n\nUrban/Buildings\nVery High\nBright White\nCorner reflectors\n\n\nForest/Vegetation\nMedium-High\nGray\nVolume scattering\n\n\nAgricultural Fields\nMedium\nLight Gray\nSurface roughness\n\n\nBare Soil (dry)\nLow-Medium\nDark Gray\nSmooth surface\n\n\nBare Soil (wet)\nMedium\nMedium Gray\nIncreased dielectric\n\n\n\n\nKey Insight: Water appears dark, structures appear bright - the basis for flood mapping!\n\n\nUnderstanding backscatter is crucial for interpreting SAR imagery. Smooth surfaces (water) reflect signals away = dark. Rough surfaces and structures reflect signals back = bright."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-imaging-modes",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-imaging-modes",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Imaging Modes",
    "text": "Sentinel-1 Imaging Modes\n\n\nInterferometric Wide Swath (IW) - 250 km swath - 5m × 20m resolution - Default over land - Philippine standard mode\nExtra Wide Swath (EW) - 400 km swath - 20m × 40m resolution - Maritime/polar regions\n\nStrip Map (SM) - 80 km swath - 5m × 5m resolution - Emergency response - High detail needed\nWave (WV) - Ocean waves - Not used for land\n\n\nInterferometric Wide (IW) mode is used over land and provides the best balance of resolution and coverage. All Philippine acquisitions use IW mode."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-data-products",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-data-products",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Data Products",
    "text": "Sentinel-1 Data Products\n\n\nLevel-1 GRD (Ground Range Detected)\n\nMulti-looked (reduced speckle)\nProjected to ground range\nMost commonly used\nFaster to process\nSmaller file size\nApplications:\n\nChange detection\nClassification\nFlood mapping\nShip detection\n\n\n\nLevel-1 SLC (Single Look Complex)\n\nPreserves phase information\nComplex-valued pixels\nRequired for InSAR\nLarger files\nApplications:\n\nGround deformation\nInterferometry\nCoherence analysis\nSubsidence monitoring\n\n\n\n\nFor most applications including flood mapping and land cover, GRD products are sufficient. SLC products are needed for advanced interferometric applications like volcano monitoring."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#grd-vs-slc---which-to-choose",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#grd-vs-slc---which-to-choose",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "GRD vs SLC - Which to Choose?",
    "text": "GRD vs SLC - Which to Choose?\n\n\n\nFactor\nGRD\nSLC\n\n\n\n\nUse Case\nMost applications\nInterferometry only\n\n\nProcessing\nReady to use\nComplex processing\n\n\nFile Size\n~1 GB\n~4 GB\n\n\nSpeckle\nReduced\nFull speckle\n\n\nPhase\nNot preserved\nPreserved\n\n\nTypical User\nMost analysts\nAdvanced specialists\n\n\n\n\nFor this training and most Philippine applications: Use GRD\n\n\nUnless you specifically need interferometry (earthquake/volcano deformation), always use GRD products. They’re easier to work with and sufficient for 90% of applications."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-pre-processing-under-the-hood",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-pre-processing-under-the-hood",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Pre-Processing “Under the Hood”",
    "text": "Sentinel-1 Pre-Processing “Under the Hood”\n\n\n\n\n\n\nWhat Happens Before You See SAR Data?\n\n\nFor this training, we use pre-processed Sentinel-1 GRD data. Here’s what happens “under the hood”:\n\n\n\nS1 Processing Pipeline:\n\nGRD Download → Raw ground-range detected amplitude\nRadiometric Calibration → Convert to backscatter coefficient (σ⁰)\nTerrain Correction (RTC) → Remove topographic distortions using DEM\nSpeckle Filtering → Reduce SAR noise (Lee, Refined Lee, or Gamma-MAP filters)\nConversion to dB → γ⁰ (gamma-naught) in decibels for visual interpretation\nTiling/Clipping → Extract area of interest\n\n\nFor Day 3 flood mapping labs: We provide analysis-ready patches with these steps already applied\n\n\nP0 IMPROVEMENT APPLIED: Explicit S1 pre-processing assumptions before U-Net flood lab (Day 3).\nKey Teaching Points: - Participants need to understand what “analysis-ready” means - RTC removes terrain effects - critical in mountainous Philippines - Gamma-naught (γ⁰) in dB is standard for land applications - Speckle filtering is essential but can blur edges - These steps are computationally intensive - we pre-process for efficiency\nTiming: 3 minutes\nTransition: “Now let’s look at Sentinel-2 optical data…”"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-applications",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-1-applications",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-1 Applications",
    "text": "Sentinel-1 Applications\n\n\nFlood Mapping\n\n\n\nCopernicus Sentinel-1 Flood Monitoring\n\n\n\nWater appears dark in SAR\nWorks through clouds\nNear real-time monitoring\n\n\nDeformation Monitoring\n\n\n\nSentinel-1C Interferogram of Northern Chile\n\n\n\nInSAR technique\nMillimeter precision\nVolcano and earthquake monitoring\n\n\n\nSentinel-1’s ability to penetrate clouds makes it invaluable for flood mapping during typhoons. The left image shows actual flood monitoring using Sentinel-1 data. The right image shows an interferogram from Sentinel-1C demonstrating ground deformation detection with millimeter precision in northern Chile."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#philippine-example-flood-monitoring",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#philippine-example-flood-monitoring",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Philippine Example: Flood Monitoring",
    "text": "Philippine Example: Flood Monitoring\n\n\n\nNovember 2020: Typhoon Ulysses\n\nExtensive flooding in Luzon\nSentinel-1 detected flood extent\nRapid mapping capability\n\n\nKey Benefits\n\nNo cloud interference\nQuick response time\nUsed for rapid damage assessment\nSupported emergency response\n\n\n\nDuring Typhoon Ulysses, Sentinel-1 provided crucial flood extent mapping when optical satellites couldn’t see through clouds. This data supported PAGASA and NDRRMC response efforts."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-overview",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-overview",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-2 Overview",
    "text": "Sentinel-2 Overview\n\n\nKey Specifications:\n\n13 spectral bands (visible, NIR, SWIR)\n10m to 60m spatial resolution\n290 km swath width\n5-day revisit (three satellites operational 2025)\nL1C & L2A processing levels\n\n\nPhilippine Example: Mayon Volcano\n\n\n\n\n\nSentinel-2 monitoring of 2018 eruption\n\nFalse-color composite highlighting lava flows\n10m resolution captures detail\n5-day revisit enables continuous monitoring\n\n\n\nThis Sentinel-2 false-color image of Mayon Volcano from 2018 shows recent lava flows. With Sentinel-2C operational in 2025, the three-satellite constellation provides 5-day global revisit."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-spectral-bands",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-spectral-bands",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-2 Spectral Bands",
    "text": "Sentinel-2 Spectral Bands\n\n\n\n\n\n\n\n\n\n\n\nBand\nName\nWavelength (nm)\nResolution\nPurpose\n\n\n\n\nB1\nCoastal Aerosol\n443\n60m\nAerosol correction, water color\n\n\nB2\nBlue\n490\n10m\nWater bodies, atmospheric\n\n\nB3\nGreen\n560\n10m\nVegetation health\n\n\nB4\nRed\n665\n10m\nVegetation discrimination\n\n\nB5\nRed Edge 1\n705\n20m\nVegetation stress detection\n\n\nB6\nRed Edge 2\n740\n20m\nVegetation classification\n\n\nB7\nRed Edge 3\n783\n20m\nVegetation stress, chlorophyll\n\n\nB8\nNIR\n842\n10m\nBiomass, water bodies\n\n\nB8A\nNIR Narrow\n865\n20m\nAtmospheric correction\n\n\nB9\nWater Vapor\n945\n60m\nAtmospheric correction\n\n\nB10\nSWIR Cirrus\n1375\n60m\nCirrus cloud detection\n\n\nB11\nSWIR 1\n1610\n20m\nMoisture content, fire\n\n\nB12\nSWIR 2\n2190\n20m\nMoisture, geology, soil\n\n\n\n\n\nAll 13 Sentinel-2 bands: Four 10m bands (B2-B4, B8) for detailed mapping, six 20m bands including the unique Red Edge trio (B5-B7) for vegetation analysis, and three 60m atmospheric bands (B1, B9, B10) for corrections. The Red Edge bands are Sentinel-2’s signature capability."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#red-edge-bands---sentinel-2s-special-capability",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#red-edge-bands---sentinel-2s-special-capability",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Red Edge Bands - Sentinel-2’s Special Capability",
    "text": "Red Edge Bands - Sentinel-2’s Special Capability\n\n\nWhat is Red Edge?\n\nTransition zone between red and NIR (700-780nm)\nThree dedicated bands (B5, B6, B7)\nSensitive to chlorophyll content\nUnique to Sentinel-2 among free satellites\n\nApplications:\n\nEarly vegetation stress detection\nCrop health monitoring\nForest disease identification\nPre-harvest yield estimation\n\n\nPhilippine Use Cases:\n\nRice crop assessment - Monitor crop health and nitrogen status during panicle initiation in Nueva Ecija rice paddies\nCoconut disease detection - Identify stem bleeding disease and pest infestations through spectral signatures\nMangrove health monitoring - Track vegetation stress and recovery in Palawan mangrove forests using red edge indices\n\n\nRed edge bands detect stress weeks before visible bands show changes\n\n\n\nRed Edge bands are Sentinel-2’s superpower. They detect vegetation stress weeks before visible bands show changes - crucial for precision agriculture and forest health monitoring."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-data-products",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-data-products",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-2 Data Products",
    "text": "Sentinel-2 Data Products\n\n\nLevel-1C (L1C)\n\nTop-of-Atmosphere reflectance\nRadiometrically corrected\nGeometrically refined\nNo atmospheric correction\nUse: If you need raw data for custom processing\n\n\nLevel-2A (L2A)\n\nBottom-of-Atmosphere (surface) reflectance\nAtmospherically corrected\nAnalysis-ready\nScene Classification Layer included\nUse: For most applications - RECOMMENDED\n\n\n\nAlways use Level-2A when available - it’s analysis-ready!\n\n\nL2A data is atmospherically corrected and ready for analysis. Unless you have a specific reason to use L1C, always choose L2A products."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-band-combinations",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentinel-2-band-combinations",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Sentinel-2 Band Combinations",
    "text": "Sentinel-2 Band Combinations\n\n\nTrue Color (Natural) - RGB: B4-B3-B2 (Red-Green-Blue) - Looks like a photograph - Good for visual inspection\nFalse Color Infrared - RGB: B8-B4-B3 (NIR-Red-Green) - Vegetation appears RED - Classic for vegetation assessment\n\nSWIR Composite (Agriculture) - RGB: B11-B8-B2 - Highlights crop moisture - Soil moisture visible\nSWIR-NIR-Red (Burn/Fire) - RGB: B12-B8-B4 - Active fires appear BRIGHT - Burn scars dark purple\n\n\nDifferent band combinations highlight different features. False Color (NIR-Red-Green) is the most common for vegetation mapping - healthy vegetation appears bright red."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#complementary-capabilities",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#complementary-capabilities",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Complementary Capabilities",
    "text": "Complementary Capabilities\n\n\n\n\n\n\n\n\nAspect\nSentinel-1\nSentinel-2\n\n\n\n\nSensor Type\nRadar (SAR)\nOptical (MSI)\n\n\nWeather\nAll-weather\nCloud-affected\n\n\nTime\nDay & night\nDaytime only\n\n\nResolution\n5m × 20m\n10m / 20m / 60m\n\n\nRevisit\n6-12 days\n5 days\n\n\nBands\nPolarizations (2)\nSpectral bands (13)\n\n\nBest For\nWater, structure, moisture\nVegetation, land cover, color\n\n\n\n\n\n\n\n\n\nS1 Flood Mapping Tip\n\n\nVV polarization shows dark water (low backscatter from smooth surfaces)\nBest practice: Compare pre-event vs post-event delta for reliability\nSynergy: Pair S1 (flood extent through clouds) with S2 (vegetation damage when clear) for complete impact assessment\n\n\n\n\nSentinel-1 and Sentinel-2 are complementary. SAR penetrates clouds and works at night, while optical provides rich spectral information about surface properties.\nMICRO-EDIT APPLIED: Added S1 vs S2 flood tip to set up Day 3 logic - VV dark water, pre/post comparison, and S1+S2 synergy."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#synergistic-use-cases",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#synergistic-use-cases",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Synergistic Use Cases",
    "text": "Synergistic Use Cases\n\n\nFlood Mapping\n\nS1: Detect water extent (through clouds)\nS2: Assess damage to vegetation/crops (when clear)\nCombined: Complete flood impact assessment\n\n\nForest Monitoring\n\nS1: Detect structural changes, biomass\nS2: Identify tree species, health\nCombined: Comprehensive forest mapping\n\n\n\n\nUsing both missions together provides more complete information. After a typhoon, S1 maps floods under clouds while S2 assesses vegetation damage where it’s clear."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#minute-break",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#minute-break",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "☕ 5-Minute Break",
    "text": "☕ 5-Minute Break\n\nStretch Break\nStand up • Grab water • Back in 5 minutes\n\n\nTiming: 5 minutes\nInstructor Actions: - Announce break clearly - Mention return time - Leave slides/browser open for questions - Be available for quick questions during break\nWhen Resuming: - Wait 1-2 minutes for everyone to return - Quick recap: “We’ve covered Sentinel-1 SAR and Sentinel-2 optical. Now we’ll explore the Philippine EO ecosystem.”"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#platform-choices-for-this-training",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#platform-choices-for-this-training",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Platform Choices for This Training",
    "text": "Platform Choices for This Training\n\n\n\n\n\n\nWhich Platform for Which Task?\n\n\nUnderstanding where to work is crucial - don’t try to train deep models on GEE or download 100GB in Colab!\n\n\n\n\n\n\n\n\n\n\n\n\nTask\nPlatform\nWhy\nLimitations\n\n\n\n\nData Prep & Exploration\nGoogle Earth Engine\nPetabyte catalog, no download, cloud composites\nExport limit 32 MB (tile large areas), no deep learning training\n\n\nML Training (RF, shallow)\nGEE or Colab\nRF works in GEE; small data in Colab\nGEE memory limits; Colab free tier quotas\n\n\nDeep Learning (CNN, U-Net)\nLocal GPU / Colab Pro\nRequires PyTorch/TensorFlow\nColab free = limited GPU time; large models need local resources\n\n\nLarge-Scale Processing\nCoPhil Mirror Site / COARE\n400TB local data, HPC resources\nRequires account; learning curve for APIs\n\n\nQuick Viz & Download\nCopernicus Browser\nInteractive, fast previews\nManual selection; bulk downloads tedious\n\n\n\n\nQuotas/Pitfalls to know: - GEE: Memory errors with large computations (tile exports!) - Colab Free: GPU disconnects after inactivity; limited sessions/day - CoPhil/Digital Space Campus: Hosts this training’s materials + local data access\n\n\nP1 IMPROVEMENT APPLIED: Explicit platform choices table prevents common mistakes.\nKey Teaching Points: - GEE is NOT for training U-Nets or downloading GBs - Colab free tier has GPU quotas - manage expectations - CoPhil infrastructure enables local processing - Choose the right tool for the right job\nTiming: 3 minutes\nTransition: “Let’s look at these platforms in detail…”"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#copernicus-data-space-ecosystem-2025",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#copernicus-data-space-ecosystem-2025",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Copernicus Data Space Ecosystem (2025)",
    "text": "Copernicus Data Space Ecosystem (2025)\n\n\n\nNew Platform (2023+)\n\nReplaced SciHub\nModern interface\nAPI access\nFree registration\n\n\nFeatures\n\nSearch by location/date\nPreview before download\nDirect download\nBulk processing\n\n\nURL: https://dataspace.copernicus.eu\n\nThe Copernicus Data Space Ecosystem is the new portal replacing the legacy SciHub. All Sentinel data are free and open."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentiboard-dashboard-october-2025",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#sentiboard-dashboard-october-2025",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "SentiBoard Dashboard (October 2025)",
    "text": "SentiBoard Dashboard (October 2025)\n\n\nReal-time mission status\nData availability insights\nAcquisition plans\nQuality metrics\nInteractive dashboard\n\n\nSentiBoard is a new feature (October 2025) providing real-time insights into Copernicus Sentinel missions and data availability."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#google-earth-engine",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#google-earth-engine",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Google Earth Engine",
    "text": "Google Earth Engine\n\n\nPlanetary-Scale Platform\n\nPetabyte-scale data catalog\nAll Sentinel-1 & Sentinel-2 data\nCloud-based processing\nFree for research/education\nNo download needed!\n\n\n\n\n\nWe’ll use GEE extensively in this training\n\nURL: https://earthengine.google.com\n\nGoogle Earth Engine provides ready access to all Sentinel data with cloud-based processing. We’ll use GEE extensively for data access and preprocessing."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#alternative-data-sources",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#alternative-data-sources",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Alternative Data Sources",
    "text": "Alternative Data Sources\n\n\nAlaska Satellite Facility (ASF)\n\nSentinel-1 specialist\nUser-friendly interface\nPreprocessing tools\nhttps://asf.alaska.edu\n\n\nAWS Open Data\n\nSentinel-2 on AWS\nCloud-optimized\nPay for compute only\nProgrammatic access\n\n\n\nNow Available: CoPhil Mirror Site in the Philippines! (Operational 2024-2025)\n\n\nMICRO-EDIT APPLIED: Mirror Site is NOW operational (not “coming soon”).\nMultiple platforms provide Sentinel data access. The CoPhil Mirror Site provides local data access in the Philippines for faster downloads - critical for reducing international bandwidth bottlenecks.\nKey Point: 400TB capacity, focused on PH scenes, free for government/academia/researchers"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#overview-of-philippine-eo-landscape",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#overview-of-philippine-eo-landscape",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Overview of Philippine EO Landscape",
    "text": "Overview of Philippine EO Landscape\n\n\nPhilSA: Space data and operations\nNAMRIA: National mapping and geospatial data\nDOST-ASTI: AI and remote sensing R&D\nPAGASA: Climate and weather data\n\n\nBeyond European satellites, the Philippines has its own geospatial data platforms and agencies that provide crucial complementary data and support."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#philsa-national-space-agency",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#philsa-national-space-agency",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "PhilSA: National Space Agency",
    "text": "PhilSA: National Space Agency\n\n\nEstablished: August 2019\nMandate:\n\nCentral civilian space agency\nPromote space data use\nBuild national capacity\nSupport DRR, CCA, NRM\nCo-chair CoPhil programme\n\n\n\n\n\n\nWebsite: https://philsa.gov.ph\n\nPhilSA is the central civilian space agency established in 2019. As co-chair of CoPhil, PhilSA is leading national efforts to leverage Copernicus data."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#siyasat-data-portal",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#siyasat-data-portal",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "SIYASAT Data Portal",
    "text": "SIYASAT Data Portal\n\n\n\nPurpose (2025)\n\nSecure data archive operational\nVisualization system\nData distribution\nMaritime & terrestrial monitoring\n\n\nData Types\n\nNovaSAR-1 radar imagery\nAIS ship tracking data\nProcessed products\nAnalysis-ready data\n\n\n\nTiming: 3 minutes\nKey Points: - SIYASAT (Secure Interactive Yield-Assessment & SAR Analytics Tools) is PhilSA’s operational data portal - 2025 Status: Fully operational with NovaSAR-1 data access - Provides secure archive for Philippine government agencies - Critical for maritime security (illegal fishing, territorial monitoring) - Registration required - priority for government agencies\nTransition: “Beyond PhilSA, DOST-ASTI is leading AI innovation for EO applications…”"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#space-data-dashboard",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#space-data-dashboard",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Space+ Data Dashboard",
    "text": "Space+ Data Dashboard\n\n\nUser-friendly web portal\nBrowse satellite imagery\nVisualization tools\nDownload datasets\nNo programming required\nOpen to government, researchers, public\n\n\nThe Space+ Data Dashboard democratizes access to satellite data, making it available to local governments, researchers, and even students without requiring programming skills."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#philsa-2025-initiatives",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#philsa-2025-initiatives",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "PhilSA 2025 Initiatives",
    "text": "PhilSA 2025 Initiatives\n\n\nSpace Business Innovation Challenge\n\nEmpowers Filipino innovators\nFree satellite data access\nBuild solutions for local needs\nEarth observation focus\nWeather & environmental data\n\n\nTraining Programs\n\nDownstream data utilization\nPractical applications\nCapacity building nationwide\nPartnership with DOST\n\n\n\nPhilSA’s 2025 initiatives focus on empowering innovators and building capacity across the Philippines to use satellite data effectively."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#coare-infrastructure",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#coare-infrastructure",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "COARE Infrastructure",
    "text": "COARE Infrastructure\n\n\nComputing and Archiving Research Environment\n\nHigh-performance computing\nData archiving capabilities\nScience cloud facilities\nSupports data-intensive research\nEnables AI/ML workflows\n\n\n\n\n\nCOARE provides the computational infrastructure needed for processing large satellite datasets and running AI/ML models at scale."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#namria-national-mapping-authority",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#namria-national-mapping-authority",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "NAMRIA: National Mapping Authority",
    "text": "NAMRIA: National Mapping Authority\n\n\nNational Mapping and Resource Information Authority\nRole:\n\nOfficial mapping agency\nAuthoritative geospatial data\nTopographic maps\nHazard maps\nLand cover datasets\n\n\n\n\nWebsite: https://www.geoportal.gov.ph\n\nNAMRIA is the national mapping agency responsible for topographic maps, hydrographic data, and authoritative geospatial datasets."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#namria-geoportal",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#namria-geoportal",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "NAMRIA Geoportal",
    "text": "NAMRIA Geoportal\n\nOne-Stop Shop for Philippine Geospatial Data\n\nNational basemaps (1:50,000 scale)\nAdministrative boundaries\nTopographic maps\nThematic maps\nDownloadable shapefiles and rasters\n\n\nThe NAMRIA Geoportal is the go-to source for official Philippine spatial data including boundaries, topographic maps, and land cover data."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#land-cover-mapping-project",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#land-cover-mapping-project",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Land Cover Mapping Project",
    "text": "Land Cover Mapping Project\n\n\n\nLatest: 2020 National Land Cover\nClasses:\n\nForest types\nAgriculture\nBuilt-up areas\nWater bodies\nWetlands\nBarren land\n\n\nData Formats:\n\nShapefile\nGeoTIFF\nCSV\nGeoJSON\nKML\nPNG\n\n\nPortal: https://land-cover-mapping-project-namria.hub.arcgis.com\n\nNAMRIA’s land cover datasets are valuable for validation and comparison with satellite-derived classifications. The 2020 national land cover map is available in multiple formats."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#hazardhunterph-portal",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#hazardhunterph-portal",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "HazardHunterPH Portal",
    "text": "HazardHunterPH Portal\n\nComprehensive Hazard Assessment Platform\n\n\nHazard Types:\n\nEarthquake-induced hazards\nActive fault lines\nTsunami susceptibility\nLiquefaction zones\nLandslide hazards\n\n\nApplications:\n\nDisaster risk assessment\nLand use planning\nInfrastructure siting\nEmergency preparedness\n\n\nURL: https://hazardhunter.georisk.gov.ph/map\n\nHazardHunterPH provides critical hazard data for DRR planning. These maps can be overlaid with satellite data for vulnerability assessments."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#how-namria-complements-sentinel-data",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#how-namria-complements-sentinel-data",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "How NAMRIA Complements Sentinel Data",
    "text": "How NAMRIA Complements Sentinel Data\n\n\nSentinel Imagery Provides:\n\nCurrent conditions\nFrequent updates\nLarge-area coverage\nMulti-temporal analysis\n\n\nNAMRIA Data Provides:\n\nGround truth for validation\nTraining labels for ML models\nHistorical baselines\nOfficial classifications\nHazard context\n\n\n\n\n\n\n\n\nMICRO-EDIT: Using NAMRIA/Space+ as Label Sources\n\n\nFor Day 2 Palawan RF lab: 1. Download NAMRIA land cover shapefile (authoritative classes) 2. Overlay on Sentinel-2 imagery 3. Extract training points per class (forest, agriculture, water, etc.) 4. Train Random Forest classifier 5. Validate predictions against NAMRIA hold-out samples\nSpace+ Dashboard also provides admin boundaries and infrastructure layers for context in all visualizations.\n\n\n\n\nExample: Use Sentinel-2 to map current land cover, validate against NAMRIA’s official 2020 map, detect changes\n\n\nMICRO-EDIT APPLIED: Explicit link between NAMRIA/Space+ as label sources & validation layers, not just context.\nNAMRIA data provides essential ground truth and context for satellite-based analysis. Official boundaries and hazard maps enhance satellite data interpretation.\nShow one example overlay: NAMRIA training polygons (land cover classes) overlaid on S2 RGB composite."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#dost-asti-overview",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#dost-asti-overview",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "DOST-ASTI Overview",
    "text": "DOST-ASTI Overview\n\n\nAdvanced Science and Technology Institute\n\nLead agency for EO and AI R&D\nRemote sensing expertise\nMachine learning development\nNational AI infrastructure\nP2.6 billion investment until 2028\n\n\n\n\nWebsite: https://asti.dost.gov.ph\n\nDOST-ASTI leads several projects at the intersection of remote sensing, AI, and big data, with significant national investment of P2.6 billion through 2028."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#datos-remote-sensing-help-desk",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#datos-remote-sensing-help-desk",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "DATOS: Remote Sensing Help Desk",
    "text": "DATOS: Remote Sensing Help Desk\n\n\nRemote Sensing and Data Science Help Desk\nRapid analytics during disasters\nFlood mapping from satellite imagery\nDamage assessment\nCrop mapping (rice, sugarcane)\nRoad network detection\nSupports emergency response agencies\n\n\nDuring past typhoons, DATOS used satellite images to produce flood maps and give them to disaster response agencies within hours. They also worked on crop mapping and infrastructure detection using AI."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#skai-pinas-programme",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#skai-pinas-programme",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "SkAI-Pinas Programme",
    "text": "SkAI-Pinas Programme\n\n\n\nPhilippine Sky AI Program (2021-2028)\n\nFlagship AI R&D programme\nPart of P2.6B DOST AI investment\nDemocratize AI across Philippines\nRemote sensing & big data focus\n\n\nImpact (2025)\n\n300+ institutions supported\nUniversities & colleges\nSMEs & research teams\nLocal government units\n\n\n\nTiming: 3 minutes\nKey Points: - 2025 Update: SkAI-Pinas is part of DOST’s P2.6 billion AI investment (2024-2028) - Focuses on democratizing AI for EO applications - Provides infrastructure, training, and support - Successfully engaged 300+ institutions nationwide - Critical for building national AI capacity\nExamples: - Universities using SkAI for flood mapping research - LGUs leveraging AI for disaster preparedness - SMEs building EO-based agricultural solutions\nTransition: “SkAI provides the platform. DIMER provides the models. PANDA brings it all together…”"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#dimer-ai-model-hub",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#dimer-ai-model-hub",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "DIMER: AI Model Hub",
    "text": "DIMER: AI Model Hub\n\n\n\nDemocratized Intelligent Model Exchange Repository\n\nDigital “model store”\nPre-trained AI models\nReady-to-use\nFilipino-specific challenges\n\n\nAvailable Models\n\nLandslide detection\nTraffic surveys\nCrop monitoring\nLand cover classification\nFlood detection\n\n\n\nDIMER lowers barriers to AI by enabling end-users to reuse optimized AI models. If you need a model for land cover or flood detection, check DIMER first before building from scratch."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#aipi-ai-processing-interface",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#aipi-ai-processing-interface",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "AIPI: AI Processing Interface",
    "text": "AIPI: AI Processing Interface\n\n\nPurpose\n\nStreamline large-scale remote sensing tasks\nReduce computational barriers\nRun AI models on ASTI servers\nProcess hundreds of images efficiently\n\n\n\n\n\nExample: Apply an AI model to 100 Sentinel-2 images over entire region without your laptop\n\n\nAIPI allows users to run large computations on ASTI’s servers without heavy local processing. This is especially valuable when processing many satellite images with AI models."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#alam-automated-labeling-machine",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#alam-automated-labeling-machine",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "ALaM: Automated Labeling Machine",
    "text": "ALaM: Automated Labeling Machine\n\n\nChallenge\nCreating labeled training data is:\n\nTime-consuming\nExpensive\nRequires expertise\nMajor bottleneck for AI\n\n\nALaM Solution\n\nAutomate labeling process\nCrowdsourcing capabilities\nExpert validation\nBuild training datasets\n\n\n\nResult: Faster creation of high-quality training data for Filipino contexts\n\n\nCreating labeled data is a big challenge in EO AI. ALaM tries to address this through automation and crowdsourcing, making it easier to build training datasets for Philippine applications."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#how-dost-asti-tools-work-together",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#how-dost-asti-tools-work-together",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "How DOST-ASTI Tools Work Together",
    "text": "How DOST-ASTI Tools Work Together\n\n\nALaM creates training data\nTrain models and share via DIMER\nProcess large datasets with AIPI\nDeploy for operational use via SkAI-Pinas\nSupport disaster response through DATOS\n\n\nThese initiatives work together to create a complete ecosystem from data labeling through model development to operational deployment."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#pagasa-weather-climate-data",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#pagasa-weather-climate-data",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "PAGASA: Weather & Climate Data",
    "text": "PAGASA: Weather & Climate Data\n\n\nPhilippine Atmospheric, Geophysical and Astronomical Services Administration\nData Types:\n\nHistorical rainfall\nTemperature records\nTyphoon tracks\nClimate forecasts\nWeather observations\n\n\n\n\n\nIntegration: Combine with satellite data for climate analysis\n\n\nPAGASA provides meteorological and climate data that can be integrated with satellite observations for comprehensive analysis. Rainfall data + flood maps, for example."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#synergy-satellite-ground-data",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#synergy-satellite-ground-data",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Synergy: Satellite + Ground Data",
    "text": "Synergy: Satellite + Ground Data\n\n\nSatellite Data (Sentinel)\n\nSpatial coverage\nConsistent acquisition\nMultiple variables\nTime series\n\n\nGround Data (Philippine agencies)\n\nPoint validation\nGround truth\nMeteorological context\nLocal expertise\n\n\n\nCombined = More robust analysis and higher confidence\n\n\nThe Philippines is developing a rich EO ecosystem. The goal of CoPhil is to ensure you can leverage both international (Sentinel) and national data together effectively."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#cophil-mirror-site",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#cophil-mirror-site",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "CoPhil Mirror Site",
    "text": "CoPhil Mirror Site\n\n\nPhilippines-based Copernicus data repository\nLocal mirror of Sentinel data\nFocus on Philippine region\nFaster access (no international bandwidth)\nReliable availability\nOperational by 2025\nHosted by PhilSA with CloudFerro support\n\n\nThe CoPhil Mirror Site will locally store Sentinel data focused on the Philippines, enabling much faster downloads than from European servers."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#digital-space-campus",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#digital-space-campus",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Digital Space Campus",
    "text": "Digital Space Campus\n\n\n\nPurpose\n\nOnline learning portal\nTraining materials repository\nSelf-paced learning\nCommunity of practice\nKnowledge sharing\n\n\nContent\n\nCourse presentations\nJupyter notebooks\nDatasets\nGuides & tutorials\nForum discussions\n\n\n\nAll materials from this 4-day training will be available on the Digital Space Campus for future reference and for colleagues who couldn’t attend."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#building-a-sustainable-eo-ecosystem",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#building-a-sustainable-eo-ecosystem",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Building a Sustainable EO Ecosystem",
    "text": "Building a Sustainable EO Ecosystem\n\nData Access: Mirror Site + Data Space Ecosystem\nProcessing: COARE + AIPI + Google Earth Engine\nModels: DIMER repository\nTraining: Digital Space Campus\nOperations: DATOS + Agency integration\nCommunity: SkAI-Pinas network\n\n\nResult: Complete infrastructure for operational EO AI/ML**\n\n\nCoPhil is building sustainable infrastructure ensuring you can continue working with Sentinel data and AI/ML tools after this training, without starting from scratch."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#key-takeaways",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#key-takeaways",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nCopernicus provides free, high-quality satellite data globally\nSentinel-1 (SAR) works day/night, all-weather - essential for tropics\nSentinel-2 (optical) provides rich spectral information - 5 day revisit\nMultiple access methods available (Data Space, GEE, Mirror Site)\nPhilippine agencies provide complementary data and expertise\nCoPhil infrastructure supports sustainable capacity building\n\n\nYou now understand the full picture from data sources through national infrastructure. This foundation supports all subsequent AI/ML work."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#how-it-all-connects",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#how-it-all-connects",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "How It All Connects",
    "text": "How It All Connects\n\n\nSatellite data + Philippine ground truth + AI models + processing infrastructure = Operational applications for DRR, CCA, and NRM"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#example-workflow-flood-mapping",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#example-workflow-flood-mapping",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Example Workflow: Flood Mapping",
    "text": "Example Workflow: Flood Mapping\n\nAcquire Sentinel-1 SAR data (GEE or Mirror Site)\nValidate with PAGASA rainfall data\nProcess using AI model from DIMER\nScale processing via AIPI\nCombine with NAMRIA hazard maps\nDeliver to NDRRMC via DATOS\n\n\nThis is what integrated EO capacity looks like!\n\n\nA real-world workflow leverages multiple data sources, AI tools, and institutional partnerships. This is exactly what CoPhil aims to enable."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#operational-cautions",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#operational-cautions",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Operational Cautions",
    "text": "Operational Cautions\n\n\n\n\n\n\n“Don’t Do This” - Common Pitfalls\n\n\nQuality & Governance additions to prevent common mistakes:\n\n\n\nModel Applicability: - ❌ Don’t apply a Palawan RF land cover model to Mindanao without re-sampling - Why: Different climate, vegetation types, seasonality - Do: Collect local training samples from Mindanao - ❌ Don’t use un-calibrated SAR (digital numbers only) - Why: Meaningless for quantitative analysis - Do: Always calibrate to σ⁰ or γ⁰ in dB\nProcessing Assumptions: - ❌ Don’t skip terrain correction (RTC) for SAR in mountainous areas - Why: Topographic distortions create false changes - Do: Apply RTC using SRTM or better DEM - ❌ Don’t mix Sentinel-2 processing baselines without harmonization - Why: DN offset of 1000 after Jan 2022 breaks time series - Do: Use HARMONIZED collections in GEE\nEvaluation: - ❌ Don’t train and test on the same tile/province - Why: Inflated accuracy, poor generalization - Do: Geographic hold-out (different tiles/provinces for test)\n\nP2 IMPROVEMENT APPLIED: Operational cautions slide with “don’t do this” examples.\nThese are real mistakes participants WILL make without explicit warnings. One slide can save hours of debugging and poor results."
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#pre-flight-checklist",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#pre-flight-checklist",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Pre-Flight Checklist",
    "text": "Pre-Flight Checklist\n\n\n\n\n\n\nP1 IMPROVEMENT: Before Day 1 Hands-On Sessions\n\n\nSend this checklist 1 week before training:\n\n\n\n✅ Accounts & Access: - [ ] Google Earth Engine account enabled (signup.earthengine.google.com) - [ ] Google Drive with ≥5 GB free space - [ ] Google Colab tested (login with same Google account) - [ ] CoPhil Infrastructure registration (application.infra.copphil.philsa.gov.ph)\n✅ Software & Data: - [ ] Downloaded sample vector/raster bundle (link provided via email) - [ ] Confirmed zip file extracts correctly - [ ] Python 3.8+ installed (if working locally) - [ ] Jupyter notebook tested (if working locally)\n✅ Troubleshooting Contacts: - [ ] Have training support email/chat details - [ ] Know how to access Digital Space Campus materials\n\nIf any issues: Contact training organizers BEFORE Day 1 to resolve access problems!\n\n\nP1 IMPROVEMENT APPLIED: Pre-flight checklist (send 1 week before).\nAction Item for Organizers: - Create downloadable “sample bundle” (small GeoPackage + COG) - Set up support email/chat channel - Send checklist email 7 days before training - Follow up 2 days before to confirm readiness"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#whats-next-in-day-1",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#whats-next-in-day-1",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "What’s Next in Day 1?",
    "text": "What’s Next in Day 1?\n\n\nSession 2 (Next)\n\nAI/ML fundamentals\nSupervised vs unsupervised learning\nNeural networks basics\nData-centric AI\n→ 5-min Concept Check (3 questions)\n\n\nSessions 3 & 4\n\nHands-on Python (GeoPandas, Rasterio)\n\nPre-run pip installs to save time\n\nGoogle Earth Engine tutorial\n\nStart from ready script, modify filters only\n\nAccess real Sentinel data\nSCL cloud/shadow masking (not QA60)\n\n\n\nNow that you understand where data comes from, we’ll learn how AI/ML can extract information from these data, then get hands-on with actual satellite imagery.\nP1 IMPROVEMENTS NOTED: - Concept check after Session 2 - Pre-run installations in Session 3 - Ready GEE script in Session 4 (modify filters live, don’t type from scratch)"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#session-summary",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#session-summary",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Session Summary",
    "text": "Session Summary\nWhat We Covered:\n✅ Copernicus Programme & free/open data policy\n✅ Sentinel-1 (SAR) - all-weather monitoring\n✅ Sentinel-2 (Optical) - 13 bands, 10m resolution\n✅ 2025 Updates: 1C & 2C operational\n✅ Philippine EO infrastructure (PhilSA, NAMRIA, DOST-ASTI)\n✅ Data access platforms (SIYASAT, Geoportal, SkAI-Pinas)\n\nTiming: 2 minutes\nKey Takeaways: - You now understand WHERE EO data comes from - You know WHAT satellites provide what data - You know HOW to access data (both EU and PH platforms) - Ready for Session 2: Learning WHAT to do with this data using AI/ML"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#qa",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#qa",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Q&A",
    "text": "Q&A\n\n\nCopernicus & Sentinel\n\nMission specifications?\nData products & formats?\nAccess methods?\nProcessing levels?\n\n\nPhilippine EO Ecosystem\n\nAgency roles & mandates?\nData access procedures?\nIntegration with Copernicus?\nP2.6B AI investment details?\n\n\n\nTiming: 3-5 minutes for Q&A\nFacilitation Tips: - Encourage questions from different agency participants - Share knowledge across government, academic, private sectors - If technical question about access, note for Session 4 (GEE hands-on) - Keep responses brief - detailed hands-on coming in Sessions 3-4\nCommon Questions & Answers: - “How do I sign up for GEE?” → Will cover in Session 4 - “Which satellite should I use?” → Depends on application (Day 2 topic) - “Is SIYASAT public?” → Registration required, priority for gov agencies - “Can I combine Sentinel-1 and -2?” → Yes! Synergistic use cases discussed today"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#core-concepts-of-aiml-for-earth-observation",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#core-concepts-of-aiml-for-earth-observation",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Core Concepts of AI/ML for Earth Observation",
    "text": "Core Concepts of AI/ML for Earth Observation\nComing up after break:\n\nWhat is AI/ML and why for EO?\nThe EO AI/ML workflow\nSupervised vs unsupervised learning\nIntroduction to deep learning\nData-centric AI approaches\n\n\nSee you in Session 2! 🚀"
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html#contact-resources",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html#contact-resources",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "Contact & Resources",
    "text": "Contact & Resources\nEuropean Platforms:\nCopernicus Data Space: https://dataspace.copernicus.eu\nCoPhil Programme: https://www.cophil.eu\nPhilippine Platforms:\nPhilSA: https://philsa.gov.ph\nNAMRIA Geoportal: https://www.geoportal.gov.ph\nDOST-ASTI: https://asti.dost.gov.ph\n\nSession 1 Complete!\nTotal timing: ~120 minutes including breaks and Q&A\nBefore Session 2: - Short bio break (5-10 minutes) - Check that all participants are ready - Ensure presentation materials for Session 2 are loaded"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#session-objectives",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#session-objectives",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Session Objectives",
    "text": "Session Objectives\n\nUnderstand what AI/ML means in Earth Observation context\nLearn the end-to-end workflow for ML projects\nDistinguish supervised vs unsupervised learning\nGrasp deep learning and neural network basics\nExplore 2025 AI innovations (foundation models, data-centric AI)\n\n\nDuration: 2 hours\n\n\nThis session covers fundamental AI/ML concepts tailored to EO applications. Mostly conceptual, but essential foundation for hands-on work in Sessions 3-4 and throughout Day 2-4."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#session-roadmap",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#session-roadmap",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Session Roadmap",
    "text": "Session Roadmap\n\n\n\nTime\nTopic\nDuration\n\n\n\n\n00-10 min\nWhat is AI/ML?\n10 min\n\n\n10-35 min\nEO Workflow & Data Pipeline\n25 min\n\n\n35-60 min\nSupervised vs Unsupervised Learning\n25 min\n\n\n60-65 min\n☕ Break\n5 min\n\n\n65-90 min\nDeep Learning & Neural Networks\n25 min\n\n\n90-110 min\nData-Centric AI & 2025 Updates\n20 min\n\n\n110-120 min\nQ&A & Summary\n10 min\n\n\n\n\nTiming: 2 minutes\nThis session is more conceptual than Session 1. Focus on building intuition and mental models. Hands-on practice comes in Sessions 3-4 and Days 2-4."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-aiml-for-earth-observation",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-aiml-for-earth-observation",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Why AI/ML for Earth Observation?",
    "text": "Why AI/ML for Earth Observation?\n\n\nTraditional Approach\n\nManual interpretation\nRule-based classification\nSimple thresholds\nTime-consuming\nHard to scale\n\n\nAI/ML Approach\n\nAutomated pattern recognition\nLearn from examples\nComplex decision boundaries\nFast processing\nScalable to large areas\n\n\n\nML can process years of satellite data in hours!\n\n\nMachine learning allows us to automatically recognize patterns in satellite imagery without hard-coding rules for every scenario. This is transformative for large-area, time-series analysis."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#defining-the-terms",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#defining-the-terms",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Defining the Terms",
    "text": "Defining the Terms\n\nAI, ML, and Deep Learning Hierarchy with EO Applications\nArtificial Intelligence (AI): Broad field of making machines “smart”\nMachine Learning (ML): Subset of AI where algorithms learn from data\nDeep Learning (DL): Subset of ML using neural networks with many layers\n\n\nAI is the broadest term. Machine Learning is a subset where computer algorithms learn patterns from data without being explicitly programmed. Deep Learning uses neural networks. This comprehensive diagram shows the nested hierarchy with specific algorithms at each level (Random Forest, CNNs, RNNs, etc.) and their Earth Observation applications like land cover classification and time series analysis."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#machine-learning-in-simple-terms",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#machine-learning-in-simple-terms",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Machine Learning in Simple Terms",
    "text": "Machine Learning in Simple Terms\n\n\nTraditional Programming\nRules + Data → Results\n\nProgrammer writes explicit rules\nFixed logic\nHard to handle complexity\n\n\nMachine Learning\nData + Results → Rules\n\nAlgorithm learns rules from examples\nAdaptive\nHandles complex patterns\n\n\n\nIn traditional programming, we tell computers what to do step-by-step. In ML, we show examples and the algorithm figures out the pattern."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#ml-in-earth-observation-context",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#ml-in-earth-observation-context",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "ML in Earth Observation Context",
    "text": "ML in Earth Observation Context\n\n\nExample: Forest vs Non-Forest\nTraditional:\nIF NDVI &gt; 0.6 THEN Forest\nELSE Non-Forest\nSimple, but breaks easily\n\nMachine Learning:\n\nShow 1000 examples of forest pixels\nShow 1000 examples of non-forest\nAlgorithm learns complex patterns\nWorks in diverse conditions\n\n\n\n\nA simple NDVI threshold might work in one region but fail in another. ML can learn the nuanced patterns that distinguish forest from non-forest across different conditions."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#end-to-end-ml-workflow",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#end-to-end-ml-workflow",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "End-to-End ML Workflow",
    "text": "End-to-End ML Workflow\n\n\nThis is the typical workflow for any ML project in Earth Observation. Understanding these steps is crucial for successful implementation."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-1-problem-definition",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-1-problem-definition",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 1: Problem Definition",
    "text": "Step 1: Problem Definition\n\n\nKey Questions\n\nWhat exactly are we trying to achieve?\nWhat decisions will this support?\nWhat level of accuracy is needed?\nWhat resources are available?\n\n\nEO Examples\n\nMap rice paddy extent\nDetect flooded areas after typhoon\nClassify land cover types\nEstimate crop yield\nMonitor deforestation\n\n\n\nClear problem definition = 50% of success\n\n\nBeing clear on the question helps design the solution. “We want to classify land cover in Palawan” is much more actionable than “We want to use AI.”"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-2-data-acquisition",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-2-data-acquisition",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 2: Data Acquisition",
    "text": "Step 2: Data Acquisition\n\n\nSatellite Imagery\n\nSentinel-1/2 (covered in Session 1!)\nLandsat\nPlanet\nHigh-resolution commercial\nMultiple dates/seasons\n\n\nGround Truth / Labels\n\nField surveys\nGPS points\nExisting maps\nPhoto interpretation\nExpert knowledge\n\n\n\nChallenge: Getting quality labels is often hardest part\n\n\nData acquisition includes both satellite images and the ground truth labels needed to train supervised models. The quality and quantity of labels directly impact model performance."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-3-data-preprocessing",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-3-data-preprocessing",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 3: Data Preprocessing",
    "text": "Step 3: Data Preprocessing\nFor Satellite Imagery:\n\nAtmospheric correction (use Level-2A!)\nCloud masking\nGeometric correction\nRadiometric calibration\nCo-registration (multiple sensors)\nTemporal compositing\n\n\n“Garbage In, Garbage Out” - preprocessing matters!\n\n\nWell-prepared input data is crucial. Even the best model will fail if fed cloudy, misaligned, or uncorrected images."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#preprocessing-example",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#preprocessing-example",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Preprocessing Example",
    "text": "Preprocessing Example\n\nCloud Removal Before and After Comparison\n\nBefore Preprocessing: - Clouds present - Atmospheric haze - Different acquisition dates\n\nAfter Preprocessing: - Clouds masked - Atmospherically corrected - Temporal composite created\n\n\nPreprocessing transforms raw satellite data into analysis-ready products. This side-by-side comparison shows the dramatic improvement from cloud masking and creating temporal composites - essential steps before any ML analysis."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-4-feature-engineering",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-4-feature-engineering",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 4: Feature Engineering",
    "text": "Step 4: Feature Engineering\n\n\nWhat are Features?\n\nInput variables for the model\nDerived from raw data\nInformative for the task\n\n\nEO Features\n\nSpectral bands (Blue, Red, NIR, etc.)\nSpectral indices (NDVI, NDWI)\nTexture measures\nTemporal statistics\nTopography (elevation, slope)\n\n\n\nDeep Learning: Often learns features automatically!\n\n\nFor traditional ML like Random Forest, we engineer features. For deep learning, the network learns features automatically from raw pixels."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#common-eo-features",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#common-eo-features",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Common EO Features",
    "text": "Common EO Features\n\n\n\n\n\n\n\n\nFeature Type\nExamples\nWhat They Capture\n\n\n\n\nSpectral Bands\nB2, B3, B4, B8\nReflectance at different wavelengths\n\n\nVegetation Indices\nNDVI, EVI, SAVI\nVegetation health, density\n\n\nWater Indices\nNDWI, MNDWI\nWater presence, moisture\n\n\nTexture\nGLCM variance, entropy\nSpatial patterns\n\n\nTemporal\nMean, std over time\nPhenology, seasonality\n\n\nTopographic\nElevation, slope, aspect\nTerrain characteristics\n\n\n\n\nDifferent features highlight different aspects of the landscape. Vegetation indices emphasize green biomass, water indices highlight water bodies, etc."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-5-model-selection-training",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-5-model-selection-training",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 5: Model Selection & Training",
    "text": "Step 5: Model Selection & Training\n\n\nModel Selection\nChoose based on:\n\nProblem type (classification vs regression)\nData size\nInterpretability needs\nComputational resources\n\n\nCommon EO Models\n\nRandom Forest\nSupport Vector Machines\nConvolutional Neural Networks\nU-Net (segmentation)\nRecurrent networks (time series)\n\n\n\nModel choice depends on your specific problem, available data, and resources. We’ll cover Random Forest on Day 2 and CNNs on Day 3."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#training-process",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#training-process",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Training Process",
    "text": "Training Process\n\n\nSplit data: Training set (70-80%) & Validation set (20-30%)\nFeed training data to model\nModel learns patterns by adjusting internal parameters\nValidate on unseen validation data\nIterate: Adjust model or data if needed\n\n\nTraining involves feeding labeled examples to the model. The model adjusts its internal parameters to minimize errors on the training data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-6-validation-evaluation",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-6-validation-evaluation",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 6: Validation & Evaluation",
    "text": "Step 6: Validation & Evaluation\n\n\nWhy Validate?\n\nEnsure model generalizes\nDetect overfitting\nCompare different models\nBuild confidence\n\n\nEvaluation Metrics\n\nOverall Accuracy\nConfusion Matrix\nPrecision & Recall\nF1-Score\nKappa coefficient\n\n\n\nUse independent test data - never validate on training data!\n\n\nRigorous validation using held-out data ensures the model works on new, unseen examples - not just memorizing training data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#confusion-matrix-example",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#confusion-matrix-example",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Confusion Matrix Example",
    "text": "Confusion Matrix Example\n\n\n\nWhat it shows:\n\nTrue Positives (correct predictions)\nFalse Positives (type I error)\nFalse Negatives (type II error)\nTrue Negatives\n\n\nDerived Metrics:\n\nPrecision = TP / (TP + FP)\nRecall = TP / (TP + FN)\nAccuracy = (TP + TN) / Total\n\n\n\nThe confusion matrix shows where your model is making mistakes. This helps identify which classes are being confused."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-7-deployment",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#step-7-deployment",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Step 7: Deployment",
    "text": "Step 7: Deployment\n\n\nDeployment Options\n\nGenerate full maps\nNear real-time monitoring\nOperational pipelines\nDecision support systems\nWeb applications\n\n\nConsiderations\n\nModel retraining schedule\nComputational requirements\nUser interface\nData updates\nMaintenance plan\n\n\n\nIf the model is satisfactory, deploy it for operational use. This might mean generating maps for entire regions or setting up automatic processing of new satellite images."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#workflow-is-iterative",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#workflow-is-iterative",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Workflow is Iterative",
    "text": "Workflow is Iterative\n\n\nPoor validation? → Go back to data acquisition or model selection\nNew data available? → Retrain model\nRequirements change? → Redefine problem\nContinuous improvement is key\n\n\nReal projects are iterative. You often loop back: if validation is poor, you might need more data, different features, or a different model."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#main-ml-paradigms",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#main-ml-paradigms",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Main ML Paradigms",
    "text": "Main ML Paradigms\n\n\nSupervised Learning (most common in EO)\nUnsupervised Learning (exploratory analysis)\nSemi-supervised Learning (combines both)\nReinforcement Learning (less common in EO)\n\n\nWe’ll focus on supervised and unsupervised learning as these are most relevant for Earth Observation applications."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-is-supervised-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-is-supervised-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "What is Supervised Learning?",
    "text": "What is Supervised Learning?\n\n\nDefinition\n\nLearning from labeled data\nKnown input-output pairs\nModel learns mapping from inputs to outputs\nLike learning with an answer key\n\n\n\n\n\nRequires ground truth labels for training\n\n\nSupervised learning is the most common in EO. The algorithm is given examples with known outcomes (labels) and learns to predict labels for new, unseen data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#two-types-of-supervised-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#two-types-of-supervised-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Two Types of Supervised Learning",
    "text": "Two Types of Supervised Learning\n\n\nClassification\n\nPredict categorical labels\nDiscrete classes\nExample outputs: “Forest”, “Water”, “Urban”\n\n\n\nRegression\n\nPredict continuous values\nNumeric outputs\nExample outputs: 25.3 tons/hectare, 15.2°C\n\n\n\n\nClassification assigns data to categories. Regression predicts numeric values. Both require labeled training data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#classification-examples-in-eo",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#classification-examples-in-eo",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Classification Examples in EO",
    "text": "Classification Examples in EO\n\n\nLand Cover Classification\n\n\nForest, agriculture, urban, water\nPixel-wise or object-based\nMulti-class problem\n\n\nCrop Type Mapping\n\n\nRice, corn, sugarcane\nSeasonal patterns important\nSupports agricultural planning\n\n\n\nLand cover classification is the classic EO supervised learning task. Each pixel or region is assigned to a class like forest, water, or urban."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#regression-examples-in-eo",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#regression-examples-in-eo",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Regression Examples in EO",
    "text": "Regression Examples in EO\n\n\nBiomass Estimation\n\n\nPredict tons of biomass per hectare\nImportant for carbon accounting\nUses SAR and optical data\n\n\nCrop Yield Prediction\n\n\nPredict tons per hectare\nSeasonal NDVI time series\nSupports food security planning\n\n\n\nRegression tasks predict continuous values like biomass density, crop yield, soil moisture, or sea surface temperature from satellite data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#common-supervised-algorithms",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#common-supervised-algorithms",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Common Supervised Algorithms",
    "text": "Common Supervised Algorithms\n\n\n\n\n\n\n\n\nAlgorithm\nStrengths\nEO Applications\n\n\n\n\nRandom Forest\nHandles high dimensions, robust\nLand cover, crop classification\n\n\nSVM\nEffective in high dimensions\nBinary classification, change detection\n\n\nNeural Networks\nLearns complex patterns\nImage classification, segmentation\n\n\nDecision Trees\nInterpretable\nQuick classifications\n\n\nk-NN\nSimple, non-parametric\nLocal classifications\n\n\n\n\nDifferent algorithms have different strengths. Random Forest is popular in EO for its robustness and ability to handle many features."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#supervised-learning-requirements",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#supervised-learning-requirements",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Supervised Learning Requirements",
    "text": "Supervised Learning Requirements\nEssential:\n\nTraining data with known labels\nRepresentative samples covering all classes\nSufficient quantity (varies by algorithm)\nQuality labels (accurate, consistent)\nIndependent validation data\n\n\nChallenge: Getting quality labels is often the bottleneck!\n\n\nSupervised learning needs ground truth. For land cover, this might be field surveys, GPS points, or careful photo interpretation. Quality matters more than quantity!"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-is-unsupervised-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-is-unsupervised-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "What is Unsupervised Learning?",
    "text": "What is Unsupervised Learning?\n\n\nDefinition\n\nLearning from unlabeled data\nNo known outputs\nDiscover hidden patterns\nLike sorting without instructions\n\n\n\n\n\nUseful for exploratory analysis and finding structure\n\n\nUnsupervised learning finds patterns or groupings inherent in the data without being told what to look for."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#clustering-main-unsupervised-technique",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#clustering-main-unsupervised-technique",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Clustering: Main Unsupervised Technique",
    "text": "Clustering: Main Unsupervised Technique\n\n\nGroup similar pixels based on spectral characteristics\nAlgorithm decides number of clusters (or you specify)\nAnalyst interprets what each cluster means\nExample: “Cluster 3 looks like water, Cluster 7 looks like forest”\n\n\nK-means clustering is a common unsupervised method. It groups pixels with similar reflectance, but you have to interpret what those groups mean."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#unsupervised-eo-applications",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#unsupervised-eo-applications",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Unsupervised EO Applications",
    "text": "Unsupervised EO Applications\n\n\nChange Detection\n\nCluster “before” and “after” images\nIdentify changed areas\nNo labels needed\n\nAnomaly Detection\n\nFind unusual pixels\nPotential forest disturbance\nData quality issues\n\n\nInitial Exploration\n\nQuick overview of spectral classes\nInform supervised approach\nGenerate training samples\n\nDimensionality Reduction\n\nPCA, t-SNE\nVisualize high-dimensional data\nFeature extraction\n\n\n\nUnsupervised methods are useful for quick initial analysis or when you don’t have ground truth labels. Results need interpretation though."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#supervised-vs-unsupervised",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#supervised-vs-unsupervised",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Supervised vs Unsupervised",
    "text": "Supervised vs Unsupervised\n\n\n\n\n\n\n\n\nAspect\nSupervised\nUnsupervised\n\n\n\n\nLabels\nRequired\nNot needed\n\n\nAccuracy\nGenerally higher\nLower, needs interpretation\n\n\nUse Case\nPrecise classification\nExploration, pattern discovery\n\n\nEffort\nHigh (collecting labels)\nLow (no labels)\n\n\nOutput\nPredefined classes\nDiscovered clusters\n\n\nControl\nHigh (you define classes)\nLow (algorithm decides groups)\n\n\n\n\nSupervised methods generally yield more accurate results when good training data is available. Unsupervised is useful when labels are unavailable or for exploratory work."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#which-to-choose",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#which-to-choose",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Which to Choose?",
    "text": "Which to Choose?\n\n\nUse Supervised When:\n\nYou have ground truth labels\nNeed specific classes\nAccuracy is critical\nOperational application\n\n\nUse Unsupervised When:\n\nNo labels available\nExploratory analysis\nDiscovering unknown patterns\nQuick initial assessment\n\n\n\nIn practice: Often combine both approaches!\n\n\nMost operational EO applications use supervised learning because accuracy and specific class definitions are important. Unsupervised helps with initial exploration."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#minute-break",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#minute-break",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "☕ 5-Minute Break",
    "text": "☕ 5-Minute Break\n\nStretch Break\nStand up • Grab water • Back in 5 minutes\n\n\nTiming: 5 minutes\nInstructor Actions: - Announce break - Mention we’ll dive into deep learning next - Be available for quick questions\nWhen Resuming: - Quick recap: “We’ve covered ML basics, workflow, supervised/unsupervised. Now: deep learning!”"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-is-deep-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-is-deep-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "What is Deep Learning?",
    "text": "What is Deep Learning?\n\n\nDeep Learning = Neural Networks with Many Layers\n\nSubset of machine learning\n“Deep” refers to multiple layers\nAutomatically learns features\nExcels at image analysis\nData-hungry\n\n\n\n\n\nDeep learning is essentially about neural networks with many layers (dozens or even hundreds). These “deep” networks can capture very complex relationships."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#neural-networks-building-blocks",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#neural-networks-building-blocks",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Neural Networks: Building Blocks",
    "text": "Neural Networks: Building Blocks\n\nArtificial Neuron:\n\nTakes multiple inputs\nMultiplies each by a weight\nAdds a bias\nApplies activation function\nProduces output\n\n\nA single neuron is like a logistic regression unit. It takes inputs, applies weights, and uses an activation function to produce an output."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#neural-network-architecture",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#neural-network-architecture",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Neural Network Architecture",
    "text": "Neural Network Architecture\n\n\n\nLayers:\n\nInput Layer: Receives data (e.g., pixel values)\nHidden Layers: Process and transform\nOutput Layer: Final prediction\n\n\nConnections:\n\nEach neuron connects to next layer\nWeights on connections\nInformation flows forward\n\n\n\nNeurons are organized into layers. The input layer receives data (pixel values), hidden layers progressively extract features, and the output layer makes predictions."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#key-concepts",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#key-concepts",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Key Concepts",
    "text": "Key Concepts\nActivation Functions\n\nIntroduce non-linearity\nCommon: ReLU, Sigmoid, Tanh\nAllow network to learn complex patterns\n\nWeights and Biases\n\nParameters the network learns\nMillions of parameters in deep networks\nAdjusted during training\n\nForward Propagation\n\nData flows input → output\nGenerate prediction\n\n\nActivation functions are crucial - they allow neural networks to learn non-linear relationships. Without them, the network would just be linear regression!"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#how-neural-networks-learn",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#how-neural-networks-learn",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "How Neural Networks Learn",
    "text": "How Neural Networks Learn\n\n\nForward pass: Input data, get prediction\nCalculate loss: How wrong is the prediction?\nBackpropagation: Calculate gradients\nUpdate weights: Adjust to reduce error\nRepeat: Thousands of times (epochs)\n\n\nTraining adjusts weights to minimize error. This happens through backpropagation - computing gradients and updating weights in the direction that reduces loss."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#loss-functions",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#loss-functions",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Loss Functions",
    "text": "Loss Functions\n\n\nClassification\nCross-Entropy Loss\n\nMeasures classification error\nHigher penalty for confident wrong predictions\nStandard for multi-class problems\n\n\nRegression\nMean Squared Error\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\n\nMeasures prediction error\nSquared difference from true value\n\n\n\nLoss functions quantify “how bad” predictions are. The training process tries to minimize this loss by adjusting weights."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#optimizers",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#optimizers",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Optimizers",
    "text": "Optimizers\n\n\nStochastic Gradient Descent (SGD)\n\nBasic optimizer\nUpdates weights based on gradients\nLearning rate controls step size\n\n\nAdam Optimizer\n\nAdaptive learning rates\nFaster convergence\nMost popular for deep learning\nGenerally works well\n\n\n\nYou don’t need to implement these - frameworks do it for you!\n\n\nOptimizers determine how weights are updated. Adam is the most popular because it adapts learning rates automatically and generally converges faster than SGD."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#convolutional-neural-networks-cnns",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#convolutional-neural-networks-cnns",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Convolutional Neural Networks (CNNs)",
    "text": "Convolutional Neural Networks (CNNs)\n\nSpecialized for images:\n\nConvolutional layers: Detect spatial patterns\nPooling layers: Reduce dimensionality\nFully connected layers: Final classification\nAutomatically learn features (edges, textures, objects)\n\n\nCNNs are neural networks specialized for grid data like images. They use convolutional layers to automatically extract spatial features."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#how-cnns-process-images",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#how-cnns-process-images",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "How CNNs Process Images",
    "text": "How CNNs Process Images\n\nHierarchical Feature Learning:\n\nEarly layers: Detect edges, simple patterns\nMiddle layers: Detect textures, parts\nLater layers: Detect objects, scenes\nNo manual feature engineering needed!\n\n\nCNNs learn increasingly complex features at each layer. Early layers detect edges, later layers detect whole objects. This happens automatically during training!"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#cnns-in-earth-observation",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#cnns-in-earth-observation",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "CNNs in Earth Observation",
    "text": "CNNs in Earth Observation\n\n\nApplications:\n\nImage classification\nObject detection (ships, buildings)\nSemantic segmentation (pixel-wise)\nChange detection\nSuper-resolution\n\n\nAdvantages:\n\nLearn features automatically\nHandle spatial context\nState-of-the-art performance\nTransfer learning possible\n\n\n\n\nCNNs have achieved state-of-the-art results in many EO tasks. They can learn to recognize complex patterns without manual feature engineering."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#popular-cnn-architectures-for-eo",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#popular-cnn-architectures-for-eo",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Popular CNN Architectures for EO",
    "text": "Popular CNN Architectures for EO\n\n\n\n\n\n\n\n\n\nArchitecture\nYear\nKey Innovation\nEO Use Cases\n\n\n\n\nResNet\n2015\nResidual connections\nClassification, backbone for detection\n\n\nU-Net\n2015\nSkip connections\nSemantic segmentation, flood mapping\n\n\nEfficientNet\n2019\nCompound scaling\nEfficient classification, mobile deployment\n\n\nDeepLabv3+\n2018\nAtrous convolution\nLand cover segmentation\n\n\nYOLOv8\n2023\nReal-time detection\nObject detection, ship/vehicle counting\n\n\n\n\nResNet and EfficientNet are most popular backbones for EO\n\n\nThese are proven architectures widely used in EO. ResNet-50 is often the starting point for transfer learning. U-Net dominates semantic segmentation tasks."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#resnet-residual-networks",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#resnet-residual-networks",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "ResNet: Residual Networks",
    "text": "ResNet: Residual Networks\n\n\n\nKey Innovation: Skip Connections\n\nAllows training very deep networks (50, 101, 152 layers)\nSolves vanishing gradient problem\nIdentity mapping preserves information\n\nCommon Variants: - ResNet-50 (25M parameters) - ResNet-101 (44M parameters) - ResNet-152 (60M parameters)\n\nEO Applications:\n\nPre-trained on ImageNet\nFine-tune for EO tasks\nBackbone for object detection\nTransfer learning baseline\n\nPerformance: - Top-5 error: 3.57% (ImageNet) - Works well with 10k+ images\n\n\nResNet revolutionized deep learning by enabling training of very deep networks. Skip connections allow gradients to flow directly through the network."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#u-net-for-semantic-segmentation",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#u-net-for-semantic-segmentation",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "U-Net for Semantic Segmentation",
    "text": "U-Net for Semantic Segmentation\n\nArchitecture: - Encoder (contracting path): Captures context - Decoder (expanding path): Enables precise localization - Skip connections: Combine low & high-level features\nWhy Dominant in EO: - Works with small datasets (hundreds of images) - Precise pixel-wise predictions - Perfect for segmentation tasks\n\nEO Applications: Flood mapping, land cover, building footprints, crop fields\n\n\nU-Net is THE architecture for semantic segmentation in EO. Originally designed for biomedical image segmentation, it’s now standard for pixel-wise classification tasks."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#deep-learning-frameworks",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#deep-learning-frameworks",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Deep Learning Frameworks",
    "text": "Deep Learning Frameworks\n\n\nTensorFlow / Keras\n\n\nGoogle’s framework\nHigh-level Keras API\nProduction-ready\nLarge ecosystem\n\n\nPyTorch\n\n\nFacebook’s framework\nPythonic and intuitive\nPopular in research\nFlexible\n\n\n\nWe’ll use TensorFlow/Keras in this training\n\n\nYou don’t implement backpropagation yourself - frameworks like TensorFlow and PyTorch handle the math. You just define the architecture and provide data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#deep-learning-considerations",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#deep-learning-considerations",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Deep Learning Considerations",
    "text": "Deep Learning Considerations\n\n\nAdvantages:\n\nAutomatic feature learning\nState-of-the-art accuracy\nHandles complex patterns\nScales to big data\n\n\nChallenges:\n\nRequires lots of training data\nComputationally intensive (need GPUs)\nLess interpretable (“black box”)\nHarder to debug\n\n\n\nStart simple (Random Forest), move to DL when you have data and compute\n\n\nDeep learning is powerful but data-hungry and computationally expensive. For many EO tasks, simpler models like Random Forest work well with less data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-benchmark-datasets-matter",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-benchmark-datasets-matter",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Why Benchmark Datasets Matter",
    "text": "Why Benchmark Datasets Matter\n\nStandardized Evaluation - Compare algorithms objectively\nTraining Resources - Pre-labeled data for model training\nTransfer Learning - Pre-train on large datasets, fine-tune locally\nResearch Reproducibility - Enable comparison across studies\nCommunity Building - Shared resources accelerate progress\n\n\nYou don’t need to label everything from scratch!\n\n\nBenchmark datasets are crucial for EO ML. They provide labeled training data and enable fair comparison of methods across research groups worldwide."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#eurosat-land-cover-classification",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#eurosat-land-cover-classification",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "EuroSAT: Land Cover Classification",
    "text": "EuroSAT: Land Cover Classification\n\n\nSpecifications: - Images: 27,000 labeled patches - Classes: 10 land cover types - Size: 64×64 pixels - Bands: All 13 Sentinel-2 bands - Source: European cities\n10 Classes: Annual Crop • Forest • Herbaceous Vegetation • Highway • Industrial • Pasture • Permanent Crop • Residential • River • Sea/Lake\n\n\nAchievement: 98.57% accuracy with CNNs\nWhy Popular: - Sentinel-2 based - Balanced classes - Easy to use\n\n\nEuroSAT is one of the most popular benchmarks for EO classification. Based on Sentinel-2, making it highly relevant for operational applications. Great starting point for CNN experiments."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#bigearthnet-large-scale-multi-label",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#bigearthnet-large-scale-multi-label",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "BigEarthNet: Large-Scale Multi-Label",
    "text": "BigEarthNet: Large-Scale Multi-Label\n\n\nMassive Scale: - Images: 590,326 Sentinel-2 patches - Coverage: 10 European countries - Labels: 43 land cover classes - Multi-label: Multiple classes per image - Multi-modal: Optical + SAR version\nReal-World Complexity: - Forest + Water - Urban + Agricultural - Reflects actual landscapes\n\n\nWhy Different:\nUnlike EuroSAT (single label), BigEarthNet has multiple overlapping classes - more realistic!\nAccess: - bigearth.net - TensorFlow Datasets - Papers With Code\n\n\nBigEarthNet’s multi-label nature makes it more challenging but also more realistic. Essential for semantic segmentation research and testing advanced architectures."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#xview-object-detection-benchmark",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#xview-object-detection-benchmark",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "xView: Object Detection Benchmark",
    "text": "xView: Object Detection Benchmark\n\n\nSpecifications: - Objects: &gt;1 million annotated - Classes: 60 object types - Resolution: 0.3m (WorldView-3) - Area: &gt;1,400 km² - Annotations: Bounding boxes\nObject Categories: - Buildings & infrastructure - Vehicles (cars, trucks, aircraft) - Ships & maritime - Storage tanks - Construction equipment\n\n\nCreated for disaster response\nApplications: - YOLO training - Faster R-CNN - Small object detection - Infrastructure mapping\n\n\nxView is THE benchmark for object detection in satellite imagery. Created for disaster response applications, now widely used for testing detection algorithms like YOLO and Faster R-CNN."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#philippine-data-resources",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#philippine-data-resources",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Philippine Data Resources",
    "text": "Philippine Data Resources\n\n\nPRiSM (PhilRice) - Rice area maps (wet/dry season) - Planting dates & growth stages - Yield estimates - Since 2014 - https://prism.philrice.gov.ph/\nPhilSA Products - Flood extent maps (DATOS) - Mangrove extent mapping - Land cover classifications - Disaster damage assessments\n\nDOST-ASTI Outputs - DATOS rapid flood mapping - Hazard susceptibility maps - AI-powered damage assessment - hazardhunter.georisk.gov.ph\nNAMRIA Geoportal - National land cover (2020) - Topographic basemaps - Administrative boundaries - Digital Elevation Models - www.geoportal.gov.ph\n\n\nUse these as training/validation data - don’t start from scratch!\n\n\nPhilippine agencies have produced operational EO products that can serve as training or validation data for your ML models. Leverage existing work!"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#paradigm-shift-model-centric-vs-data-centric",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#paradigm-shift-model-centric-vs-data-centric",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Paradigm Shift: Model-Centric vs Data-Centric",
    "text": "Paradigm Shift: Model-Centric vs Data-Centric\n\n\n\nModel-Centric (Traditional)\n\nFocus on improving algorithms\nKeep data fixed\nTry different models\nTune hyperparameters\n\n\nData-Centric (Modern)\n\nFocus on improving data\nKeep model fixed\nClean and augment data\nBetter annotations\n\n\n\nA lot of early ML progress focused on model algorithms. Data-Centric AI, popularized by Andrew Ng, advocates that improving your data often yields bigger gains."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-data-centric-matters-for-eo",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-data-centric-matters-for-eo",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Why Data-Centric Matters for EO",
    "text": "Why Data-Centric Matters for EO\nEO-Specific Data Challenges:\n\nCloud contamination\nAtmospheric effects\nSensor artifacts and noise\nLabel uncertainty\nGeographic variability\nTemporal dynamics\nClass imbalance\n\n\n“Better data beats a cleverer model” in most cases\n\n\nIn EO, the “food” you feed your AI matters more than fancy model tweaks. Cloudy images, mislabeled points, or biased samples can derail any model."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#research-data-efficiency",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#research-data-efficiency",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "2025 Research: Data Efficiency",
    "text": "2025 Research: Data Efficiency\n\nKey Finding (ArXiv 2025):\n\nSome EO datasets reach optimal accuracy with &lt;20% of temporal instances\nSingle band from single modality can be sufficient\nData efficiency crucial for operational systems\nQuality over quantity\n\n\nRecent research shows you don’t always need all available data. Smart selection of temporal instances and bands can achieve similar accuracy with much less data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#four-pillars-of-data-centric-ai",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#four-pillars-of-data-centric-ai",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Four Pillars of Data-Centric AI",
    "text": "Four Pillars of Data-Centric AI\n\n\n1. Data Quality\n\n\nCloud/shadow removal\nAtmospheric correction\nSensor calibration\nGeometric accuracy\n\n\n2. Data Quantity\n\n\nSufficient training samples\nBalanced classes\nData augmentation\nTransfer learning"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#four-pillars-continued",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#four-pillars-continued",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Four Pillars (Continued)",
    "text": "Four Pillars (Continued)\n\n\n3. Data Diversity\n\n\nMultiple seasons\nDifferent regions\nVarious conditions\nClass variations\n\n\n4. Label Quality\n\n\nClear definitions\nConsistent protocols\nExpert validation\nAccurate geolocation\n\n\n\nThese four aspects - quality, quantity, diversity, and labels - determine model success more than architectural choices."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-quality-in-eo",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-quality-in-eo",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Data Quality in EO",
    "text": "Data Quality in EO\n\n\nCommon Issues:\n\nClouds and shadows\nHaze and aerosols\nSensor artifacts (striping, banding)\nGeometric misalignment\nRadiometric inconsistencies\nMixed pixels at boundaries\n\n\nSolutions:\n\nUse Level-2A products\nRigorous cloud masking\nQuality flag filtering\nMulti-temporal compositing\nValidation checks\nDocument preprocessing\n\n\n\nSatellite data can be noisy. Cloud masking, using atmospherically corrected products, and careful preprocessing are essential."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#quality-example-cloud-masking",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#quality-example-cloud-masking",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Quality Example: Cloud Masking",
    "text": "Quality Example: Cloud Masking\n\n\nWithout Cloud Masking\n\n\nClouds misclassified\nShadows cause errors\nPoor model performance\n\n\nWith Proper Masking\n\n\nClean training data\nAccurate classifications\nBetter generalization\n\n\n\nOne cloudy image can ruin your training data!\n\n\nEven a few cloudy training samples can teach the model wrong patterns. Rigorous cloud masking is non-negotiable."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#how-much-data-do-you-need",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#how-much-data-do-you-need",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "How Much Data Do You Need?",
    "text": "How Much Data Do You Need?\nDepends on:\n\nModel complexity (DL needs more)\nProblem difficulty\nClass separability\nAvailable features\n\nGeneral Guidelines:\n\nTraditional ML: 100s to 1000s of samples per class\nDeep Learning: 1000s to 10,000s per class\nTransfer Learning: Can work with 100s per class\n\n\nDeep learning is data-hungry. Random Forest can work with smaller datasets. Transfer learning (starting from pre-trained models) reduces data requirements."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-augmentation",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-augmentation",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\nTechniques:\n\nRotation (90°, 180°, 270°)\nFlipping (horizontal, vertical)\nBrightness/contrast adjustment\nAdding noise\nElastic deformations\n\n\nResult: 10x more training samples from existing data!\n\n\nData augmentation synthetically increases dataset size by creating modified versions of existing samples. This helps models generalize better."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#transfer-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#transfer-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\n\n\nConcept:\n\nStart with model pre-trained on large dataset\nFine-tune on your specific task\nRequires much less data\n\n\nEO Applications:\n\nUse ImageNet pre-trained models\nNASA-IBM Geospatial Foundation Model\nDomain-specific pre-training\n\n\n\nTransfer learning leverages knowledge learned on large datasets and adapts it to your specific problem with much less data."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-diversity-matters",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#why-diversity-matters",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Why Diversity Matters",
    "text": "Why Diversity Matters\n\n\nProblem: Biased Training\n\n\nAll samples from one season\nOne geographic region only\nSimilar conditions\nResult: Model fails elsewhere\n\n\nSolution: Diverse Training\n\n\nMultiple seasons\nDifferent regions\nVarious conditions\nResult: Model generalizes\n\n\n\nModels trained on narrow datasets often fail when deployed in different conditions. Diversity in training data leads to better generalization."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#sources-of-diversity-needed",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#sources-of-diversity-needed",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Sources of Diversity Needed",
    "text": "Sources of Diversity Needed\nTemporal Diversity:\n\nDifferent seasons (wet/dry)\nMultiple years\nPhenological stages\n\nGeographic Diversity:\n\nDifferent regions\nVarious elevations\nCoastal vs inland\n\nAtmospheric Diversity:\n\nClear vs hazy days\nDifferent solar angles\nSeasonal lighting\n\nClass Diversity:\n\nVariations within classes\nEdge cases\nTransitional zones\n\n\nFor robust models, ensure your training data covers the range of conditions the model will encounter in operational use."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#example-urban-classification",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#example-urban-classification",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Example: Urban Classification",
    "text": "Example: Urban Classification\n\n\nPoor Diversity\n\nOnly Metro Manila samples\nOnly concrete roofs\nOnly high-density areas\nFails in other cities\n\n\nGood Diversity\n\nLarge cities, small towns\nVarious roof materials (concrete, metal, nipa)\nDifferent architectural styles\nDifferent densities\nWorks across Philippines\n\n\n\nIf training only on Metro Manila, the model might not recognize small towns or rural settlements with different characteristics."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#label-quality-is-critical",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#label-quality-is-critical",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Label Quality is Critical",
    "text": "Label Quality is Critical\n\n\nCommon Label Issues:\n\nMislabeled samples\nPositional errors (GPS drift)\nTemporal mismatch (old labels, new image)\nAmbiguous classes\nInconsistent definitions\nMixed pixels\n\n\nImpact:\n\nModel learns wrong patterns\nContradictory signals\nPoor generalization\nLow confidence predictions\nWasted compute\n\n\n\nOne bad label can corrupt model learning!\n\n\nGround truth labels might have errors - GPS inaccuracy, outdated information, or human mistakes. These errors propagate to model predictions."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#label-quality-best-practices",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#label-quality-best-practices",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Label Quality Best Practices",
    "text": "Label Quality Best Practices\n1. Clear Class Definitions\n\nWrite explicit criteria\nProvide examples\nDefine edge cases\nDocument ambiguities\n\n2. Consistent Protocols\n\nStandard operating procedures\nSame interpretation rules\nCalibration sessions\nRegular training for labelers\n\n3. Multiple Annotators\n\nIndependent labeling\nCompare for consistency\nResolve disagreements\nBuild consensus labels\n\n4. Expert Validation\n\nDomain experts review samples\nRandom quality checks\nIterative improvement\n\n\nInvest time in defining classes clearly and training labelers. Consistency matters more than speed."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#label-quality-example",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#label-quality-example",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Label Quality Example",
    "text": "Label Quality Example\n\n\nPoor Labels\n\n\n“Forest” defined inconsistently\nMixed with shrubland\nTemporal mismatch\nPositional errors\n\n\nHigh-Quality Labels\n\n\nClear forest definition\nCareful boundary delineation\nImage-label temporal match\nValidated position\n\n\n\nHigh-quality labels are worth the effort. A smaller dataset with accurate labels often outperforms a larger dataset with noisy labels."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#alam-project-addressing-labels",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#alam-project-addressing-labels",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "ALaM Project: Addressing Labels",
    "text": "ALaM Project: Addressing Labels\n\nDOST-ASTI’s Automated Labeling Machine\n\nAutomates labeling process\nCrowdsourcing capabilities\nExpert validation workflow\nAddresses EO’s biggest bottleneck\n\n\nRemember from Session 1: DOST-ASTI’s ALaM project specifically addresses the label quality and quantity challenge through automation and crowdsourcing."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-centric-workflow",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-centric-workflow",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Data-Centric Workflow",
    "text": "Data-Centric Workflow\nBefore Training:\n\nAudit your data: Visualize samples, check distributions\nClean aggressively: Remove clouds, fix labels, filter outliers\nBalance classes: Address imbalances through sampling or augmentation\nDocument everything: Track data sources, preprocessing, versions\n\nDuring Training:\n\nAnalyze errors: Which samples does model get wrong?\nIdentify patterns: Are errors systematic? (e.g., all in one region)\nFix data: Add more diverse samples, improve labels\nIterate: Retrain with better data\n\n\nData-centric approach means continuously improving data quality based on model feedback. Look at errors to understand what data you’re missing."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-quality-checklist",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#data-quality-checklist",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Data Quality Checklist",
    "text": "Data Quality Checklist\n\nAtmospherically corrected (Level-2A)?\nClouds and shadows masked?\nGeometric alignment verified?\nTemporal consistency checked?\nLabel accuracy validated?\nClasses clearly defined?\nTraining data balanced?\nGeographic diversity ensured?\nSeasonal coverage adequate?\nEdge cases included?\nQuality flags documented?\n\n\nUse this checklist before training any model. Addressing data issues upfront saves time and improves results."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#case-study-better-data-better-results",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#case-study-better-data-better-results",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Case Study: Better Data = Better Results",
    "text": "Case Study: Better Data = Better Results\n\n\nScenario:\nCoral reef mapping project\nInitial Results:\n\n70% accuracy\nFails in turbid water\nConfuses reef with sand\n\nProblem Identified:\nAll training data from clear water\n\nData-Centric Solution:\n\nAdd turbid water samples\nInclude reef-sand transition zones\nMore diverse depths\nImprove label precision\n\nNew Results:\n\n90% accuracy\nWorks in turbid water\nBetter boundary detection\n\n\n\n10x improvement from better data, same model!\n\n\nReal example of how data improvements had bigger impact than model tuning. The data was the key, not the algorithm."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#foundation-models-for-eo",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#foundation-models-for-eo",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Foundation Models for EO",
    "text": "Foundation Models for EO\n\nWhat are Foundation Models?\n\nLarge models pre-trained on massive EO datasets\nLearn general representations\nFine-tune for specific tasks\nDramatically reduce labeled data needs\n\nExamples (2025):\n\nGoogle AlphaEarth Foundations (DeepMind, 2025) - 1.4 trillion embeddings/year in GEE\nNASA-IBM Geospatial Foundation Model (open-source, Aug 2024)\nPrithvi (IBM/NASA/ESA collaboration)\nClay Foundation Model (open-source)\nPlanet Labs + Anthropic Claude integration\n\n\nTiming: 4 minutes\nKey Points: - 2025 Update: Foundation models are THE major innovation in EO AI - Google AlphaEarth Foundations: Virtual satellite model, 10x10m resolution, integrates Sentinel-1/2 + Landsat + radar, available in Earth Engine, 16x less storage than other AI systems - NASA-IBM model released August 2024 as open-source - Trained on massive Sentinel-2 datasets (1 billion parameters) - Can be fine-tuned with just hundreds of labeled samples (vs thousands before) - Philippine Application: Use foundation models to jumpstart projects with limited labeled data - AlphaEarth embeddings already in GEE!\nExample: “Instead of manually labeling 10,000 images for rice mapping, use AlphaEarth embeddings in GEE or fine-tune Prithvi with just 500 samples and achieve similar accuracy”"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#on-board-ai-processing",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#on-board-ai-processing",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "On-Board AI Processing",
    "text": "On-Board AI Processing\n\n\n\nESA Φsat-2 (Launched 2024)\n\n22×10×33 cm CubeSat\nOnboard AI computer (Intel Myriad X VPU)\nReal-time cloud detection\nProcess before downlink\nSaves bandwidth\n\n\nSatellogic Edge Computing\n\n“AI First” satellites\nOnboard GPUs\nReal-time processing\nImmediate insights\nShip/object detection\n\n\n\nTiming: 3 minutes\nKey Points: - 2025 Update: On-board AI is operational on multiple satellites - ESA’s Φsat-2 launched 2024 with Intel AI chip - Processes images on-orbit before transmitting - Use case: Only download cloud-free portions, save 90% bandwidth - Future: Real-time disaster detection from space\nPhilippine Relevance: “Imagine typhoon damage detected and reported automatically from orbit within minutes, not hours”"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#self-supervised-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#self-supervised-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Self-Supervised Learning",
    "text": "Self-Supervised Learning\n\n\nConcept:\n\nLearn from unlabeled data\nDefine pretext tasks (e.g., predict missing patches)\nModel learns useful representations\nFine-tune with small labeled dataset\n\nWhy Important for EO:\n\nAbundance of unlabeled satellite imagery\nHigh cost of labeling\nImproves transferability\n\n\n\n\n\nSelf-supervised learning is particularly relevant for EO due to abundance of unlabeled imagery. Models learn useful features without expensive labeling."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#explainable-ai-xai",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#explainable-ai-xai",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Explainable AI (XAI)",
    "text": "Explainable AI (XAI)\n\nWhy XAI Matters:\n\nUnderstand model decisions\nBuild trust in AI systems\nDebug and improve models\nRegulatory compliance\n\nMethods:\n\nSHAP: Feature importance\nLIME: Local explanations\nGrad-CAM: Visual attention maps\nSaliency Maps: What pixels matter?\n\n\nAs AI systems make important decisions (disaster response, resource allocation), understanding why they make those decisions becomes crucial."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-we-covered",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#what-we-covered",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "What We Covered",
    "text": "What We Covered\n\nAI/ML Basics: What it is and why it’s powerful for EO\nML Workflow: 7-step process from problem to deployment\nSupervised Learning: Classification and regression with labeled data\nUnsupervised Learning: Clustering and pattern discovery\nDeep Learning: Neural networks and CNNs for images\nData-Centric AI: Quality, quantity, diversity, labels\n2025 Trends: Foundation models, on-board AI, XAI\n\n\nWe’ve covered the fundamental concepts you need to understand before diving into hands-on implementation."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#key-takeaways",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#key-takeaways",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n1. Focus on Data First\n\nQuality beats quantity\nDiversity enables generalization\nGood labels are gold\n\n2. Start Simple\n\nTry traditional ML before deep learning\nRandom Forest is often enough\nAdd complexity only when needed\n\n3. Iterate Continuously\n\nAnalyze errors\nImprove data\nRetrain models\nDeployment is not the end\n\n\nThese principles will serve you well throughout your AI/ML journey. Data quality and iterative improvement are more important than fancy algorithms."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#practical-advice",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#practical-advice",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Practical Advice",
    "text": "Practical Advice\nFor Your Projects:\n\nDefine the problem clearly before collecting data\nInvest in high-quality training data - it’s worth it\nValidate rigorously on independent data\nDocument everything (data sources, preprocessing, model versions)\nStart with baselines (simple models, existing methods)\nIterate based on errors - let failures guide improvements\nConsider operational constraints early\n\n\nThese practical tips come from real-world experience. Following them will save you time and frustration."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#the-data-centric-mindset",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#the-data-centric-mindset",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "The Data-Centric Mindset",
    "text": "The Data-Centric Mindset\n\n\nWhen model performs poorly, ask:\n\nIs my data clean?\nAre labels accurate?\nIs training data representative?\nDo I have enough diversity?\nAre there systematic biases?\n\n\nBefore trying:\n\nMore complex model\nMore epochs\nDifferent hyperparameters\nNew architecture\n\nCheck your data first!\n\n\n“Better data beats a cleverer model” - Andrew Ng\n\n\nAdopt a data-centric mindset. When models underperform, investigate data issues before blaming the algorithm."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#connection-to-sessions-3-4",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#connection-to-sessions-3-4",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Connection to Sessions 3 & 4",
    "text": "Connection to Sessions 3 & 4\n\n\nSession 3: Python Basics\n\nLoad and explore data\nGeoPandas (vector)\nRasterio (raster)\nFoundation for all ML work\n\n\nSession 4: Google Earth Engine\n\nAccess Sentinel data at scale\nCloud masking (data quality!)\nTemporal compositing\nExport for ML workflows\n\n\n\nEverything builds on these concepts!\n\n\nThe hands-on sessions this afternoon put these concepts into practice. You’ll actually work with data and see these principles in action."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#looking-ahead-days-2-4",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#looking-ahead-days-2-4",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Looking Ahead: Days 2-4",
    "text": "Looking Ahead: Days 2-4\n\n\nDay 2:\n\nRandom Forest classification\nLand cover mapping\nCNN basics\nTensorFlow/Keras intro\n\n\nDays 3-4:\n\nU-Net for segmentation\nFlood mapping (DRR focus)\nTime series with LSTMs\nFoundation models\nExplainable AI\n\n\n\nOver the next three days, we’ll implement these concepts in real EO applications for DRR, CCA, and NRM."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#resources-for-continued-learning",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#resources-for-continued-learning",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Resources for Continued Learning",
    "text": "Resources for Continued Learning\nOnline Courses:\n\nNASA ARSET: ML for Earth Science\nEO College: Introduction to ML for EO\nCoursera: Machine Learning (Andrew Ng)\nFast.ai: Practical Deep Learning\n\nPapers & Tutorials:\n\n“Data-Centric ML for Earth Observation” (ArXiv 2025)\nGoogle Earth Engine tutorials\nTensorFlow Earth Observation tutorials\n\nCommunities:\n\nSkAI-Pinas network\nDigital Space Campus (CoPhil)\nDIMER model repository\n\n\nThese resources will support your continued learning after the training. The Digital Space Campus will have all our materials for reference."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#session-summary",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#session-summary",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Session Summary",
    "text": "Session Summary\nWhat We Covered:\n✅ AI/ML/DL definitions and relationships\n✅ End-to-end ML workflow for EO\n✅ Supervised learning (classification, regression)\n✅ Unsupervised learning (clustering)\n✅ Deep learning & CNNs for satellite imagery\n✅ Data-centric AI philosophy\n✅ 2025 innovations: Foundation models, on-board AI\n\nTiming: 2 minutes\nYou now have conceptual foundation for all ML work in this course. Sessions 3-4 today and Days 2-4 will implement these concepts."
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#qa",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#qa",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Q&A",
    "text": "Q&A\n\n\nAI/ML Concepts\n\nSupervised vs unsupervised?\nWhen to use deep learning?\nFoundation models for my use case?\n\n\nPractical Questions\n\nData quality challenges?\nLabel collection strategies?\nComputing requirements?\n\n\n\nTiming: 5-8 minutes for Q&A\nCommon Questions: - “Do I need a GPU?” → Not for Random Forest, yes for deep learning - “How many labels do I need?” → Depends: 100s with foundation models, 1000s for CNN from scratch - “Which algorithm should I use?” → Start simple (RF), then deep learning if needed - “Can I use foundation models for Philippines?” → Yes! They’re global and open-source"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#hands-on-python-for-geospatial-data",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#hands-on-python-for-geospatial-data",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Hands-on Python for Geospatial Data",
    "text": "Hands-on Python for Geospatial Data\nComing up after 15-minute break:\n\nGoogle Colab environment setup\nGeoPandas for vector data\nRasterio for raster data\nWork with Philippine boundaries\nLoad and visualize Sentinel-2 imagery\nCalculate NDVI\n\n\nGet ready to code! 💻"
  },
  {
    "objectID": "day1/presentations/02_session2_ai_ml_fundamentals.html#resources",
    "href": "day1/presentations/02_session2_ai_ml_fundamentals.html#resources",
    "title": "Core Concepts of AI/ML for Earth Observation",
    "section": "Resources",
    "text": "Resources\nFoundation Models:\nNASA-IBM Geospatial: https://huggingface.co/ibm-nasa-geospatial\nPrithvi: https://github.com/NASA-IMPACT/Prithvi\nClay: https://clay-foundation.github.io\nLearning:\nNASA ARSET: https://appliedsciences.nasa.gov/arset\nEO College: https://eo-college.org\nSkAI-Pinas: https://asti.dost.gov.ph/skai-pinas\n\nSession 2 Complete!\n15-minute break before Session 3. Make sure participants have: - Google Colab access working - GEE account registration started (will finalize in Session 4)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#session-overview",
    "href": "day1/presentations/03_session3_python_geospatial.html#session-overview",
    "title": "Python for Geospatial Data Analysis",
    "section": "Session Overview",
    "text": "Session Overview\nPython for Geospatial Data Analysis\n\nHands-on introduction to working with vector and raster data using Python\n\n\nFormat: Brief conceptual intro + Extended hands-on coding\n\n\nDuration: 2 hours (15-20 min presentation + 100 min hands-on)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#learning-objectives",
    "href": "day1/presentations/03_session3_python_geospatial.html#learning-objectives",
    "title": "Python for Geospatial Data Analysis",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you will be able to:\n\nSet up and use Google Colaboratory for geospatial analysis\nLoad, explore, and visualize vector data with GeoPandas\nRead, process, and visualize raster data with Rasterio\nPerform basic geospatial operations (clipping, reprojecting, cropping)\nPrepare data for AI/ML workflows"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#session-roadmap",
    "href": "day1/presentations/03_session3_python_geospatial.html#session-roadmap",
    "title": "Python for Geospatial Data Analysis",
    "section": "Session Roadmap",
    "text": "Session Roadmap\n\n\n\nTime\nTopic\nDuration\n\n\n\n\n00-15 min\nSetup & Python Basics Recap\n15 min\n\n\n15-55 min\nGeoPandas for Vector Data (HANDS-ON)\n40 min\n\n\n55-60 min\n☕ Break\n5 min\n\n\n60-110 min\nRasterio for Raster Data (HANDS-ON)\n50 min\n\n\n110-120 min\nSummary & Next Steps\n10 min\n\n\n\n\nTiming: 2 minutes\nKey Point: This is a HANDS-ON session. Participants code along in their notebooks."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#notebook-access",
    "href": "day1/presentations/03_session3_python_geospatial.html#notebook-access",
    "title": "Python for Geospatial Data Analysis",
    "section": "Notebook Access",
    "text": "Notebook Access\n📓 Google Colab Notebook:\nDay1_Session3_Python_Geospatial_Data.ipynb\n\nOpen link from course materials\nSave a copy to your Drive\nRun first cell to install packages\nFollow along as we code together\n\n\nTiming: 3 minutes\nInstructor Actions: - Share notebook link in chat - Wait for participants to open and save copy - Verify everyone can see the notebook - Explain Colab basics (cells, shift+enter)\nTroubleshooting: - “Can’t access?” → Check Google account login - “Packages fail?” → Restart runtime"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#todays-focus",
    "href": "day1/presentations/03_session3_python_geospatial.html#todays-focus",
    "title": "Python for Geospatial Data Analysis",
    "section": "Today’s Focus",
    "text": "Today’s Focus\n\n\nVector Data:\n\nAdministrative boundaries\nPoints of interest\nRoads, rivers\nTraining sample polygons\nUsing GeoPandas\n\n\nRaster Data:\n\nSatellite imagery\nDigital elevation models\nLand cover maps\nAI model outputs\nUsing Rasterio\n\n\n\nIntegration: Combining vector and raster for complete EO workflows"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#the-python-advantage",
    "href": "day1/presentations/03_session3_python_geospatial.html#the-python-advantage",
    "title": "Python for Geospatial Data Analysis",
    "section": "The Python Advantage",
    "text": "The Python Advantage\nWhy Python is the Leading Language for EO:\n\nRich Ecosystem\n\nHundreds of specialized libraries\nActive development and community\n\nEasy to Learn\n\nClear syntax, readable code\nGentle learning curve\n\nPowerful Integration\n\nConnects data sources, processing, ML\nSingle environment for complete workflows\n\nFree and Open Source\n\nNo licensing costs\nTransparent and reproducible"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#python-geospatial-ecosystem",
    "href": "day1/presentations/03_session3_python_geospatial.html#python-geospatial-ecosystem",
    "title": "Python for Geospatial Data Analysis",
    "section": "Python Geospatial Ecosystem",
    "text": "Python Geospatial Ecosystem\n\nComplete Python Earth Observation Ecosystem organized by function\nThis diagram shows the complete Python ecosystem for Earth Observation, organized by function: Data Access (Earth Engine, Sentinel Hub), Geospatial Processing (GeoPandas, Rasterio), Data Science (NumPy, Pandas), Machine Learning (Scikit-learn, TensorFlow, PyTorch), and Visualization tools. Notice how data flows from access through processing to analysis and visualization."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#integration-capabilities",
    "href": "day1/presentations/03_session3_python_geospatial.html#integration-capabilities",
    "title": "Python for Geospatial Data Analysis",
    "section": "Integration Capabilities",
    "text": "Integration Capabilities\nPython Connects Everything:\n\n# Example workflow\nimport geopandas as gpd\nimport rasterio\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load vector training data\ntraining = gpd.read_file('samples.geojson')\n\n# Load satellite raster\nwith rasterio.open('sentinel2.tif') as src:\n    image = src.read()\n\n# Extract features and train model\nX, y = extract_features(image, training)\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\n# Predict on full image\nprediction = model.predict(image)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#community-and-resources",
    "href": "day1/presentations/03_session3_python_geospatial.html#community-and-resources",
    "title": "Python for Geospatial Data Analysis",
    "section": "Community and Resources",
    "text": "Community and Resources\nVibrant Python Geospatial Community:\nDocumentation:\n\nComprehensive guides for all libraries\nTutorials and examples\nAPI references\n\nCommunity Support:\n\nStack Overflow\nGitHub discussions\nGIS Stack Exchange\nDedicated forums\n\nLearning Resources:\n\nFree courses (Coursera, Udemy)\nBooks (Automating GIS Processes)\nBlogs and tutorials\nConference workshops"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#why-colab-for-this-training",
    "href": "day1/presentations/03_session3_python_geospatial.html#why-colab-for-this-training",
    "title": "Python for Geospatial Data Analysis",
    "section": "Why Colab for This Training?",
    "text": "Why Colab for This Training?\nAdvantages for Learning:\n\nNo Setup Hassles\n\nWorks immediately\nNo environment configuration\nConsistent for all participants\n\nAccessible Anywhere\n\nJust need a browser\nWorks on any computer\nEven tablets\n\nPowerful Resources\n\nFree GPU for deep learning\n12+ GB RAM\nSufficient for all exercises\n\nEasy Sharing\n\nShare notebooks instantly\nCollaborative editing\nCoPhil materials readily accessible"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#colab-interface-overview",
    "href": "day1/presentations/03_session3_python_geospatial.html#colab-interface-overview",
    "title": "Python for Geospatial Data Analysis",
    "section": "Colab Interface Overview",
    "text": "Colab Interface Overview\n\n\nMain Components:\n\n\nMenu Bar\n\nFile, Edit, View, Insert, Runtime\n\n\n\n\n\nToolbar\n\nPlay button to run cells\nAdd code/text cells\n\n\n\n\n\nNotebook Area\n\nCode cells (executable)\nText cells (Markdown)\n\n\n\n\n\nSidebar\n\nTable of contents\nFiles browser\nCode snippets\n\n\n\n\n\n\n\nColab Interface"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#running-code-in-colab",
    "href": "day1/presentations/03_session3_python_geospatial.html#running-code-in-colab",
    "title": "Python for Geospatial Data Analysis",
    "section": "Running Code in Colab",
    "text": "Running Code in Colab\nTwo Ways to Execute Cells:\n\n1. Click the Play Button\n\nLeft side of each code cell\nRuns that specific cell\n\n\n\n2. Keyboard Shortcuts\n\nShift + Enter: Run cell and move to next\nCtrl + Enter: Run cell, stay on current\nCtrl + M then A: Add cell above\nCtrl + M then B: Add cell below\n\n\n\nOutput Appears Below Cell:\nText, plots, tables, errors all display inline."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#google-drive-integration",
    "href": "day1/presentations/03_session3_python_geospatial.html#google-drive-integration",
    "title": "Python for Geospatial Data Analysis",
    "section": "Google Drive Integration",
    "text": "Google Drive Integration\nMounting Your Google Drive:\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\nBenefits:\n\nPersistent storage (Colab resets)\nUpload/download data\nSave outputs\nShare files between notebooks\n\n\n\nAccess Your Files:\n# Your Drive files appear at:\n# /content/drive/MyDrive/\n\n# Example:\nimport geopandas as gpd\ngdf = gpd.read_file('/content/drive/MyDrive/data/boundaries.shp')"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#installing-additional-packages",
    "href": "day1/presentations/03_session3_python_geospatial.html#installing-additional-packages",
    "title": "Python for Geospatial Data Analysis",
    "section": "Installing Additional Packages",
    "text": "Installing Additional Packages\nMost Common Libraries Pre-Installed:\nNumPy, Pandas, Matplotlib, Scikit-learn\n\nFor Geospatial Libraries:\n# GeoPandas (usually pre-installed, but check version)\n!pip install geopandas\n\n# Rasterio\n!pip install rasterio\n\n# Other useful libraries\n!pip install earthengine-api\n!pip install folium\n\n\nNote: Packages need reinstalling each session (Colab resets runtime)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#what-is-geopandas",
    "href": "day1/presentations/03_session3_python_geospatial.html#what-is-geopandas",
    "title": "Python for Geospatial Data Analysis",
    "section": "What is GeoPandas?",
    "text": "What is GeoPandas?\n\n\nPandas + Geometry = GeoPandas\n\nDefinition:\nExtension of Pandas for working with geospatial vector data\n\n\nKey Concept:\nLike a spreadsheet/table where one column contains geometries (points, lines, polygons)\n\n\nBuilt On:\n\nPandas - Data manipulation\nShapely - Geometric operations\nFiona - File I/O\nPyProj - Coordinate systems\n\n\n\n\nGeoPandas\nPandas + Geometry"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#the-geodataframe-concept",
    "href": "day1/presentations/03_session3_python_geospatial.html#the-geodataframe-concept",
    "title": "Python for Geospatial Data Analysis",
    "section": "The GeoDataFrame Concept",
    "text": "The GeoDataFrame Concept\nSimilar to Pandas DataFrame:\n\n\nRegular DataFrame:\n\n\n\nName\nPopulation\nArea\n\n\n\n\nManila\n1.78M\n42.88\n\n\nCebu\n0.92M\n315\n\n\nDavao\n1.63M\n2444\n\n\n\n\nGeoDataFrame:\n\n\n\nName\nPopulation\nArea\ngeometry\n\n\n\n\nManila\n1.78M\n42.88\nPOLYGON(…)\n\n\nCebu\n0.92M\n315\nPOLYGON(…)\n\n\nDavao\n1.63M\n2444\nPOLYGON(…)\n\n\n\n\n\nSpecial “geometry” Column:\nContains spatial information (coordinates defining shapes)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#common-vector-data-operations",
    "href": "day1/presentations/03_session3_python_geospatial.html#common-vector-data-operations",
    "title": "Python for Geospatial Data Analysis",
    "section": "Common Vector Data Operations",
    "text": "Common Vector Data Operations\nWhat You Can Do with GeoPandas:\n\nRead/Write\n\nShapefiles, GeoJSON, GeoPackage, PostGIS\n\nExplore\n\nView attributes, examine geometries\n\nVisualize\n\nQuick plotting, interactive maps\n\nGeoprocessing\n\nBuffer, intersection, union, clip\n\nSpatial Joins\n\nCombine datasets based on location\n\nCoordinate Transformations\n\nReproject to different CRS"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#geopandas-code-example",
    "href": "day1/presentations/03_session3_python_geospatial.html#geopandas-code-example",
    "title": "Python for Geospatial Data Analysis",
    "section": "GeoPandas Code Example",
    "text": "GeoPandas Code Example\nLoading and Visualizing:\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\n\n# Read Philippine provinces shapefile\nprovinces = gpd.read_file('philippines_provinces.shp')\n\n# Examine data\nprint(provinces.head())\nprint(provinces.crs)  # Check coordinate system\n\n# Simple plot\nprovinces.plot(figsize=(10, 10),\n               edgecolor='black',\n               facecolor='lightblue')\nplt.title('Philippine Provinces')\nplt.show()"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#visualization-capabilities",
    "href": "day1/presentations/03_session3_python_geospatial.html#visualization-capabilities",
    "title": "Python for Geospatial Data Analysis",
    "section": "Visualization Capabilities",
    "text": "Visualization Capabilities\nGeoPandas Plotting:\n\n# Color by attribute\nprovinces.plot(column='population',\n               cmap='YlOrRd',\n               legend=True,\n               figsize=(12, 10))\nplt.title('Population by Province')\n\n# Add basemap (with contextily)\nimport contextily as ctx\nprovinces_web_mercator = provinces.to_crs(epsg=3857)\nax = provinces_web_mercator.plot(figsize=(15, 15),\n                                   alpha=0.5)\nctx.add_basemap(ax)\n\n\nGeoPandas integrates seamlessly with Matplotlib for static plots and can work with Folium or Plotly for interactive maps. For training, we’ll focus on quick visualization for QA and exploration."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#philippine-coordinate-reference-systems",
    "href": "day1/presentations/03_session3_python_geospatial.html#philippine-coordinate-reference-systems",
    "title": "Python for Geospatial Data Analysis",
    "section": "Philippine Coordinate Reference Systems",
    "text": "Philippine Coordinate Reference Systems\nCommon CRS for Philippines:\n\n\n\nEPSG\nName\nType\nUnits\nUse Case\n\n\n\n\n4326\nWGS84\nGeographic\nDegrees\nWeb maps, lat/lon\n\n\n32651\nUTM Zone 51N\nProjected\nMeters\nWestern PH, Manila\n\n\n32652\nUTM Zone 52N\nProjected\nMeters\nEastern PH, Mindanao\n\n\n3123\nPRS92 Zone III\nProjected\nMeters\nNational standard\n\n\n\n\nRule: Use geographic (4326) for storage, projected (UTM) for analysis\n\n\nPhilippines spans two UTM zones. Zone 51N covers Manila, Palawan, western areas. Zone 52N covers Mindanao and eastern regions. Always reproject to UTM for area/distance calculations!"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#philippine-utm-zones",
    "href": "day1/presentations/03_session3_python_geospatial.html#philippine-utm-zones",
    "title": "Python for Geospatial Data Analysis",
    "section": "Philippine UTM Zones",
    "text": "Philippine UTM Zones\n\n\nUTM Zone 51N (EPSG:32651)\nCoverage: - Metro Manila - Palawan - Western Luzon - Western Visayas\nMost common for: - Urban analysis - Palawan studies - Manila projects\n\nUTM Zone 52N (EPSG:32652)\nCoverage: - Mindanao - Eastern Visayas - Bicol Region - Eastern Luzon\nMost common for: - Mindanao analysis - Disaster mapping - Agricultural studies\n\n\nCode Example:\n# Reproject to UTM 51N for area calculation\ngdf_utm = gdf.to_crs(epsg=32651)\ngdf_utm['area_km2'] = gdf_utm.geometry.area / 1_000_000\n\n\nThe UTM zone boundary runs roughly through the middle of the Philippines. For national-scale work, pick one zone and reproject everything to it, or use PRS92."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#philippine-geospatial-data-sources",
    "href": "day1/presentations/03_session3_python_geospatial.html#philippine-geospatial-data-sources",
    "title": "Python for Geospatial Data Analysis",
    "section": "Philippine Geospatial Data Sources",
    "text": "Philippine Geospatial Data Sources\n\n\nNAMRIA Geoportal - Administrative boundaries - Topographic maps - Land cover 2020 - DEMs - www.geoportal.gov.ph\nPhilSA - Satellite imagery - EO products - philsa.gov.ph\n\nPSA - Census boundaries - Barangay data - psa.gov.ph\nOpenStreetMap - Roads, buildings - extract.bbbike.org\nNatural Earth - Country boundaries - naturalearthdata.com\n\n\nAll work with GeoPandas - just gpd.read_file()!\n\n\nNAMRIA is the official source for government work. OSM is community-maintained but very detailed for urban areas. PhilSA provides satellite-derived products."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#what-is-rasterio",
    "href": "day1/presentations/03_session3_python_geospatial.html#what-is-rasterio",
    "title": "Python for Geospatial Data Analysis",
    "section": "What is Rasterio?",
    "text": "What is Rasterio?\n\n\nPython Wrapper for GDAL\n\nDefinition:\nClean, idiomatic Python library for reading and writing geospatial raster data\n\n\nWhy Not Use GDAL Directly?\n\nGDAL Python bindings are cumbersome\nRasterio is more Pythonic\nBetter integration with NumPy\nCleaner syntax\n\n\n\nWorks With:\nAll formats GDAL supports - GeoTIFF, COG, NetCDF, HDF, etc.\n\n\n\nRasterio\nPython Wrapper for GDAL"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#raster-data-structure",
    "href": "day1/presentations/03_session3_python_geospatial.html#raster-data-structure",
    "title": "Python for Geospatial Data Analysis",
    "section": "Raster Data Structure",
    "text": "Raster Data Structure\nHow Rasterio Represents Imagery:\n\n3D NumPy Array:\n(bands, rows, columns)\n\n\nExample: Sentinel-2 10m Bands:\n# 4 bands (Blue, Green, Red, NIR)\n# 1098 rows (10980 m / 10 m)\n# 1098 columns (10980 m / 10 m)\n# Shape: (4, 1098, 1098)\n\narray[0, :, :]  # Band 1 (Blue)\narray[1, :, :]  # Band 2 (Green)\narray[2, :, :]  # Band 3 (Red)\narray[3, :, :]  # Band 4 (NIR)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#reading-raster-data-with-rasterio",
    "href": "day1/presentations/03_session3_python_geospatial.html#reading-raster-data-with-rasterio",
    "title": "Python for Geospatial Data Analysis",
    "section": "Reading Raster Data with Rasterio",
    "text": "Reading Raster Data with Rasterio\nBasic Workflow:\n\nimport rasterio\nimport numpy as np\n\n# Open raster file\nwith rasterio.open('sentinel2_10m.tif') as src:\n    # Read all bands\n    data = src.read()\n\n    # Read specific band\n    red_band = src.read(3)\n\n    # Get metadata\n    print(f\"CRS: {src.crs}\")\n    print(f\"Transform: {src.transform}\")\n    print(f\"Width: {src.width}, Height: {src.height}\")\n    print(f\"Bounds: {src.bounds}\")\n    print(f\"Number of bands: {src.count}\")\n\n\nThe ‘with’ statement ensures the file is properly closed after reading. This is good practice and prevents file locking issues."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#array-operations",
    "href": "day1/presentations/03_session3_python_geospatial.html#array-operations",
    "title": "Python for Geospatial Data Analysis",
    "section": "Array Operations",
    "text": "Array Operations\nRasterio + NumPy = Powerful Processing\n\n# Calculate NDVI\nwith rasterio.open('sentinel2_10m.tif') as src:\n    red = src.read(3).astype(float)\n    nir = src.read(4).astype(float)\n\n# NDVI formula\nndvi = (nir - red) / (nir + red + 1e-8)  # Small value prevents division by zero\n\n# Apply threshold\nvegetation_mask = ndvi &gt; 0.3\n\n# Calculate statistics\nprint(f\"Mean NDVI: {np.mean(ndvi):.3f}\")\nprint(f\"Vegetation pixels: {np.sum(vegetation_mask)}\")"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#common-spectral-indices",
    "href": "day1/presentations/03_session3_python_geospatial.html#common-spectral-indices",
    "title": "Python for Geospatial Data Analysis",
    "section": "Common Spectral Indices",
    "text": "Common Spectral Indices\nKey Indices for EO Analysis:\n\n\n\n\n\n\n\n\n\nIndex\nFormula\nPurpose\nRange\n\n\n\n\nNDVI\n(NIR - Red) / (NIR + Red)\nVegetation health\n-1 to +1\n\n\nEVI\n2.5 × (NIR - Red) / (NIR + 6×Red - 7.5×Blue + 1)\nEnhanced vegetation\n-1 to +1\n\n\nNDWI\n(Green - NIR) / (Green + NIR)\nWater bodies\n-1 to +1\n\n\nNDBI\n(SWIR - NIR) / (SWIR + NIR)\nBuilt-up areas\n-1 to +1\n\n\n\n\nSentinel-2 Bands: Blue (B2), Green (B3), Red (B4), NIR (B8), SWIR (B11, B12)\n\n\nThese indices are fundamental for EO analysis. NDVI is most common for vegetation. EVI better in high biomass areas (tropical forests). NDWI for flood mapping."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#philippine-application-rice-monitoring",
    "href": "day1/presentations/03_session3_python_geospatial.html#philippine-application-rice-monitoring",
    "title": "Python for Geospatial Data Analysis",
    "section": "Philippine Application: Rice Monitoring",
    "text": "Philippine Application: Rice Monitoring\nUsing NDVI for Philippine Rice Paddies:\n# Calculate NDVI for Central Luzon rice area\nwith rasterio.open('sentinel2_central_luzon.tif') as src:\n    red = src.read(4).astype(float)   # Band 4\n    nir = src.read(8).astype(float)   # Band 8\n\n# NDVI calculation\nndvi = (nir - red) / (nir + red + 1e-8)\n\n# Classify vegetation health\nbare_soil = ndvi &lt; 0.2      # Recently planted / fallow\ngrowing = (ndvi &gt;= 0.2) & (ndvi &lt; 0.5)  # Early growth\nmature = (ndvi &gt;= 0.5) & (ndvi &lt; 0.8)   # Peak biomass\nvery_dense = ndvi &gt;= 0.8    # Maximum vegetation\n\n# Calculate rice area statistics\npixel_area = 100  # 10m x 10m = 100 m²\nmature_rice_area_ha = np.sum(mature) * pixel_area / 10000\n\nprint(f\"Mature rice area: {mature_rice_area_ha:.2f} hectares\")\n\nThis workflow is used by PRiSM for operational rice monitoring across the Philippines. NDVI time series tracks crop phenology from planting to harvest."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#visualization-with-rasterio",
    "href": "day1/presentations/03_session3_python_geospatial.html#visualization-with-rasterio",
    "title": "Python for Geospatial Data Analysis",
    "section": "Visualization with Rasterio",
    "text": "Visualization with Rasterio\nDisplaying Satellite Imagery:\n\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\n\n# Open and display\nwith rasterio.open('sentinel2_10m.tif') as src:\n    # Show true color composite (RGB)\n    show((src, [3, 2, 1]), title='True Color')\n\n    # Show false color composite (NIR, Red, Green)\n    show((src, [4, 3, 2]), title='False Color (NIR-R-G)')\n\n# Or read and plot with matplotlib\nwith rasterio.open('sentinel2_10m.tif') as src:\n    data = src.read([3, 2, 1])  # RGB\n    # Scale to 0-255 for display\n    data_scaled = np.clip(data / 3000, 0, 1)\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(np.moveaxis(data_scaled, 0, -1))\n    plt.title('Sentinel-2 True Color')\n    plt.axis('off')\n    plt.show()"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#what-well-build-today",
    "href": "day1/presentations/03_session3_python_geospatial.html#what-well-build-today",
    "title": "Python for Geospatial Data Analysis",
    "section": "What We’ll Build Today",
    "text": "What We’ll Build Today\nNotebook 1: Vector Data with GeoPandas\n\nLoad Philippine administrative boundaries\nExplore and visualize provinces\nFilter specific regions (e.g., Central Luzon)\nCalculate area and basic statistics\nSpatial operations (buffer, clip)\nCreate training sample polygons"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#what-well-build-today-1",
    "href": "day1/presentations/03_session3_python_geospatial.html#what-well-build-today-1",
    "title": "Python for Geospatial Data Analysis",
    "section": "What We’ll Build Today",
    "text": "What We’ll Build Today\nNotebook 2: Raster Data with Rasterio\n\nLoad Sentinel-2 image subset\nExamine metadata and properties\nVisualize true and false color composites\nCalculate vegetation indices (NDVI, EVI)\nCrop to area of interest\nExtract pixel values at point locations\nSave processed outputs"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#integrating-vector-and-raster",
    "href": "day1/presentations/03_session3_python_geospatial.html#integrating-vector-and-raster",
    "title": "Python for Geospatial Data Analysis",
    "section": "Integrating Vector and Raster",
    "text": "Integrating Vector and Raster\nCombining Both Data Types:\n\nimport geopandas as gpd\nimport rasterio\nfrom rasterio.mask import mask\n\n# Load vector boundary\naoi = gpd.read_file('study_area.geojson')\n\n# Load raster\nwith rasterio.open('sentinel2.tif') as src:\n    # Clip raster to vector boundary\n    clipped_data, clipped_transform = mask(\n        src,\n        aoi.geometry,\n        crop=True\n    )\n\n    # Update metadata for output\n    out_meta = src.meta.copy()\n    out_meta.update({\n        \"height\": clipped_data.shape[1],\n        \"width\": clipped_data.shape[2],\n        \"transform\": clipped_transform\n    })"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#preparing-for-ml-workflows",
    "href": "day1/presentations/03_session3_python_geospatial.html#preparing-for-ml-workflows",
    "title": "Python for Geospatial Data Analysis",
    "section": "Preparing for ML Workflows",
    "text": "Preparing for ML Workflows\nWhat You’ll Learn:\n\nExtract Training Data\n\nSample raster values at polygon locations\nCreate feature matrix (X) and labels (y)\n\nSpatial Data Splits\n\nAvoid spatial autocorrelation in train/test\n\nData Formatting\n\nStructure for Scikit-learn, TensorFlow, PyTorch\n\nQuality Control\n\nCheck for NaN values, outliers\nValidate spatial alignment"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#link-to-colab-notebooks",
    "href": "day1/presentations/03_session3_python_geospatial.html#link-to-colab-notebooks",
    "title": "Python for Geospatial Data Analysis",
    "section": "Link to Colab Notebooks",
    "text": "Link to Colab Notebooks\n\nAccess Today’s Notebooks:\n\n\nNotebook 1: Vector Data\n[Link will be provided in chat]\n\n\nNotebook 2: Raster Data\n[Link will be provided in chat]\n\n\nMake a Copy:\nFile → Save a copy in Drive (so you can edit and experiment)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#tips-for-success",
    "href": "day1/presentations/03_session3_python_geospatial.html#tips-for-success",
    "title": "Python for Geospatial Data Analysis",
    "section": "Tips for Success",
    "text": "Tips for Success\nAs We Work Through Notebooks:\n\nRun Cells Sequentially\n\nTop to bottom order matters\nEach cell may depend on previous\n\nRead the Comments\n\nExplanations included in code\nLearn the “why” not just “how”\n\nExperiment\n\nModify parameters\nTry different visualizations\nBreak things and learn\n\nAsk Questions\n\nUse chat or raise hand\nNo question is too basic\n\nTake Notes\n\nUseful patterns and code snippets\nErrors and solutions"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#python-geospatial-ecosystem-1",
    "href": "day1/presentations/03_session3_python_geospatial.html#python-geospatial-ecosystem-1",
    "title": "Python for Geospatial Data Analysis",
    "section": "Python Geospatial Ecosystem",
    "text": "Python Geospatial Ecosystem\nGeoPandas:\n\nDataFrame + geometry column\nVector data operations\nEasy visualization\nIntegration with Pandas\n\nRasterio:\n\nClean GDAL wrapper\nNumPy array representation\nComprehensive metadata handling\nEfficient I/O\n\nIntegration:\n\nBoth work together seamlessly\nComplete EO workflows in Python\nFoundation for ML/AI applications"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#why-these-skills-matter",
    "href": "day1/presentations/03_session3_python_geospatial.html#why-these-skills-matter",
    "title": "Python for Geospatial Data Analysis",
    "section": "Why These Skills Matter",
    "text": "Why These Skills Matter\nFor AI/ML in Earth Observation:\n\nData Preparation\n\nLoading and preprocessing is 80% of work\nQuality in → Quality out\n\nFeature Engineering\n\nCalculate indices, textures, derivatives\nNumPy operations on raster arrays\n\nTraining Data Creation\n\nSample raster at polygon locations\nExtract features for supervised learning\n\nModel Deployment\n\nApply trained models to new imagery\nGenerate prediction maps\n\nValidation and QA\n\nCompare predictions to ground truth\nCalculate accuracy metrics"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#building-blocks-for-this-week",
    "href": "day1/presentations/03_session3_python_geospatial.html#building-blocks-for-this-week",
    "title": "Python for Geospatial Data Analysis",
    "section": "Building Blocks for This Week",
    "text": "Building Blocks for This Week\nToday’s Skills Enable:\nDay 2:\n\nRandom Forest land cover classification\nPreparing training data for ML\n\nDay 3:\n\nDeep learning data pipelines\nU-Net flood mapping inputs\n\nDay 4:\n\nTime series data preparation\nLSTM input formatting\n\n\nMastering these fundamentals now will make everything else smoother."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#minute-break",
    "href": "day1/presentations/03_session3_python_geospatial.html#minute-break",
    "title": "Python for Geospatial Data Analysis",
    "section": "☕ 5-Minute Break",
    "text": "☕ 5-Minute Break\n\nStretch Break\nStand up • Grab water • Back in 5 minutes\n\n\nTiming: 5 minutes\nAfter Break: Continue with Rasterio section (50 minutes of hands-on coding)"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#transition-to-hands-on",
    "href": "day1/presentations/03_session3_python_geospatial.html#transition-to-hands-on",
    "title": "Python for Geospatial Data Analysis",
    "section": "Transition to Hands-On",
    "text": "Transition to Hands-On\n\nOpen Your Notebooks\n\n\nWe’ll start with:\nVector Data Analysis using GeoPandas\n\n\nRemember:\n\nMake a copy of the notebook\nMount your Google Drive\nRun cells in order\nAsk questions anytime"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#support-during-hands-on",
    "href": "day1/presentations/03_session3_python_geospatial.html#support-during-hands-on",
    "title": "Python for Geospatial Data Analysis",
    "section": "Support During Hands-On",
    "text": "Support During Hands-On\nInstructors Available:\n\nMain instructor demonstrating\nTeaching assistants in chat\nScreen sharing for debugging\n\nPacing:\n\nWe’ll work through together\nPause points for questions\nExtra exercises for fast finishers\n\nGoal:\nEveryone completes core exercises, understands concepts, ready for GEE"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#session-summary",
    "href": "day1/presentations/03_session3_python_geospatial.html#session-summary",
    "title": "Python for Geospatial Data Analysis",
    "section": "Session Summary",
    "text": "Session Summary\nWhat You’ve Learned:\n✅ Google Colab setup for geospatial work\n✅ GeoPandas for vector data (load, visualize, analyze)\n✅ Rasterio for raster data (read, process, visualize)\n✅ Coordinate systems and projections\n✅ Basic geospatial operations (clip, reproject, crop)\n✅ Data preparation for ML workflows\n\nTiming: 3 minutes\nYou now have foundational Python geospatial skills. Session 4 builds on this with Google Earth Engine."
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#qa",
    "href": "day1/presentations/03_session3_python_geospatial.html#qa",
    "title": "Python for Geospatial Data Analysis",
    "section": "Q&A",
    "text": "Q&A\nCommon Questions:\n\n\n\nGeoPandas vs Shapely?\nWhen to use Rasterio vs GDAL?\nCRS issues and solutions?\nMemory errors with large files?\n\n\n\nBest practices for file paths?\nNoData values handling?\nVisualization tips?\nIntegration with ML pipelines?\n\n\n\nCommon Answers: - GeoPandas uses Shapely under the hood - Rasterio = Pythonic GDAL wrapper - Always check CRS before operations - Use chunking/windowing for large rasters"
  },
  {
    "objectID": "day1/presentations/03_session3_python_geospatial.html#google-earth-engine",
    "href": "day1/presentations/03_session3_python_geospatial.html#google-earth-engine",
    "title": "Python for Geospatial Data Analysis",
    "section": "Google Earth Engine",
    "text": "Google Earth Engine\nComing up:\n\nCloud-based EO data processing\nAccess to entire Sentinel archive\nPlanetary-scale analysis\nPython API (geemap)\nCloud masking & compositing\nExport workflows\n\n\nGet ready for GEE! 🌍\n\nEveryone completes core exercises with understanding, not just copying code."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#final-session-of-day-1",
    "href": "day1/presentations/04_session4_google_earth_engine.html#final-session-of-day-1",
    "title": "Introduction to Google Earth Engine",
    "section": "Final Session of Day 1!",
    "text": "Final Session of Day 1!\n\nGoogle Earth Engine\nPlanetary-scale geospatial analysis in the cloud\n\n\nDuration: 2 hours (Hands-on with Python API)\n\n\nThis session introduces Google Earth Engine using Python exclusively (no JavaScript Code Editor). Participants will use geemap library for Python-based GEE access."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#learning-objectives",
    "href": "day1/presentations/04_session4_google_earth_engine.html#learning-objectives",
    "title": "Introduction to Google Earth Engine",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you will be able to:\n\nUnderstand what GEE is and why it’s powerful\nAuthenticate and initialize GEE Python API\nAccess Sentinel-1 and Sentinel-2 imagery\nFilter image collections (spatial, temporal, property)\nApply cloud masking to Sentinel-2\nCreate temporal composites (median, mean)\nCalculate spectral indices (NDVI, NDWI)\nVisualize results with geemap\nExport data for further analysis"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#session-roadmap",
    "href": "day1/presentations/04_session4_google_earth_engine.html#session-roadmap",
    "title": "Introduction to Google Earth Engine",
    "section": "Session Roadmap",
    "text": "Session Roadmap\n\n\n\nTime\nTopic\nDuration\n\n\n\n\n00-15 min\nGEE Overview & Authentication\n15 min\n\n\n15-55 min\nCore Concepts & Sentinel Access (HANDS-ON)\n40 min\n\n\n55-60 min\n☕ Break\n5 min\n\n\n60-110 min\nProcessing & Visualization (HANDS-ON)\n50 min\n\n\n110-120 min\nExport & Summary\n10 min\n\n\n\n\nTiming: 2 minutes\nLike Session 3, this is heavily hands-on. Most time spent coding in notebooks following instructor demonstration."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#what-is-google-earth-engine",
    "href": "day1/presentations/04_session4_google_earth_engine.html#what-is-google-earth-engine",
    "title": "Introduction to Google Earth Engine",
    "section": "What is Google Earth Engine?",
    "text": "What is Google Earth Engine?\n\n\nCloud-Based Platform for Geospatial Analysis\n\nMassive data catalog (petabytes)\nPowerful compute (Google’s infrastructure)\nFree for research & education\nNo download needed\nProcess at scale\n\n\n\n\n\n“Planetary-scale geospatial analysis”\n\n\nTiming: 3 minutes\nKey Points: - GEE hosts 40+ years of satellite imagery - Entire Landsat, Sentinel, MODIS archives - Processing happens on Google’s servers, not your laptop - Analyze entire countries in minutes - Free tier sufficient for most research/education"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#gee-architecture-and-workflow",
    "href": "day1/presentations/04_session4_google_earth_engine.html#gee-architecture-and-workflow",
    "title": "Introduction to Google Earth Engine",
    "section": "GEE Architecture and Workflow",
    "text": "GEE Architecture and Workflow\n\nGoogle Earth Engine complete architecture showing User Interface, Cloud Processing, Data Catalog, and Outputs\nThis comprehensive diagram shows GEE’s architecture: multiple user interfaces (Code Editor, Python API, Apps), the massive data catalog (70+ PB including Landsat, Sentinel, MODIS), distributed processing engine, common operations (filtering, compositing, indices, classification), and various output options (maps, exports, charts, training data)."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#why-gee-for-this-training",
    "href": "day1/presentations/04_session4_google_earth_engine.html#why-gee-for-this-training",
    "title": "Introduction to Google Earth Engine",
    "section": "Why GEE for This Training?",
    "text": "Why GEE for This Training?\nAddresses Key Challenges:\n\n❌ Traditional: Download 100s of GB of Sentinel data\n✅ GEE: Access entire archive without downloading\n❌ Traditional: Need powerful computer for processing\n✅ GEE: Google’s infrastructure does the work\n❌ Traditional: Complex cloud masking & preprocessing\n✅ GEE: Built-in algorithms & analysis-ready data\n❌ Traditional: Time-series analysis is painful\n✅ GEE: Designed for temporal analysis\n\n\nPerfect for Philippine-scale analysis!"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#gee-data-catalog",
    "href": "day1/presentations/04_session4_google_earth_engine.html#gee-data-catalog",
    "title": "Introduction to Google Earth Engine",
    "section": "GEE Data Catalog",
    "text": "GEE Data Catalog\nDatasets Available:\n\n\nSatellite Imagery:\n\nSentinel-1, 2, 3, 5P\nLandsat (entire archive!)\nMODIS\nPlanet, SkySat (some)\nMany more…\n\n\nGeophysical:\n\nClimate data\nElevation (SRTM, ASTER)\nWeather data\nPopulation datasets\nLand cover products\n\n\nBrowse: https://developers.google.com/earth-engine/datasets\n\nGEE hosts 800+ public datasets. All preprocessed and analysis-ready. Focus today on Sentinel-1 and Sentinel-2."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#python-api-vs-javascript-code-editor",
    "href": "day1/presentations/04_session4_google_earth_engine.html#python-api-vs-javascript-code-editor",
    "title": "Introduction to Google Earth Engine",
    "section": "Python API vs JavaScript Code Editor",
    "text": "Python API vs JavaScript Code Editor\n\n\nJavaScript Code Editor\n\nWeb-based IDE\nInteractive visualization\nQuick prototyping\nBuilt-in examples\n\n\nPython API (Our Focus)\n\nJupyter notebooks\nIntegration with ML libraries\nFamiliar Python ecosystem\ngeemap package for visualization\n\n\n\nToday: Python-only approach using geemap\n\n\nWhy Python for this training: - Integrates with ML workflows (scikit-learn, TensorFlow, PyTorch) - Familiar to data scientists - geemap provides all visualization capabilities - Better for reproducible research"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#geemap-package",
    "href": "day1/presentations/04_session4_google_earth_engine.html#geemap-package",
    "title": "Introduction to Google Earth Engine",
    "section": "geemap Package",
    "text": "geemap Package\n\nPython package for interactive GEE mapping\n\nBuilt on ipyleaflet\nInteractive map visualization\nLayer controls\nInspector tool\nSplit-panel comparison\nExport functionality\nMakes Python GEE as easy as Code Editor\n\n\ngeemap by Dr. Qiusheng Wu. Makes GEE accessible from Jupyter. We’ll use it extensively today."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#sign-up-for-gee",
    "href": "day1/presentations/04_session4_google_earth_engine.html#sign-up-for-gee",
    "title": "Introduction to Google Earth Engine",
    "section": "Sign Up for GEE",
    "text": "Sign Up for GEE\n\n\n\n\n\n\nBefore We Code\n\n\nYou need a Google Earth Engine account!\nSign up: https://earthengine.google.com/signup\n\n\n\nSteps:\n\nVisit signup page\nUse Gmail account\nSelect “Research/Education”\nWait for approval (usually instant)\n\n\nAlready have account? Great! Let’s authenticate.\n\n\nTiming: 2 minutes\nCheck: Ask participants if everyone has GEE access approved. If not, they can follow along and authenticate later."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#authentication-process",
    "href": "day1/presentations/04_session4_google_earth_engine.html#authentication-process",
    "title": "Introduction to Google Earth Engine",
    "section": "Authentication Process",
    "text": "Authentication Process\n83d Open Notebook: Day1_Session4_Google_Earth_Engine.ipynb\nAuthentication Code:\nimport ee\nimport geemap\n\n# Authenticate (first time only)\nee.Authenticate()\n\n# Initialize\nee.Initialize()\n\nprint(\"GEE Initialized Successfully!\")\n\nTiming: 5 minutes - participants authenticate\nSteps: 1. Run ee.Authenticate() - opens browser tab 2. Sign in with Google account 3. Allow Earth Engine access 4. Copy token back to notebook 5. Run ee.Initialize() 6. Confirm success message\nTroubleshooting: - “Authentication failed” → Check GEE account approved - “Module not found” → Install geemap: pip install geemap"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#key-gee-objects",
    "href": "day1/presentations/04_session4_google_earth_engine.html#key-gee-objects",
    "title": "Introduction to Google Earth Engine",
    "section": "Key GEE Objects",
    "text": "Key GEE Objects\n\n\nee.Image\n\nSingle raster image\nMultiple bands\nProperties (metadata)\n\nee.ImageCollection\n\nStack of images\nTime series\nFilter and reduce\n\n\nee.Geometry\n\nPoints, lines, polygons\nDefine areas of interest\n\nee.Feature / FeatureCollection\n\nVector data with attributes\nShapefiles, GeoJSON\n\n\n\nEverything is server-side! Code describes operations, execution happens on Google’s servers."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#server-side-vs-client-side",
    "href": "day1/presentations/04_session4_google_earth_engine.html#server-side-vs-client-side",
    "title": "Introduction to Google Earth Engine",
    "section": "Server-Side vs Client-Side",
    "text": "Server-Side vs Client-Side\n\n\n**Server-Side (ee.*):**\n# Runs on Google servers\nimage = ee.Image('COPERNICUS/S2/...')\nndvi = image.normalizedDifference(['B8', 'B4'])\nmean_ndvi = ndvi.reduceRegion(\n    reducer=ee.Reducer.mean(),\n    geometry=aoi,\n    scale=10\n)\nFast, scalable\n\nClient-Side (Python):\n# Runs on your computer\nresult = mean_ndvi.getInfo()\nprint(result)  # Downloads result\n\n# Visualization\nMap = geemap.Map()\nMap.addLayer(ndvi)\nMap  # Display\nFor viewing results\n\n\nKey concept: Build server-side computation, then download only final result. Never download raw petabytes!"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#filtering",
    "href": "day1/presentations/04_session4_google_earth_engine.html#filtering",
    "title": "Introduction to Google Earth Engine",
    "section": "Filtering",
    "text": "Filtering\nThree main filter types:\n1. Spatial (filterBounds):\naoi = ee.Geometry.Rectangle([120.5, 14.5, 121.0, 15.0])  # Metro Manila\nimages = collection.filterBounds(aoi)\n2. Temporal (filterDate):\nimages = collection.filterDate('2024-01-01', '2024-12-31')\n3. Property (filter):\n# Cloud cover &lt; 20%\nimages = collection.filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\nChain filters together!"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#reducers",
    "href": "day1/presentations/04_session4_google_earth_engine.html#reducers",
    "title": "Introduction to Google Earth Engine",
    "section": "Reducers",
    "text": "Reducers\nAggregate data across space or time:\n\n\nTemporal Reduction:\n# Median composite\nmedian = collection.median()\n\n# Mean\nmean = collection.mean()\n\n# Max NDVI\nmax_ndvi = collection.max()\n\nSpatial Reduction:\n# Mean value in region\nmean_val = image.reduceRegion(\n    reducer=ee.Reducer.mean(),\n    geometry=aoi,\n    scale=10\n)\n\n\nMost common: Median composite to remove clouds"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#accessing-sentinel-2",
    "href": "day1/presentations/04_session4_google_earth_engine.html#accessing-sentinel-2",
    "title": "Introduction to Google Earth Engine",
    "section": "Accessing Sentinel-2",
    "text": "Accessing Sentinel-2\n83dLive Coding Exercise 1\n# Define area of interest (Palawan)\naoi = ee.Geometry.Rectangle([118.0, 8.0, 120.5, 11.5])\n\n# Load Sentinel-2 collection\ns2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\\\n    .filterBounds(aoi) \\\\\n    .filterDate('2024-01-01', '2024-12-31') \\\\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\n# Print collection info\nprint('Number of images:', s2.size().getInfo())\n\n# Get first image\nfirst_image = s2.first()\nprint('Bands:', first_image.bandNames().getInfo())\n\nLive coding - participants follow along\nKey Teaching Points: - S2_SR_HARMONIZED is Surface Reflectance (L2A) - Always filter by cloud cover - Use filterBounds before filterDate (more efficient)"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#visualizing-sentinel-2",
    "href": "day1/presentations/04_session4_google_earth_engine.html#visualizing-sentinel-2",
    "title": "Introduction to Google Earth Engine",
    "section": "Visualizing Sentinel-2",
    "text": "Visualizing Sentinel-2\n83dLive Coding Exercise 2\n# Create map\nMap = geemap.Map(center=[9.5, 118.5], zoom=8)\n\n# Visualization parameters - True Color\nvis_params_rgb = {\n    'bands': ['B4', 'B3', 'B2'],\n    'min': 0,\n    'max': 3000,\n    'gamma': 1.4\n}\n\n# Add layer\nMap.addLayer(first_image, vis_params_rgb, 'Sentinel-2 True Color')\nMap\n\nKey points: - Create interactive map - Add Sentinel-2 layer - Zoom, pan, inspect - Explain band selection for RGB"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#false-color-composite",
    "href": "day1/presentations/04_session4_google_earth_engine.html#false-color-composite",
    "title": "Introduction to Google Earth Engine",
    "section": "False Color Composite",
    "text": "False Color Composite\n83dLive Coding Exercise 3\n# False color (vegetation = red)\nvis_params_false = {\n    'bands': ['B8', 'B4', 'B3'],  # NIR, Red, Green\n    'min': 0,\n    'max': 3000\n}\n\nMap.addLayer(first_image, vis_params_false, 'False Color')\n\nVegetation appears bright red!\n\n\nFalse color makes vegetation stand out. Useful for forest monitoring, agriculture."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#accessing-sentinel-1",
    "href": "day1/presentations/04_session4_google_earth_engine.html#accessing-sentinel-1",
    "title": "Introduction to Google Earth Engine",
    "section": "Accessing Sentinel-1",
    "text": "Accessing Sentinel-1\n83dLive Coding Exercise 4\n# Load Sentinel-1 collection\ns1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\\\n    .filterBounds(aoi) \\\\\n    .filterDate('2024-01-01', '2024-12-31') \\\\\n    .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\\\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\\\n    .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n\n# Get median composite\ns1_median = s1.select('VV').median()\n\n# Visualize\nvis_params_s1 = {'min': -25, 'max': 0}\nMap.addLayer(s1_median, vis_params_s1, 'Sentinel-1 VV')\n\nSAR visualization: Dark = smooth (water), Bright = rough (urban, forest)"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#minute-break",
    "href": "day1/presentations/04_session4_google_earth_engine.html#minute-break",
    "title": "Introduction to Google Earth Engine",
    "section": "☕ 5-Minute Break",
    "text": "☕ 5-Minute Break\n\nStretch Break\nStand up • Grab water • Back in 5 minutes\n\n\nAfter break: Cloud masking, indices, compositing, export"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#cloud-masking",
    "href": "day1/presentations/04_session4_google_earth_engine.html#cloud-masking",
    "title": "Introduction to Google Earth Engine",
    "section": "Cloud Masking",
    "text": "Cloud Masking\n83dLive Coding Exercise 5\ndef maskS2clouds(image):\n    \"\"\"Mask clouds using QA60 band\"\"\"\n    qa = image.select('QA60')\n    \n    # Bits 10 and 11 are clouds and cirrus\n    cloudBitMask = 1 &lt;&lt; 10\n    cirrusBitMask = 1 &lt;&lt; 11\n    \n    # Both flags should be zero (clear)\n    mask = qa.bitwiseAnd(cloudBitMask).eq(0) \\\\\n        .And(qa.bitwiseAnd(cirrusBitMask).eq(0))\n    \n    return image.updateMask(mask)\n\n# Apply to collection\ns2_masked = s2.map(maskS2clouds)\n\n# Create cloud-free composite\ncomposite = s2_masked.median()\n\nMap.addLayer(composite, vis_params_rgb, 'Cloud-Free Composite')\n\nKey concept: QA60 band contains cloud information. Mask clouds before compositing for best results."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#understanding-bitwise-operations",
    "href": "day1/presentations/04_session4_google_earth_engine.html#understanding-bitwise-operations",
    "title": "Introduction to Google Earth Engine",
    "section": "Understanding Bitwise Operations",
    "text": "Understanding Bitwise Operations\nHow QA60 Band Stores Cloud Information:\n\n\nQA60 value = 1024 (binary: 10000000000)\n                            ↑\n                         Bit 10 set → Cloud present\n\nBit mask operation:\ncloud_bit_mask = 1 &lt;&lt; 10  # Shift 1 left by 10 = 1024\nqa.bitwiseAnd(cloud_bit_mask)  # Extract bit 10\n\nWhy Bitwise?\n\nEfficient storage (multiple flags in one band)\nBit 10 = Opaque clouds\nBit 11 = Cirrus clouds\nCan check multiple conditions\n\n\n\nQA60 Bit Flags:\n\n\n\nBit\nFlag\n\n\n\n\n10\nOpaque clouds\n\n\n11\nCirrus clouds\n\n\n\nExample Values:\n\n0 = Clear (00000000000)\n1024 = Clouds (10000000000)\n2048 = Cirrus (100000000000)\n3072 = Both (110000000000)\n\n\n\nBitwise operations allow efficient checking of multiple flags stored in a single band. This is common in satellite QA bands."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#advanced-cloud-masking-scl-band",
    "href": "day1/presentations/04_session4_google_earth_engine.html#advanced-cloud-masking-scl-band",
    "title": "Introduction to Google Earth Engine",
    "section": "Advanced Cloud Masking: SCL Band",
    "text": "Advanced Cloud Masking: SCL Band\nScene Classification Layer (SCL) - More Detailed Classification:\ndef mask_s2_clouds_scl(image):\n    \"\"\"Advanced cloud masking using SCL band\"\"\"\n    scl = image.select('SCL')\n\n    # SCL Classification Values:\n    # 3 = Cloud shadows\n    # 4 = Vegetation\n    # 5 = Bare soil\n    # 6 = Water\n    # 8 = Cloud medium probability\n    # 9 = Cloud high probability\n    # 10 = Thin cirrus\n    # 11 = Snow/ice\n\n    # Keep only clear land/water pixels\n    mask = scl.eq(4).Or(scl.eq(5)).Or(scl.eq(6))\n\n    return image.updateMask(mask).divide(10000)\n\nSCL vs QA60: SCL provides more granular classification but requires loading additional band\n\n\nSCL band is available in Sentinel-2 L2A products. Provides detailed scene classification including shadows, vegetation, water, clouds, cirrus, and snow."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#calculating-ndvi",
    "href": "day1/presentations/04_session4_google_earth_engine.html#calculating-ndvi",
    "title": "Introduction to Google Earth Engine",
    "section": "Calculating NDVI",
    "text": "Calculating NDVI\n83dLive Coding Exercise 6\n# Calculate NDVI\nndvi = composite.normalizedDifference(['B8', 'B4']).rename('NDVI')\n\n# Visualization parameters\nndvi_vis = {\n    'min': -0.2,\n    'max': 0.8,\n    'palette': ['brown', 'yellow', 'green', 'darkgreen']\n}\n\nMap.addLayer(ndvi, ndvi_vis, 'NDVI')\n\nDark green = healthy vegetation\n\n\nNDVI = (NIR - Red) / (NIR + Red)\nRanges from -1 to +1: - Negative: Water - 0-0.2: Bare soil - 0.2-0.5: Sparse vegetation - 0.5-0.9: Dense vegetation"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#other-indices",
    "href": "day1/presentations/04_session4_google_earth_engine.html#other-indices",
    "title": "Introduction to Google Earth Engine",
    "section": "Other Indices",
    "text": "Other Indices\n83dLive Coding Exercise 7\n# NDWI (water)\nndwi = composite.normalizedDifference(['B3', 'B8']).rename('NDWI')\n\n# NDBI (built-up)\nndbi = composite.normalizedDifference(['B11', 'B8']).rename('NDBI')\n\n# Add to map\nMap.addLayer(ndwi, {'min': -0.5, 'max': 0.5, 'palette': ['white', 'blue']}, 'NDWI')\nMap.addLayer(ndbi, {'min': -0.5, 'max': 0.5, 'palette': ['green', 'gray']}, 'NDBI')\n\n\nNDWI highlights water bodies\nNDBI highlights urban areas\nMany more indices available for different applications"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#temporal-compositing",
    "href": "day1/presentations/04_session4_google_earth_engine.html#temporal-compositing",
    "title": "Introduction to Google Earth Engine",
    "section": "Temporal Compositing",
    "text": "Temporal Compositing\nCompare different time periods:\n# Dry season (Jan-Mar)\ndry = s2_masked.filterDate('2024-01-01', '2024-03-31').median()\n\n# Wet season (Jul-Sep)\nwet = s2_masked.filterDate('2024-07-01', '2024-09-30').median()\n\n# Calculate NDVI for both\nndvi_dry = dry.normalizedDifference(['B8', 'B4'])\nndvi_wet = wet.normalizedDifference(['B8', 'B4'])\n\n# Difference\nndvi_change = ndvi_wet.subtract(ndvi_dry)\n\nMap.addLayer(ndvi_change, {'min': -0.5, 'max': 0.5, \n                            'palette': ['red', 'white', 'green']}, \n             'NDVI Change')\n\nGreen = vegetation increase, Red = vegetation decrease"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#composite-methods-comparison",
    "href": "day1/presentations/04_session4_google_earth_engine.html#composite-methods-comparison",
    "title": "Introduction to Google Earth Engine",
    "section": "Composite Methods Comparison",
    "text": "Composite Methods Comparison\nDifferent ways to create composites:\n\n\n1. Median Composite\ncomposite = collection.median()\n\nMost common\nReduces outliers\nGood for cloud removal\n\n\n2. Mean Composite\ncomposite = collection.mean()\n\nAverage of all values\nSmooth results\nCan blur features\n\n\n3. Greenest Pixel\ndef add_ndvi(img):\n    ndvi = img.normalizedDifference(['B8','B4'])\n    return img.addBands(ndvi.rename('NDVI'))\n\ncomposite = collection.map(add_ndvi).qualityMosaic('NDVI')\n\nMaximum NDVI pixel\nBest vegetation condition\nIdeal for crop mapping\n\n\n\nGreenest pixel composite selects the pixel with highest NDVI at each location across the time series. Perfect for agricultural applications."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#greenest-pixel-composite-example",
    "href": "day1/presentations/04_session4_google_earth_engine.html#greenest-pixel-composite-example",
    "title": "Introduction to Google Earth Engine",
    "section": "Greenest Pixel Composite Example",
    "text": "Greenest Pixel Composite Example\nPhilippine Rice Monitoring Application:\n# Define Central Luzon rice area\nrice_aoi = ee.Geometry.Rectangle([120.5, 15.0, 121.5, 16.0])\n\n# Load Sentinel-2 for growing season\ns2_rice = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(rice_aoi)\n    .filterDate('2024-06-01', '2024-10-31')  # Main rice season\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .map(maskS2clouds))\n\n# Add NDVI band to each image\ndef add_ndvi_band(image):\n    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n    return image.addBands(ndvi)\n\ns2_with_ndvi = s2_rice.map(add_ndvi_band)\n\n# Create greenest pixel composite\ngreenest_composite = s2_with_ndvi.qualityMosaic('NDVI')\n\n# Visualize\nMap.addLayer(greenest_composite, vis_params_rgb, 'Greenest Pixel - Rice Season')\n\nResult: Captures peak rice biomass across entire growing season\n\n\nGreenest pixel composite is particularly useful for rice monitoring in the Philippines. It captures the peak vegetation condition for each pixel, showing maximum crop development."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#time-series-analysis",
    "href": "day1/presentations/04_session4_google_earth_engine.html#time-series-analysis",
    "title": "Introduction to Google Earth Engine",
    "section": "Time Series Analysis",
    "text": "Time Series Analysis\nExtract time series at a point:\n# Define point (Manila)\npoint = ee.Geometry.Point([121.0, 14.6])\n\n# Function to add date and NDVI\ndef addNDVI(image):\n    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n    return image.addBands(ndvi).set('date', image.date().format('YYYY-MM-dd'))\n\n# Add NDVI to collection\ns2_ndvi = s2_masked.map(addNDVI)\n\n# Extract time series\nts = s2_ndvi.select('NDVI').getRegion(point, 10).getInfo()\n\n# Convert to pandas DataFrame\nimport pandas as pd\ndf = pd.DataFrame(ts[1:], columns=ts[0])\nprint(df.head())\n\nTime series analysis powerful for monitoring changes over time. Can track vegetation seasonality, crop growth, etc."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#philippine-example-rice-monitoring",
    "href": "day1/presentations/04_session4_google_earth_engine.html#philippine-example-rice-monitoring",
    "title": "Introduction to Google Earth Engine",
    "section": "Philippine Example: Rice Monitoring",
    "text": "Philippine Example: Rice Monitoring\n83dLive Coding Exercise 8 - Complete Workflow\n# Rice growing area (Central Luzon)\nrice_aoi = ee.Geometry.Rectangle([120.5, 15.0, 121.0, 15.5])\n\n# One year of data\nrice_s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\\\n    .filterBounds(rice_aoi) \\\\\n    .filterDate('2024-01-01', '2024-12-31') \\\\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30)) \\\\\n    .map(maskS2clouds)\n\n# Monthly composites\ndef monthlyComposite(month):\n    start = ee.Date.fromYMD(2024, month, 1)\n    end = start.advance(1, 'month')\n    return rice_s2.filterDate(start, end).median() \\\\\n        .set('month', month)\n\n# Create 12 monthly NDVI composites\nmonths = ee.List.sequence(1, 12)\nmonthly_ndvi = ee.ImageCollection(months.map(monthlyComposite)) \\\\\n    .map(lambda img: img.normalizedDifference(['B8', 'B4']))\n\n# Visualize (add to map, create chart, etc.)\n\nComplete workflow: AOI, filtering, cloud masking, compositing, index calculation. This pattern applies to many EO applications."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#exporting-data",
    "href": "day1/presentations/04_session4_google_earth_engine.html#exporting-data",
    "title": "Introduction to Google Earth Engine",
    "section": "Exporting Data",
    "text": "Exporting Data\nExport to Google Drive:\n# Export image\ntask = ee.batch.Export.image.toDrive(\n    image=composite,\n    description='Palawan_S2_Composite',\n    folder='GEE_Exports',\n    region=aoi,\n    scale=10,\n    crs='EPSG:4326',\n    maxPixels=1e9\n)\n\n# Start task\ntask.start()\n\n# Check status\nprint('Task Status:', task.status())\n\nFind exported file in Google Drive!\n\n\nExports run in background. Can take minutes to hours depending on size. Check task manager for progress."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#export-options",
    "href": "day1/presentations/04_session4_google_earth_engine.html#export-options",
    "title": "Introduction to Google Earth Engine",
    "section": "Export Options",
    "text": "Export Options\n\n\nExport Types:\n\ntoDrive() - Google Drive\ntoAsset() - GEE Asset (reuse in GEE)\ntoCloudStorage() - Google Cloud Storage\n\nData Types:\n\nImage (raster)\nTable (vector)\nVideo (time series animation)\n\n\nBest Practices:\n\nSet appropriate scale (resolution)\nDefine region (don’t export globally!)\nUse maxPixels wisely\nCheck crs matches your needs\nMonitor tasks in Code Editor"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#integration-with-ml-workflows",
    "href": "day1/presentations/04_session4_google_earth_engine.html#integration-with-ml-workflows",
    "title": "Introduction to Google Earth Engine",
    "section": "Integration with ML Workflows",
    "text": "Integration with ML Workflows\nGEE → Python ML Pipeline:\n# 1. Process in GEE (fast, scalable)\ncomposite = s2_masked.median()\nndvi = composite.normalizedDifference(['B8', 'B4'])\n\n# 2. Sample training data\ntraining = ndvi.sampleRegions(\n    collection=training_polygons,\n    scale=10\n)\n\n# 3. Export to Drive\nee.batch.Export.table.toDrive(\n    collection=training,\n    description='training_data',\n    fileFormat='CSV'\n).start()\n\n# 4. Download and use in scikit-learn/TensorFlow (Day 2!)\n\nGEE perfect for preprocessing. Then export for ML training. We’ll do this extensively in Days 2-4."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#geemap-advanced-features",
    "href": "day1/presentations/04_session4_google_earth_engine.html#geemap-advanced-features",
    "title": "Introduction to Google Earth Engine",
    "section": "geemap Advanced Features",
    "text": "geemap Advanced Features\nSplit-panel comparison:\nleft_layer = geemap.ee_tile_layer(dry, vis_params, 'Dry Season')\nright_layer = geemap.ee_tile_layer(wet, vis_params, 'Wet Season')\n\nMap = geemap.Map()\nMap.split_map(left_layer, right_layer)\nMap\nTime slider:\nMap.add_time_slider(monthly_ndvi, vis_params, date_format='YYYY-MM')\nInteractive charting, legends, colorbars, and more!\n\ngeemap has many advanced features. Explore documentation for more."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#case-study-1-typhoon-impact-assessment",
    "href": "day1/presentations/04_session4_google_earth_engine.html#case-study-1-typhoon-impact-assessment",
    "title": "Introduction to Google Earth Engine",
    "section": "Case Study 1: Typhoon Impact Assessment",
    "text": "Case Study 1: Typhoon Impact Assessment\nScenario: Assess vegetation damage from Typhoon Odette (Rai) - December 2021\n\n\n# Define affected region (Bohol & Cebu)\nvisayas_aoi = ee.Geometry.Rectangle([123.5, 9.5, 125.0, 11.0])\n\n# Pre-typhoon (November 2021)\npre_typhoon = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(visayas_aoi)\n    .filterDate('2021-11-01', '2021-11-30')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .map(maskS2clouds)\n    .median())\n\n# Post-typhoon (January 2022)\npost_typhoon = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(visayas_aoi)\n    .filterDate('2022-01-15', '2022-02-15')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .map(maskS2clouds)\n    .median())\n\n# Calculate NDVI change\nndvi_pre = pre_typhoon.normalizedDifference(['B8', 'B4'])\nndvi_post = post_typhoon.normalizedDifference(['B8', 'B4'])\nndvi_damage = ndvi_post.subtract(ndvi_pre)\n\nMap.addLayer(ndvi_damage,\n    {'min': -0.5, 'max': 0.1, 'palette': ['red', 'yellow', 'white']},\n    'Vegetation Damage')\n\nAnalysis:\n\nRed areas = severe damage\nYellow = moderate damage\nCoastal coconut plantations heavily affected\nRapid assessment for disaster response\n\nOutput: Damage map for NDRRMC\n\n\nTyphoon Odette (international name Rai) was one of the strongest typhoons to hit the Philippines in 2021. GEE enabled rapid damage assessment."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#case-study-2-manila-bay-water-quality",
    "href": "day1/presentations/04_session4_google_earth_engine.html#case-study-2-manila-bay-water-quality",
    "title": "Introduction to Google Earth Engine",
    "section": "Case Study 2: Manila Bay Water Quality",
    "text": "Case Study 2: Manila Bay Water Quality\nScenario: Monitor turbidity and suspended sediment in Manila Bay\n# Define Manila Bay AOI\nmanila_bay = ee.Geometry.Polygon([\n    [[120.7, 14.4], [120.95, 14.4], [121.0, 14.65],\n     [120.75, 14.75], [120.7, 14.4]]\n])\n\n# Load Sentinel-2 (dry season 2024)\ns2_manila = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(manila_bay)\n    .filterDate('2024-02-01', '2024-04-30')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n    .map(maskS2clouds)\n    .median())\n\n# Calculate Turbidity Index (Red/Green ratio)\nturbidity = s2_manila.select('B4').divide(s2_manila.select('B3'))\n\nMap.addLayer(turbidity,\n    {'min': 0.5, 'max': 2.0, 'palette': ['blue', 'cyan', 'yellow', 'red']},\n    'Manila Bay Turbidity')\n\nApplication: Monitor rehabilitation progress, identify pollution sources\n\n\nManila Bay rehabilitation is a major government initiative. Satellite monitoring provides objective, regular assessment of water quality across the entire bay."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#case-study-3-rice-paddy-phenology-sentinel-1",
    "href": "day1/presentations/04_session4_google_earth_engine.html#case-study-3-rice-paddy-phenology-sentinel-1",
    "title": "Introduction to Google Earth Engine",
    "section": "Case Study 3: Rice Paddy Phenology (Sentinel-1)",
    "text": "Case Study 3: Rice Paddy Phenology (Sentinel-1)\nScenario: Track rice growth stages using SAR in Central Luzon\n# Define rice area (Nueva Ecija)\nrice_region = ee.Geometry.Rectangle([120.8, 15.3, 121.3, 15.8])\n\n# Load Sentinel-1 time series (wet season 2024)\ns1_rice = (ee.ImageCollection('COPERNICUS/S1_GRD')\n    .filterBounds(rice_region)\n    .filterDate('2024-06-01', '2024-11-30')\n    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n    .select('VH'))  # VH sensitive to rice canopy\n\n# Create time series chart\nchart = geemap.image_series_by_region(\n    s1_rice, rice_region, reducer='mean',\n    scale=100, x_property='system:time_start'\n)\nchart\n\nPhenology Pattern:\n\nLow VH = flooding/transplanting\nRising VH = vegetative growth\nPeak VH = heading/flowering\nDeclining VH = maturity/harvest\n\n\n\nSentinel-1 SAR penetrates clouds, making it ideal for monitoring rice in the rainy season. VH backscatter correlates with rice biomass and growth stage."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#case-study-4-mangrove-monitoring-in-palawan",
    "href": "day1/presentations/04_session4_google_earth_engine.html#case-study-4-mangrove-monitoring-in-palawan",
    "title": "Introduction to Google Earth Engine",
    "section": "Case Study 4: Mangrove Monitoring in Palawan",
    "text": "Case Study 4: Mangrove Monitoring in Palawan\nScenario: Map and monitor mangrove forest extent in Puerto Princesa\n# Define Palawan coastal area\npalawan_coast = ee.Geometry.Rectangle([118.7, 9.5, 119.0, 10.0])\n\n# Load recent Sentinel-2\ns2_mangrove = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(palawan_coast)\n    .filterDate('2024-01-01', '2024-12-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n    .map(maskS2clouds)\n    .median())\n\n# Mangrove index: NDVI + NDWI combination\nndvi = s2_mangrove.normalizedDifference(['B8', 'B4'])\nndwi = s2_mangrove.normalizedDifference(['B3', 'B8'])\n\n# Simple mangrove classifier\nmangrove_mask = ndvi.gt(0.3).And(ndwi.gt(-0.1))\n\nMap.addLayer(mangrove_mask.selfMask(),\n    {'palette': ['green']},\n    'Potential Mangrove Areas')\n\n# Calculate area\nmangrove_area = mangrove_mask.multiply(ee.Image.pixelArea()).reduceRegion(\n    reducer=ee.Reducer.sum(),\n    geometry=palawan_coast,\n    scale=10,\n    maxPixels=1e9\n)\n\nprint('Mangrove area (hectares):',\n      ee.Number(mangrove_area.get('nd')).divide(10000).getInfo())\n\nMangroves are critical coastal ecosystems in the Philippines. GEE enables monitoring changes over time for conservation planning."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#philippine-applications-summary",
    "href": "day1/presentations/04_session4_google_earth_engine.html#philippine-applications-summary",
    "title": "Introduction to Google Earth Engine",
    "section": "Philippine Applications Summary",
    "text": "Philippine Applications Summary\nWhat GEE Enables for Philippines:\nDisaster Response: - Flood mapping during typhoons - Damage assessment - Recovery monitoring\nAgricultural Monitoring: - Rice area mapping (PRiSM program) - Crop health assessment - Yield prediction\nEnvironmental Management: - Forest cover change - Mangrove monitoring - Water quality assessment\nUrban Planning: - Land cover mapping - Urban expansion tracking - Infrastructure development\n\nAll at national scale, updated regularly, cloud-free!\n\n\nGEE’s planetary-scale capabilities make it ideal for Philippines-wide monitoring. Free access democratizes satellite data analysis for all agencies and researchers."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#session-summary",
    "href": "day1/presentations/04_session4_google_earth_engine.html#session-summary",
    "title": "Introduction to Google Earth Engine",
    "section": "Session Summary",
    "text": "Session Summary\nWhat You’ve Learned:\n✅ GEE platform & Python API authentication ✅ Core concepts: Image, ImageCollection, filtering, reducing ✅ Accessing Sentinel-1 and Sentinel-2 data ✅ Cloud masking (QA60 bitwise operations & SCL band) ✅ Calculating spectral indices (NDVI, NDWI, NDBI) ✅ Temporal compositing (median, mean, greenest pixel) ✅ Time series analysis and multi-temporal comparison ✅ Visualization with geemap ✅ Exporting data for ML workflows ✅ Philippine case studies (typhoon, water quality, rice, mangroves)\n\nTiming: 3 minutes\nYou now have GEE skills to access and process satellite data at scale. Perfect foundation for Days 2-4 ML work."
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#qa",
    "href": "day1/presentations/04_session4_google_earth_engine.html#qa",
    "title": "Introduction to Google Earth Engine",
    "section": "Q&A",
    "text": "Q&A\nCommon Questions:\n\n\n\nGEE free tier limits?\nJavaScript vs Python trade-offs?\nHow to handle large exports?\nBest practices for efficiency?\n\n\n\nWorking with Landsat data?\nCustom algorithms in GEE?\nIntegration with QGIS?\nWhere to learn more?\n\n\n\nCommon Answers: - Free tier: 250GB cloud storage, generous compute - Python better for ML integration - Export in tiles if too large - Filter early, compute late - Landsat similar to Sentinel-2 - Custom: Use .map() with functions - QGIS: Use Earth Engine plugin"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#resources",
    "href": "day1/presentations/04_session4_google_earth_engine.html#resources",
    "title": "Introduction to Google Earth Engine",
    "section": "Resources",
    "text": "Resources\nOfficial Documentation:\nhttps://developers.google.com/earth-engine\nPython API:\nhttps://geemap.org\nTutorials:\nhttps://developers.google.com/earth-engine/tutorials\nCommunity:\nhttps://groups.google.com/forum/#!forum/google-earth-engine-developers\nAwesome GEE:\nhttps://github.com/giswqs/Awesome-GEE"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#amazing-progress-today",
    "href": "day1/presentations/04_session4_google_earth_engine.html#amazing-progress-today",
    "title": "Introduction to Google Earth Engine",
    "section": "Amazing Progress Today!",
    "text": "Amazing Progress Today!\nYou’ve mastered:\n\n✅ Copernicus & Philippine EO ecosystem\n✅ AI/ML fundamentals for EO\n✅ Python geospatial libraries (GeoPandas, Rasterio)\n✅ Google Earth Engine Python API\n\n\nTomorrow: Apply these skills to real ML problems!"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#day-2-preview",
    "href": "day1/presentations/04_session4_google_earth_engine.html#day-2-preview",
    "title": "Introduction to Google Earth Engine",
    "section": "Day 2 Preview",
    "text": "Day 2 Preview\nMachine Learning for Earth Observation\n\n\nMorning: - Random Forest classification - Training data preparation - Model evaluation - Palawan land cover mapping\n\nAfternoon: - Deep learning introduction - CNN for imagery - Transfer learning - Building damage assessment\n\n\nSee you tomorrow! 🚀"
  },
  {
    "objectID": "day1/presentations/04_session4_google_earth_engine.html#excellent-work-today",
    "href": "day1/presentations/04_session4_google_earth_engine.html#excellent-work-today",
    "title": "Introduction to Google Earth Engine",
    "section": "Excellent Work Today!",
    "text": "Excellent Work Today!\n\nRest well.\nTomorrow we build AI models!\n\n\nDay 1 Complete!\nEnsure participants: - Save their notebooks - Review exercises if needed - Come prepared for hands-on ML tomorrow\nTotal training time Day 1: ~8 hours (with breaks)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CoPhil EO AI/ML Training",
    "section": "",
    "text": "Advanced training for Philippine EO professionals on AI/ML applications for Disaster Risk Reduction, Climate Change Adaptation, and Natural Resource Management"
  },
  {
    "objectID": "index.html#about-this-training",
    "href": "index.html#about-this-training",
    "title": "CoPhil EO AI/ML Training",
    "section": "About This Training",
    "text": "About This Training\nWelcome to the 4-Day Advanced Online Training on AI/ML for Earth Observation for Philippine EO Professionals. This comprehensive training is part of the CoPhil Programme (EU-Philippines Copernicus Capacity Support Programme), a flagship initiative under the European Union’s Global Gateway strategy.\n\n\n\n\n\n\nNoteTraining Details\n\n\n\nDates: October 20-23, 2025 Instructor: Stylianos Kotsopoulos Programme: EU-Philippines CoPhil Programme\nThis course strengthens the Philippines’ capacity to use Copernicus Earth Observation data for:\n\nDisaster Risk Reduction (DRR) - Flood mapping, typhoon monitoring, landslide assessment\nClimate Change Adaptation (CCA) - Drought monitoring, agricultural resilience, coastal changes\nNatural Resource Management (NRM) - Forest monitoring, land cover mapping, marine resources"
  },
  {
    "objectID": "index.html#course-curriculum",
    "href": "index.html#course-curriculum",
    "title": "CoPhil EO AI/ML Training",
    "section": "Course Curriculum",
    "text": "Course Curriculum\nThis intensive 4-day program takes you from EO data fundamentals to deploying operational AI/ML solutions. Each day builds on previous concepts through hands-on exercises using Philippine case studies.\nFormat: 4 days × 4 sessions × 2 hours = 32 hours total | Mode: Online via Google Colab | Level: Intermediate to Advanced\n\nLearning Journey\n\n\n\n\n\ngraph LR\n    A[Day 1&lt;br/&gt;EO Data &&lt;br/&gt;Fundamentals] --&gt; B[Day 2&lt;br/&gt;Machine&lt;br/&gt;Learning]\n    B --&gt; C[Day 3&lt;br/&gt;Deep&lt;br/&gt;Learning]\n    C --&gt; D[Day 4&lt;br/&gt;Advanced&lt;br/&gt;Topics]\n\n    style A fill:#003399,stroke:#003399,stroke-width:3px,color:#fff\n    style B fill:#f8f9fa,stroke:#dee2e6,stroke-width:2px,color:#495057\n    style C fill:#f8f9fa,stroke:#dee2e6,stroke-width:2px,color:#495057\n    style D fill:#f8f9fa,stroke:#dee2e6,stroke-width:2px,color:#495057\n\n\n\n\n\n\n\n\nWhat You’ll Achieve\nBy completing this training, you will:\n\n🎯 Master EO Data - Work confidently with Sentinel-1 SAR and Sentinel-2 optical data, understanding their characteristics and Philippine EO infrastructure (SIYASAT, NAMRIA, DOST-ASTI). Navigate the Copernicus Data Space Ecosystem and implement preprocessing pipelines.\n🤖 Build AI/ML Models - Design, train, and evaluate machine learning (Random Forest, SVM) and deep learning models (CNNs, U-Net, YOLO) for land cover classification, flood mapping, and disaster monitoring. Master model validation, hyperparameter tuning, and deployment workflows.\n🚀 Deploy Solutions - Create scalable processing pipelines using cloud platforms and integrate with Philippine operational systems. Implement real-time monitoring solutions for DRR, CCA, and NRM applications.\n🌏 Apply to Real Challenges - Complete hands-on projects addressing Philippine environmental priorities using real datasets from Palawan (land cover), Central Luzon (flood mapping), Mindanao (drought monitoring), and Metro Manila (urban monitoring).\n\n\n\nDaily Breakdown\n\n\n\n\n01\n\n\n\nEO Data & AI/ML Fundamentals\n\n 4 sessions × 2 hours\n\n\n\nWhat you’ll learn: Copernicus Sentinel missions • Philippine EO ecosystem (PhilSA, DOST-ASTI, NAMRIA) • AI/ML core concepts • Python geospatial libraries (GeoPandas, Rasterio) • Google Earth Engine\nHands-on practice: Python geospatial data processing, GEE data access, visualization workflows\nStart Day 1 →\n\n\n\n\n\n02\n\n\n\nMachine Learning for EO\n\n 4 sessions × 2 hours\n\n\n\nWhat you’ll learn: Random Forest & SVM classification • K-means clustering • Feature engineering for spectral indices • CNN fundamentals • Transfer learning concepts\nHands-on practice: Palawan land cover classification, model evaluation, feature importance analysis\nStart Day 2 →\n\n\n\n\n\n03\n\n\n\nDeep Learning for EO\n\n 4 sessions × 2 hours\n\n\n\nWhat you’ll learn: U-Net semantic segmentation • Object detection (YOLO, Faster R-CNN) • SAR flood mapping (Central Luzon) • Building detection & damage assessment • Transfer learning strategies\nHands-on practice: U-Net flood mapping implementation, YOLO building detection, model fine-tuning\nStart Day 3 →\n\n\n\n\n\n04\n\n\n\nAdvanced Topics & Projects\n\n 4 sessions × 2 hours\n\n\n\nWhat you’ll learn: LSTMs for time series • Drought monitoring (Mindanao) • Foundation models (NASA-IBM Prithvi, Clay) • Self-supervised learning • Explainable AI (XAI)\nHands-on practice: Time series forecasting, foundation model fine-tuning, capstone project development\nStart Day 4 →"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "CoPhil EO AI/ML Training",
    "section": "Prerequisites",
    "text": "Prerequisites\nGet ready for the training with these simple requirements:\n\n\n\n\n\nRequired\n\n\n\nTechnical Setup:\n\nGoogle account (for Colab and Earth Engine)\nGoogle Earth Engine account (free signup)\nBasic Python knowledge (variables, loops, functions)\nModern web browser (Chrome or Firefox)\n\n\n\n\n\n\n\nTipNo Installation Needed!\n\n\n\n\n\nAll exercises run in Google Colaboratory. No local software installation required.\n\n\n\n\n\n\n\n\n\nRecommended Background\n\n\n\nHelpful Knowledge:\n\nBasic remote sensing concepts (bands, resolution, sensors)\nFamiliarity with machine learning terminology\nUnderstanding of Philippine geography and environmental challenges\nExperience with Jupyter notebooks (helpful but not required)\n\n\n\n\n\n\n\nNoteNew to EO or ML?\n\n\n\n\n\nDon’t worry! Day 1 covers all foundational concepts. We start from the basics.\n\n\n\n\n\n\n\n\nReady to get started? Follow our step-by-step setup guide to prepare your environment.\n\nComplete Setup Guide →"
  },
  {
    "objectID": "index.html#course-resources",
    "href": "index.html#course-resources",
    "title": "CoPhil EO AI/ML Training",
    "section": "Course Resources",
    "text": "Course Resources\nEverything you need for a successful learning experience:\n\n\n\n\nSetup Guide\nComplete technical setup instructions for Google Colab, Earth Engine, and required accounts.\nView Guide →\n\n\n\n\n\nPhilippine EO Resources\nDirectory of Philippine EO platforms, agencies, and data access portals.\nExplore Resources →\n\n\n\n\n\nDownload Materials\nAccess all course notebooks, datasets, presentations, and supplementary materials.\nDownload →\n\n\n\n\n\nFAQ\nCommon questions and troubleshooting tips for technical issues and course content.\nBrowse FAQ →\n\n\n\n\n\nGlossary\nComprehensive definitions of EO, AI/ML, and geospatial terminology used throughout the course.\nView Glossary →\n\n\n\n\n\nCheat Sheets\nQuick reference guides for Python, geospatial libraries, and machine learning workflows.\nGet Cheat Sheets →"
  },
  {
    "objectID": "index.html#about-cophil-programme",
    "href": "index.html#about-cophil-programme",
    "title": "CoPhil EO AI/ML Training",
    "section": "About CoPhil Programme",
    "text": "About CoPhil Programme\n\n\n\n\n\n\nTipTechnical Assistance for Philippines’ Copernicus Capacity Support\n\n\n\nThe CoPhil Programme is part of the EU-Philippines cooperation programme and the EU’s Global Gateway strategy.\nKey Partners:\n\nPhilippine Space Agency (PhilSA) - Co-chair and space data authority\nDepartment of Science and Technology (DOST) - Co-chair and technology advancement\nEuropean Union - Funding and technical cooperation\nEuropean Space Agency (ESA) - Copernicus programme expertise\n\nProgramme Objectives:\n\nEstablish Copernicus Mirror Site in the Philippines\nBuild capacity in EO data analysis and AI/ML applications\nCo-develop pilot services for DRR, CCA, and NRM\nCreate sustainable Digital Space Campus for continued learning\nFoster Philippine EO community of practice"
  },
  {
    "objectID": "index.html#course-delivery",
    "href": "index.html#course-delivery",
    "title": "CoPhil EO AI/ML Training",
    "section": "Course Delivery",
    "text": "Course Delivery\n\n\n\n\n\n\nNoteTraining Format\n\n\n\n\nLanguage: English\nCertificate: Issued upon completion of all days and capstone project\nSupport: Live instructors and teaching assistants"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "CoPhil EO AI/ML Training",
    "section": "Getting Started",
    "text": "Getting Started\n\n\n\n\n\n\nImportantReady to Begin?\n\n\n\n\nComplete Setup - Follow our Setup Guide\nReview Prerequisites - Ensure you have the required accounts\nStart Day 1 - Begin with foundational concepts\nAsk Questions - Use FAQ and instructor support throughout\n\nStart with Setup Guide → Jump to Day 1 →"
  },
  {
    "objectID": "index.html#need-help",
    "href": "index.html#need-help",
    "title": "CoPhil EO AI/ML Training",
    "section": "Need Help?",
    "text": "Need Help?\nThroughout the training, you can:\n\nAsk questions during live sessions\nConsult the FAQ for common issues\nCheck the Glossary for term definitions\nDownload Cheat Sheets for quick reference\nAccess the Philippine EO Resources directory\nContact instructors or teaching assistants"
  },
  {
    "objectID": "index.html#technical-support",
    "href": "index.html#technical-support",
    "title": "CoPhil EO AI/ML Training",
    "section": "Technical Support",
    "text": "Technical Support\n\n\n\n\n\n\nNoteGetting Help\n\n\n\nFor technical issues: - Google Colab problems: See Setup Guide - Data access issues: Check session-specific troubleshooting - Course questions: Contact instructors during sessions - Email support: info@philsa.gov.ph"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "CoPhil EO AI/ML Training",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis training is made possible through the partnership between:\n\nEuropean Union (Global Gateway Initiative)\nPhilippine Space Agency (PhilSA)\nDepartment of Science and Technology (DOST)\nEuropean Space Agency (ESA)\nCoPhil Programme Consortium\n\n\nFunded by the European Union under the Global Gateway initiative and delivered in partnership with the Philippine Space Agency (PhilSA) and the Department of Science and Technology (DOST)."
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "",
    "text": "EU-Philippines Copernicus Capacity Support Programme",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#cophil-4-day-advanced-training-on-aiml-for-earth-observation",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#cophil-4-day-advanced-training-on-aiml-for-earth-observation",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "",
    "text": "EU-Philippines Copernicus Capacity Support Programme",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#learning-objectives",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#learning-objectives",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you will be able to:\n\nSet up a Python geospatial environment in Google Colab\nLoad, inspect, and visualize vector data using GeoPandas\nLoad, inspect, and visualize raster data using Rasterio\nPerform basic geospatial operations (filtering, clipping, cropping)\nCalculate vegetation indices (NDVI, NDWI) from Sentinel-2 imagery\nCombine vector and raster data for integrated analysis\nApply these skills to Philippine EO applications (DRR, CCA, NRM)",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#why-this-session-matters",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#why-this-session-matters",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Why This Session Matters",
    "text": "Why This Session Matters\nPython geospatial skills are the foundation of ALL AI/ML workflows in Earth Observation.\nYou cannot: - Train a model without loading training data ✗ - Preprocess satellite images without raster operations ✗ - Validate results without vector boundaries ✗ - Deploy solutions without understanding data formats ✗\nThis session gives you the superpowers to: - Handle Sentinel-2 imagery like a pro ✓ - Work with Philippine administrative boundaries ✓ - Prepare analysis-ready datasets ✓ - Build production-ready EO applications ✓",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#prerequisites",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#prerequisites",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic Python knowledge (variables, loops, functions)\nGoogle account for Colab access\nCompletion of Sessions 1-2 (Copernicus overview, AI/ML concepts)",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#session-structure",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#session-structure",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Session Structure",
    "text": "Session Structure\nPart 1: Environment Setup (10 min) Part 2: Python Basics Recap (10 min) Part 3: GeoPandas for Vector Data (40 min) Part 4: Rasterio for Raster Data (50 min) Part 5: Combined Operations (30 min)\nTotal: ~2 hours with exercises",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#part-1-environment-setup",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#part-1-environment-setup",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Part 1: Environment Setup",
    "text": "Part 1: Environment Setup\n\n1.1 Mount Google Drive\nWe’ll use Google Drive to: - Access sample datasets - Save outputs and results - Share data between sessions\n\n# Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Create working directory\nimport os\nwork_dir = '/content/drive/MyDrive/CoPhil_Training'\nos.makedirs(work_dir, exist_ok=True)\nos.makedirs(f'{work_dir}/outputs', exist_ok=True)\n\nprint(f\"✓ Google Drive mounted successfully!\")\nprint(f\"✓ Working directory: {work_dir}\")\n\n\n\n1.2 Install Required Packages\nCore geospatial libraries: - geopandas - Vector data (shapefiles, GeoJSON) - rasterio - Raster data (GeoTIFF, satellite imagery) - shapely - Geometric operations - pyproj - Coordinate reference systems\nInstallation time: 1-2 minutes\n\n# Install geospatial libraries (suppress output for cleaner notebook)\n!pip install geopandas rasterio shapely pyproj matplotlib contextily -q\n\nprint(\"✓ All packages installed successfully!\")\n\n\n\n1.3 Import Libraries and Verify Installation\n\n# Core scientific libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\n\n# Geospatial libraries\nimport geopandas as gpd\nimport rasterio\nfrom rasterio.plot import show\nfrom rasterio.mask import mask\nfrom rasterio.warp import calculate_default_transform, reproject, Resampling\nfrom shapely.geometry import Point, Polygon, box\nimport warnings\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# Set visualization defaults for professional-looking plots\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 11\nplt.rcParams['axes.titlesize'] = 13\nplt.rcParams['xtick.labelsize'] = 9\nplt.rcParams['ytick.labelsize'] = 9\nplt.rcParams['legend.fontsize'] = 10\n\n# Print versions\nprint(\"✓ All libraries imported successfully!\\n\")\nprint(\"Library Versions:\")\nprint(f\"  • NumPy: {np.__version__}\")\nprint(f\"  • Pandas: {pd.__version__}\")\nprint(f\"  • GeoPandas: {gpd.__version__}\")\nprint(f\"  • Rasterio: {rasterio.__version__}\")\nprint(f\"  • Matplotlib: {plt.matplotlib.__version__}\")\nprint(\"\\n\" + \"=\"*60)",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#part-2-python-basics-quick-recap",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#part-2-python-basics-quick-recap",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Part 2: Python Basics Quick Recap",
    "text": "Part 2: Python Basics Quick Recap\nBefore diving into geospatial operations, let’s review Python fundamentals you’ll encounter throughout this notebook.\nIf you’re comfortable with Python, feel free to skim this section.\n\n2.1 Data Types and Structures\n\n# Strings - text data\nprovince_name = \"Palawan\"\nregion = \"MIMAROPA\"\n\n# Numbers - integers and floats\npopulation = 1200000  # integer\narea_km2 = 14649.73   # float (decimal)\n\n# Lists - ordered collections (can be modified)\nphilippine_islands = [\"Luzon\", \"Visayas\", \"Mindanao\"]\nband_numbers = [2, 3, 4, 8]  # Sentinel-2 bands\n\n# Dictionaries - key-value pairs\nprovince_data = {\n    \"name\": \"Palawan\",\n    \"capital\": \"Puerto Princesa\",\n    \"population\": 1200000,\n    \"area_km2\": 14649.73,\n    \"coordinates\": [118.73, 9.85]\n}\n\n# Accessing data\nprint(f\"Province: {province_name}\")\nprint(f\"First island: {philippine_islands[0]}\")\nprint(f\"Capital: {province_data['capital']}\")\nprint(f\"Population density: {population / area_km2:.1f} people/km²\")\n\n\n\n2.2 Control Structures - Loops and Conditionals\n\n# For loops - iterate over collections\nprint(\"Philippine Island Groups:\")\nfor island in philippine_islands:\n    print(f\"  • {island}\")\n\n# If-elif-else - conditional execution\nndvi_value = 0.65\n\nif ndvi_value &lt; 0:\n    vegetation_class = \"Water/Bare soil\"\nelif ndvi_value &lt; 0.2:\n    vegetation_class = \"Sparse vegetation\"\nelif ndvi_value &lt; 0.5:\n    vegetation_class = \"Moderate vegetation\"\nelse:\n    vegetation_class = \"Dense vegetation\"\n\nprint(f\"\\nNDVI = {ndvi_value} → {vegetation_class}\")\n\n# List comprehension - compact way to create lists\nband_names = [f\"Band_{b}\" for b in band_numbers]\nprint(f\"\\nBand names: {band_names}\")\n\n\n\n2.3 Functions - Reusable Code Blocks\n\ndef calculate_ndvi(nir, red):\n    \"\"\"\n    Calculate Normalized Difference Vegetation Index.\n    \n    NDVI = (NIR - Red) / (NIR + Red)\n    \n    Parameters:\n    -----------\n    nir : array-like\n        Near-infrared band values\n    red : array-like\n        Red band values\n    \n    Returns:\n    --------\n    ndvi : array-like\n        NDVI values (-1 to 1)\n    \"\"\"\n    # Convert to float to avoid integer division\n    nir = nir.astype(float)\n    red = red.astype(float)\n    \n    # Calculate NDVI, handling division by zero\n    denominator = nir + red\n    ndvi = np.where(denominator != 0, (nir - red) / denominator, 0)\n    \n    return ndvi\n\n# Test the function\nnir_test = np.array([5000, 3000, 1000])\nred_test = np.array([1500, 1200, 900])\nresult = calculate_ndvi(nir_test, red_test)\n\nprint(\"NDVI Calculation Test:\")\nfor i in range(len(result)):\n    print(f\"  NIR={nir_test[i]}, Red={red_test[i]} → NDVI={result[i]:.3f}\")",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#part-3-geopandas-for-vector-data",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#part-3-geopandas-for-vector-data",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Part 3: GeoPandas for Vector Data",
    "text": "Part 3: GeoPandas for Vector Data\nGeoPandas extends pandas for geospatial vector data (points, lines, polygons).\n\nWhy GeoPandas?\n\n✓ Read/write multiple formats (Shapefile, GeoJSON, KML, etc.)\n✓ Spatial operations (intersection, buffer, union)\n✓ Coordinate reference system (CRS) transformations\n✓ Easy visualization\n✓ Integration with pandas (filtering, grouping, etc.)\n\n\n\n3.1 Creating Sample Philippine Administrative Data\nNote: In production, you would load actual shapefiles from sources like: - NAMRIA Geoportal: https://www.geoportal.gov.ph/ - HDX Philippines: https://data.humdata.org/group/phl - PhilSA: https://philsa.gov.ph/\n\n# Create sample Philippine province polygons for demonstration\n# In practice, load from: gdf = gpd.read_file('philippines_provinces.shp')\n\nprovinces_data = [\n    # (name, region, island_group, population, minx, miny, maxx, maxy)\n    (\"Palawan\", \"MIMAROPA\", \"Luzon\", 1200000, 117.5, 8.5, 119.5, 11.5),\n    (\"Metro Manila\", \"NCR\", \"Luzon\", 13000000, 120.8, 14.4, 121.2, 14.8),\n    (\"Cebu\", \"Central Visayas\", \"Visayas\", 5200000, 123.2, 9.5, 124.0, 11.3),\n    (\"Davao del Sur\", \"Davao Region\", \"Mindanao\", 700000, 125.0, 6.3, 125.7, 7.2),\n    (\"Iloilo\", \"Western Visayas\", \"Visayas\", 2150000, 122.3, 10.4, 123.2, 11.6),\n    (\"Cagayan\", \"Cagayan Valley\", \"Luzon\", 1280000, 121.2, 17.4, 122.3, 18.6),\n]\n\n# Create geometries and build GeoDataFrame\ngeometries = []\nattributes = []\n\nfor name, region, island, pop, minx, miny, maxx, maxy in provinces_data:\n    # Create bounding box polygon\n    geom = box(minx, miny, maxx, maxy)\n    geometries.append(geom)\n    \n    # Calculate area and density\n    area = geom.area * 111 * 111  # Rough conversion to km² at Philippines latitude\n    density = pop / area\n    \n    attributes.append({\n        'Province': name,\n        'Region': region,\n        'Island_Group': island,\n        'Population': pop,\n        'Area_km2': area,\n        'Density': density\n    })\n\n# Create GeoDataFrame\nphilippines_gdf = gpd.GeoDataFrame(\n    attributes,\n    geometry=geometries,\n    crs='EPSG:4326'  # WGS84 geographic coordinates\n)\n\nprint(\"✓ Philippine provinces GeoDataFrame created!\")\nprint(f\"  Provinces: {len(philippines_gdf)}\")\nprint(f\"  CRS: {philippines_gdf.crs.name}\")\n\n\n\n3.2 Inspecting the GeoDataFrame\n\n# Display first few rows\nprint(\"First 3 provinces:\")\ndisplay(philippines_gdf.head(3))\n\n# Check data types\nprint(\"\\nColumn data types:\")\nprint(philippines_gdf.dtypes)\n\n# Summary statistics\nprint(\"\\nSummary statistics:\")\ndisplay(philippines_gdf[['Population', 'Area_km2', 'Density']].describe())\n\n\n# Coordinate Reference System (CRS) information\nprint(\"CRS Details:\")\nprint(f\"  Name: {philippines_gdf.crs.name}\")\nprint(f\"  EPSG Code: {philippines_gdf.crs.to_epsg()}\")\nprint(f\"  Units: {philippines_gdf.crs.axis_info[0].unit_name}\")\n\n# Bounds (extent)\nbounds = philippines_gdf.total_bounds\nprint(f\"\\nGeographic Extent:\")\nprint(f\"  Min Longitude: {bounds[0]:.2f}°\")\nprint(f\"  Min Latitude:  {bounds[1]:.2f}°\")\nprint(f\"  Max Longitude: {bounds[2]:.2f}°\")\nprint(f\"  Max Latitude:  {bounds[3]:.2f}°\")\n\n\n\n3.3 Filtering and Querying Vector Data\n\n# Filter by attribute: Select provinces in Mindanao\nmindanao = philippines_gdf[philippines_gdf['Island_Group'] == 'Mindanao']\nprint(\"Mindanao Provinces:\")\nprint(mindanao[['Province', 'Population', 'Area_km2']])\n\n# Filter by condition: High-density provinces\nhigh_density = philippines_gdf[philippines_gdf['Density'] &gt; 1000]\nprint(\"\\nHigh Density Provinces (&gt;1000 people/km²):\")\nprint(high_density[['Province', 'Density']].sort_values('Density', ascending=False))\n\n# Multiple conditions: Large AND populous\nmajor_provinces = philippines_gdf[\n    (philippines_gdf['Population'] &gt; 1000000) & \n    (philippines_gdf['Area_km2'] &gt; 5000)\n]\nprint(\"\\nMajor Provinces (&gt;1M pop AND &gt;5000 km²):\")\nprint(major_provinces[['Province', 'Population', 'Area_km2']])\n\n\n\n3.4 Spatial Operations\n\n# Calculate centroids\nphilippines_gdf['centroid'] = philippines_gdf.geometry.centroid\n\n# Calculate area in km² (more accurate than bounding box)\n# For accurate area, project to equal-area CRS\nphilippines_utm = philippines_gdf.to_crs('EPSG:32651')  # UTM Zone 51N\nphilippines_gdf['Area_km2_precise'] = philippines_utm.geometry.area / 1e6\n\nprint(\"Centroid coordinates:\")\nfor idx, row in philippines_gdf.iterrows():\n    print(f\"  {row['Province']:&lt;20} ({row['centroid'].x:.3f}, {row['centroid'].y:.3f})\")\n\n# Create buffer around Metro Manila (50km)\nmanila = philippines_gdf[philippines_gdf['Province'] == 'Metro Manila']\nmanila_utm = manila.to_crs('EPSG:32651')  # Project to UTM for accurate buffering\nmanila_buffer = manila_utm.buffer(50000)  # 50km buffer in meters\nmanila_buffer = manila_buffer.to_crs('EPSG:4326')  # Back to geographic\n\nprint(f\"\\n✓ Created 50km buffer around Metro Manila\")\nprint(f\"  Original area: {manila.geometry.area.values[0] * 111 * 111:.0f} km²\")\nprint(f\"  Buffer area: {manila_buffer.area.values[0] * 111 * 111:.0f} km²\")\n\n\n\n3.5 Visualizing Vector Data\n\n# Simple plot - all provinces\nfig, ax = plt.subplots(figsize=(12, 10))\n\nphilippines_gdf.plot(\n    ax=ax,\n    color='lightblue',\n    edgecolor='darkblue',\n    linewidth=1.5,\n    alpha=0.6\n)\n\n# Add province labels\nfor idx, row in philippines_gdf.iterrows():\n    ax.annotate(\n        text=row['Province'],\n        xy=(row['centroid'].x, row['centroid'].y),\n        ha='center',\n        fontsize=8,\n        fontweight='bold'\n    )\n\nax.set_title('Sample Philippine Provinces', fontsize=14, fontweight='bold', pad=20)\nax.set_xlabel('Longitude (°E)', fontsize=11)\nax.set_ylabel('Latitude (°N)', fontsize=11)\nax.grid(True, linestyle='--', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n# Choropleth map - color by population\nfig, ax = plt.subplots(figsize=(12, 10))\n\nphilippines_gdf.plot(\n    ax=ax,\n    column='Population',\n    cmap='YlOrRd',\n    edgecolor='black',\n    linewidth=1,\n    legend=True,\n    legend_kwds={\n        'label': 'Population',\n        'orientation': 'horizontal',\n        'shrink': 0.8\n    }\n)\n\nax.set_title('Philippine Provinces by Population', fontsize=14, fontweight='bold', pad=20)\nax.set_xlabel('Longitude (°E)', fontsize=11)\nax.set_ylabel('Latitude (°N)', fontsize=11)\nax.grid(True, linestyle='--', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n# Categorical map - color by island group\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Define custom colors for each island group\nisland_colors = {'Luzon': '#2ecc71', 'Visayas': '#3498db', 'Mindanao': '#e74c3c'}\nphilippines_gdf['color'] = philippines_gdf['Island_Group'].map(island_colors)\n\nphilippines_gdf.plot(\n    ax=ax,\n    color=philippines_gdf['color'],\n    edgecolor='black',\n    linewidth=1,\n    alpha=0.7\n)\n\n# Create custom legend\nlegend_elements = [\n    Patch(facecolor='#2ecc71', label='Luzon'),\n    Patch(facecolor='#3498db', label='Visayas'),\n    Patch(facecolor='#e74c3c', label='Mindanao')\n]\nax.legend(handles=legend_elements, loc='upper right', title='Island Group')\n\nax.set_title('Philippine Provinces by Island Group', fontsize=14, fontweight='bold', pad=20)\nax.set_xlabel('Longitude (°E)', fontsize=11)\nax.set_ylabel('Latitude (°N)', fontsize=11)\nax.grid(True, linestyle='--', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n📝 Exercise 1: Select and Plot Your Home Province\nTask: 1. Select a province from the GeoDataFrame 2. Calculate its population density 3. Create a focused map showing only that province 4. Add informative labels\nHint: Use boolean filtering: gdf[gdf['Province'] == 'YourProvince']\n\n# YOUR CODE HERE\n# Example solution (uncomment and modify):\n\n# my_province = philippines_gdf[philippines_gdf['Province'] == 'Palawan']\n# density = my_province['Population'].values[0] / my_province['Area_km2'].values[0]\n\n# fig, ax = plt.subplots(figsize=(10, 8))\n# my_province.plot(ax=ax, color='green', edgecolor='black', linewidth=2, alpha=0.6)\n# ax.set_title(f\"{my_province['Province'].values[0]} Province\\nDensity: {density:.1f} people/km²\",\n#              fontsize=14, fontweight='bold')\n# plt.show()\n\n\n\nClick to see solution\n\n# Select Palawan\nmy_province = philippines_gdf[philippines_gdf['Province'] == 'Palawan']\n\n# Calculate density\npop = my_province['Population'].values[0]\narea = my_province['Area_km2'].values[0]\ndensity = pop / area\n\n# Create visualization\nfig, ax = plt.subplots(figsize=(10, 8))\nmy_province.plot(\n    ax=ax,\n    color='forestgreen',\n    edgecolor='darkgreen',\n    linewidth=2,\n    alpha=0.6\n)\n\n# Add info text\ninfo_text = f\"Population: {pop:,}\\nArea: {area:.0f} km²\\nDensity: {density:.1f} people/km²\"\nax.text(0.02, 0.98, info_text,\n        transform=ax.transAxes,\n        fontsize=10,\n        verticalalignment='top',\n        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nax.set_title(f\"{my_province['Province'].values[0]} Province\",\n             fontsize=14, fontweight='bold', pad=20)\nax.set_xlabel('Longitude (°E)')\nax.set_ylabel('Latitude (°N)')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#part-4-rasterio-for-raster-data",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#part-4-rasterio-for-raster-data",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Part 4: Rasterio for Raster Data",
    "text": "Part 4: Rasterio for Raster Data\nRasterio is the go-to library for working with raster/gridded data like satellite imagery.\n\nWhy Rasterio?\n\n✓ Read/write GeoTIFF and other raster formats\n✓ NumPy integration for fast array operations\n✓ Handles multi-band imagery (Sentinel-2 has 13 bands!)\n✓ Georeferencing and coordinate transformations\n✓ Masking, clipping, resampling, reprojection\n\n\n\n4.1 Creating Synthetic Sentinel-2 Data\nFor this demo, we’ll create realistic synthetic Sentinel-2 imagery for Palawan.\nIn production, you would:\nwith rasterio.open('sentinel2_L2A_palawan.tif') as src:\n    data = src.read()\n\nfrom rasterio.transform import from_bounds\nfrom rasterio.crs import CRS\n\n# Create synthetic Sentinel-2 data for Palawan\n# Palawan approximate bounds\npalawan_bounds = (117.5, 8.5, 119.5, 11.5)  # (minx, miny, maxx, maxy)\nwidth, height = 400, 600  # Image dimensions (pixels)\n\n# Calculate affine transform (pixel coords → geographic coords)\ntransform = from_bounds(*palawan_bounds, width, height)\n\n# Create realistic synthetic bands\nnp.random.seed(42)  # For reproducibility\n\n# Sentinel-2 L2A typical reflectance values (0-10000 scale)\n# Simulate different land covers: forest, water, urban\n\n# Create spatial patterns\ny, x = np.ogrid[0:height, 0:width]\nx_norm = x / width\ny_norm = y / height\n\n# Base pattern (simulates vegetation gradient)\nvegetation_pattern = 0.5 + 0.3 * np.sin(x_norm * 4 * np.pi) * np.cos(y_norm * 3 * np.pi)\n\n# Water mask (lower left corner)\nwater_mask = ((x_norm &lt; 0.3) & (y_norm &gt; 0.7)).astype(float)\n\n# Urban pattern (scattered bright spots)\nurban_pattern = np.random.random((height, width)) &gt; 0.98\n\n# Band 2 (Blue, 490nm) - 10m resolution\nband_blue = np.random.randint(500, 1500, size=(height, width), dtype=np.uint16)\nband_blue = (band_blue * (1 - water_mask * 0.6) + water_mask * 800).astype(np.uint16)\n\n# Band 3 (Green, 560nm) - 10m resolution  \nband_green = np.random.randint(800, 2000, size=(height, width), dtype=np.uint16)\nband_green = (band_green * (1 - water_mask * 0.5) + water_mask * 1000).astype(np.uint16)\n\n# Band 4 (Red, 665nm) - 10m resolution\nband_red = np.random.randint(400, 1800, size=(height, width), dtype=np.uint16)\nband_red = (band_red * vegetation_pattern * (1 - water_mask * 0.8)).astype(np.uint16)\nband_red[urban_pattern] = np.random.randint(2000, 3000, size=np.sum(urban_pattern))\n\n# Band 8 (NIR, 842nm) - 10m resolution\n# NIR is HIGH over vegetation, LOW over water\nband_nir = np.random.randint(2500, 5500, size=(height, width), dtype=np.uint16)\nband_nir = (band_nir * vegetation_pattern * (1 - water_mask * 0.9)).astype(np.uint16)\nband_nir[water_mask &gt; 0.5] = np.random.randint(200, 600, size=np.sum(water_mask &gt; 0.5))\nband_nir[urban_pattern] = np.random.randint(1500, 2500, size=np.sum(urban_pattern))\n\nprint(\"✓ Synthetic Sentinel-2 bands created!\")\nprint(f\"  Dimensions: {width} x {height} pixels\")\nprint(f\"  Bands: Blue (B2), Green (B3), Red (B4), NIR (B8)\")\nprint(f\"  Resolution: ~5km x 5km area at 10m/pixel\")\nprint(f\"  Simulated features: Vegetation, Water, Urban areas\")\n\n\n\n4.2 Writing Raster to File\n\n# Save as GeoTIFF\nraster_path = '/tmp/palawan_sentinel2_sample.tif'\n\nwith rasterio.open(\n    raster_path,\n    'w',\n    driver='GTiff',\n    height=height,\n    width=width,\n    count=4,  # 4 bands\n    dtype=np.uint16,\n    crs=CRS.from_epsg(4326),\n    transform=transform,\n    compress='lzw'  # Compression for smaller file size\n) as dst:\n    # Write each band\n    dst.write(band_blue, 1)\n    dst.write(band_green, 2)\n    dst.write(band_red, 3)\n    dst.write(band_nir, 4)\n    \n    # Set band descriptions\n    dst.set_band_description(1, 'Blue (B2)')\n    dst.set_band_description(2, 'Green (B3)')\n    dst.set_band_description(3, 'Red (B4)')\n    dst.set_band_description(4, 'NIR (B8)')\n\nprint(f\"✓ Raster saved: {raster_path}\")\nprint(f\"  File size: {os.path.getsize(raster_path) / 1024:.1f} KB\")\n\n\n\n4.3 Opening and Inspecting Raster Metadata\n\n# Open raster file (context manager ensures proper closure)\nsrc = rasterio.open(raster_path)\n\nprint(\"=\"*60)\nprint(\"RASTER METADATA\")\nprint(\"=\"*60)\n\nprint(f\"\\nFile Information:\")\nprint(f\"  Driver: {src.driver}\")\nprint(f\"  Format: {src.driver} (GeoTIFF)\")\nprint(f\"  Compression: {src.profile.get('compress', 'None')}\")\n\nprint(f\"\\nDimensions:\")\nprint(f\"  Width: {src.width} pixels\")\nprint(f\"  Height: {src.height} pixels\")\nprint(f\"  Bands: {src.count}\")\n\nprint(f\"\\nData Type:\")\nprint(f\"  dtype: {src.dtypes[0]}\")\nprint(f\"  bits: {np.iinfo(src.dtypes[0]).bits}\")\nprint(f\"  Value range: {np.iinfo(src.dtypes[0]).min} to {np.iinfo(src.dtypes[0]).max}\")\n\nprint(f\"\\nCoordinate Reference System (CRS):\")\nprint(f\"  CRS: {src.crs.to_string()}\")\nprint(f\"  EPSG: {src.crs.to_epsg()}\")\n\nprint(f\"\\nGeographic Extent (Bounds):\")\nprint(f\"  Left (minx):   {src.bounds.left:.4f}°\")\nprint(f\"  Bottom (miny): {src.bounds.bottom:.4f}°\")\nprint(f\"  Right (maxx):  {src.bounds.right:.4f}°\")\nprint(f\"  Top (maxy):    {src.bounds.top:.4f}°\")\n\nprint(f\"\\nSpatial Resolution:\")\nres_x = (src.bounds.right - src.bounds.left) / src.width\nres_y = (src.bounds.top - src.bounds.bottom) / src.height\nprint(f\"  X resolution: {res_x:.6f}° (~{res_x * 111:.1f} km at equator)\")\nprint(f\"  Y resolution: {res_y:.6f}° (~{res_y * 111:.1f} km)\")\n\nprint(f\"\\nAffine Transform:\")\nprint(f\"{src.transform}\")\n\nprint(f\"\\nBand Descriptions:\")\nfor i in range(1, src.count + 1):\n    desc = src.descriptions[i-1] or f\"Band {i}\"\n    print(f\"  Band {i}: {desc}\")\n\nprint(\"\\n\" + \"=\"*60)\n\n\n\n4.4 Reading Raster Data as NumPy Arrays\n\n# Read individual bands\nblue = src.read(1)   # Band 1 (Blue)\ngreen = src.read(2)  # Band 2 (Green)\nred = src.read(3)    # Band 3 (Red)\nnir = src.read(4)    # Band 4 (NIR)\n\nprint(\"Band Arrays:\")\nprint(f\"  Blue:  shape={blue.shape}, dtype={blue.dtype}\")\nprint(f\"  Green: shape={green.shape}, dtype={green.dtype}\")\nprint(f\"  Red:   shape={red.shape}, dtype={red.dtype}\")\nprint(f\"  NIR:   shape={nir.shape}, dtype={nir.dtype}\")\n\n# Read all bands at once\nall_bands = src.read()  # Returns (bands, rows, cols)\nprint(f\"\\nAll bands: shape={all_bands.shape}\")\nprint(f\"  (bands, rows, columns) = ({all_bands.shape[0]}, {all_bands.shape[1]}, {all_bands.shape[2]})\")\n\n\n\n4.5 Calculating Band Statistics\n\n# Calculate statistics for each band\nbands_dict = {\n    'Blue (B2)': blue,\n    'Green (B3)': green,\n    'Red (B4)': red,\n    'NIR (B8)': nir\n}\n\nprint(\"=\"*80)\nprint(\"BAND STATISTICS (Sentinel-2 Reflectance, 0-10000 scale)\")\nprint(\"=\"*80)\nprint(f\"{'Band':&lt;15} {'Min':&gt;8} {'Max':&gt;8} {'Mean':&gt;10} {'Median':&gt;10} {'Std Dev':&gt;10}\")\nprint(\"-\"*80)\n\nfor band_name, band_data in bands_dict.items():\n    print(f\"{band_name:&lt;15} \"\n          f\"{band_data.min():&gt;8} \"\n          f\"{band_data.max():&gt;8} \"\n          f\"{band_data.mean():&gt;10.1f} \"\n          f\"{np.median(band_data):&gt;10.1f} \"\n          f\"{band_data.std():&gt;10.1f}\")\n\nprint(\"=\"*80)\n\n# Calculate percentiles\nprint(\"\\nPercentile Analysis (Red band):\")\npercentiles = [5, 25, 50, 75, 95]\nvalues = np.percentile(red, percentiles)\nfor p, v in zip(percentiles, values):\n    print(f\"  {p}th percentile: {v:.0f}\")\n\n\n\n4.6 Visualizing Single Bands\n\n# Visualize NIR band (grayscale)\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Convert to reflectance (0-1 scale)\nnir_refl = nir / 10000.0\n\nim = ax.imshow(nir_refl, cmap='gray', vmin=0, vmax=0.6)\ncbar = plt.colorbar(im, ax=ax, shrink=0.8)\ncbar.set_label('NIR Reflectance', fontsize=11)\n\nax.set_title('Sentinel-2 Near-Infrared Band (B8)', fontsize=14, fontweight='bold', pad=15)\nax.set_xlabel('Column (pixel)', fontsize=11)\nax.set_ylabel('Row (pixel)', fontsize=11)\n\n# Add explanation text\nexplanation = (\n    \"NIR (Near-Infrared):\\n\"\n    \"• Bright = High reflectance (vegetation)\\n\"\n    \"• Dark = Low reflectance (water, bare soil)\"\n)\nax.text(0.02, 0.98, explanation,\n        transform=ax.transAxes,\n        fontsize=9,\n        verticalalignment='top',\n        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n\nplt.tight_layout()\nplt.show()\n\n\n# Visualize all 4 bands in subplots\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\naxes = axes.flatten()\n\nbands_to_plot = [\n    (blue, 'Blue (B2)', 'Blues'),\n    (green, 'Green (B3)', 'Greens'),\n    (red, 'Red (B4)', 'Reds'),\n    (nir, 'NIR (B8)', 'gray')\n]\n\nfor idx, (band, title, cmap) in enumerate(bands_to_plot):\n    im = axes[idx].imshow(band / 10000.0, cmap=cmap, vmin=0, vmax=0.6)\n    axes[idx].set_title(title, fontsize=12, fontweight='bold')\n    axes[idx].set_xlabel('Column', fontsize=9)\n    axes[idx].set_ylabel('Row', fontsize=9)\n    plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n\nplt.suptitle('Sentinel-2 Multispectral Bands - Palawan', \n             fontsize=15, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.show()\n\n\n\n4.7 Creating RGB True Color Composite\n\n# Stack RGB bands (Red, Green, Blue)\nrgb = np.dstack([red, green, blue])\n\n# Convert to reflectance (0-1 scale)\nrgb_refl = rgb / 10000.0\n\n# Apply contrast stretch for better visualization\n# Method 1: Simple linear stretch (2nd to 98th percentile)\np2, p98 = np.percentile(rgb_refl, (2, 98))\nrgb_stretched = np.clip((rgb_refl - p2) / (p98 - p2), 0, 1)\n\n# Create side-by-side comparison\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n\n# Original\nax1.imshow(rgb_refl)\nax1.set_title('True Color (Original)', fontsize=12, fontweight='bold')\nax1.set_xlabel('Column')\nax1.set_ylabel('Row')\nax1.text(0.02, 0.98, 'May appear dark\\ndue to reflectance scale',\n         transform=ax1.transAxes, fontsize=9,\n         verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n\n# Contrast stretched\nax2.imshow(rgb_stretched)\nax2.set_title('True Color (2-98% Stretch)', fontsize=12, fontweight='bold')\nax2.set_xlabel('Column')\nax2.set_ylabel('Row')\nax2.text(0.02, 0.98, 'Enhanced contrast\\nfor better visualization',\n         transform=ax2.transAxes, fontsize=9,\n         verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n\nplt.suptitle('Sentinel-2 True Color Composite (RGB) - Palawan',\n             fontsize=14, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.show()\n\nprint(\"✓ True color composite created!\")\nprint(\"  This is how the area would look from space in natural color\")\n\n\n\n4.8 False Color Composites\nFalse color composites use non-visible bands to highlight specific features.\n\n# False Color Composite: NIR-Red-Green (Vegetation appears bright red)\nfalse_color_nrg = np.dstack([nir, red, green]) / 10000.0\n\n# Apply stretch\np2, p98 = np.percentile(false_color_nrg, (2, 98))\nfalse_color_nrg_stretched = np.clip((false_color_nrg - p2) / (p98 - p2), 0, 1)\n\n# Display\nfig, ax = plt.subplots(figsize=(12, 10))\n\nax.imshow(false_color_nrg_stretched)\nax.set_title('False Color Composite (NIR-R-G) - Vegetation Analysis',\n             fontsize=14, fontweight='bold', pad=15)\nax.set_xlabel('Column', fontsize=11)\nax.set_ylabel('Row', fontsize=11)\n\n# Add legend\nlegend_text = (\n    \"False Color Interpretation:\\n\"\n    \"• Bright Red = Dense vegetation\\n\"\n    \"• Pink/Light Red = Moderate vegetation\\n\"\n    \"• Dark Blue/Black = Water\\n\"\n    \"• Gray/White = Urban, bare soil\\n\\n\"\n    \"Band Assignment:\\n\"\n    \"R = NIR (B8), G = Red (B4), B = Green (B3)\"\n)\nax.text(1.02, 0.5, legend_text,\n        transform=ax.transAxes,\n        fontsize=9,\n        verticalalignment='center',\n        bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.9))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Why False Color?\")\nprint(\"  • Vegetation reflects STRONGLY in NIR (invisible to human eye)\")\nprint(\"  • By mapping NIR to Red channel, vegetation appears bright red\")\nprint(\"  • Makes vegetation identification much easier!\")\nprint(\"  • Critical for agriculture, forestry, and NRM applications\")\n\n\n\n4.9 Calculating NDVI (Normalized Difference Vegetation Index)\nNDVI is THE most important vegetation index in remote sensing.\n\\[NDVI = \\frac{NIR - Red}{NIR + Red}\\]\nInterpretation: - -1 to 0: Water, bare soil, snow - 0 to 0.2: Sparse vegetation, rock - 0.2 to 0.5: Shrubs, grassland - 0.5 to 0.8: Dense vegetation, healthy crops - 0.8 to 1: Very dense vegetation (tropical forest)\n\n# Calculate NDVI using our function from earlier\nndvi = calculate_ndvi(nir, red)\n\n# Print statistics\nprint(\"=\"*60)\nprint(\"NDVI STATISTICS\")\nprint(\"=\"*60)\nprint(f\"Minimum:   {ndvi.min():.4f}\")\nprint(f\"Maximum:   {ndvi.max():.4f}\")\nprint(f\"Mean:      {ndvi.mean():.4f}\")\nprint(f\"Median:    {np.median(ndvi):.4f}\")\nprint(f\"Std Dev:   {ndvi.std():.4f}\")\nprint(\"=\"*60)\n\n# Calculate area by vegetation class\npixel_area_km2 = (res_x * 111) * (res_y * 111)  # Approximate pixel area\n\nwater_pixels = np.sum(ndvi &lt; 0)\nsparse_pixels = np.sum((ndvi &gt;= 0) & (ndvi &lt; 0.2))\nmoderate_pixels = np.sum((ndvi &gt;= 0.2) & (ndvi &lt; 0.5))\ndense_pixels = np.sum((ndvi &gt;= 0.5) & (ndvi &lt; 0.8))\nvery_dense_pixels = np.sum(ndvi &gt;= 0.8)\n\nprint(\"\\nVegetation Cover Analysis:\")\nprint(f\"  Water/Bare (&lt;0):       {water_pixels:&gt;6} pixels ({water_pixels * pixel_area_km2:.1f} km²)\")\nprint(f\"  Sparse (0-0.2):        {sparse_pixels:&gt;6} pixels ({sparse_pixels * pixel_area_km2:.1f} km²)\")\nprint(f\"  Moderate (0.2-0.5):    {moderate_pixels:&gt;6} pixels ({moderate_pixels * pixel_area_km2:.1f} km²)\")\nprint(f\"  Dense (0.5-0.8):       {dense_pixels:&gt;6} pixels ({dense_pixels * pixel_area_km2:.1f} km²)\")\nprint(f\"  Very Dense (&gt;0.8):     {very_dense_pixels:&gt;6} pixels ({very_dense_pixels * pixel_area_km2:.1f} km²)\")\n\n# Calculate vegetation percentage\nveg_pixels = moderate_pixels + dense_pixels + very_dense_pixels\ntotal_pixels = width * height\nveg_percentage = (veg_pixels / total_pixels) * 100\n\nprint(f\"\\n✓ Overall Vegetation Coverage: {veg_percentage:.1f}%\")\n\n\n# Visualize NDVI\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Use diverging colormap (red-yellow-green)\nim = ax.imshow(ndvi, cmap='RdYlGn', vmin=-0.2, vmax=0.9)\ncbar = plt.colorbar(im, ax=ax, shrink=0.8, extend='both')\ncbar.set_label('NDVI', fontsize=12, fontweight='bold')\n\n# Add horizontal lines for class boundaries\ncbar.ax.axhline(y=0, color='blue', linewidth=2, linestyle='--', alpha=0.7)\ncbar.ax.axhline(y=0.2, color='orange', linewidth=1.5, linestyle='--', alpha=0.7)\ncbar.ax.axhline(y=0.5, color='yellow', linewidth=1.5, linestyle='--', alpha=0.7)\ncbar.ax.axhline(y=0.8, color='darkgreen', linewidth=1.5, linestyle='--', alpha=0.7)\n\nax.set_title('NDVI - Normalized Difference Vegetation Index',\n             fontsize=14, fontweight='bold', pad=15)\nax.set_xlabel('Column (pixel)', fontsize=11)\nax.set_ylabel('Row (pixel)', fontsize=11)\n\n# Add interpretation legend\nlegend_text = (\n    \"NDVI Interpretation:\\n\\n\"\n    \"&lt; 0 (Red/Brown)\\n\"\n    \"  Water, bare soil\\n\\n\"\n    \"0 - 0.2 (Orange/Yellow)\\n\"\n    \"  Sparse vegetation\\n\\n\"\n    \"0.2 - 0.5 (Light Green)\\n\"\n    \"  Moderate vegetation\\n\\n\"\n    \"0.5 - 0.8 (Green)\\n\"\n    \"  Dense vegetation\\n\\n\"\n\"&gt; 0.8 (Dark Green)\\n\"\n    \"  Very dense vegetation\"\n)\nax.text(1.15, 0.5, legend_text,\n        transform=ax.transAxes,\n        fontsize=9,\n        verticalalignment='center',\n        bbox=dict(boxstyle='round', facecolor='white', alpha=0.9),\n        family='monospace')\n\nplt.tight_layout()\nplt.show()\n\n\n\n4.10 NDVI Histogram and Distribution Analysis\n\n# Create comprehensive histogram\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n# Histogram\nax1.hist(ndvi.flatten(), bins=100, color='green', alpha=0.7, edgecolor='darkgreen')\nax1.axvline(ndvi.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {ndvi.mean():.3f}')\nax1.axvline(np.median(ndvi), color='blue', linestyle='--', linewidth=2, label=f'Median: {np.median(ndvi):.3f}')\n\n# Add class boundary lines\nax1.axvline(0, color='black', linestyle=':', linewidth=1.5, alpha=0.5)\nax1.axvline(0.2, color='orange', linestyle=':', linewidth=1.5, alpha=0.5)\nax1.axvline(0.5, color='yellow', linestyle=':', linewidth=1.5, alpha=0.5)\nax1.axvline(0.8, color='darkgreen', linestyle=':', linewidth=1.5, alpha=0.5)\n\nax1.set_xlabel('NDVI Value', fontsize=11, fontweight='bold')\nax1.set_ylabel('Frequency (pixel count)', fontsize=11, fontweight='bold')\nax1.set_title('NDVI Distribution', fontsize=13, fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Box plot\nbox_data = [ndvi[ndvi &lt; 0].flatten(),\n            ndvi[(ndvi &gt;= 0) & (ndvi &lt; 0.2)].flatten(),\n            ndvi[(ndvi &gt;= 0.2) & (ndvi &lt; 0.5)].flatten(),\n            ndvi[(ndvi &gt;= 0.5) & (ndvi &lt; 0.8)].flatten(),\n            ndvi[ndvi &gt;= 0.8].flatten()]\n\nbp = ax2.boxplot(box_data, \n                 labels=['Water\\n(&lt;0)', 'Sparse\\n(0-0.2)', 'Moderate\\n(0.2-0.5)', \n                        'Dense\\n(0.5-0.8)', 'Very Dense\\n(&gt;0.8)'],\n                 patch_artist=True)\n\n# Color boxes\ncolors = ['brown', 'orange', 'yellow', 'green', 'darkgreen']\nfor patch, color in zip(bp['boxes'], colors):\n    patch.set_facecolor(color)\n    patch.set_alpha(0.6)\n\nax2.set_ylabel('NDVI Value', fontsize=11, fontweight='bold')\nax2.set_title('NDVI by Vegetation Class', fontsize=13, fontweight='bold')\nax2.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\n\n\n📝 Exercise 2: Calculate and Visualize NDWI (Water Index)\nNDWI (Normalized Difference Water Index) is used to detect water bodies.\n\\[NDWI = \\frac{Green - NIR}{Green + NIR}\\]\nTask: 1. Write a function to calculate NDWI 2. Calculate NDWI from the Green and NIR bands 3. Create a visualization showing water bodies 4. Calculate statistics (min, max, mean)\nHints: - NDWI &gt; 0.3: Water - NDWI 0 to 0.3: Wetlands/moist soil - NDWI &lt; 0: Dry land/vegetation\n\n# YOUR CODE HERE\n# Step 1: Write NDWI function\n\n# def calculate_ndwi(green, nir):\n#     \"\"\"\n#     Calculate Normalized Difference Water Index.\n#     NDWI = (Green - NIR) / (Green + NIR)\n#     \"\"\"\n#     # Your code here\n#     pass\n\n# Step 2: Calculate NDWI\n# ndwi = calculate_ndwi(green, nir)\n\n# Step 3: Visualize\n# fig, ax = plt.subplots(figsize=(12, 10))\n# im = ax.imshow(ndwi, cmap='Blues', vmin=-0.5, vmax=0.5)\n# # Add colorbar, title, labels\n# plt.show()\n\n# Step 4: Calculate statistics\n# print(f\"NDWI Statistics:\")\n# print(f\"  Min: {ndwi.min():.3f}\")\n# # ... etc\n\n\n\nClick to see solution\n\ndef calculate_ndwi(green, nir):\n    \"\"\"\n    Calculate Normalized Difference Water Index.\n    NDWI = (Green - NIR) / (Green + NIR)\n    \"\"\"\n    green = green.astype(float)\n    nir = nir.astype(float)\n    \n    denominator = green + nir\n    ndwi = np.where(denominator != 0, (green - nir) / denominator, 0)\n    \n    return ndwi\n\n# Calculate NDWI\nndwi = calculate_ndwi(green, nir)\n\n# Statistics\nprint(\"NDWI Statistics:\")\nprint(f\"  Min:    {ndwi.min():.3f}\")\nprint(f\"  Max:    {ndwi.max():.3f}\")\nprint(f\"  Mean:   {ndwi.mean():.3f}\")\nprint(f\"  Median: {np.median(ndwi):.3f}\")\n\n# Water area calculation\nwater_pixels = np.sum(ndwi &gt; 0.3)\nwater_area_km2 = water_pixels * pixel_area_km2\nprint(f\"\\nWater bodies (NDWI &gt; 0.3): {water_area_km2:.1f} km²\")\n\n# Visualization\nfig, ax = plt.subplots(figsize=(12, 10))\n\nim = ax.imshow(ndwi, cmap='Blues', vmin=-0.5, vmax=0.5)\ncbar = plt.colorbar(im, ax=ax, shrink=0.8)\ncbar.set_label('NDWI', fontsize=12, fontweight='bold')\n\nax.set_title('NDWI - Normalized Difference Water Index',\n             fontsize=14, fontweight='bold', pad=15)\nax.set_xlabel('Column (pixel)', fontsize=11)\nax.set_ylabel('Row (pixel)', fontsize=11)\n\n# Add legend\nlegend_text = (\n    \"NDWI Interpretation:\\n\\n\"\n    \"&gt; 0.3 (Dark Blue)\\n\"\n    \"  Water bodies\\n\\n\"\n    \"0 to 0.3 (Light Blue)\\n\"\n    \"  Wetlands, moist soil\\n\\n\"\n    \"&lt; 0 (White/Gray)\\n\"\n    \"  Dry land, vegetation\"\n)\nax.text(1.12, 0.5, legend_text,\n        transform=ax.transAxes,\n        fontsize=9,\n        verticalalignment='center',\n        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.9),\n        family='monospace')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#part-5-combined-operations---vector-and-raster-integration",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#part-5-combined-operations---vector-and-raster-integration",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Part 5: Combined Operations - Vector and Raster Integration",
    "text": "Part 5: Combined Operations - Vector and Raster Integration\nThe real power of geospatial analysis comes from combining vector and raster data.\nCommon workflows: - Clip raster to administrative boundaries - Extract statistics per province/region - Overlay boundaries on satellite imagery - Sample raster values at point locations\n\n5.1 Clipping Raster to Vector Boundary\n\nfrom rasterio.mask import mask as rasterio_mask\n\n# Select Palawan province\npalawan_gdf = philippines_gdf[philippines_gdf['Province'] == 'Palawan']\n\n# Get geometry in format rasterio expects (GeoJSON-like)\npalawan_geom = [palawan_gdf.geometry.values[0].__geo_interface__]\n\n# Open raster and clip\nwith rasterio.open(raster_path) as src:\n    # Clip raster to Palawan boundary\n    out_image, out_transform = rasterio_mask(src, palawan_geom, crop=True, filled=True)\n    out_meta = src.meta.copy()\n\n# Update metadata\nout_meta.update({\n    \"height\": out_image.shape[1],\n    \"width\": out_image.shape[2],\n    \"transform\": out_transform\n})\n\nprint(\"✓ Raster clipped to Palawan boundary!\")\nprint(f\"  Original size: {height} x {width} pixels\")\nprint(f\"  Clipped size:  {out_image.shape[1]} x {out_image.shape[2]} pixels\")\nprint(f\"  Reduction:     {(1 - (out_image.shape[1] * out_image.shape[2]) / (height * width)) * 100:.1f}%\")\n\n# Extract clipped bands\nclipped_red = out_image[2, :, :]\nclipped_nir = out_image[3, :, :]\n\n# Calculate NDVI for clipped area\nclipped_ndvi = calculate_ndvi(clipped_nir, clipped_red)\n\nprint(f\"\\nClipped NDVI statistics:\")\nprint(f\"  Mean: {clipped_ndvi.mean():.3f}\")\nprint(f\"  Min:  {clipped_ndvi.min():.3f}\")\nprint(f\"  Max:  {clipped_ndvi.max():.3f}\")\n\n\n# Visualize clipped NDVI\nfig, ax = plt.subplots(figsize=(12, 10))\n\nim = ax.imshow(clipped_ndvi, cmap='RdYlGn', vmin=-0.2, vmax=0.9)\ncbar = plt.colorbar(im, ax=ax, shrink=0.8)\ncbar.set_label('NDVI', fontsize=12)\n\nax.set_title('NDVI - Palawan Province Only (Clipped)',\n             fontsize=14, fontweight='bold', pad=15)\nax.set_xlabel('Column', fontsize=11)\nax.set_ylabel('Row', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n\n\n\n5.2 Overlay Vector Boundaries on Raster\n\n# Create combined visualization\nfig, ax = plt.subplots(figsize=(14, 12))\n\n# Display NDVI as background\nextent = [src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top]\nim = ax.imshow(ndvi, cmap='RdYlGn', vmin=-0.2, vmax=0.9,\n               extent=extent, origin='upper')\n\n# Overlay province boundaries\nphilippines_gdf.boundary.plot(ax=ax, edgecolor='blue', linewidth=2, label='Province Boundaries')\n\n# Highlight Palawan\npalawan_gdf.boundary.plot(ax=ax, edgecolor='red', linewidth=3, label='Palawan (highlighted)')\n\n# Add colorbar\ncbar = plt.colorbar(im, ax=ax, shrink=0.7, pad=0.02)\ncbar.set_label('NDVI', fontsize=12)\n\nax.set_xlabel('Longitude (°E)', fontsize=11)\nax.set_ylabel('Latitude (°N)', fontsize=11)\nax.set_title('NDVI with Province Boundaries Overlay',\n             fontsize=14, fontweight='bold', pad=20)\nax.legend(loc='upper right', fontsize=10)\nax.grid(True, alpha=0.3, linestyle='--')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"✓ Combined vector-raster visualization created!\")\nprint(\"  This demonstrates spatial integration of different data types\")\n\n\n\n5.3 Zonal Statistics - Calculate Mean NDVI per Province\n\nfrom rasterio.features import rasterize\nfrom rasterio.transform import rowcol\n\n# Simple approach: Sample NDVI at province centroids\n# For full zonal statistics, use rasterstats library (not installed by default)\n\ndef sample_raster_at_point(lon, lat, raster_array, transform):\n    \"\"\"\n    Sample raster value at given coordinates.\n    \"\"\"\n    from rasterio.transform import rowcol\n    \n    # Convert geographic to pixel coordinates\n    row, col = rowcol(transform, lon, lat)\n    \n    # Check bounds\n    if 0 &lt;= row &lt; raster_array.shape[0] and 0 &lt;= col &lt; raster_array.shape[1]:\n        return raster_array[row, col]\n    else:\n        return np.nan\n\n# Sample NDVI at each province centroid\nndvi_values = []\nfor idx, row in philippines_gdf.iterrows():\n    centroid = row['centroid']\n    ndvi_val = sample_raster_at_point(centroid.x, centroid.y, ndvi, src.transform)\n    ndvi_values.append(ndvi_val)\n\nphilippines_gdf['NDVI_Centroid'] = ndvi_values\n\nprint(\"=\"*60)\nprint(\"MEAN NDVI BY PROVINCE (sampled at centroids)\")\nprint(\"=\"*60)\nprint(f\"{'Province':&lt;20} {'NDVI':&gt;10} {'Vegetation Class':&gt;20}\")\nprint(\"-\"*60)\n\nfor idx, row in philippines_gdf.iterrows():\n    ndvi_val = row['NDVI_Centroid']\n    if np.isnan(ndvi_val):\n        veg_class = \"Outside raster\"\n    elif ndvi_val &lt; 0:\n        veg_class = \"Water/Bare\"\n    elif ndvi_val &lt; 0.2:\n        veg_class = \"Sparse\"\n    elif ndvi_val &lt; 0.5:\n        veg_class = \"Moderate\"\n    elif ndvi_val &lt; 0.8:\n        veg_class = \"Dense\"\n    else:\n        veg_class = \"Very Dense\"\n    \n    print(f\"{row['Province']:&lt;20} {ndvi_val:&gt;10.3f} {veg_class:&gt;20}\")\n\nprint(\"=\"*60)\nprint(\"\\nNote: For accurate zonal statistics, use rasterstats library\")\nprint(\"      This provides full polygon statistics (mean, median, min, max)\")\n\n\n\n5.4 Saving Results\n\n# Save NDVI as GeoTIFF\nndvi_path = f'{work_dir}/outputs/palawan_ndvi.tif'\n\n# Copy metadata from source\nndvi_meta = src.meta.copy()\nndvi_meta.update({\n    'count': 1,\n    'dtype': 'float32',\n    'nodata': -9999\n})\n\nwith rasterio.open(ndvi_path, 'w', **ndvi_meta) as dst:\n    dst.write(ndvi.astype('float32'), 1)\n    dst.set_band_description(1, 'NDVI')\n\nprint(f\"✓ NDVI saved: {ndvi_path}\")\n\n# Save updated GeoDataFrame with NDVI values\nvector_path = f'{work_dir}/outputs/provinces_with_ndvi.geojson'\n\n# Create a copy for saving (to avoid modifying original)\ngdf_to_save = philippines_gdf.copy()\n\n# Convert centroid geometry column to string (WKT format) to avoid conflicts\nif 'centroid' in gdf_to_save.columns:\n    gdf_to_save['centroid_lon'] = gdf_to_save['centroid'].x\n    gdf_to_save['centroid_lat'] = gdf_to_save['centroid'].y\n    gdf_to_save = gdf_to_save.drop(columns=['centroid'])\n\n# Save to GeoJSON\ngdf_to_save.to_file(vector_path, driver='GeoJSON')\n\nprint(f\"✓ Vector data saved: {vector_path}\")\nprint(f\"  Attributes saved: Province, Region, Island_Group, Population, Area, Density, NDVI\")\nprint(f\"  Centroid coordinates saved as: centroid_lon, centroid_lat\")\nprint(f\"\\nAll outputs saved to: {work_dir}/outputs/\")",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#part-6-best-practices-and-common-pitfalls",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#part-6-best-practices-and-common-pitfalls",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Part 6: Best Practices and Common Pitfalls",
    "text": "Part 6: Best Practices and Common Pitfalls\n\n6.1 Memory Management\n\nprint(\"BEST PRACTICES FOR MEMORY MANAGEMENT:\\n\")\n\nprint(\"1. ALWAYS use context managers (with statements):\")\nprint(\"   ✓ with rasterio.open('file.tif') as src:\")\nprint(\"       data = src.read()\")\nprint(\"   ✗ src = rasterio.open('file.tif')  # Don't forget to close!\\n\")\n\nprint(\"2. Read only what you need:\")\nprint(\"   ✓ band = src.read(1)  # Single band\")\nprint(\"   ✗ all_bands = src.read()  # All bands (if you only need one)\\n\")\n\nprint(\"3. Use windowed reading for large files:\")\nprint(\"   from rasterio.windows import Window\")\nprint(\"   window = Window(0, 0, 1000, 1000)  # 1000x1000 subset\")\nprint(\"   data = src.read(1, window=window)\\n\")\n\nprint(\"4. Process in chunks for very large datasets:\")\nprint(\"   for ji, window in src.block_windows(1):\")\nprint(\"       data = src.read(1, window=window)\")\nprint(\"       # Process chunk\")\nprint(\"       # Write result\\n\")\n\nprint(\"5. Delete large arrays when done:\")\nprint(\"   del large_array\")\nprint(\"   import gc; gc.collect()  # Force garbage collection\")\n\n\n\n6.2 CRS Alignment - CRITICAL!\n\nprint(\"CRS (Coordinate Reference System) ALIGNMENT:\\n\")\n\nprint(\"ALWAYS check CRS before combining data!\\n\")\n\n# Example: Check and align CRS\nprint(\"Step 1: Check CRS\")\nprint(f\"  Vector CRS: {philippines_gdf.crs}\")\nprint(f\"  Raster CRS: {src.crs}\")\n\nprint(\"\\nStep 2: Reproject if needed\")\nprint(\"  if vector.crs != raster.crs:\")\nprint(\"      vector = vector.to_crs(raster.crs)\")\nprint(\"      print('Vector reprojected!')\\n\")\n\nprint(\"COMMON CRS IN PHILIPPINES:\")\nprint(\"  EPSG:4326  - WGS84 Geographic (lat/lon in degrees)\")\nprint(\"  EPSG:32651 - WGS84 / UTM Zone 51N (meters, for Luzon/Visayas)\")\nprint(\"  EPSG:32652 - WGS84 / UTM Zone 52N (meters, for Mindanao)\")\nprint(\"  EPSG:3123  - PRS92 / Philippines Zone I\")\nprint(\"  EPSG:3124  - PRS92 / Philippines Zone II\")\nprint(\"  EPSG:3125  - PRS92 / Philippines Zone III\\n\")\n\nprint(\"PRO TIP: Use UTM for accurate area/distance calculations!\")\n\n\n\n6.3 Handling NoData Values\n\nprint(\"HANDLING NODATA VALUES:\\n\")\n\n# Check for nodata value\nprint(f\"Current raster nodata value: {src.nodata}\")\n\nprint(\"\\nMethod 1: Read with masked=True\")\nprint(\"  data = src.read(1, masked=True)  # Returns np.ma.MaskedArray\")\nprint(\"  valid_mean = data.mean()  # Automatically ignores nodata\")\n\nprint(\"\\nMethod 2: Manual masking\")\nprint(\"  data = src.read(1)\")\nprint(\"  if src.nodata is not None:\")\nprint(\"      valid_data = data[data != src.nodata]\")\nprint(\"      valid_mean = valid_data.mean()\")\n\nprint(\"\\nMethod 3: NumPy masked arrays\")\nprint(\"  import numpy.ma as ma\")\nprint(\"  masked_data = ma.masked_equal(data, src.nodata)\")\nprint(\"  valid_mean = masked_data.mean()\")\n\nprint(\"\\nWHY IT MATTERS:\")\nprint(\"  NoData pixels can skew statistics if not handled!\")\nprint(\"  Example: mean() of [100, 100, -9999] = -3266 (WRONG!)\")\nprint(\"           mean() excluding nodata = 100 (CORRECT!)\")\n\n\n\n6.4 Common Errors and Solutions\n\nprint(\"COMMON ERRORS AND SOLUTIONS:\\n\")\nprint(\"=\"*70)\n\nprint(\"\\n1. 'ValueError: cannot set EPSG:4326 CRS'\")\nprint(\"   CAUSE: CRS already set or incompatible\")\nprint(\"   FIX: gdf.set_crs('EPSG:4326', allow_override=True)\\n\")\n\nprint(\"2. 'IndexError: index 1 is out of bounds'\")\nprint(\"   CAUSE: Trying to read band that doesn't exist\")\nprint(\"   FIX: Check src.count before reading\")\nprint(\"        bands = src.read([1, 2, 3])  # Read multiple\\n\")\n\nprint(\"3. 'TypeError: integer argument expected, got float'\")\nprint(\"   CAUSE: Pixel coordinates must be integers\")\nprint(\"   FIX: row, col = int(row), int(col)\\n\")\n\nprint(\"4. 'MemoryError: Unable to allocate array'\")\nprint(\"   CAUSE: Trying to load massive raster into memory\")\nprint(\"   FIX: Use windowed reading or downsample\")\nprint(\"        data = src.read(1, out_shape=(500, 500))\\n\")\n\nprint(\"5. 'RuntimeWarning: invalid value encountered in divide'\")\nprint(\"   CAUSE: Division by zero in NDVI/NDWI calculation\")\nprint(\"   FIX: Use np.where() to handle zero denominators\")\nprint(\"        ndvi = np.where(denom != 0, (nir-red)/denom, 0)\\n\")\n\nprint(\"6. 'GeoDataFrame.to_file() slow for large datasets'\")\nprint(\"   CAUSE: Shapefile format is slow\")\nprint(\"   FIX: Use GeoPackage or GeoJSON\")\nprint(\"        gdf.to_file('data.gpkg', driver='GPKG')  # Faster!\\n\")\n\nprint(\"=\"*70)",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#summary-and-key-takeaways",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#summary-and-key-takeaways",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nWhat You’ve Learned Today:\n\n1. GeoPandas for Vector Data\n✓ Loading and inspecting shapefiles/GeoJSON\n✓ Filtering by attributes and spatial queries\n✓ CRS transformations and projections\n✓ Creating professional maps and visualizations\n✓ Spatial operations (buffer, intersection, union)\n\n\n2. Rasterio for Raster Data\n✓ Reading multi-band satellite imagery\n✓ Extracting metadata and band information\n✓ Processing bands as NumPy arrays\n✓ Calculating statistics and percentiles\n✓ Creating RGB and false color composites\n\n\n3. Vegetation Indices\n✓ NDVI calculation and interpretation\n✓ NDWI for water body detection\n✓ Histogram analysis and thresholding\n✓ Land cover classification based on indices\n\n\n4. Integrated Workflows\n✓ Clipping rasters to vector boundaries\n✓ Overlaying vectors on rasters\n✓ Zonal statistics (per-province analysis)\n✓ Saving results in multiple formats\n\n\n5. Best Practices\n✓ Memory management techniques\n✓ CRS alignment (CRITICAL!)\n✓ NoData value handling\n✓ Error prevention and debugging\n\n\n\n\nWhy This Matters for AI/ML\nThese skills are ESSENTIAL for:\n\nData Preparation\n\nLoading training data (labeled polygons)\nPreprocessing satellite imagery\nCreating feature layers for models\n\nFeature Engineering\n\nCalculating spectral indices (NDVI, NDWI, etc.)\nExtracting texture features\nCreating multi-temporal composites\n\nModel Training\n\nSampling training pixels\nCreating validation datasets\nBalancing class distributions\n\nResult Analysis\n\nVisualizing model predictions\nCalculating accuracy metrics\nValidating against ground truth\n\nDeployment\n\nProcessing new satellite scenes\nGenerating operational products\nCreating decision support maps\n\n\n\n\n\nPhilippine EO Applications\nYou can now build applications for:\nDisaster Risk Reduction (DRR): - Flood extent mapping using NDWI - Landslide susceptibility analysis - Typhoon damage assessment\nClimate Change Adaptation (CCA): - Vegetation health monitoring (NDVI) - Drought impact assessment - Coastal erosion detection\nNatural Resource Management (NRM): - Forest cover monitoring - Agricultural land mapping - Marine protected area monitoring",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#next-session-google-earth-engine-python-api",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#next-session-google-earth-engine-python-api",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Next Session: Google Earth Engine Python API",
    "text": "Next Session: Google Earth Engine Python API\nSession 4 will cover: - Accessing petabytes of satellite data in the cloud - Processing Sentinel-1 and Sentinel-2 at scale - Cloud masking and temporal compositing - Exporting data for ML workflows - Integrating GEE with local Python analysis\nPreview:\nimport ee\nee.Initialize()\n\n# Access entire Sentinel-2 archive\ns2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(palawan) \\\n    .filterDate('2024-01-01', '2024-12-31') \\\n    .map(mask_clouds)\n\n# Create cloud-free composite\ncomposite = s2.median()\n\n# Calculate NDVI at planetary scale!\nndvi = composite.normalizedDifference(['B8', 'B4'])",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#additional-resources",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#additional-resources",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nDocumentation\n\nGeoPandas: https://geopandas.org/\nRasterio: https://rasterio.readthedocs.io/\nNumPy: https://numpy.org/doc/\nMatplotlib: https://matplotlib.org/\n\n\n\nTutorials\n\nCarpentries Geospatial Python: https://carpentries-incubator.github.io/geospatial-python/\nEarth Data Science: https://www.earthdatascience.org/\nPython for Geospatial Analysis: https://www.tomasbeuzen.com/python-for-geospatial-analysis/\n\n\n\nPhilippine Data Sources\n\nPhilSA: https://philsa.gov.ph/\nNAMRIA Geoportal: https://www.geoportal.gov.ph/\nDOST-ASTI DATOS: https://asti.dost.gov.ph/\nHDX Philippines: https://data.humdata.org/group/phl\nHazardHunterPH: https://hazardhunter.georisk.gov.ph/\n\n\n\nBooks\n\nGeoprocessing with Python (Garrard)\nLearning Geospatial Analysis with Python (Lawhead)\nPython for Data Analysis (McKinney)",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#practice-exercises-optional-homework",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#practice-exercises-optional-homework",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Practice Exercises (Optional Homework)",
    "text": "Practice Exercises (Optional Homework)\nTo reinforce your learning:\n\nExercise A: Multi-Province Analysis\nCalculate and compare NDVI statistics for all provinces in one island group.\n\n\nExercise B: Time-Series Simulation\nCreate multiple synthetic images representing different seasons and analyze NDVI changes.\n\n\nExercise C: Custom Index\nResearch and implement another vegetation index (EVI, SAVI, or MSAVI).\n\n\nExercise D: Real Data\nDownload actual Sentinel-2 data from Copernicus Data Space and apply these techniques.\n\n\nExercise E: Water Detection\nUse NDWI to create a binary water mask and calculate total water area.",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#clean-up",
    "href": "day1/notebooks/Day1_Session3_Python_Geospatial_Data.html#clean-up",
    "title": "Day 1, Session 3: Python for Geospatial Data",
    "section": "Clean Up",
    "text": "Clean Up\n\n# Close raster file\nsrc.close()\n\n# Clean up temporary files (optional)\nimport os\ntemp_files = [raster_path]\n\nfor f in temp_files:\n    if os.path.exists(f):\n        os.remove(f)\n        print(f\"Removed: {f}\")\n\nprint(\"\\n✓ Cleanup complete!\")\nprint(f\"\\nYour outputs are saved in: {work_dir}/outputs/\")",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 3: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html",
    "href": "day1/notebooks/notebook2.html",
    "title": "Notebook 2: Google Earth Engine",
    "section": "",
    "text": "This session includes TWO complementary notebooks designed for progressive learning:\n\n\nComplete introduction to Google Earth Engine with standard cloud masking (QA60)\n\n\n\nAdvanced techniques using SCL and s2cloudless for production-quality results\n\n\n\nBy completing both notebooks, you will:\n\nAuthenticate and initialize Google Earth Engine\nAccess Sentinel-1 SAR and Sentinel-2 optical data\nFilter ImageCollections by location, date, and metadata\nApply basic (QA60) AND advanced (SCL, s2cloudless) cloud masking\nUnderstand when to use each masking approach\nCreate temporal composites with best practices\nCalculate vegetation indices (NDVI)\nExport processed data for ML workflows\nApply concepts to Philippine use cases",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#session-4-hands-on-notebooks",
    "href": "day1/notebooks/notebook2.html#session-4-hands-on-notebooks",
    "title": "Notebook 2: Google Earth Engine",
    "section": "",
    "text": "This session includes TWO complementary notebooks designed for progressive learning:\n\n\nComplete introduction to Google Earth Engine with standard cloud masking (QA60)\n\n\n\nAdvanced techniques using SCL and s2cloudless for production-quality results\n\n\n\nBy completing both notebooks, you will:\n\nAuthenticate and initialize Google Earth Engine\nAccess Sentinel-1 SAR and Sentinel-2 optical data\nFilter ImageCollections by location, date, and metadata\nApply basic (QA60) AND advanced (SCL, s2cloudless) cloud masking\nUnderstand when to use each masking approach\nCreate temporal composites with best practices\nCalculate vegetation indices (NDVI)\nExport processed data for ML workflows\nApply concepts to Philippine use cases",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#notebook-1-gee-fundamentals-start-here-1",
    "href": "day1/notebooks/notebook2.html#notebook-1-gee-fundamentals-start-here-1",
    "title": "Notebook 2: Google Earth Engine",
    "section": "📘 Notebook 1: GEE Fundamentals (Start Here)",
    "text": "📘 Notebook 1: GEE Fundamentals (Start Here)\n\nWhat You’ll Learn\n\nComplete GEE setup and authentication\nCore concepts: Image, ImageCollection, Geometry\nFiltering by location, date, and metadata\nStandard QA60 cloud masking (simple, fast)\nMedian composite creation\nNDVI calculation and visualization\nBasic export workflows\nPhilippine case studies\n\n\n\nOpen in Google Colab\n\n\n\n\nOpen In Colab\n\n\n\n\n\n\n\n\n\nNoteFirst Time Using This Notebook?\n\n\n\nIf you get a “Not Found” error: 1. The notebook files need to be pushed to GitHub first 2. Alternative: Download the notebook below and upload to your own Google Drive 3. Then open from Drive in Colab\n\n\n\n\n\n\n\n\nImportantEarth Engine Account Required\n\n\n\nYou must have a registered Google Earth Engine account to run this notebook. If you haven’t registered yet, see the Setup Guide.\nRegistration takes 24-48 hours for approval.\n\n\nWhy Start with Notebook 1: - Covers all GEE fundamentals systematically - Standard QA60 masking is easier to understand first - Complete workflow from setup to export - ~60 cells, ~2 hours\n\nDownload Option:\nDownload Notebook 1 .ipynb",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#notebook-2-advanced-cloud-masking-p0-best-practices-1",
    "href": "day1/notebooks/notebook2.html#notebook-2-advanced-cloud-masking-p0-best-practices-1",
    "title": "Notebook 2: Google Earth Engine",
    "section": "📗 Notebook 2: Advanced Cloud Masking (P0 Best Practices)",
    "text": "📗 Notebook 2: Advanced Cloud Masking (P0 Best Practices)\n\nWhat You’ll Learn\n🆕 P0 IMPROVEMENT: This notebook implements the expert-recommended cloud masking improvements:\n\nScene Classification Layer (SCL) masking\n\nDetects clouds AND shadows (QA60 misses shadows!)\n12-class comprehensive classification\nBetter for NDVI time series and land cover\n\ns2cloudless integration\n\nMost accurate ML-based cloud detection\nAdjustable probability thresholds\nProduction-grade quality\n\nComparative analysis\n\nVisual comparison: QA60 vs SCL vs s2cloudless\nWhen to use each method\nPerformance trade-offs\n\nExport templates\n\nOptimized for ML training data\nBatch export workflows\nSample point extraction\n\n\n\n\nWhen to Use This Notebook\n✅ After completing Notebook 1 ✅ For operational/production work ✅ When QA60 results have “ghost” clouds or shadows ✅ For ML training data preparation ✅ For NDVI time series analysis\n\n\nOpen in Google Colab\n\n\n\n\n\n\nNoteNotebook 2 Coming Soon\n\n\n\nThe advanced SCL cloud masking notebook is currently under development. For now, use Notebook 1 which covers all essential GEE concepts including QA60 cloud masking.\n\n\nWhy Notebook 2 (When Available): - Implements P0 improvement from expert review - Cleaner composites for ML training - Production-ready workflows - Focused and concise (~10-15 cells, ~30 minutes)",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#recommended-learning-path",
    "href": "day1/notebooks/notebook2.html#recommended-learning-path",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Recommended Learning Path",
    "text": "Recommended Learning Path\n\n🎯 For Beginners / First-Time GEE Users:\n\n✅ Complete Notebook 1 in full (2 hours)\n✅ Practice exercises\n✅ (Optional) Try Notebook 2 to see advanced techniques\n\n\n\n🎯 For Experienced Users / Refresher:\n\n✅ Skim Notebook 1 sections 1-3 (setup and basics)\n✅ Focus on Notebook 2 for P0 improvements\n✅ Use as reference for production workflows\n\n\n\n🎯 For Production / Operational Work:\n\n✅ Use Notebook 2 as template\n✅ Reference Notebook 1 for comprehensive examples\n✅ Adapt SCL or s2cloudless masking to your use case",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#quick-comparison",
    "href": "day1/notebooks/notebook2.html#quick-comparison",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Quick Comparison",
    "text": "Quick Comparison\n\n\n\nAspect\nNotebook 1\nNotebook 2\n\n\n\n\nFocus\nComprehensive GEE introduction\nAdvanced cloud masking\n\n\nCloud Masking\nQA60 (standard)\nSCL + s2cloudless (advanced)\n\n\nLength\n~66 cells, 2 hours\n~10-15 cells, 30 min\n\n\nComplexity\nBeginner-friendly\nIntermediate\n\n\nUse For\nLearning GEE fundamentals\nProduction workflows\n\n\nP0 Alignment\nStandard approach\n✅ Implements P0 improvement",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#whats-covered-across-both-notebooks",
    "href": "day1/notebooks/notebook2.html#whats-covered-across-both-notebooks",
    "title": "Notebook 2: Google Earth Engine",
    "section": "What’s Covered Across Both Notebooks",
    "text": "What’s Covered Across Both Notebooks\n\nNotebook 1 Topics\n\nEarth Engine Fundamentals\n\nAuthentication and initialization\nee.Image and ee.ImageCollection\nGeometry definitions\nData catalog navigation\n\nSentinel-1 & Sentinel-2 Access\n\nCOPERNICUS/S2_SR_HARMONIZED collection\nCOPERNICUS/S1_GRD collection\nBand names and metadata\n\nFiltering & Processing\n\nSpatial, temporal, metadata filters\nQA60 cloud masking\nMedian composites\nNDVI calculation\n\nPhilippine Case Studies\n\nMetro Manila monitoring\nPalawan land cover\nFlood detection\nAgricultural monitoring\n\nExport Workflows\n\nGoogle Drive export\nScale and region parameters\nTask management\n\n\n\n\nNotebook 2 Additional Topics\n\nAdvanced Cloud Masking (P0)\n\nSCL 12-class classification\nShadow detection\ns2cloudless ML-based detection\nMethod comparison\n\nProduction Optimization\n\nExport templates for ML\nSample point extraction\nBatch processing workflows\nBest practices",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#prerequisites",
    "href": "day1/notebooks/notebook2.html#prerequisites",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting this notebook, ensure you have:\n\n✅ Google Earth Engine account (registered and approved)\n✅ Completed Setup Guide\n✅ Understanding of Session 4 concepts\n✅ Basic Python knowledge\n✅ Familiarity with Jupyter/Colab",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#notebook-contents",
    "href": "day1/notebooks/notebook2.html#notebook-contents",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Notebook Contents",
    "text": "Notebook Contents\nThe full interactive notebook includes:\n\n20+ code cells with step-by-step instructions\n15+ visualizations including interactive maps\n4 Philippine case studies with real-world applications\nExport workflows for downloading processed data\nTroubleshooting section for common errors\nExercises to reinforce learning",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#key-concepts-covered",
    "href": "day1/notebooks/notebook2.html#key-concepts-covered",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Key Concepts Covered",
    "text": "Key Concepts Covered\n\nEarth Engine Architecture\n# Basic Earth Engine workflow\nimport ee\nee.Initialize()\n\n# Define area of interest\nphilippines = ee.Geometry.Rectangle([116.0, 4.0, 127.0, 21.0])\n\n# Access Sentinel-2 collection\ncollection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n    .filterBounds(philippines) \\\n    .filterDate('2024-01-01', '2024-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\n# Create composite\ncomposite = collection.median()\n\n# Calculate NDVI\nndvi = composite.normalizedDifference(['B8', 'B4'])\n\n\nInteractive Mapping with geemap\nimport geemap\n\n# Create interactive map\nMap = geemap.Map()\nMap.centerObject(philippines, 6)\n\n# Add layers\nMap.addLayer(composite, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3000}, 'True Color')\nMap.addLayer(ndvi, {'min': 0, 'max': 1, 'palette': ['red', 'yellow', 'green']}, 'NDVI')\n\nMap",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#philippine-use-cases",
    "href": "day1/notebooks/notebook2.html#philippine-use-cases",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Philippine Use Cases",
    "text": "Philippine Use Cases\n\nCase Study 1: Metro Manila Urban Monitoring\nTrack urban expansion and changes in the National Capital Region using multi-temporal Sentinel-2 data.\n\n\nCase Study 2: Palawan Forest Cover\nMonitor forest cover and detect deforestation in Palawan Province using NDVI time series.\n\n\nCase Study 3: Central Luzon Flood Mapping\nDetect flood extents using Sentinel-1 SAR backscatter changes before and after typhoon events.\n\n\nCase Study 4: Mindanao Agricultural Drought\nAssess agricultural drought impacts using vegetation indices and SWIR bands.",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#common-errors-solutions",
    "href": "day1/notebooks/notebook2.html#common-errors-solutions",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Common Errors & Solutions",
    "text": "Common Errors & Solutions\n\nError: “Please set project ID”\nCause: Earth Engine not authenticated\nSolution:\nee.Authenticate()  # Follow prompts\nee.Initialize()\n\n\nError: “User memory limit exceeded”\nSolution: Reduce spatial or temporal scope, increase scale parameter\n\n\nError: “Too many concurrent aggregations”\nSolution: Add .limit() to reduce collection size\nSee the FAQ for more troubleshooting help.",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#support",
    "href": "day1/notebooks/notebook2.html#support",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Support",
    "text": "Support\n\nDuring the Training\n\nAsk questions in live session\nConsult teaching assistants\nShare your results with the group\n\n\n\nAfter the Training\n\nReview Earth Engine Cheat Sheet\nCheck FAQ\nJoin GEE Community Forum",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#related-resources",
    "href": "day1/notebooks/notebook2.html#related-resources",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Related Resources",
    "text": "Related Resources\n\nSession Materials: - Session 4: Introduction to Google Earth Engine - Session 4 Presentation Slides\nQuick References: - Earth Engine Python API Cheat Sheet - Sentinel Missions Reference\nOfficial Documentation: - Earth Engine Python API Guide - Earth Engine Data Catalog - geemap Documentation\nCommunity Resources: - Awesome Earth Engine - Earth Engine Tutorials",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook2.html#next-steps",
    "href": "day1/notebooks/notebook2.html#next-steps",
    "title": "Notebook 2: Google Earth Engine",
    "section": "Next Steps",
    "text": "Next Steps\nAfter completing this notebook:\n\n✅ Practice with different Philippine regions\n✅ Experiment with other satellites (Landsat, MODIS)\n✅ Prepare for Day 2: Machine Learning for Land Cover Classification\n✅ Export data for your own projects\n\n\n\n\n\n\n\n\nTipReady to Explore Petabytes of Data?\n\n\n\nOpen the notebook in Colab and start accessing the entire Sentinel archive from your browser!\n\n\nAll processing happens in the cloud - no downloads required!",
    "crumbs": [
      "Notebooks",
      "Notebook 2: Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html",
    "href": "day1/sessions/session1.html",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "",
    "text": "Home › Day 1 › Session 1",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#session-overview",
    "href": "day1/sessions/session1.html#session-overview",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Session Overview",
    "text": "Session Overview\nThis session introduces the European Copernicus Earth Observation program, focusing on Sentinel-1 (SAR) and Sentinel-2 (Optical) missions with 2025 updates. You’ll explore the Philippine EO landscape, including key agencies and data platforms that complement Copernicus data for disaster risk reduction, climate adaptation, and resource management applications.\n\nLearning Objectives\nBy the end of this session, you will be able to:\n\nDescribe the Copernicus programme and its mission for global Earth monitoring\nCompare Sentinel-1 SAR and Sentinel-2 optical satellite characteristics\nIdentify 2025 constellation updates (Sentinel-2C operational, Sentinel-1C launched)\nNavigate the Copernicus Data Space Ecosystem and SentiBoard dashboard\nLocate Philippine EO data through PhilSA SIYASAT, NAMRIA Geoportal, and DOST-ASTI platforms\nExplain how CoPhil integrates European and Philippine EO capabilities\nAccess the CoPhil Mirror Site and Digital Space Campus for continued learning",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#presentation-slides",
    "href": "day1/sessions/session1.html#presentation-slides",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Presentation Slides",
    "text": "Presentation Slides",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#part-1-the-copernicus-programme",
    "href": "day1/sessions/session1.html#part-1-the-copernicus-programme",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 1: The Copernicus Programme",
    "text": "Part 1: The Copernicus Programme\n\nWhat is Copernicus?\nCopernicus is the European Union’s flagship Earth Observation programme providing free and open satellite data for environmental monitoring and societal applications worldwide. It represents the world’s largest single Earth observation programme, with a multi-billion euro investment.\n\n\n\n\n\n\nNoteWhy “Copernicus”?\n\n\n\nNamed after Nicolaus Copernicus (1473-1543), the Renaissance astronomer who formulated the heliocentric model of the universe. Just as Copernicus revolutionized our understanding of our place in the cosmos, the Copernicus programme transforms our understanding of Earth systems.\n\n\n\n\nCopernicus Components\nThe programme consists of:\n\nSpace Component - The Sentinel satellite family (1-6) and contributing missions\nIn Situ Component - Ground-based and airborne observations\nServices - Six thematic information products:\n\nAtmosphere Monitoring Service (CAMS) - Air quality, emissions, aerosols\nMarine Environment Monitoring Service (CMEMS) - Ocean state, sea ice, biogeochemistry\nLand Monitoring Service - Land cover, vegetation, water bodies\nClimate Change Service (C3S) - Climate indicators, projections, reanalysis\nEmergency Management Service (CEMS) - Rapid mapping, early warning, risk assessment\nSecurity Service - Border surveillance, maritime security\n\n\n\n\n\n\n\ngraph TD\n    A[Copernicus Programme] --&gt; B[Space Component]\n    A --&gt; C[In Situ Component]\n    A --&gt; D[Services]\n\n    B --&gt; B1[Sentinel Missions 1-6]\n    B --&gt; B2[Contributing Missions]\n\n    C --&gt; C1[Ground Stations]\n    C --&gt; C2[Airborne Sensors]\n    C --&gt; C3[Maritime Buoys]\n\n    D --&gt; D1[CAMS&lt;br/&gt;Atmosphere]\n    D --&gt; D2[CMEMS&lt;br/&gt;Marine]\n    D --&gt; D3[Land Monitoring]\n    D --&gt; D4[C3S&lt;br/&gt;Climate Change]\n    D --&gt; D5[CEMS&lt;br/&gt;Emergency]\n    D --&gt; D6[Security]\n\n    B1 -.-&gt;|Data| D\n    C1 -.-&gt;|Data| D\n\n    D --&gt; E[End Users]\n    E --&gt; E1[Governments]\n    E --&gt; E2[Researchers]\n    E --&gt; E3[Private Sector]\n    E --&gt; E4[Citizens]\n\n    style A fill:#0066cc,stroke:#003d7a,stroke-width:3px,color:#fff\n    style B fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff\n    style C fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff\n    style D fill:#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff\n    style E fill:#cc0066,stroke:#880044,stroke-width:2px,color:#fff\n\n\n Copernicus Programme Architecture \n\n\n\n\n\n\n\n\n\nImportant2024 Climate Highlights from C3S\n\n\n\nAccording to the Copernicus Climate Change Service, 2024 was the hottest year on record with a global average temperature of 15.10°C. The year also saw the highest atmospheric water vapor content on record, contributing to increased intensity of heavy rainfall events—directly impacting countries like the Philippines.\n\n\n\n\nThe Sentinel Family\n\n\n\n\n\n\n\n\n\nSentinel\nType\nPrimary Application\nStatus\n\n\n\n\nSentinel-1\nC-band SAR\nAll-weather radar imaging\n1A operational, 1C launched Dec 2024\n\n\nSentinel-2\nMultispectral Optical\nLand monitoring\n2B operational, 2C operational Jan 2025\n\n\nSentinel-3\nOcean & Land\nMarine/land surface monitoring\nOperational\n\n\nSentinel-4\nGeostationary\nAir quality monitoring\nIn development\n\n\nSentinel-5P\nAtmospheric\nAir quality and pollution\nOperational\n\n\nSentinel-6\nRadar Altimetry\nOcean surface topography\nOperational\n\n\n\nFor this training, we focus on Sentinel-1 and Sentinel-2 - the most widely used for DRR, CCA, and NRM applications.\n\n\nCopernicus Expansion Missions\nFollowing the UK’s re-entry to the EU Copernicus programme, funding has been confirmed for six expansion missions:\n\nSentinel-7 (CO2M): Atmospheric CO2 and methane measurement\nSentinel-8 (LSTM): High-resolution land surface temperature for water scarcity monitoring\nSentinel-9 (CRISTAL): Polar ice and snow topography (launch 2028)\nSentinel-10 (CHIME): Hyperspectral imaging for agriculture and resources\nSentinel-11 (CIMR): Microwave radiometry for sea ice, SST, salinity\nSentinel-12 (ROSE-L): L-band SAR (no earlier than 2028)",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#part-2-sentinel-1---synthetic-aperture-radar",
    "href": "day1/sessions/session1.html#part-2-sentinel-1---synthetic-aperture-radar",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 2: Sentinel-1 - Synthetic Aperture Radar",
    "text": "Part 2: Sentinel-1 - Synthetic Aperture Radar\n\nMission Overview\n\nSentinel-1A (2014)\n\n\nSentinel-1B (2016-2022)\n\n\nSentinel-1C (Dec 2024)\n\nSentinel-1 is a C-band Synthetic Aperture Radar (SAR) mission providing all-weather, day-and-night imaging capability.\n\n\n2024-2025 Constellation Updates\n\nSentinel-1A: Operational since April 2014, continuing strong performance\nSentinel-1B: Operational 2016-2022 (anomaly ended operations December 23, 2021)\nSentinel-1C: Successfully launched December 5, 2024 aboard Vega-C rocket\n\nCurrently in commissioning phase\nWill restore dual-satellite constellation with 1A\nImproved AIS (Automatic Identification System) for enhanced vessel tracking\n\nSentinel-1D: Scheduled for launch November 4, 2025\n\n\n\n\n\n\n\nTipWhy SAR Matters for the Philippines\n\n\n\nThe Philippines experiences:\n\nFrequent cloud cover (tropical climate)\nMonsoon seasons with persistent rain\nNighttime disasters (earthquakes, floods)\n~20 typhoons annually\n\nSAR sees through clouds and operates at night - critical for disaster response when optical satellites are blinded. The restored two-satellite constellation will provide 6-day revisit time over the Philippines, enhancing rapid response capabilities.\n\n\n\n\nKey Technical Specifications\n\n\n\n\n\n\n\nParameter\nSpecification\n\n\n\n\nSensor Type\nC-band Synthetic Aperture Radar\n\n\nFrequency\n5.405 GHz (wavelength ~5.6 cm)\n\n\nOrbit\nSun-synchronous, near-polar\n\n\nAltitude\n693 km\n\n\nInclination\n98.18°\n\n\nOrbital Period\n98.6 minutes\n\n\nRevisit Time\n6 days (two satellites), 12 days (single)\n\n\nSwath Width\nUp to 410 km (EW mode), 250 km (IW mode)\n\n\nSpatial Resolution\nDown to 5 m × 20 m (range × azimuth, IW mode)\n\n\nPolarization\nDual (VV+VH or HH+HV) or single\n\n\nData Storage\n1,443 Gbit (168 GiB) onboard\n\n\nDownlink Rate\n520 Mbit/s (X-band)\n\n\n\n\n\nSAR Operating Principle\n\n\n\n\n\nflowchart LR\n    A[Satellite&lt;br/&gt;693km altitude] --&gt;|Transmit&lt;br/&gt;Radar Pulse| B[Ground Surface]\n    B --&gt;|Backscatter&lt;br/&gt;Returns| A\n\n    A --&gt; C[Signal Processing]\n    C --&gt; D[SAR Image]\n\n    D --&gt; E1[Amplitude&lt;br/&gt;Surface roughness]\n    D --&gt; E2[Phase&lt;br/&gt;InSAR deformation]\n    D --&gt; E3[Polarization&lt;br/&gt;Surface properties]\n\n    style A fill:#0066cc,stroke:#003d7a,stroke-width:2px,color:#fff\n    style B fill:#8b4513,stroke:#654321,stroke-width:2px,color:#fff\n    style C fill:#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff\n    style D fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff\n\n\n Sentinel-1 SAR Imaging Process \n\n\n\nKey Advantages:\n\nAll-weather: Penetrates clouds, rain, smoke\nDay-night: Active sensor with own illumination\nSurface sensitivity: Detects roughness, moisture, structure\nInterferometry: Measures mm-scale deformation\n\n\n\nImaging Modes\nSentinel-1 operates in four primary acquisition modes:\n\n\n\n\n\ngraph LR\n    A[Sentinel-1&lt;br/&gt;Imaging Modes] --&gt; B[SM&lt;br/&gt;Strip Map]\n    A --&gt; C[IW&lt;br/&gt;Interferometric&lt;br/&gt;Wide Swath]\n    A --&gt; D[EW&lt;br/&gt;Extra Wide&lt;br/&gt;Swath]\n    A --&gt; E[WV&lt;br/&gt;Wave Mode]\n\n    B --&gt; B1[80 km swath&lt;br/&gt;5m resolution&lt;br/&gt;Specific targets]\n    C --&gt; C1[250 km swath&lt;br/&gt;5x20m resolution&lt;br/&gt;Standard land mode]\n    D --&gt; D1[410 km swath&lt;br/&gt;20x40m resolution&lt;br/&gt;Maritime/ice/polar]\n    E --&gt; E1[20x20 km vignettes&lt;br/&gt;5m resolution&lt;br/&gt;Ocean waves]\n\n    style A fill:#0066cc,stroke:#003d7a,stroke-width:3px,color:#fff\n    style C fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff\n    style C1 fill:#00aa44,stroke:#006622,stroke-width:1px,color:#fff\n\n\n Sentinel-1 Acquisition Modes Comparison \n\n\n\n\n1. Interferometric Wide Swath (IW) Mode\n\nPrimary Use: Main land acquisition mode\nSwath Width: 250 km\nSpatial Resolution: 5 m × 20 m (single look)\nPixel Spacing: 10 m\nTechnique: TOPSAR (Terrain Observation with Progressive Scanning SAR)\nPolarization: Dual (VV+VH or HH+HV) or single\nApplication: Land monitoring, disaster response, agriculture\n\nKey Feature: TOPSAR eliminates scalloping effects and ensures homogeneous image quality across the entire swath.\n\n\n2. Extra Wide Swath (EW) Mode\n\nPrimary Use: Maritime and polar region monitoring\nSwath Width: 400 km\nSpatial Resolution: 20 m × 40 m\nPixel Spacing: 40 m\nApplication: Wide-area coastal monitoring, sea ice detection\n\n\n\n3. Strip Map (SM) Mode\n\nSwath Width: 80 km\nSpatial Resolution: 5 m × 5 m\nApplication: High-resolution applications requiring smaller coverage\n\n\n\n4. Wave (WV) Mode\n\nPrimary Use: Ocean wave spectra measurements\nAcquisition: 20 km × 20 km vignettes every 100 km\nApplication: Wave height and direction, maritime safety\n\nFor Philippine land applications: IW mode is standard.\n\n\n\nUnderstanding Polarization\nSAR polarization refers to the orientation of the radar wave’s electric field. Different surface types respond differently to different polarizations:\nVV (Vertical-Vertical): - Transmit and receive in vertical polarization - Sensitive to surface roughness - Best for: Ocean monitoring, flood detection, bare soil\nVH (Vertical-Horizontal) - Cross-Polarization: - Transmit vertical, receive horizontal - Sensitive to volume scattering - Best for: Forest biomass, vegetation structure, crop classification\nHH (Horizontal-Horizontal): - Transmit and receive in horizontal polarization - Sensitive to surface scattering - Best for: Sea ice classification, soil moisture\nHV (Horizontal-Vertical) - Cross-Polarization: - Transmit horizontal, receive vertical - Volume scattering sensitivity - Best for: Vegetation monitoring\n\nBackscatter Characteristics by Target Type\n\nWater (calm): Very low backscatter (specular reflection) → very dark\nWater (rough/waves): Low to moderate backscatter → dark to medium gray\nBare soil: Moderate backscatter → medium gray\nVegetation: Moderate to high backscatter (volume scattering) → medium to light gray\nUrban/buildings: Very high backscatter (double-bounce) → very bright\nFlooded vegetation: Very high backscatter (double-bounce from water-plant) → bright\n\n\n\n\nData Products\n\nLevel-1 Single Look Complex (SLC)\n\nType: Complex (amplitude + phase)\nGeometry: Slant range\nUse Cases:\n\nInSAR (Interferometry) for ground deformation\nCoherence analysis\nAdvanced polarimetric analysis\n\nApplications: Earthquake monitoring, volcanic deformation, landslide detection, subsidence\n\n\n\nLevel-1 Ground Range Detected (GRD)\n\nType: Detected, multi-looked intensity\nGeometry: Ground range projected (WGS84 ellipsoid)\nProcessing: Focused, detected, multi-looked, projected\nValues: Amplitude only (no phase information)\nPixel Spacing: 10 m (IW, SM), 40 m (EW)\nMost common for: Change detection, flood mapping, land cover classification\n\n\n\nLevel-2 Ocean (OCN)\n\nType: Geophysical ocean products\nParameters: Wind fields, wave spectra, surface radial velocity\nFormat: NetCDF\nApplications: Maritime safety, weather forecasting\n\n\n\n\n\n\n\nNoteGRD vs SLC - When to Use Each\n\n\n\nUse GRD when: - Detecting changes in backscatter intensity - Mapping floods or water bodies - Classifying land cover - Working with single-date images - You need simpler, faster processing\nUse SLC when: - Measuring ground deformation with InSAR - Calculating coherence between image pairs - Performing polarimetric analysis - You need phase information - Precision measurements are critical\n\n\n\n\n\nSentinel-1 Applications\nDisaster Risk Reduction: - Flood extent mapping - Water appears dark due to specular reflection - Earthquake damage assessment - Coherence change detection - Landslide detection - Temporal change analysis - Volcanic deformation - InSAR millimeter-level precision\nNatural Resource Management: - Deforestation monitoring - All-weather forest change detection - Rice paddy mapping - Temporal backscatter analysis - Mangrove extent - Coastal wetland monitoring - Soil moisture - Agricultural water management\nMaritime Applications: - Ship detection and tracking (enhanced with Sentinel-1C/1D AIS) - Oil spill monitoring - Dampening of Bragg scattering - Sea ice classification - Ice type and concentration - Illegal fishing detection\n\n\nFive Powerful Sentinel-1 Use Cases (2024 ESA Report)\nAccording to the Copernicus OBSERVER 2024 report, these are the five key applications leveraging Sentinel-1C:\n\nArctic Sea Ice Monitoring - All-season navigation route planning\nLand Subsidence Detection - InSAR with millimeter-level precision for urban areas\nAgricultural Land Use - Crop type classification using temporal backscatter signatures\nOil Spill Detection - Rapid response for maritime pollution incidents\nForest Structure Mapping - Canopy height and biomass assessment\n\n\n\nExample: Sentinel-1 Flood Mapping\nSAR backscatter characteristics during flooding:\n\nPre-flood agricultural land: Moderate backscatter (vegetation) → medium gray\nDuring flood: Low backscatter (water surface) → dark\nFlooded urban areas: Very high backscatter (double-bounce) → bright\n\n\n\n\n\n\n\nTipFlood Mapping Best Practice\n\n\n\nVV polarization shows dark water (low backscatter from smooth surfaces)\nKey technique: Compare pre-event vs post-event delta for reliability - reduces false positives\nSynergy: Pair S1 (flood extent through clouds) with S2 (vegetation damage assessment when clear)\n\n\n\nSentinel-1 Pre-Processing “Under the Hood”\n\n\n\n\n\n\nNoteP0 Improvement: Understanding Analysis-Ready SAR Data\n\n\n\nFor Day 3 flood mapping labs, we provide pre-processed Sentinel-1 patches. Here’s what has been applied:\n\n\nStandard S1 GRD Processing Pipeline:\n\nGRD Download → Raw ground-range detected amplitude\nRadiometric Calibration → Convert to backscatter coefficient (σ⁰)\nTerrain Correction (RTC) → Remove topographic distortions using DEM (critical in mountainous Philippines)\nSpeckle Filtering → Reduce SAR noise (Lee, Refined Lee, or Gamma-MAP filters)\nConversion to dB → γ⁰ (gamma-naught) in decibels for visual interpretation\nTiling/Clipping → Extract area of interest\n\nWhy this matters: - RTC is essential for mountainous areas to remove terrain-induced distortions - Gamma-naught (γ⁰) in dB is the standard for land applications - Speckle filtering improves interpretability but can blur edges - These steps are computationally intensive - we pre-process for training efficiency\nWorkflow: 1. Acquire pre-flood and during-flood Sentinel-1 GRD images 2. Apply pre-processing pipeline above (or use pre-processed patches) 3. Change detection or thresholding 4. Extract flood extent polygon 5. Deliver map within 10-20 minutes of data availability\nPhilippine Example: DOST-ASTI’s DATOS Help Desk uses this approach to provide near-real-time flood maps to disaster response agencies using AI-powered automated processing.",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#part-3-sentinel-2---multispectral-optical",
    "href": "day1/sessions/session1.html#part-3-sentinel-2---multispectral-optical",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 3: Sentinel-2 - Multispectral Optical",
    "text": "Part 3: Sentinel-2 - Multispectral Optical\n\nMission Overview\n\nSentinel-2A (2015)\n\n\nSentinel-2B (2017)\n\n\nSentinel-2C (Jan 2025)\n\nSentinel-2 is a multispectral optical imaging mission focused on land monitoring with high spatial, spectral, and temporal resolution.\n\n\n2025 Constellation Update - Three-Satellite Configuration!\nMajor Milestone: January 21, 2025 - Sentinel-2C became operational, replacing Sentinel-2A\nThree-Satellite Constellation Trial: For the first time ever, three Sentinel satellites of the same type are working together:\n\nSentinel-2C at 0° orbital phase (primary operational)\nSentinel-2B at 180° phase (primary operational)\nSentinel-2A at 144° phase (experimental year-long trial mission)\n\nRevolutionary Benefits: - Enhanced temporal resolution beyond the standard 5-day revisit - Improved probability of cloud-free acquisitions - Better monitoring of rapid changes - Assessment of three-satellite constellation utility for future missions\n\n\n\n\n\n\nImportantWhat This Means for Philippine EO\n\n\n\nWith three Sentinel-2 satellites operational:\n\nMore frequent observations over the Philippines and Southeast Asia\nHigher chance of capturing cloud-free imagery during monsoon seasons\nBetter temporal coverage for crop phenology monitoring\nImproved disaster response and damage assessment capabilities\nEnhanced monitoring of fast-changing events (typhoon impacts, flooding)\n\nThis configuration will be studied throughout 2025 to inform future Copernicus expansion decisions.\n\n\n\n\nKey Technical Specifications\n\n\n\n\n\n\n\nParameter\nSpecification\n\n\n\n\nSensor\nMultiSpectral Instrument (MSI)\n\n\nSpectral Bands\n13 (VIS to SWIR: 443-2190 nm)\n\n\nOrbit\nSun-synchronous, polar\n\n\nAltitude\n786 km\n\n\nInclination\n98.62°\n\n\nOrbital Period\n101 minutes\n\n\nSwath Width\n290 km\n\n\nRevisit Time\n5 days at equator (2 satellites), improved with 3rd satellite\n\n\nTile Size\n100 km × 100 km (MGRS grid, UTM/WGS84)\n\n\nLocal Overpass Time\n10:30 AM (descending node)\n\n\n\n\n\nSpectral Bands - The MSI Advantage\nSentinel-2’s 13 bands span visible, near-infrared, and shortwave infrared, optimized for land applications:\n\n\n\n\n\ngraph TB\n    A[Sentinel-2 MSI&lt;br/&gt;13 Spectral Bands] --&gt; B[10m Resolution&lt;br/&gt;TRUE COLOR + NIR]\n    A --&gt; C[20m Resolution&lt;br/&gt;RED EDGE + SWIR]\n    A --&gt; D[60m Resolution&lt;br/&gt;ATMOSPHERIC]\n\n    B --&gt; B1[B2 - Blue&lt;br/&gt;490nm]\n    B --&gt; B2[B3 - Green&lt;br/&gt;560nm]\n    B --&gt; B3[B4 - Red&lt;br/&gt;665nm]\n    B --&gt; B4[B8 - NIR&lt;br/&gt;842nm]\n\n    C --&gt; C1[B5 - RedEdge 1&lt;br/&gt;705nm]\n    C --&gt; C2[B6 - RedEdge 2&lt;br/&gt;740nm]\n    C --&gt; C3[B7 - RedEdge 3&lt;br/&gt;783nm]\n    C --&gt; C4[B8A - Narrow NIR&lt;br/&gt;865nm]\n    C --&gt; C5[B11 - SWIR 1&lt;br/&gt;1610nm]\n    C --&gt; C6[B12 - SWIR 2&lt;br/&gt;2190nm]\n\n    D --&gt; D1[B1 - Coastal&lt;br/&gt;443nm]\n    D --&gt; D2[B9 - Water Vapor&lt;br/&gt;945nm]\n    D --&gt; D3[B10 - Cirrus&lt;br/&gt;1373nm]\n\n    B1 -.-&gt;|Applications| E1[RGB Composites&lt;br/&gt;Water mapping]\n    B4 -.-&gt;|Applications| E2[Vegetation indices&lt;br/&gt;Biomass estimation]\n    C1 -.-&gt;|Applications| E3[Vegetation stress&lt;br/&gt;Crop monitoring]\n    C5 -.-&gt;|Applications| E4[Soil moisture&lt;br/&gt;Fire detection]\n    D1 -.-&gt;|Applications| E5[Atmospheric&lt;br/&gt;correction]\n\n    style A fill:#0066cc,stroke:#003d7a,stroke-width:3px,color:#fff\n    style B fill:#00cc00,stroke:#008800,stroke-width:2px,color:#fff\n    style C fill:#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff\n    style D fill:#6666cc,stroke:#444499,stroke-width:2px,color:#fff\n\n\n Sentinel-2 Spectral Bands Overview \n\n\n\n\n10-meter Resolution Bands (True Color + NIR)\n\n\n\n\n\n\n\n\n\n\nBand\nName\nWavelength (nm)\nBandwidth (nm)\nPrimary Application\n\n\n\n\nB2\nBlue\n490\n98\nAerosol detection, water body mapping\n\n\nB3\nGreen\n560\n45\nVegetation vigor, water clarity\n\n\nB4\nRed\n665\n38\nChlorophyll absorption, vegetation discrimination\n\n\nB8\nNIR\n842\n145\nBiomass, vegetation health, water delineation\n\n\n\n\n\n20-meter Resolution Bands (Red Edge + SWIR)\n\n\n\n\n\n\n\n\n\n\nBand\nName\nWavelength (nm)\nBandwidth (nm)\nPrimary Application\n\n\n\n\nB5\nRed Edge 1\n705\n19\nVegetation classification, chlorophyll\n\n\nB6\nRed Edge 2\n740\n18\nVegetation stress detection\n\n\nB7\nRed Edge 3\n783\n28\nVegetation monitoring, LAI estimation\n\n\nB8A\nNarrow NIR\n865\n33\nAtmospheric correction, cloud screening\n\n\nB11\nSWIR 1\n1610\n143\nSnow/ice/cloud discrimination, soil moisture\n\n\nB12\nSWIR 2\n2190\n242\nFire detection, vegetation moisture content\n\n\n\n\n\n60-meter Resolution Bands (Atmospheric)\n\n\n\n\n\n\n\n\n\n\nBand\nName\nWavelength (nm)\nBandwidth (nm)\nPrimary Application\n\n\n\n\nB1\nCoastal Aerosol\n443\n27\nAerosol detection, coastal water\n\n\nB9\nWater Vapor\n945\n26\nWater vapor detection, atm. correction\n\n\nB10\nCirrus\n1373\n75\nCirrus cloud detection (L1C only)\n\n\n\n\n\n\n\n\n\nNoteRed Edge Bands - Sentinel-2’s Special Capability\n\n\n\nBands 5, 6, and 7 (the “red edge” region) are particularly sensitive to: - Chlorophyll content variations - Early vegetation stress (before visible to human eye) - Crop health and disease detection - Subtle changes in forest canopy\nThis makes Sentinel-2 superior to Landsat for agricultural and forest monitoring applications.\n\n\n\n\n\nData Products\n\nLevel-1C (Top-of-Atmosphere Reflectance)\nProcessing: - Radiometric and geometric corrections applied - Orthorectification using DEM - Sub-pixel multispectral and multitemporal registration\nOutput: - Top-of-atmosphere reflectance - 100 km × 100 km tiles (MGRS grid) - UTM/WGS84 projection - Cloud and land/water masks included\nFormat: JPEG2000 (.jp2) in SAFE archive, or GeoTIFF on some platforms\n\n\nLevel-2A (Bottom-of-Atmosphere Surface Reflectance)\nProcessing: - Atmospheric correction using Sen2Cor processor - Accounts for: - Gaseous absorption (O2, H2O, O3) - Rayleigh scattering - Aerosol scattering and absorption - Cirrus correction - Terrain effects\nOutput Products: - Surface reflectance for all bands except B10 - Scene Classification Layer (SCL) at 20 m: - No data, saturated/defective, cloud shadows, vegetation, bare soils, water, cloud (low/medium/high probability), cirrus, snow/ice - Aerosol Optical Thickness (AOT) at 60 m - Water Vapor (WV) at 60 m - Cloud probability masks at 60 m - Snow probability masks at 60 m\nWhy Level-2A is Recommended: - Physically meaningful surface reflectance values - Directly comparable across dates and seasons - Built-in quality masks (clouds, shadows, snow) - CEOS Analysis Ready Data (ARD) compliant (Processing Baseline 04.00+)\n\n\n\n\n\n\nTipAlways Use Level-2A When Available\n\n\n\nFor vegetation indices, land cover classification, and change detection: - Level-2A removes atmospheric effects - Provides consistent reflectance values - Includes quality masks (clouds, shadows) - Ready for analysis without preprocessing\nException: Use Level-1C only when you need to apply custom atmospheric correction or work with very recent data before Level-2A is available.\n\n\n\n\nScene Classification Layer (SCL) - Better Cloud Masking\n\n\n\n\n\n\nImportantP0 Improvement: Use SCL Instead of QA60\n\n\n\nWhy SCL is superior: - QA60 provides only basic cloud/cirrus detection via bitmasks - SCL provides comprehensive 20m classification layer with 12 classes: - No data (0), Saturated/defective (1) - Cloud shadows (3) - QA60 misses these! - Vegetation (4), Bare soils/rocks (5), Water (6) - Cloud low probability (7), Cloud medium probability (8), Cloud high probability (9) - Cirrus (10), Snow/ice (11)\nIn Session 4 GEE lab: We’ll mask pixels where SCL ∉ {3, 8, 9, 10, 11} (exclude shadows, clouds, cirrus)\nOptional enhancement: Apply 1-pixel dilation to remove cloud fringes\n\n\nResult: Cleaner composites with both clouds AND shadows removed - critical for NDVI time series and land cover classification in Day 2.\n\n\n\nProcessing Baseline Updates\nCurrent Baseline: 05.11 (deployed July 23, 2024) - Supports Sentinel-2C and future 2D specifications - Implements Product Specification Document (PSD) v15.0\nImportant Note: After January 25, 2022 (Processing Baseline 04.00), Sentinel-2 DN values were shifted by 1000 to accommodate improved radiometric performance.\nSolution: Use HARMONIZED collections in Google Earth Engine that correct for this shift, ensuring consistent values across the entire 2015-present time series.\n\n\nSentinel-2 Band Combinations for Visualization\n\nNatural Color (True Color)\nRGB = B4, B3, B2 (Red, Green, Blue) - Appears as human eye sees - Good for general interpretation - Urban areas, water bodies, vegetation\n\n\nFalse Color Infrared\nRGB = B8, B4, B3 (NIR, Red, Green) - Vegetation appears bright red (high NIR reflectance) - Water appears dark blue/black - Urban areas appear cyan/gray - Excellent for: Vegetation health, water body delineation\n\n\nSWIR Composite (Agriculture)\nRGB = B11, B8, B2 (SWIR1, NIR, Blue) - Highlights moisture content - Agricultural fields appear in various colors based on crop type and water content - Excellent for: Crop differentiation, soil moisture assessment\n\n\nSWIR-NIR-Red (Burn Scar)\nRGB = B12, B8, B4 (SWIR2, NIR, Red) - Burned areas appear bright red/magenta - Healthy vegetation appears green - Excellent for: Post-fire assessment, burn severity mapping\n\n\nBathymetric\nRGB = B4, B3, B1 (Red, Green, Coastal Aerosol) - Penetrates water for shallow bathymetry - Coral reefs and seagrass beds visible - Excellent for: Coastal and marine habitat mapping\n\n\n\nCommon Spectral Indices\nNDVI (Normalized Difference Vegetation Index):\n\\[\nNDVI = \\frac{NIR - Red}{NIR + Red} = \\frac{B8 - B4}{B8 + B4}\n\\]\n\nRange: -1 to +1\nInterpretation:\n\n&lt; 0: Water, clouds, snow\n0 - 0.2: Bare soil, rock, sand\n0.2 - 0.4: Sparse vegetation (grassland, shrubs)\n0.4 - 0.6: Moderate vegetation (crops, degraded forest)\n0.6 - 0.8: Dense vegetation (healthy crops, forest)\n\n0.8: Very dense vegetation\n\n\n\nEVI (Enhanced Vegetation Index):\n\\[\nEVI = 2.5 \\times \\frac{B8 - B4}{B8 + 6 \\times B4 - 7.5 \\times B2 + 1}\n\\]\n\nReduces saturation in dense vegetation\nBetter sensitivity in high-biomass areas\nCorrects for atmospheric and soil background effects\n\nNDWI (Normalized Difference Water Index):\n\\[\nNDWI = \\frac{Green - NIR}{Green + NIR} = \\frac{B3 - B8}{B3 + B8}\n\\]\n\nHigh values (&gt;0.3): Open water\nLow values: Land surfaces\nUse: Water body mapping, flood detection\n\nMNDWI (Modified NDWI):\n\\[\nMNDWI = \\frac{Green - SWIR1}{Green + SWIR1} = \\frac{B3 - B11}{B3 + B11}\n\\]\n\nBetter than NDWI for built-up areas\nSuppresses built-up land features\n\nNBR (Normalized Burn Ratio):\n\\[\nNBR = \\frac{NIR - SWIR2}{NIR + SWIR2} = \\frac{B8 - B12}{B8 + B12}\n\\]\n\nPre-fire: High values (healthy vegetation)\nPost-fire: Low values (burned areas)\ndNBR (differenced NBR): NBR_pre - NBR_post = burn severity\n\nSAVI (Soil-Adjusted Vegetation Index):\n\\[\nSAVI = \\frac{(NIR - Red)}{(NIR + Red + L)} \\times (1 + L)\n\\]\nwhere L = 0.5 (general use)\n\nReduces soil background effects\nBetter for areas with sparse vegetation\n\nMSI (Moisture Stress Index):\n\\[\nMSI = \\frac{SWIR1}{NIR} = \\frac{B11}{B8}\n\\]\n\nIndicates vegetation water content\nHigher values = higher water stress\n\n\n\nSentinel-2 Applications\nNatural Resource Management: - Land cover/land use classification - Forest type and health monitoring - Crop type mapping and growth monitoring - Agricultural productivity assessment - Mangrove and coral reef monitoring - Wetland mapping and change detection\nClimate Change Adaptation: - Drought monitoring via vegetation stress indices - Crop phenology and growing season tracking - Snow cover extent and glacier retreat - Coastal erosion and shoreline change - Urban heat island identification\nDisaster Risk Reduction: - Post-fire burn severity mapping - Landslide identification and mapping - Flood impact assessment (damage to vegetation/crops) - Earthquake-induced surface changes - Infrastructure damage assessment",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#part-4-accessing-copernicus-data-2025",
    "href": "day1/sessions/session1.html#part-4-accessing-copernicus-data-2025",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 4: Accessing Copernicus Data (2025)",
    "text": "Part 4: Accessing Copernicus Data (2025)\n\nPlatform Choices for This Training\n\n\n\n\n\n\nImportantP1 Improvement: Which Platform for Which Task?\n\n\n\nUnderstanding where to work is crucial - don’t try to train deep models on GEE or download 100GB in Colab!\n\n\n\n\n\n\n\n\n\n\n\nTask\nPlatform\nWhy\nLimitations\n\n\n\n\nData Prep & Exploration\nGoogle Earth Engine\nPetabyte catalog, no download, cloud composites\nExport limit 32 MB (tile large areas), no deep learning training\n\n\nML Training (RF, shallow)\nGEE or Colab\nRF works in GEE; small data in Colab\nGEE memory limits; Colab free tier quotas\n\n\nDeep Learning (CNN, U-Net)\nLocal GPU / Colab Pro\nRequires PyTorch/TensorFlow\nColab free = limited GPU time; large models need local resources\n\n\nLarge-Scale Processing\nCoPhil Mirror Site / COARE\n400TB local data, HPC resources\nRequires account; learning curve for APIs\n\n\nQuick Viz & Download\nCopernicus Browser\nInteractive, fast previews\nManual selection; bulk downloads tedious\n\n\n\nQuotas/Pitfalls to Know:\n\nGEE: Memory errors with large computations - tile exports for large areas\nColab Free: GPU disconnects after inactivity; limited sessions per day; 12-hour max runtime\nCoPhil/Digital Space Campus: Hosts training materials + local data access (400TB)\n\n\n\n\nCopernicus Data Space Ecosystem\nNEW in 2023-2025: The legacy Copernicus Open Access Hub (SciHub) has been replaced by the comprehensive Copernicus Data Space Ecosystem.\n\n\n\n\n\n\nNoteCopernicus Data Space Ecosystem\n\n\n\nURL: https://dataspace.copernicus.eu\nOfficial Launch: January 2023, with continuous updates through 2024-2025\nFeatures: - Unified access to all Sentinel missions (1-6) - Full archive from mission start to present - Interactive Copernicus Browser for visualization - Multiple API access methods (STAC, OData, Sentinel Hub, openEO) - JupyterHub for cloud-based analysis - Direct S3 cloud storage access - SentiBoard real-time mission dashboard\nFree Access: No download limits, free processing quotas for registered users\n\n\n\n\nAvailable Data Collections\nThe CDSE provides comprehensive access to:\nSentinel Missions: - Sentinel-1: Full archive (2014-present) - GRD, SLC, OCN products - Sentinel-2: Level-1C and Level-2A (2015-present) - Sentinel-3: OLCI, SLSTR, SRAL products - Sentinel-5P: TROPOMI atmospheric products - Sentinel-6: Poseidon-4 altimetry\nContributing Missions: - Commercial and partner satellite data - High-resolution imagery for validation\n\n\nAccess Methods\n\n1. Copernicus Browser\nURL: https://browser.dataspace.copernicus.eu\nCapabilities: - Interactive visualization and exploration - Time-slider for temporal analysis - Custom band combinations - Cloud filtering - Compare mode (side-by-side dates) - On-the-fly index calculations (NDVI, NDWI, etc.) - Export images in various formats - No registration required for viewing\n\n\n2. APIs for Programmatic Access\nOData API: - Full-text search - Product metadata retrieval - Download links\nSTAC API (SpatioTemporal Asset Catalog): - Standardized search interface - Integration with modern GIS tools - Cloud-native workflows\nSentinel Hub API: - On-demand processing - Custom script execution - Statistical API for time-series - Batch Processing API for large-scale operations\nopenEO API: - Standardized EO data processing - Cloud-based analysis workflows - Python, R, JavaScript support\n\n\n3. JupyterHub\n\nCloud-based Python notebooks\nPre-configured EO libraries\nDirect access to Sentinel data\n20 GB storage for general users\nScalable computing resources\n\n\n\n4. S3 Interface\n\nDirect cloud storage access\nHigh-performance for large-scale processing\nIntegration with cloud computing platforms\n\n\n\n\nSentiBoard - Real-Time Mission Insights\nSentiBoard is an interactive dashboard providing real-time insights into Sentinel missions:\nAccess: Via Data Availability page on Copernicus Data Space Ecosystem\nInformation Provided: - Current satellite orbital positions - Upcoming acquisition schedules - Data ingestion status and latency - Processing baseline versions - System performance metrics - Planned maintenance windows - Data availability by region\nBenefits: - Plan acquisition dates for your area of interest - Understand data delays during emergencies - Track when new satellites become operational - Monitor data quality indicators\n\n\nRecent CDSE Enhancements (2024)\nSentinel Hub Batch API V2 (December 2024): - Custom tiling grid support - Focus on areas of interest - Optimized processing unit usage - Available to Copernicus Service Level Users\nSentinel-2 On-Demand Production (September 2024): - Generate Level-2A for historical data - Custom processing baseline selection - Fill gaps in archive\n\n\nAlternative Access Methods\nBeyond the Copernicus Data Space, you can access Sentinel data through:\n\nGoogle Earth Engine\nURL: https://earthengine.google.com\nSentinel Collections: - COPERNICUS/S1_GRD - Sentinel-1 Ground Range Detected - COPERNICUS/S2_SR_HARMONIZED - Sentinel-2 Surface Reflectance (harmonized) - COPERNICUS/S2_HARMONIZED - Sentinel-2 Top-of-Atmosphere (harmonized) - COPERNICUS/S3/OLCI - Sentinel-3 Ocean and Land Color\nAdvantages: - No data download required - Parallel processing in the cloud - Massive time-series analysis - JavaScript and Python APIs\nNote: Google Earth Engine will be covered in detail in Session 4.\n\n\nAWS Registry of Open Data\nSentinel-2: https://registry.opendata.aws/sentinel-2/ Sentinel-1: https://registry.opendata.aws/sentinel-1/\nFeatures: - Cloud-Optimized GeoTIFFs (COGs) - Direct S3 access - STAC API via Earth-Search - Integration with AWS services\n\n\nMicrosoft Planetary Computer\n\nSentinel-2 Level-2A in cloud-optimized formats\nIntegrated with Azure cloud services\nPython SDK and STAC API\n\n\n\n\nPre-Flight Checklist\n\n\n\n\n\n\nNoteP1 Improvement: Before Day 1 Hands-On Sessions\n\n\n\nThis checklist should be sent to participants 1 week before training:\n\n\n✅ Accounts & Access: - [ ] Google Earth Engine account enabled (signup.earthengine.google.com) - [ ] Google Drive with ≥5 GB free space - [ ] Google Colab tested (login with same Google account) - [ ] CoPhil Infrastructure registration (application.infra.copphil.philsa.gov.ph)\n✅ Software & Data: - [ ] Downloaded sample vector/raster bundle (link provided via email) - [ ] Confirmed zip file extracts correctly - [ ] Python 3.8+ installed (if working locally) - [ ] Jupyter notebook tested (if working locally)\n✅ Troubleshooting Contacts: - [ ] Have training support email/chat details - [ ] Know how to access Digital Space Campus materials\nIf any issues: Contact training organizers BEFORE Day 1 to resolve access problems!\nOrganizer Action Items: - Create downloadable “sample bundle” (small GeoPackage + COG raster) - Set up support email/chat channel - Send checklist email 7 days before training - Follow up 2 days before to confirm participant readiness\n\nASF DAAC (Alaska Satellite Facility)\nURL: https://search.asf.alaska.edu\nSpecialization: - Sentinel-1 archive - InSAR processing tools - RTC (Radiometric Terrain Correction) products - HyP3 on-demand processing\n\n\n\nPlatform Comparison\n\n\n\n\n\n\n\n\n\nFeature\nCDSE\nGoogle Earth Engine\nAWS\n\n\n\n\nRegistration\nFree\nFree (non-commercial)\nAWS account\n\n\nData Format\nOriginal + COG\nAnalysis-ready\nCOG\n\n\nProcessing\nJupyterHub, APIs\nCode Editor, Python\nUser-managed\n\n\nDownload\nUnlimited\nLimited export\nS3 direct\n\n\nSentinel-1\nFull archive\nGRD (2014+)\nGRD COG\n\n\nSentinel-2\nL1C, L2A\nHarmonized SR, TOA\nL1C, L2A COG\n\n\nBest For\nOfficial access\nLarge-scale analysis\nAWS integration",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#part-5-the-philippine-eo-ecosystem",
    "href": "day1/sessions/session1.html#part-5-the-philippine-eo-ecosystem",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 5: The Philippine EO Ecosystem",
    "text": "Part 5: The Philippine EO Ecosystem\n\nOverview\nThe Philippines is building a robust Earth observation ecosystem with multiple agencies and platforms providing complementary data and services. These local resources enhance Copernicus data with Philippine-specific context, ground truth, and operational applications.\n\n\n\n\n\ngraph TB\n    subgraph Space[\"SPACE SEGMENT\"]\n        S1[Copernicus Sentinels&lt;br/&gt;EU Programme]\n        S2[Diwata-2&lt;br/&gt;PhilSA]\n        S3[MULA 2026&lt;br/&gt;PhilSA/SSTL]\n        S4[Maya CubeSats&lt;br/&gt;STAMINA4Space]\n    end\n\n    subgraph Agencies[\"GOVERNMENT AGENCIES\"]\n        A1[PhilSA&lt;br/&gt;Space Agency]\n        A2[NAMRIA&lt;br/&gt;Mapping Authority]\n        A3[PAGASA&lt;br/&gt;Weather Service]\n        A4[DOST-ASTI&lt;br/&gt;Science Tech]\n        A5[DENR&lt;br/&gt;Environment]\n        A6[DA&lt;br/&gt;Agriculture]\n    end\n\n    subgraph Platforms[\"DATA PLATFORMS\"]\n        P1[SIYASAT&lt;br/&gt;PhilSA Portal]\n        P2[DATOS&lt;br/&gt;DOST Platform]\n        P3[PRiSM&lt;br/&gt;Rice Monitoring]\n        P4[ALaM&lt;br/&gt;Agri Monitor]\n        P5[CoPhil Mirror&lt;br/&gt;2025]\n    end\n\n    subgraph Apps[\"APPLICATIONS\"]\n        AP1[Disaster Response&lt;br/&gt;NDRRMC/OCD]\n        AP2[Climate Adaptation&lt;br/&gt;PAGASA/CCC]\n        AP3[Agriculture&lt;br/&gt;DA/PhilRice]\n        AP4[Forestry&lt;br/&gt;DENR-FMB]\n        AP5[Urban Planning&lt;br/&gt;LGUs/NEDA]\n    end\n\n    S1 -.-&gt;|Free Data| P5\n    S2 -.-&gt;|5m Resolution| P1\n    S3 -.-&gt;|2026 Launch| P1\n\n    A1 --&gt;|Operates| P1\n    A1 --&gt;|Co-chairs| P5\n    A4 --&gt;|Develops| P2\n    A4 --&gt;|Develops| P4\n\n    P1 --&gt;|Imagery| Agencies\n    P2 --&gt;|Processing| Agencies\n    P5 --&gt;|Sentinel Data| Agencies\n\n    Agencies --&gt;|Services| Apps\n\n    style Space fill:#003d7a,stroke:#001f3d,stroke-width:2px,color:#fff\n    style A1 fill:#cc0000,stroke:#880000,stroke-width:2px,color:#fff\n    style P5 fill:#0066cc,stroke:#003d7a,stroke-width:2px,color:#fff\n    style Apps fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff\n\n\n Philippine Earth Observation Ecosystem \n\n\n\n\n\n\nPhilippine Space Agency (PhilSA)\n\n\nPhilSA\n\n\nPhilippine Space Agency\nEstablished: August 8, 2019 (Republic Act No. 11363) Website: https://philsa.gov.ph Email: info@philsa.gov.ph Phone: +632 8568 99 31 Current Office: 29th Floor, Cyber One Building, Eastwood Ave., Quezon City Future HQ: National Space Center, New Clark City, Capas, Tarlac (under construction)\nSix Mission Areas: 1. National Security and Development 2. Hazard Management and Climate Studies 3. Space Research and Development 4. Space Industry Capacity Building 5. Space Education and Awareness 6. International Cooperation\nRole in CoPhil: - Programme co-chair with DOST - Host of Copernicus Mirror Site infrastructure - Provider of training and capacity building\n\n\n\nPhilippine Satellite Program\nDiwata Microsatellites:\nDiwata-1 (PHL-Microsat-1): - Launch: April 27, 2016 (ISS deployment) - Mass: 50 kg - Significance: First microsatellite designed and constructed by Filipinos\nDiwata-2: - Launch: October 29, 2018 (Tanegashima Space Center, Japan) - Mass: 50 kg - Images captured: Over 112,049 (as of 2024) - Philippine coverage: 94.03% (282,088 km² of landmass) - Status: Operational\nMaya CubeSats: - Maya-5 and Maya-6 launched June 5, 2023 to ISS - Part of STAMINA4Space program\nMULA Satellite (Upcoming): - Full Name: Multispectral Unit for Land Assessment - Planned Launch: February 2026 (NET - No Earlier Than) - Launch Vehicle: SpaceX Falcon 9, Transporter-16 - Mass: 130 kg (Philippines’ largest satellite to date) - Resolution: 5-meter TrueColor camera - Swath Width: 120 km - Spectral Bands: 9 bands - Status: Testing phase as of June 2025 - Development Partner: Surrey Satellite Technology Ltd. (SSTL)\nApplications: - Food security monitoring - Disaster resilience - Environment conservation - National security - Land use/land cover change mapping - Crop monitoring - Forestry management\n\n\nSIYASAT Data Portal\nSIYASAT is PhilSA’s secure data archive, visualization, and distribution system for satellite data access.\nFull Name: Surveillance, Identification, and Assessment Using Satellites\nData Sources: - NovaSAR-1 satellite (S-band SAR) - 10% capacity share agreement - Diwata-2 optical imagery - Partner satellite data (KompSat-3, KompSat-5)\nCoverage: - Maritime domain awareness (4 passes per day over Philippines) - Terrestrial monitoring - Coastal zone surveillance\nData Types: - Synthetic Aperture Radar (SAR) imagery - Automatic Identification System (AIS) ship tracking data - Optical high-resolution imagery - Derived products and analytics\nApplications: - Maritime security and illegal fishing monitoring - Port and harbor monitoring - Coastal zone management - Disaster response (floods, landslides) - Environmental monitoring - Agricultural monitoring\nAccess: Through PhilSA official channels, PEDRO Center, and partner agreements. Free for government agencies, LGUs, and state universities/colleges.\n\n\nPEDRO Center (Philippine Earth Data Resource Observation)\nStatus: Currently operated by DOST-ASTI, scheduled for transition to PhilSA\nPurpose: Gateway to satellite images and spaceborne data from Philippine and foreign satellites\nGround Receiving Stations (3 locations):\n\nQuezon City Station\n\nLocation: DOST-ASTI facility, UP Diliman\nAntenna: 3.7 meters satellite tracking\nStatus: First operational station\n\nDavao Station\n\nLaunch: June 30, 2019\nLocation: Francisco Bangoy International Airport\nAntenna: 7.3 meters satellite tracking\nCoverage: Enhanced southern Philippines reception\n\nIloilo Station\n\nLaunch: August 2022\nLocation: Dumangas, Iloilo\nFunding: Japan International Cooperation Agency (JICA)\nCapability: Multi-mission support\n\n\nSatellite Data Access: - Direct download: Diwata-1, Diwata-2, KompSat-3, KompSat-5, NovaSAR-1 - Optical high-resolution and multispectral imagery - Radar (SAR) cloud-penetrating, day-night imaging\n\n\nSpace Data Dashboard\nAccess: https://spacedata.philsa.gov.ph\nDescription: Robust platform designed to revolutionize space data access, built using open-source technologies.\nAvailable Data Types: - Ship traffic monitoring (AIS data) - Air quality indicators - Water quality monitoring - Night lights (urbanization, economic activity) - Land cover mapping - Traffic monitoring\nDevelopment: Jointly developed by PhilSA, DOST-ASTI, and STAMINA4Space during COVID-19 pandemic.\nCapacity Building: Annual Space Data Dashboard Media Workshop training journalists to generate stories using space data.\nAdditional Tool: LEO Satellite Tracker - https://leo-tracker.philsa.gov.ph/tracker\n\n\n2025 PhilSA Initiatives\nPINAS Workshops (PhilSA Integrated Network for Space-Enabled Actions): - April 23-25, 2025: Malolos City, Bulacan - Focus: Disaster Risk Reduction and Management - Topics: Flood mapping, drought assessment, ground motion monitoring, impact assessment\nSpace Business Innovation Challenge (SBIC) 2025: - Empower Filipino innovators - Build solutions using free satellite data - Access to EO, weather, and environmental datasets\nTraining Course on Downstream Data Utilization: - Dates: June 23-27, 2025 - Location: Mandaluyong City - Focus: Practical application of satellite data for end-users\nPhilSA Ad Astra Scholarship Program: - Space science and technology education scholarships - Build national capacity in space sector\n\n\nCOARE Infrastructure\nCOARE (Computing and Archiving Research Environment)\nEstablished: 2014 Location: DOST-ASTI, UP Diliman campus\nTechnical Specifications: - Thousands of CPU cores - GPU acceleration for AI/ML - Several petabytes of storage - 10 Gbps network speed\nServices: 1. High-Performance Computing (HPC) for AI model training 2. Science Cloud for scalable computing 3. Data Archiving for satellite imagery and research data\nAccess Policy: Free for students, researchers, and data analysts\nApplications: - AI model training and inference - Large-scale remote sensing data processing - Climate modeling and disaster simulation\nIntegration: Supports DATOS, SkAI-Pinas, DIMER, and academic research\n\n\n\n\nNAMRIA (National Mapping and Resource Information Authority)\n\n\nNAMRIA\n\n\nNational Mapping and Resource Information Authority\nParent Agency: Department of Environment and Natural Resources (DENR) Established: 1987 Website: https://www.namria.gov.ph Main Portal: https://www.geoportal.gov.ph\nOffice Address: Lawton Avenue, Fort Bonifacio, Taguig City Email: css.gismb@namria.gov.ph, oss@namria.gov.ph Phone: (02) 8887-5466, (02) 8810-4831\nMandate: - National mapping agency - Central repository for geospatial data - Cartographic and hydrographic services - Land administration and resource monitoring support\nRelevance for EO: - Authoritative reference data for validation - Base maps for context and visualization - Hazard maps for risk assessment - Historical land cover data for change detection\n\n\n\nGeoportal Philippines\nMain Portal: https://www.geoportal.gov.ph\nPurpose: Find and access geospatial data and services for strategic planning, decision making, and situational analysis\nApplications: - Disaster risk management - Land use planning - Infrastructure management - Surveying and mapping - Strategic planning\nAvailable Data: - Base Maps: Multiscale topographic maps (1:50,000, 1:250,000) - Administrative Boundaries: Provinces, municipalities, barangays - Infrastructure: Roads, bridges, airports, ports - Thematic Layers: Land cover, geology, soil types - Hazard Information: Links to HazardHunterPH\nFeatures: - Web-GIS mapping tools - Data discovery and catalog - Download services - Feedback system for queries\n\n\nNAMRIA eMapa\nURL: https://isportal.namria.gov.ph/eMapa\nDescription: Interactive mapping application for visualizing NAMRIA datasets with user-friendly interface.\n\n\nLand Cover Mapping Project\nPortal: https://land-cover-mapping-project-namria.hub.arcgis.com\nData Formats Available: - CSV, KML, Zip - GeoJSON - GeoTIFF (raster) - PNG (visualization) - Web services (WMS, WFS)\nCoverage: National-scale land cover datasets for environmental monitoring and planning\nTime Series: Historical and recent land cover classifications\nApplications: - Baseline data for change detection - Training data for satellite image classification - Validation of EO-derived products - Environmental assessment - Policy and planning support\n\n\nHazardHunterPH\nURL: https://hazardhunter.georisk.gov.ph/map\nTagline: “Hazard assessment at your fingertips”\nPurpose: National hazard assessment and visualization portal\nHazard Types Covered: - Earthquake-related: Ground shaking intensity, liquefaction potential - Tsunami: Inundation susceptibility zones - Landslide: Susceptibility maps (high, moderate, low) - Flood: Hazard maps (collaboration with PAGASA)\nIntegration with EO: - Reference hazard zones for satellite-based disaster mapping - Validation of EO-derived flood and landslide extents - Risk-informed prioritization of monitoring areas - Multi-hazard exposure assessment\nUse Case Example: During flood events, overlay Sentinel-1 flood extent maps with HazardHunterPH flood hazard zones to identify high-risk areas requiring immediate response.\n\n\nUsing NAMRIA Data as Training Labels\n\n\n\n\n\n\nTipMicro-Edit: NAMRIA/Space+ as Label Sources & Validation Layers\n\n\n\nNAMRIA data is not just context - it’s a critical source of training labels for machine learning:\nExample Workflow for Day 2 Palawan RF Lab: 1. Download NAMRIA land cover shapefile (authoritative class labels) 2. Overlay on Sentinel-2 imagery in GEE or QGIS 3. Extract training points per class: - Forest (closed canopy, open canopy) - Agriculture (crops, plantations) - Water bodies - Built-up areas - Barren land 4. Train Random Forest classifier on Sentinel-2 bands + indices 5. Validate predictions against NAMRIA hold-out samples from different tiles 6. Generate updated land cover map for current date\nSpace+ Dashboard provides: - Administrative boundaries for stratified sampling - Infrastructure layers for context in visualizations - Overlay capabilities for validation\nOne Example Visualization: NAMRIA training polygons (color-coded by land cover class) overlaid on Sentinel-2 RGB true-color composite - this is what participants will create in Day 2.\n\n\nIntegration Best Practice: Use NAMRIA as baseline truth for training, then detect changes with multi-temporal Sentinel data. This combines authoritative national mapping with frequent satellite updates.\n\n\nRecent Collaborations (2024-2025)\nPhilSA-NAMRIA Partnership (February 10, 2025): - Agreement on satellite data utilization - Focus Areas: - Land cover mapping and monitoring - Topographic mapping - Bathymetric mapping - Coastal resource monitoring - Environmental monitoring - Natural resource mapping - Objectives: Address climate change, biodiversity conservation, scientific research\nDOST-ASTI Collaboration: - Access to COARE computing facilities - Remote sensing and GIS expertise (DATOS) - Satellite imagery from PEDRO Center - Land use/land cover classification projects\n\n\nBathymetric Mapping Initiative\nPartnership: Seabed 2030 Project (announced October 2022)\nObjectives: - Map entire ocean floor by 2030 - Contribute Philippine waters data to GEBCO global grid - Support maritime navigation and ocean research\nData Collection: - Hydrographic surveys of archipelagic waters - Port and harbor surveys (40 locations) - Municipal waters delineation for LGUs\n\n\n\n\nDOST-ASTI AI Initiatives\n\n\nASTI\n\n\nDepartment of Science and Technology - Advanced Science and Technology Institute\nParent Agency: Department of Science and Technology (DOST) Established: R&D institute for advanced S&T applications Website: https://asti.dost.gov.ph\nOffice Address: ASTI Building, C.P. Garcia Avenue, UP Campus, Diliman, Quezon City Phone: +63 2 8426 3572, +63 2 8249 8500 Email: franz.deleon@asti.dost.gov.ph\nFocus: - ICT research and development - Remote sensing and AI applications - Software and systems development - Technology innovation and deployment\nNational Investment: P2.6 billion AI budget (until 2028) supporting government, academia, industry, and over 300 institutions\n\n\n\nSkAI-Pinas (Sky Artificial Intelligence Program)\nFull Name: Philippine Sky Artificial Intelligence Program\nTimeline: - 2017: Foundation laid by Dr. Jose Ildefonso U. Rubrico (DOST Balik Scientist) - 2021: Prototype evolved into flagship R&D initiative - 2024-present: Ongoing expansion with national investment\nFunding: DOST-PCIEERD (Philippine Council for Industry, Energy, and Emerging Technology Research and Development)\nMission: Bridge the gap between massive remote sensing data and sustainable processing frameworks through AI democratization.\nVision: Make AI “part of daily decision-making and national progress” across the Philippines\nImpact: - Supports over 300 institutions: - State universities and colleges - Small and medium enterprises (SMEs) - Research teams - Local government units - Government agencies\nComponents: - AI Knowledge Base: Repository of experts, protocols, best practices - AI Model Repository: Pre-trained models and labeled training images - Training Programs: Capacity building across sectors - Processing Infrastructure: Scalable compute via COARE\nKey Statistics (as of 2024): - Over 1 million manually labeled images - 5 million identified objects in training datasets - More than 200 trained AI models - Supporting decision-making across agriculture, disaster response, environment, transportation\nAnnual Event: SkAI-Pinas Congress (3rd Congress held 2024/2025) showcasing innovations and promoting inclusive AI development\n\n\nDIMER (Democratized Intelligent Model Exchange Repository)\nDeveloper: ASTI-ALaM Project Nature: AI Model Hub for the Philippines\nPurpose: Democratize access to optimized AI models for Filipino researchers, government agencies, and innovators\nDescription: Intuitive “AI model store” designed specifically for Filipino contexts, lowering technical barriers to AI adoption\nKey Features: - Pre-trained AI model repository - Model sharing platform - Quick deployment capabilities - Inference engine for real-time geospatial intelligence - Quality assurance and versioning\nTarget Users: - AI researchers and engineers - Domain experts (agriculture, disaster, environment) - Government planners and disaster risk managers - Enthusiasts and hobbyists\nApplications: - Flood detection from SAR imagery - Landslide detection from optical and radar data - Traffic pattern analysis from high-resolution imagery - Crop health monitoring from multispectral data - Building/infrastructure mapping for urban planning\nHow It Works: 1. Browse model catalog by application domain 2. Select pre-trained model optimized for Philippine conditions 3. Deploy model via AIPI or local environment 4. Eliminate need to train from scratch - accelerates deployment\nImpact: Dramatically reduces time and expertise required to apply AI to Earth observation data\n\n\nAIPI (AI Processing Interface)\nFull Name: AI Processing Interface Developed by: ALaM-LSI (Large-Scale Inference) team Status: Key accomplishment of SkAI-Pinas program\nPurpose: Streamline large-scale remote sensing processing tasks\nCapabilities: - Large-scale satellite image processing - Process entire provinces or regions - Batch inference on AI models from DIMER - Distributed computing coordination across COARE infrastructure - User-friendly interface - No deep coding expertise required - Reduces computational barriers for users without powerful hardware\nHow It Works: 1. Upload satellite imagery (Sentinel, Diwata, etc.) 2. Select AI model from DIMER catalog 3. AIPI orchestrates processing on backend HPC infrastructure 4. Download results (classifications, detections, predictions) 5. Visualize and export geospatial products\nBenefit: Users can perform complex AI-based remote sensing analysis without: - Powerful local computers - Deep programming knowledge - Manual model deployment - Complex infrastructure setup\nIntegration: Seamlessly connected to COARE for compute resources and DIMER for AI models\n\n\nALaM Project (Automated Labeling Machine)\nFull Name: ASTI-Automated Labeling Machine Component of: SkAI-Pinas Program\nWebsite: https://asti.dost.gov.ph/projects/alam-project\nPurpose: Address the training data scarcity bottleneck that limits AI development\nKey Features: - Automated data labeling using AI-assisted annotation - Crowdsourcing capabilities for human-in-the-loop labeling - Quality control mechanisms ensuring label accuracy - Integration with DIMER for model training pipelines\nWhy It Matters: Creating labeled training data is: - Time-consuming (weeks to months) - Expensive (requires expert annotators) - The main bottleneck in AI development\nALaM dramatically reduces this burden through automation and distributed annotation.\nEvolution: - Initial prototype: Automated labeling for satellite images - Expansion: Comprehensive AI development platform - Outputs: DIMER platform, AIPI interface, trained models\nApplication Domains: - Mapping (land cover, infrastructure) - Computer vision (object detection, segmentation) - Disaster response (damage assessment) - Environmental monitoring (deforestation, coastal changes)\n\n\nDATOS (Remote Sensing and Data Science Help Desk)\nFull Name: Remote Sensing and Data Science Help Desk\nWebsite: https://asti.dost.gov.ph/projects/datos\nMission: Produce and communicate relevant disaster information to agencies and end-users to complement existing government efforts\nTechnology Approach: - Integration of GIS, Remote Sensing, and Data Science - AI-powered automated mapping system - Near-real-time disaster response (10-20 minutes from satellite overpass)\nCore Capabilities:\n\nAI-Based Flood Mapping\n\nTechnology: AI-based near-real-time flood extent mapping\nData Sources:\n\nC-Band Sentinel-1 SAR images\nS-Band NovaSAR-1 images (SARwAIS Project)\n\nSpeed: 10-20 minutes from data acquisition to map distribution\nDelivery: Maps sent to DOST Regional Offices and LGUs\nCoverage: Areas affected by severe weather disturbances\n\nProcess: 1. Satellite overpass during typhoon/monsoon 2. SAR image automatically downloaded 3. AI model identifies flood extent 4. Map generated and validated 5. Distributed to disaster response agencies 6. Posted to DATOS dashboard\n2024 Enhancement: NovaSAR-1 integration augments Sentinel-1 for redundancy and improved temporal resolution\n\n\nOther Disaster Mapping Services\n\nLandslide monitoring: Rain-induced susceptibility mapping\nForest fire detection: Active fire and burn scar mapping\nDrought assessment: Vegetation stress analysis\nAgricultural impact: Crop damage evaluation\n\nOperational Model: On-demand analysis responding to agency requests and automatic triggers during disasters\nCase Study: Makilala, Cotabato Landslide Response (2019) - PEDRO Center and DATOS provided timely data to government agencies\nTraining and Capacity Building: - Regional DOST office training (2021-present) - AI and machine learning workshops - Flood mapping technology transfer to LGUs\nPublications: - “Near-Realtime Flood Detection from Multi-Temporal Sentinel Radar Images Using Artificial Intelligence” (2020) - Multiple ISPRS conference papers (2023-2024)\n\n\n\nSARwAIS Project\nFull Name: SAR with Automatic Identification System Start Date: 2018 Duration: Multi-year project\nWebsite: https://asti.dost.gov.ph/space-technology/sarwais\nNovaSAR-1 Satellite Partnership: - Capacity Share: 10% of NovaSAR-1 imaging capacity - Launch: September 2018 - Developer: Surrey Satellite Technology Ltd. (SSTL) - Satellite Type: S-Band SAR minisatellite - Pass Frequency: 4 times per day over Philippines\nKey Capabilities: - Cloud-penetrating S-band radar imaging - Day/night Earth observation - Marine vessel detection via AIS receiver - Simultaneous SAR and AIS acquisition - Flood and disaster mapping - Aquaculture monitoring\nApplications:\nTerrestrial Monitoring: - Land cover classification - Environmental monitoring - Agricultural monitoring (e.g., sugarcane temporal signatures) - Forest change detection\nDisaster Management: - Flood extent mapping (augments Sentinel-1) - Landslide detection - Infrastructure damage assessment\nMaritime Domain: - Ship detection and tracking - Illegal fishing monitoring - Maritime security - Aquaculture monitoring\nStakeholder Training Programs: - Bureau of Fisheries and Aquatic Resources (BFAR) - Maritime Research and Information Center (MRIC) - National Coast Watch Council (NCWC) - Philippine Coast Guard (PCG)\nResearch Publications: - “Augmenting the Philippines’ DOST-ASTI’s Potential Flood Extents Mapping Service with S-Band NovaSAR-1 Images” (2024) - “Advancing the Philippines’ Climate Change Response through DOST-ASTI’s SARwAIS Project” (2024)\n\n\n\n\nPAGASA (Philippine Atmospheric, Geophysical and Astronomical Services Administration)\n\n\nPAGASA\n\n\nPAGASA\nParent Agency: Department of Science and Technology (DOST) Function: National Meteorological and Hydrological Services (NMHS) agency Website: http://www.pagasa.dost.gov.ph\nOffice Address: PAGASA Science Garden Complex, Sen. Miriam Defensor-Santiago Avenue, Brgy. Central, Diliman, Quezon City\nPhone: (02) 8284-0800 Email: information@pagasa.dost.gov.ph, cadpagasa@gmail.com (climate data)\nCore Functions: - Flood and typhoon warnings - Public weather forecasts and advisories - Meteorological, astronomical, and climatological services - Geophysical monitoring - Climate change analysis\n\n\n\nPANaHON (PAGASA National Hydro-Met Observing Network)\nURL: https://www.panahon.gov.ph\nDescription: System showing near real-time information from PAGASA’s weather stations across the country\nStation Types: - Automatic weather stations (AWS) - Manned observation stations\nData Displayed: - Rainfall (hourly, daily accumulation) - Temperature (current, min/max) - Humidity (relative humidity %) - Wind speed and direction - Atmospheric pressure\nAdditional Features: - Weather forecasts using ECMWF data - Historical data access - Station location mapping - Data export capabilities\n\n\nCliMap v2 (Climate Information Map)\nURL: https://www.pagasa.dost.gov.ph/climate/climate-change/dynamic-downscaling/climap-v2\nDescription: Online platform with interactive map for exploring and downloading climate data\nCoverage: Entire Philippines at municipal level\nData Types: - Climate projections (temperature, precipitation) - Historical climate data - Downscaled climate scenarios - Temperature trends - Precipitation patterns - Climate change indicators\nAccess Process: 1. Click on map over desired municipality 2. Select needed information 3. Complete online registration 4. Receive download links via email\n\n\nSatellite Data Usage by PAGASA\nEarth Observation Capabilities: - Weather satellite imagery analysis (Himawari, GOES, others) - Cloud pattern monitoring - Typhoon tracking and intensity estimation - Sea surface temperature monitoring - Precipitation estimation\nIntegration with EO:\nRainfall + SAR flood mapping: - Validate satellite-derived flood extents - Understand precipitation drivers - Improve flood forecasting models\nDrought indices + NDVI time series: - Agricultural drought monitoring - Crop stress early warning - Water resource management\nTyphoon tracks + damage assessment: - Post-disaster prioritization - Impact forecasting - Recovery planning\nClimate data + land cover change: - Climate change attribution studies - Ecosystem vulnerability assessment - Adaptation strategy development\n\n\nClimate Services and Publications\nPublications: - State of the Philippine Climate (annual reports) - 2022 Report available - Comprehensive climate indicators - Impact assessments - Climate assessment reports - Climate change impact studies\nServices: - Climate change projections - Climate risk management tools - Seasonal forecasts - Climate variability analysis (ENSO, monsoon) - Agrometeorological services",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#part-6-cophil-programme---bridging-europe-and-the-philippines",
    "href": "day1/sessions/session1.html#part-6-cophil-programme---bridging-europe-and-the-philippines",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 6: CoPhil Programme - Bridging Europe and the Philippines",
    "text": "Part 6: CoPhil Programme - Bridging Europe and the Philippines\n\nCoPhil Overview\nFull Name: National Copernicus Capacity Support Action Programme for the Philippines (CoPhil)\nLaunch Date: April 24, 2023\nBudget: €10 million (approximately PHP 624 million)\nProgramme Duration: 2023-2027\nSignificance: First Copernicus Earth observation data storing and processing facility in Asia\n\n\n\n\n\n\nImportantCoPhil - A Landmark Cooperation\n\n\n\nCoPhil represents a flagship programme of the EU’s Global Gateway strategy, demonstrating international commitment to space technology for sustainable development. It’s the first space cooperation agreement of its kind in Southeast Asia, positioning the Philippines as a regional hub for Earth observation.\n\n\n\n\nStrategic Objectives\nCoPhil focuses on three core pillars:\n1. Infrastructure Development - Establish Copernicus Data Centre in the Philippines - Provide free, open, and immediate access to Copernicus EO data - Host data focused on Philippine region and Southeast Asia\n2. Service Development - Co-develop Earth Observation pilot services in three thematic areas - Tailor services to Philippine needs (DRR, NRM, CCA) - Integrate with national monitoring systems\n3. Knowledge Transfer - Conduct awareness-raising activities - Provide training and skills development - Offer scholarships for advanced EO studies - Build sustainable local capacity\n\n\nTimeline of Major Milestones\nJanuary 24, 2023: - Contribution Agreement signed in Brussels - ESA Director General Josef Aschbacher and EU Commission representatives - €10 million funding commitment announced\nApril 24, 2023: - Official programme launch in the Philippines - PhilSA, DOST, and EU jointly launch CoPhil\nJune 30, 2023: - Administrative Arrangement signing - PhilSA and European Commission (simultaneous ceremonies in Quezon City and Brussels) - Formalized establishment of Copernicus Mirror Site\nOctober 17, 2024: - “First Light” Inaugural Event in Manila - Unveiling of Copernicus Data Centre at PhilSA premises - First demonstrations of pilot services - Launch of CoPhil Infrastructure website - Moderated by broadcast journalist Atom Araullo\nNovember 2024: - CoPhil Infrastructure website fully operational - First training courses initiated with 80+ participants\n2025: - Full operational capacity of infrastructure - Continuation of training and service development - EU CoPhil Scholarship Programme active\n\n\nInstitutional Framework\nManaging Entities: - European Union Delegation to the Philippines: Overall programme management - European Space Agency (ESA): Technical implementation lead\nPhilippine Government Partners: - Philippine Space Agency (PhilSA): Primary implementing partner, infrastructure host - Department of Science and Technology (DOST): Strategic planning and coordination\nImplementation Consortium: - CloudFerro (Poland): Infrastructure provider, operates Copernicus Mirror Site - Stantec (Belgium): Technical assistance, scholarship programme - Paris Lodron University of Salzburg (Austria): Training and Digital Campus lead - University of Twente ITC (Netherlands): Ground motion service development - GeoVille (Austria): Processing environment development - CLS (France): Service development and transfer lead\n\n\n\nCoPhil Infrastructure (Mirror Site)\nStatus: Operational as of late 2024, reaching full capacity in 2025\nLocation: Philippine Space Agency headquarters, Manila\nPurpose: Philippines-based data center hosting a mirror of Copernicus data focused on the Philippine region\nTechnical Architecture: - Private cloud environment provided by CloudFerro - Fully scalable platform optimized for EO data - Built using effective combination of open-source and specialized software\nCapacity: - Expected storage: 400 TB of resources - State-of-the-art discovery, cataloguing, and serving tools - Support for 8 satellite types - 5+ Earth Observation data collections - Daily updates from Sentinel constellation\nCoverage: - Entire territory of the Philippines - Islands and offshore areas - Over 1.8 million square kilometers total coverage - Southeast Asian regional context\nBenefits:\nFaster Access: - Local hosting reduces latency - Improved download speeds for Philippine users - Reduced dependence on international bandwidth\nDisaster Resilience: - Local backup during regional crises - Reliable access during emergencies - Critical for rapid disaster response\nCapacity Building: - Local expertise in data management - Philippine control of infrastructure - Foundation for regional leadership\nCost Savings: - Free access for Philippine institutions - Reduced international data transfer costs - Enables larger-scale processing\n\n\nAccess Methods\n\n1. Data Explorer\nURL: https://explore.infra.copphil.philsa.gov.ph/search\nCapabilities: - User-friendly search and browsing - Browse Copernicus EO data by attributes: - Geographic area (map selection or coordinates) - Date range - Satellite/sensor type - Cloud cover percentage - Processing level - Preview thumbnails and quicklooks - Download selected products - Bulk download via data packages\n\n\n2. JupyterLab Environment\nURL: https://jupyter.infra.copphil.philsa.gov.ph\nFeatures: - Web-based interactive development environment - Process Copernicus EO data using Python, Julia, R - Access to Copernicus Data Space Ecosystem data - Pre-configured with EO libraries (rasterio, geopandas, xarray) - Machine learning libraries (scikit-learn, tensorflow) - Integrated analysis and visualization tools\n\n\n3. S3/Zipper Service\n\nCloud data access via object storage (S3 protocol)\nZipped download interfaces for bulk data\nEfficient transfer for large datasets\nAPI access for automated workflows\n\n\n\n\nRegistration and Access\nRegistration Portal: https://application.infra.copphil.philsa.gov.ph\nAccess Levels: - copphil-registered role: Immediate minimal access upon registration - eodata-low role: Basic data access - Organization-level access: Additional quotas and roles\nCurrent Status (late 2024): - 184 registered users - Growing community across government, academia, research - Multiple organizations actively using platform\nDocumentation: Knowledge Base: https://knowledgebase.infra.copphil.philsa.gov.ph\n\n\n\nCoPhil Digital Space Campus\nPlatform URL: https://courses.copphil.philsa.gov.ph\nPurpose: Comprehensive online training platform for Earth Observation capacity building\nManagement: Paris Lodron University of Salzburg (PLUS) leads knowledge and capacity development\nTraining Approach: Mix of online self-paced and hybrid (online + on-site) courses\n\n\nCoPhil Pilot Services\nCoPhil is co-developing three operational pilot services tailored to Philippine priorities:\n\n1. Ground Motion Monitoring Service\nObjective: Improve hazard management, reduce risks, support sustainable urban development\nTechnology: - InSAR (Interferometric SAR) processing using Sentinel-1 data - Open-access tools integrated with GeoVille processing environment - Time-series analysis for deformation trends\nApplications: 1. Seismic Activity Monitoring - Context: 5-20 earthquakes daily in the Philippines - Precise deformation from earthquakes - Early warning support - Post-earthquake damage assessment\n\nVolcanic Deformation\n\nContext: 53 active volcanoes in the Philippines\nGround swelling detection before eruptions\nEvacuation planning support\n\nLandslide Risk Assessment\n\nSlow-moving landslide detection\nRisk mapping for vulnerable areas\n\nLand Subsidence Monitoring\n\nUrban subsidence detection\nGroundwater extraction impacts\nMining-induced subsidence\n\nMining Area Monitoring\n\nGround stability assessment\nIllegal mining detection\nEnvironmental impact monitoring\n\n\nTarget Users: LGUs, urban planners, DOST, PHIVOLCS, mining agencies, infrastructure developers\n\n\n2. Land Cover, Forest, and Crop Mapping Service\nObjective: Support local authorities, farmers, and environmental managers in sustainable land use and agricultural productivity\nTechnology: - Multi-temporal Sentinel-1 SAR and Sentinel-2 optical - Sen4Stat processor for crop type mapping - Machine learning classification - Vegetation indices time series\nSub-Services:\nA. Crop Monitoring and Mapping - Crop type identification and mapping - Crop growth monitoring throughout season - Yield prediction and estimation - Irrigation monitoring - Products: Seasonal crop maps, biophysical parameters, vegetation indices\nB. Forest Monitoring - Forest area and type mapping - Tree cover density assessment - Deforestation monitoring - Illegal logging detection - Reforestation planning and monitoring\n2024 Products Developed: - Consolidated Forest Area and Type product - Tree Cover Density maps - Forest Cover Change product (near completion)\nC. Land Cover Classification - National and regional mapping - Urban expansion monitoring - Land use planning - Environmental monitoring\nTarget Users: Department of Agriculture, farmers, DENR, forest agencies, LGUs, environmental researchers\n\n\n3. Coastal Marine (Benthic) Habitat Monitoring Service\nObjective: Support sustainable coastal management, preserve biodiversity, maintain marine habitat health\nTechnology: - Sentinel-2 and Sentinel-3 optical data for coastal waters - Benthic habitat classification algorithms - Water quality indicators - Coastal zone change detection\nApplications: 1. Coral Reef Monitoring - Coral bleaching detection and tracking - Reef health assessment - Climate change impact monitoring\n\nSeagrass Mapping\n\nExtent and density mapping\nHealth indicators\nChange detection\n\nWater Quality Monitoring\n\nTurbidity and suspended sediment\nAlgal bloom detection\nPollution monitoring\n\nCoastal Zone Management\n\nShoreline change detection\nErosion monitoring\nMangrove mapping\n\nMarine Protected Areas\n\nBaseline habitat mapping\nProtection effectiveness monitoring\n\n\nTarget Users: Marine researchers, DENR, coastal management authorities, MPA managers, fisheries agencies\n\n\n\nTraining Series on Digital Campus\nThe platform offers specialized training across three main thematic areas:\n1. Ground Motion Monitoring Series - InSAR principles and SAR processing - Ground deformation analysis - Applications in geo-hazard assessment - Urban planning and emergency preparedness\n2. Land Cover, Forest, and Crop Monitoring Series - Introduction to Satellite Data and EO - Sen4Stat Training for crop monitoring - Crop Monitoring Practical Training - Forest Monitoring principles and applications - Land Cover Classification workflows\n3. Benthic (Coastal Marine) Habitat Monitoring Series - Coastal zone dynamics - Marine habitat classification - Coral reef and seagrass monitoring - Water quality assessment\nCourse Structure: - Part I - Fundamentals: Theoretical concepts - Part II - Practical: Hands-on with tools and data - Part III - Application: Real-world use cases\nMajor Training Events (2024):\nNovember 25, 2024 - First Training Course: - Hosted at CoPhil Centre - Over 80 participants from government agencies, academia, other sectors - Starting point for ten-course training series - Positive feedback from participants\n\n\nEU CoPhil Scholarship Programme\nLaunch: Late 2024 Implementation: Stantec-led consortium\nFinancial Support: Up to €33,500 (approximately PHP 2,072,712) per scholarship\nStudy Duration: September 2025 to July 2027 (approximately 2 years)\nStudy Level: Master’s degree (MSc) in Earth Observation and Remote Sensing\nStudy Location: European universities in EU Member States\nEligibility: Filipino citizens working in specific government agencies: - Philippine Space Agency (PhilSA) - Department of Science and Technology (DOST) - Department of Environment and Natural Resources (DENR) - Department of Human Settlements and Urban Development (DSHUD) - Department of Agriculture - Mindanao Development Authority (MinDA)\nApplication Deadline: March 21, 2025\nObjective: Develop advanced Philippine expertise in EO, create pool of specialists, strengthen institutional capacity for long-term sustainability\n\n\nSuccess Stories and Impact\n\nCase Study: MT Terra Nova Oil Spill Response (July 2024)\nIncident: - Philippine tanker MT Terra Nova carrying 1.4 million liters of industrial fuel sank in Manila Bay on July 25, 2024 - Occurred during Typhoon Gaemi (Typhoon Carina) - Significant environmental emergency\nCoPhil Response: - Copernicus satellites activated from Europe - Rapid Sentinel-1 SAR data acquisition - All-weather monitoring during typhoon conditions - Data provided to Philippine authorities\nCollaboration: - DENR, PhilSA, UP Marine Science Institute, other agencies\nOutcomes: - Oil spill extent mapped - Trajectory modeling for cleanup - Support for containment operations - First major real-world application demonstrating CoPhil’s operational readiness\n\n\nKey Achievements (2024)\nInfrastructure: - First Copernicus facility in Asia operational - 400 TB capacity, 184 registered users - Support for 8 satellite types\nService Development: - Forest monitoring products developed - Land cover and crop services demonstrated - Ground motion InSAR chain established\nCapacity Building: - 80+ professionals trained in first cohort - Digital Campus platform launched - Hybrid training model proven - Growing EO user community\nRecognition: - Featured at UN-SPIDER as model cooperation programme - Presented at Asia-Pacific DRR conferences - Highlighted in EU Global Gateway communications",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#part-7-synergies---combining-european-and-philippine-eo-data",
    "href": "day1/sessions/session1.html#part-7-synergies---combining-european-and-philippine-eo-data",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Part 7: Synergies - Combining European and Philippine EO Data",
    "text": "Part 7: Synergies - Combining European and Philippine EO Data\n\nWhy Combine Data Sources?\nEuropean Copernicus data provides: - Global coverage and consistency - High temporal frequency (5-6 day revisit) - Consistent data quality and standards - Long-term archives (2014/2015-present) - Free and open access policy - Multiple spectral and sensor types\nPhilippine platforms provide: - National reference datasets (base maps, boundaries) - Local hazard information and risk maps - Ground truth and validation data - Operational AI models tuned for Philippine conditions - Weather and climate contextual data - Filipino language support and localized services - Rapid response infrastructure (DATOS)\n\n\nIntegrated Workflow Examples\n\nExample 1: Flood Mapping After Typhoon\nData Integration: 1. PAGASA rainfall data → Understand precipitation drivers and intensity 2. Sentinel-1 SAR (via CoPhil) → Detect flood extent under clouds 3. DOST-ASTI DATOS AI → Rapid processing (10-20 minutes) 4. NAMRIA HazardHunterPH → Identify pre-existing high-risk zones 5. NAMRIA Geoportal → Administrative boundaries and infrastructure context\nWorkflow: 1. PAGASA issues typhoon warning with rainfall forecast 2. Sentinel-1 automatically acquired during and after typhoon 3. DATOS AI processes SAR data, extracts flood extent 4. Overlay with HazardHunterPH flood hazard zones 5. Identify affected barangays using NAMRIA boundaries 6. Deliver maps to NDRRMC, DOST ROs, LGUs within hours\nValue: Rapid, science-based disaster response with multi-source validation\n\n\nExample 2: Land Cover Change Detection\nData Integration: 1. Sentinel-2 time series (via CoPhil) → Multi-temporal optical imagery 2. NAMRIA land cover basemap → Reference classification for baseline 3. SkAI-Pinas DIMER models → Apply pre-trained classification model 4. AIPI processing → Process large areas efficiently 5. COARE infrastructure → High-performance computing\nWorkflow: 1. Download Sentinel-2 time series for province from CoPhil 2. Load NAMRIA land cover as training reference 3. Select land cover classification model from DIMER 4. Deploy via AIPI for batch processing 5. Validate changes against NAMRIA basemap 6. Deliver updated land cover maps to DENR and LGUs\nValue: Scalable, cost-effective national mapping with local validation\n\n\nExample 3: Agricultural Drought Monitoring\nData Integration: 1. Sentinel-2 NDVI time series (CoPhil) → Vegetation health indicator 2. PAGASA rainfall and SPEI → Meteorological drought indices 3. SkAI-Pinas models → Predict crop stress levels 4. DOST agencies → Deliver alerts to DA and LGUs\nWorkflow: 1. Calculate NDVI anomaly from Sentinel-2 (current vs. historical average) 2. Integrate PAGASA rainfall deficit data 3. Apply SkAI-Pinas crop stress model 4. Generate drought severity map by municipality 5. Issue early warning to Department of Agriculture 6. Support targeted agricultural assistance programs\nValue: Early warning enables proactive intervention, reduces crop losses\n\n\nExample 4: Coral Reef Health Monitoring\nData Integration: 1. Sentinel-2 coastal imagery (CoPhil) → Shallow water habitat 2. CoPhil Benthic Habitat pilot service → Classification algorithms 3. NAMRIA coastal basemaps → Reef locations and boundaries 4. PhilSA Diwata-2 imagery → Higher resolution validation\nWorkflow: 1. Acquire Sentinel-2 Level-2A over coral reef areas 2. Apply CoPhil benthic habitat classification 3. Detect bleaching events through spectral change 4. Validate with NAMRIA reef maps and Diwata-2 imagery 5. Track recovery over time 6. Support DENR marine protected area management\nValue: Cost-effective long-term reef monitoring at national scale",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#activity-exploring-the-data-platforms",
    "href": "day1/sessions/session1.html#activity-exploring-the-data-platforms",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Activity: Exploring the Data Platforms",
    "text": "Activity: Exploring the Data Platforms\n\n\n\n\n\n\nTipHands-On Exploration (15 minutes)\n\n\n\nTask: Navigate to the following platforms and explore their interfaces:\n1. Copernicus Data Space Ecosystem - Go to https://dataspace.copernicus.eu - Explore the Browser - Find the SentiBoard (Data Availability page) - Search for a recent Sentinel-2 image over the Philippines - Note the three-satellite constellation status\n2. CoPhil Infrastructure - Visit https://copphil.philsa.gov.ph - Explore “About” section - Check “First Light” event highlights - Browse pilot services descriptions\n3. PhilSA Website - Visit https://philsa.gov.ph - Explore Space Data Dashboard - Read about MULA satellite - Check recent news and training opportunities\n4. NAMRIA Geoportal - Go to https://www.geoportal.gov.ph - Browse available map layers - Visit HazardHunterPH - Explore hazard maps for your province\n5. DOST-ASTI - Visit https://asti.dost.gov.ph - Read about SkAI-Pinas, DIMER, and AIPI initiatives - Explore DATOS project page - Check COARE infrastructure description\nDiscussion Questions: - What types of data or services are most relevant to your work? - How could you combine Copernicus data with Philippine platforms? - Which training courses on CoPhil Digital Campus interest you most?",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#operational-cautions",
    "href": "day1/sessions/session1.html#operational-cautions",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Operational Cautions",
    "text": "Operational Cautions\n\n\n\n\n\n\nWarning“Don’t Do This” - Common Pitfalls (P2 Quality & Governance)\n\n\n\nPrevent common mistakes that will waste time and produce poor results:\n\n\n\nModel Applicability\n❌ Don’t apply a Palawan RF land cover model to Mindanao without re-sampling - Why: Different climate zones, vegetation types, rainfall seasonality, and soil properties - Do: Collect local training samples from target region (Mindanao) or use transfer learning with local fine-tuning\n❌ Don’t use un-calibrated SAR (raw digital numbers) - Why: Digital numbers are sensor-dependent and meaningless for quantitative analysis or comparison - Do: Always calibrate to σ⁰ (sigma-naught) or γ⁰ (gamma-naught) in decibels\n\n\nProcessing Assumptions\n❌ Don’t skip terrain correction (RTC) for SAR in mountainous areas - Why: Topographic distortions create false changes; slopes appear brighter/darker regardless of surface properties - Do: Apply Radiometric Terrain Correction (RTC) using SRTM 30m DEM or better (ALOS DEM)\n❌ Don’t mix Sentinel-2 processing baselines without harmonization - Why: DN offset of +1000 after January 25, 2022 (Processing Baseline 04.00) breaks time series analysis - Do: Use HARMONIZED collections in Google Earth Engine (COPERNICUS/S2_SR_HARMONIZED)\n❌ Don’t use QA60 for cloud masking (use SCL instead) - Why: QA60 misses cloud shadows; leads to “ghost” clouds in composites and contaminated NDVI - Do: Use Scene Classification Layer (SCL) to mask clouds, shadows, and cirrus\n\n\nEvaluation & Validation\n❌ Don’t train and test on the same geographic tile or province - Why: Inflated accuracy metrics; model doesn’t generalize to new areas - Do: Use geographic hold-out - train on tiles from western Luzon, test on eastern Luzon or Visayas\n❌ Don’t rely solely on overall accuracy for land cover classification - Why: Class imbalance (e.g., 90% forest) inflates accuracy; rare classes perform poorly - Do: Report per-class precision/recall, F1-scores, and area-weighted accuracy; use stratified sampling\n❌ Don’t assume 10m Sentinel-2 resolution is sufficient for building detection - Why: Individual buildings are often sub-pixel at 10m; yields poor mAP scores - Do: Frame as “built-up / settlement extent detection” or use higher-resolution data for true building footprints\n\n\nData-Centric Best Practices\n✅ Always check: - Data provenance (where did training labels come from?) - Temporal alignment (satellite date matches ground truth collection date?) - Class balance (are minority classes adequately sampled?) - Spatial autocorrelation (are train/test points truly independent?)\n✅ Document: - Processing steps applied (calibration, filtering, atmospheric correction) - Training data sources and collection methods - Known limitations and appropriate use cases",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#key-takeaways",
    "href": "day1/sessions/session1.html#key-takeaways",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nImportantSession 1 Summary\n\n\n\nCopernicus Programme: 1. World’s leading Earth observation programme with free Sentinel data globally 2. Sentinel-1C launched December 2024, restoring dual-satellite constellation 3. Sentinel-2 three-satellite configuration (2025): 2C operational, 2B operational, 2A experimental trial 4. Copernicus Data Space Ecosystem is the 2025 unified access portal with SentiBoard monitoring\nSentinel-1 SAR: - C-band radar sees through clouds and at night - essential for tropical disaster monitoring - 6-day revisit with two satellites, 250 km swath (IW mode) - GRD products for change detection, SLC for InSAR deformation - Applications: Flood mapping, deforestation, ground motion, maritime surveillance\nSentinel-2 Optical: - 13 spectral bands (VIS-NIR-SWIR) at 10-60 m resolution - 5-day revisit with improved temporal coverage from three satellites - Level-2A surface reflectance recommended (ARD-compliant) - Red edge bands unique for vegetation stress detection - Applications: Land cover, agriculture, forest monitoring, coral reefs\nPhilippine EO Ecosystem: - PhilSA: SIYASAT portal, Space Data Dashboard, MULA satellite (2026), COARE HPC - NAMRIA: Geoportal Philippines, HazardHunterPH, Land Cover Mapping, bathymetry - DOST-ASTI: SkAI-Pinas, DIMER (AI models), AIPI (processing), ALaM (labeling), DATOS (disaster response) - PAGASA: PANaHON weather data, CliMap climate projections, typhoon tracking - P2.6 billion DOST AI investment (until 2028) supporting 300+ institutions\nCoPhil Programme: - €10 million EU-Philippines cooperation (2023-2027) - First Copernicus mirror site in Asia - operational late 2024 - 400 TB capacity, 184 users, free access for Philippine institutions - Three pilot services: Ground motion (InSAR), Land/forest/crop mapping, Benthic habitats - Digital Space Campus: Comprehensive training platform - EU Scholarship Programme: Master’s degrees in EO for government personnel - Success: MT Terra Nova oil spill response (July 2024) demonstrated operational capability\nSynergies: - Combining European and Philippine data creates powerful DRR, CCA, and NRM applications - Integrated workflows leverage global coverage with local context and rapid response - CoPhil infrastructure enables local storage, processing, and capacity building\nYour Next Steps: - Register for CoPhil Infrastructure access - Explore CoPhil Digital Campus training courses - Consider EU CoPhil Scholarship Programme (if eligible) - Engage with DOST-ASTI initiatives (SkAI-Pinas, DIMER, DATOS) - Integrate Copernicus data into your operational workflows",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session1.html#further-reading-and-resources",
    "href": "day1/sessions/session1.html#further-reading-and-resources",
    "title": "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem",
    "section": "Further Reading and Resources",
    "text": "Further Reading and Resources\n\nCopernicus Programme\n\nCopernicus Programme Overview\nCopernicus Data Space Ecosystem\nSentinel Online\nSentinel-1 User Handbook\nSentinel-2 User Handbook\nESA Earth Online\n\n\n\nCoPhil Programme\n\nCoPhil Main Portal\nCoPhil Infrastructure\nCoPhil Digital Campus\nESA CoPhil Announcement\n\n\n\nPhilippine EO Resources\n\nPhilSA Official Website\nPhilSA Space Data Dashboard\nNAMRIA Geoportal Philippines\nNAMRIA HazardHunterPH\nDOST-ASTI Website\nDOST-ASTI DATOS Project\nPAGASA Website\nPAGASA PANaHON\n\n\n\nTechnical Documentation\n\nSentinel-1 Product Specification\nSentinel-2 Product Specification\nSen2Cor Atmospheric Correction\nGoogle Earth Engine Sentinel Data Catalog",
    "crumbs": [
      "Sessions",
      "Session 1: Copernicus Sentinel Data Deep Dive & Philippine EO Ecosystem"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html",
    "href": "day1/sessions/session3.html",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "",
    "text": "Home › Day 1 › Session 3",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#session-overview",
    "href": "day1/sessions/session3.html#session-overview",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Session Overview",
    "text": "Session Overview\nThis hands-on session teaches you how to work with geospatial data in Python - the foundation of Earth Observation workflows. Python has become the dominant language for EO and ML applications due to its rich ecosystem of libraries, active community, and seamless integration with operational systems. You’ll use Google Colab (no installation needed!), learn vector data operations with GeoPandas, and master raster processing with Rasterio. By the end, you’ll understand why Python powers Philippine EO platforms like DATOS, SkAI-Pinas, and PRiSM, and you’ll be able to prepare satellite data for AI/ML workflows.\n\n\nLearning Objectives\nBy the end of this session, you will be able to:\n\nExplain why Python is the dominant language for EO and ML applications\nSet up Google Colaboratory for geospatial analysis\nMount Google Drive for data access and storage\nInstall Python geospatial libraries (GeoPandas, Rasterio)\nRead and inspect vector data (shapefiles, GeoJSON) with GeoPandas\nTransform coordinate reference systems for Philippine UTM zones\nQuery and filter geospatial DataFrames spatially and by attributes\nVisualize vector data with static and interactive maps\nOpen and examine raster files and their metadata with Rasterio\nRead raster bands into NumPy arrays efficiently\nCalculate spectral indices (NDVI, EVI, NDWI) from Sentinel-2 bands\nCreate RGB and false-color composites from multispectral imagery\nClip rasters to vector boundaries for area-of-interest analysis\nSample raster values at point locations for validation\nApply preprocessing workflows for ML-ready data\nCombine vector and raster operations for complete EO workflows",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#presentation-slides",
    "href": "day1/sessions/session3.html#presentation-slides",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Presentation Slides",
    "text": "Presentation Slides",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#part-1-why-python-for-earth-observation",
    "href": "day1/sessions/session3.html#part-1-why-python-for-earth-observation",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 1: Why Python for Earth Observation?",
    "text": "Part 1: Why Python for Earth Observation?\n\nThe Python EO Ecosystem\nPython has become the de facto standard for Earth Observation and Machine Learning. Understanding why helps you leverage the right tools for your projects.\n\nKey Advantages\n\n\n\n\n\ngraph TB\n    subgraph DataAccess[\"DATA ACCESS\"]\n        DA1[Earth Engine&lt;br/&gt;Python API]\n        DA2[Sentinel Hub&lt;br/&gt;sentinelhub-py]\n        DA3[NASA CMR&lt;br/&gt;earthaccess]\n        DA4[STAC&lt;br/&gt;pystac-client]\n    end\n\n    subgraph Geospatial[\"GEOSPATIAL PROCESSING\"]\n        GP1[GeoPandas&lt;br/&gt;Vector data]\n        GP2[Rasterio&lt;br/&gt;Raster I/O]\n        GP3[Xarray&lt;br/&gt;N-D arrays]\n        GP4[Rioxarray&lt;br/&gt;Raster+Xarray]\n        GP5[GDAL/OGR&lt;br/&gt;Low-level ops]\n    end\n\n    subgraph DataScience[\"DATA SCIENCE\"]\n        DS1[NumPy&lt;br/&gt;Array operations]\n        DS2[Pandas&lt;br/&gt;Tabular data]\n        DS3[SciPy&lt;br/&gt;Scientific computing]\n        DS4[Dask&lt;br/&gt;Parallel computing]\n    end\n\n    subgraph ML[\"MACHINE LEARNING\"]\n        ML1[Scikit-learn&lt;br/&gt;Traditional ML]\n        ML2[TensorFlow&lt;br/&gt;Deep Learning]\n        ML3[PyTorch&lt;br/&gt;Deep Learning]\n        ML4[TorchGeo&lt;br/&gt;Geospatial DL]\n    end\n\n    subgraph Viz[\"VISUALIZATION\"]\n        VZ1[Matplotlib&lt;br/&gt;Static plots]\n        VZ2[Folium&lt;br/&gt;Interactive maps]\n        VZ3[Plotly&lt;br/&gt;Interactive viz]\n        VZ4[Cartopy&lt;br/&gt;Map projections]\n    end\n\n    DataAccess --&gt; Geospatial\n    Geospatial --&gt; DataScience\n    DataScience --&gt; ML\n    Geospatial --&gt; Viz\n    ML --&gt; Viz\n\n    style DataAccess fill:#e6f3ff,stroke:#0066cc,stroke-width:2px\n    style Geospatial fill:#e6ffe6,stroke:#00aa44,stroke-width:2px\n    style DataScience fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style ML fill:#ffe6e6,stroke:#cc0044,stroke-width:2px\n    style Viz fill:#f0e6ff,stroke:#6666cc,stroke-width:2px\n\n\n Python Earth Observation Ecosystem \n\n\n\n1. Rich Library Ecosystem - Geospatial Processing: GeoPandas, Rasterio, Xarray, Rioxarray, GDAL bindings - Data Science: NumPy, Pandas, SciPy for array operations and analysis - Machine Learning: Scikit-learn, TensorFlow, PyTorch, TorchGeo - Visualization: Matplotlib, Seaborn, Folium, Plotly for static and interactive maps\n2. Seamless Integration - Cloud Platforms: Google Earth Engine Python API, AWS, Azure - Big Data: Dask for parallel/distributed computing on large rasters - Workflows: Easy integration between data access → preprocessing → ML → visualization\n3. Active Community - Extensive documentation and tutorials - Stack Overflow, GitHub discussions, forums - Open-source contributions and rapid bug fixes - Jupyter notebooks for reproducible research\n4. Industry and Research Standard - NASA, ESA, USGS use Python for operational systems - Scientific papers increasingly publish Python code - Enterprise solutions (Planet Labs, Descartes Labs) built on Python\n\n\n\nPython in Philippine EO Systems\nDATOS (DOST-ASTI) - Python backend for automated processing - Integrates Sentinel data access and analysis - Generates data products for government use\nPRiSM (PhilRice/IRRI) - Python workflows for SAR-based rice monitoring - Combines Sentinel-1, field data, and crop models - Operational since 2014\nSkAI-Pinas (PhilSA) - Python-based AI/ML training platform - Pre-configured with GeoPandas, Rasterio, TensorFlow - Jupyter notebooks for capacity building\nALaM Project (DOST) - Python for agricultural land monitoring - Automated crop classification pipelines - Integration with local government data systems\nKey Takeaway: Learning Python for EO opens doors to operational systems, not just research.\n\n\nConnecting to Session 2: Preprocessing for ML\nIn Session 2, you learned that data preprocessing is critical for ML success:\n\nAtmospheric correction: Sen2Cor converts Sentinel-2 Level-1C → Level-2A\nNormalization: Scaling pixel values for neural networks\nCloud masking: Removing contaminated observations\nData augmentation: Rotation, flipping, cropping\n\nThis session provides the Python skills to implement these preprocessing steps for your own projects. By the end, you’ll prepare analysis-ready data for the ML models in Sessions 5-7.",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#part-2-setting-up-google-colaboratory",
    "href": "day1/sessions/session3.html#part-2-setting-up-google-colaboratory",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 2: Setting Up Google Colaboratory",
    "text": "Part 2: Setting Up Google Colaboratory\n\nWhat is Google Colab?\nGoogle Colaboratory (Colab) is a free cloud-based Jupyter notebook environment that allows you to:\n\nWrite and execute Python code in your browser\nAccess free GPU/TPU for machine learning\nCollaborate with others in real-time\nSave notebooks to Google Drive\nNo local installation required!\n\nPerfect for this training: Everyone has the same environment, no dependency issues, accessible from anywhere.\n\n\n\n\n\n\nTipWhy Colab for Philippine Trainees?\n\n\n\n\nNo hardware requirements: Works on any laptop/computer with a browser\nInternet connectivity: Even with limited bandwidth, code runs on Google’s servers\nFree GPU access: Train deep learning models without expensive hardware\nReproducibility: Notebooks can be shared and rerun by collaborators\nCOARE compatibility: Skills transfer to DOST’s COARE HPC environment\n\n\n\n\n\nCreating Your First Colab Notebook\n\n\n\n\n\n\nNoteAccess Colab\n\n\n\nURL: https://colab.research.google.com\nRequirements: - Google account - Modern web browser (Chrome, Firefox, Safari, Edge) - Stable internet connection\n\n\nSteps:\n\nGo to colab.research.google.com\nSign in with your Google account\nClick File → New Notebook\nRename: File → Rename → “Day1_Session3_Geospatial_Python”\n\n\n\nUnderstanding the Colab Interface\nKey components:\n\nCode cells: Where you write Python code (click or press Enter to edit)\nText cells: Markdown for documentation (Insert → Text cell)\nRun button: ▶ Execute current cell (or press Shift+Enter)\nRuntime menu: Manage execution environment (restart, change runtime type)\nTable of Contents: Navigate long notebooks (left sidebar icon)\n\nTry it: Create a code cell and run:\nprint(\"Hello from Google Colab!\")\nprint(\"This is the CoPhil EO AI/ML Training\")\n\n# Check Python version\nimport sys\nprint(f\"Python version: {sys.version}\")\nPress Shift+Enter to execute. You should see the output below the cell.\n\n\nConnecting Google Drive\nWhy mount Google Drive?\n\nAccess data files stored in Drive\nSave outputs permanently (Colab sessions are temporary!)\nShare data with collaborators\nStore trained models and results\n\nMount Drive:\nfrom google.colab import drive\ndrive.mount('/content/drive')\nWhat happens:\n\nClick the link that appears\nSelect your Google account\nClick “Allow” to grant access\nCopy the authorization code\nPaste into the input field and press Enter\n\nVerification:\nimport os\nos.listdir('/content/drive/MyDrive')\nYou should see your Google Drive folders listed!\n\n\n\n\n\n\nTipOrganizing Your Data\n\n\n\nCreate a folder structure in Google Drive for this training:\nMyDrive/\n  CoPhil_Training/\n    data/\n      vector/          # Shapefiles, GeoJSON (Philippine boundaries, AOIs)\n      raster/          # Satellite imagery (Sentinel-2 tiles, NDVI, etc.)\n    outputs/           # Processed results (clipped rasters, classification maps)\n    notebooks/         # Saved Colab notebooks\nPro Tip: Use descriptive filenames with dates: palawan_sentinel2_20240615.tif\nThis keeps your training materials organized and makes paths easier to reference in code.\n\n\n\n\nUnderstanding Colab Runtime\nRuntime Types:\n\nCPU: Default, free, sufficient for most geospatial tasks\nGPU: Free, excellent for deep learning (Sessions 5-7)\nTPU: Free, specialized for TensorFlow models\n\nSession Limits:\n\nIdle timeout: 90 minutes of inactivity\nMaximum runtime: 12 hours continuous execution\nDisk space: ~100 GB temporary storage\n\nBest Practices:\n# Periodically save important results to Drive\nimport shutil\nshutil.copy('important_result.tif', '/content/drive/MyDrive/CoPhil_Training/outputs/')",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#part-3-installing-geospatial-libraries",
    "href": "day1/sessions/session3.html#part-3-installing-geospatial-libraries",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 3: Installing Geospatial Libraries",
    "text": "Part 3: Installing Geospatial Libraries\n\nRequired Libraries\nGoogle Colab comes with many libraries pre-installed (NumPy, Pandas, Matplotlib), but specialized geospatial tools need installation.\nCore libraries we’ll use:\n\n\n\n\n\n\n\n\nLibrary\nPurpose\nWhy Important\n\n\n\n\nGeoPandas\nVector data (shapefiles, GeoJSON, polygons, points)\nSpatial operations, CRS transforms, easy plotting\n\n\nRasterio\nRaster data (GeoTIFF, satellite imagery)\nRead/write rasters, metadata, windowed reading\n\n\nShapely\nGeometric operations (included with GeoPandas)\nCreate/manipulate geometries, spatial predicates\n\n\nMatplotlib\nVisualization\nPublication-quality figures, customizable plots\n\n\nNumPy\nArray operations (pre-installed)\nFoundation for all numerical computing\n\n\n\n\n\nInstallation\nRun this cell (may take 1-2 minutes):\n# Install geospatial libraries\n!pip install geopandas rasterio fiona shapely pyproj -q\n\nprint(\"Installation complete! ✓\")\nThe -q flag makes installation quiet (less output).\nWhat’s being installed:\n\ngeopandas: Main vector library (also installs Pandas)\nrasterio: Main raster library (also installs GDAL bindings)\nfiona: File I/O for vector formats (dependency of GeoPandas)\nshapely: Geometric operations (dependency of GeoPandas)\npyproj: Coordinate reference system transformations\n\nIf you see warnings: Usually safe to ignore. If errors occur, try:\n!pip install --upgrade geopandas rasterio\n\n\nVerify Installation\nimport geopandas as gpd\nimport rasterio\nfrom rasterio.plot import show\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nprint(\"✓ GeoPandas version:\", gpd.__version__)\nprint(\"✓ Rasterio version:\", rasterio.__version__)\nprint(\"✓ NumPy version:\", np.__version__)\nprint(\"✓ All libraries imported successfully!\")\n\n\n\n\n\n\nWarningRuntime Restart\n\n\n\nOccasionally, Colab may ask you to restart the runtime after installing libraries:\nRuntime → Restart runtime\nThen re-run your import cell. This is normal and ensures clean library loading!",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#part-4-python-basics-refresher",
    "href": "day1/sessions/session3.html#part-4-python-basics-refresher",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 4: Python Basics Refresher",
    "text": "Part 4: Python Basics Refresher\nQuick recap of Python essentials for geospatial work:\n\nData Types\n# Numbers\npopulation = 1780148        # Integer\narea_km2 = 42.88           # Float\n\n# Strings\ncity = \"Manila\"\nprovince = \"Metro Manila\"\n\n# Lists (ordered collections)\nregions = [\"Luzon\", \"Visayas\", \"Mindanao\"]\ncoordinates = [14.5995, 120.9842]  # [latitude, longitude]\n\n# Dictionaries (key-value pairs)\nlocation_info = {\n    \"city\": \"Quezon City\",\n    \"population\": 2960048,\n    \"region\": \"NCR\"\n}\n\nprint(f\"{city} has population {population:,}\")\nprint(f\"Regions: {regions}\")\nprint(f\"Coordinates: {coordinates}\")\n\n\nControl Structures\n# If statements\ncloud_cover = 15\n\nif cloud_cover &lt; 10:\n    quality = \"Excellent\"\nelif cloud_cover &lt; 30:\n    quality = \"Good\"\nelse:\n    quality = \"Poor\"\n\nprint(f\"Cloud cover {cloud_cover}%: {quality} for optical imagery\")\n\n# For loops\nprovinces = [\"Palawan\", \"Zambales\", \"Quezon\"]\nfor province in provinces:\n    print(f\"Processing {province}...\")\n\n# List comprehension (Pythonic way)\nprovince_lengths = [len(p) for p in provinces]\nprint(\"Province name lengths:\", province_lengths)\n\n\nFunctions\ndef calculate_ndvi(nir, red):\n    \"\"\"\n    Calculate Normalized Difference Vegetation Index.\n\n    Parameters:\n    -----------\n    nir : float or ndarray\n        Near-infrared reflectance values\n    red : float or ndarray\n        Red reflectance values\n\n    Returns:\n    --------\n    ndvi : float or ndarray\n        NDVI values ranging from -1 to +1\n    \"\"\"\n    # Add small value to avoid division by zero\n    ndvi = (nir - red) / (nir + red + 1e-10)\n    return ndvi\n\n# Example usage\nnir_value = 0.8\nred_value = 0.2\nresult = calculate_ndvi(nir_value, red_value)\nprint(f\"NDVI: {result:.3f}\")\nKey Python concepts for geospatial:\n\nIndentation matters: Use 4 spaces to define code blocks\n0-indexed: First element is list[0], not list[1]\nMethod chaining: gdf.filter(...).plot() applies operations sequentially\nContext managers: with rasterio.open() as src: auto-closes files (prevents memory leaks)",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#part-5-vector-data-with-geopandas",
    "href": "day1/sessions/session3.html#part-5-vector-data-with-geopandas",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 5: Vector Data with GeoPandas",
    "text": "Part 5: Vector Data with GeoPandas\n\nWhat is Vector Data?\nVector data represents discrete features as geometric objects:\n\nPoints: Cities, field sites, observation locations, GPS waypoints\nLines (LineStrings): Roads, rivers, transects, boundaries\nPolygons: Administrative boundaries, land parcels, watersheds, protected areas\n\n\n\n\n\n\ngraph TB\n    subgraph VectorTypes[\"VECTOR GEOMETRY TYPES\"]\n        V1[Point&lt;br/&gt;Single coordinate&lt;br/&gt;Cities, GPS points]\n        V2[LineString&lt;br/&gt;Connected coordinates&lt;br/&gt;Roads, rivers]\n        V3[Polygon&lt;br/&gt;Closed ring&lt;br/&gt;Boundaries, parcels]\n        V4[MultiPoint&lt;br/&gt;Multiple points&lt;br/&gt;Observation sites]\n        V5[MultiLineString&lt;br/&gt;Multiple lines&lt;br/&gt;Road networks]\n        V6[MultiPolygon&lt;br/&gt;Multiple polygons&lt;br/&gt;Archipelago]\n    end\n\n    subgraph GDF[\"GEOPANDAS GEODATAFRAME\"]\n        GDF1[Geometry Column&lt;br/&gt;Shapely objects&lt;br/&gt;Point/Line/Polygon]\n        GDF2[Attribute Columns&lt;br/&gt;Name, Population&lt;br/&gt;Area, Type, etc.]\n        GDF3[CRS Information&lt;br/&gt;EPSG:4326, 32651&lt;br/&gt;Projection metadata]\n        GDF4[Spatial Index&lt;br/&gt;R-tree for fast&lt;br/&gt;spatial queries]\n    end\n\n    subgraph Formats[\"FILE FORMATS\"]\n        F1[Shapefile&lt;br/&gt;.shp + .shx + .dbf + .prj]\n        F2[GeoJSON&lt;br/&gt;.geojson, .json]\n        F3[GeoPackage&lt;br/&gt;.gpkg]\n        F4[KML/KMZ&lt;br/&gt;.kml, .kmz]\n    end\n\n    V1 --&gt; GDF1\n    V2 --&gt; GDF1\n    V3 --&gt; GDF1\n\n    GDF1 --&gt; Operations[Spatial Operations&lt;br/&gt;Buffer, Intersect&lt;br/&gt;Union, Dissolve&lt;br/&gt;Clip, Overlay]\n    GDF2 --&gt; Operations\n    GDF3 --&gt; Operations\n\n    F1 --&gt;|gpd.read_file| GDF\n    F2 --&gt;|gpd.read_file| GDF\n    F3 --&gt;|gpd.read_file| GDF\n\n    style VectorTypes fill:#e6f3ff,stroke:#0066cc,stroke-width:2px\n    style GDF fill:#e6ffe6,stroke:#00aa44,stroke-width:2px\n    style Formats fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style Operations fill:#ffe6ff,stroke:#cc00cc,stroke-width:2px\n\n\n Vector Data Types and GeoPandas GeoDataFrame Structure \n\n\n\nCommon formats: Shapefile (.shp + .shx + .dbf + .prj), GeoJSON (.geojson, .json), KML (.kml), GeoPackage (.gpkg)\nShapefile components: - .shp: Geometry (points, lines, polygons) - .shx: Shape index - .dbf: Attribute table (database) - .prj: Coordinate reference system - .cpg: Character encoding (optional)\n\n\nUnderstanding Coordinate Reference Systems (CRS)\nBefore we load data, it’s essential to understand CRS - a critical concept for accurate geospatial analysis.\nWhat is a CRS?\nA Coordinate Reference System defines how coordinates map to locations on Earth. It consists of:\n\nCoordinate System: Geographic (lat/lon) or Projected (x/y in meters)\nDatum: Reference ellipsoid approximating Earth’s shape (e.g., WGS84)\nProjection: Mathematical transformation from 3D Earth to 2D map\n\nCommon CRS for Philippines:\n\n\n\n\n\n\n\n\n\n\nEPSG Code\nName\nType\nUnits\nUse Case\n\n\n\n\n4326\nWGS84\nGeographic\nDegrees\nGPS, web maps, global datasets\n\n\n32651\nUTM Zone 51N\nProjected\nMeters\nNorthern/Western Philippines\n\n\n32652\nUTM Zone 52N\nProjected\nMeters\nEastern Philippines\n\n\n3123\nPRS92 / Philippines Zone III\nProjected\nMeters\nPhilippine national standard\n\n\n\nWhy CRS matters:\n\nDistance calculations: Geographic CRS (degrees) gives wrong distances; use projected CRS (meters)\nArea calculations: Same issue - must use projected CRS for accurate areas\nSpatial operations: CRS must match for intersections, buffers, clipping\nVisualization: Web maps expect EPSG:4326; analysis needs meters\n\n\n\n\n\n\nflowchart TD\n    A[GPS Data&lt;br/&gt;EPSG:4326&lt;br/&gt;Lat: 14.5995°&lt;br/&gt;Lon: 120.9842°] --&gt; B{Need distance&lt;br/&gt;or area&lt;br/&gt;calculations?}\n\n    B --&gt;|Yes| C[Transform to&lt;br/&gt;Projected CRS&lt;br/&gt;UTM Zone 51N&lt;br/&gt;EPSG:32651]\n\n    B --&gt;|No, just&lt;br/&gt;visualization| D[Keep Geographic&lt;br/&gt;EPSG:4326&lt;br/&gt;For web maps]\n\n    C --&gt; E[Accurate Calculations&lt;br/&gt;X: 279,000 m&lt;br/&gt;Y: 1,615,000 m]\n\n    E --&gt; F[Spatial Operations&lt;br/&gt;Buffer 1000m&lt;br/&gt;Calculate area in km²&lt;br/&gt;Distance in meters]\n\n    F --&gt; G{Export for&lt;br/&gt;web mapping?}\n\n    G --&gt;|Yes| H[Transform back&lt;br/&gt;to EPSG:4326]\n    G --&gt;|No| I[Save in UTM&lt;br/&gt;For further analysis]\n\n    H --&gt; D\n\n    D --&gt; J[Interactive Map&lt;br/&gt;Folium, Leaflet&lt;br/&gt;Google Maps]\n\n    subgraph PhilippineCRS[\"PHILIPPINE CRS OPTIONS\"]\n        P1[EPSG:4326&lt;br/&gt;WGS84 Geographic&lt;br/&gt;Global standard]\n        P2[EPSG:32651&lt;br/&gt;UTM Zone 51N&lt;br/&gt;Western PH]\n        P3[EPSG:32652&lt;br/&gt;UTM Zone 52N&lt;br/&gt;Eastern PH]\n        P4[EPSG:3123&lt;br/&gt;PRS92 Zone III&lt;br/&gt;National standard]\n    end\n\n    C -.-&gt;|Choose| P2\n    C -.-&gt;|Choose| P3\n    C -.-&gt;|Choose| P4\n\n    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px\n    style C fill:#ffe6e6,stroke:#cc0044,stroke-width:2px\n    style E fill:#e6ffe6,stroke:#00aa44,stroke-width:2px\n    style D fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style PhilippineCRS fill:#f0e6ff,stroke:#6666cc,stroke-width:2px\n\n\n Coordinate Reference System Transformation Workflow \n\n\n\n\n\n\n\n\n\nImportantPhilippine UTM Zones\n\n\n\nThe Philippines spans two UTM zones:\n\nUTM Zone 51N (EPSG:32651): Covers western Philippines including Manila, Palawan, western Luzon, western Visayas\nUTM Zone 52N (EPSG:32652): Covers eastern Philippines including Mindanao, eastern Visayas, Bicol\n\nRule of thumb: Manila and most populated areas use Zone 51N. When in doubt, check your AOI’s central longitude: - Longitude &lt; 120° → Zone 51N - Longitude ≥ 120° → Zone 52N\nFor national-scale analysis, PRS92 (EPSG:3123) is the Philippine standard.\n\n\n\n\nLoading Philippine Administrative Boundaries\nLet’s load provincial boundaries of the Philippines:\n# Option 1: From Google Drive (if you uploaded data)\nprovinces = gpd.read_file('/content/drive/MyDrive/CoPhil_Training/data/vector/philippines_provinces.shp')\n\n# Option 2: From URL (using sample data from PhilGIS)\nurl = \"https://raw.githubusercontent.com/altcoder/philippines-json-maps/master/geojson/provinces/hires/BOHOL.json\"\nsample_province = gpd.read_file(url)\n\n# For this example, let's use the sample\ngdf = sample_province\nprint(\"Loaded successfully! ✓\")\nprint(f\"Data type: {type(gdf)}\")\n\n\nPhilippine Geospatial Data Sources\nOfficial Government Sources: - NAMRIA Geoportal: Official administrative boundaries, topographic maps - Website: https://www.namria.gov.ph/ - Note: May require registration for downloads - PhilGIS: Open-source Philippine GIS data repository - Roads, rivers, boundaries, land use - PSA (Philippine Statistics Authority): Statistical boundaries matching census data - LMB (Land Management Bureau): Cadastral boundaries, land parcel data\nOpen Data Platforms: - GADM: Global Administrative Areas - free Philippine boundaries - Website: https://gadm.org/download_country.html (select Philippines) - OpenStreetMap (OSM): Roads, buildings, POIs, waterways - Export via: https://export.hotosm.org/ or Overpass Turbo - Humanitarian Data Exchange (HDX): Disaster-related boundaries and data - PhilSA Data Portal: Satellite-derived products and boundaries\nPro Tip: Download shapefiles to your Google Drive data/vector/ folder before the session for offline access.\n\n\nInspecting a GeoDataFrame\nView first rows:\ngdf.head()\nThis shows: - Attribute columns (NAME, PROVINCE, REGION, ADM1_EN, etc.) - geometry column: The shapes themselves (Polygon or MultiPolygon)\nCheck coordinate reference system:\nprint(\"CRS:\", gdf.crs)\nprint(\"CRS name:\", gdf.crs.name if gdf.crs else \"Not defined\")\nGet basic information:\nprint(f\"Number of features: {len(gdf)}\")\nprint(f\"Geometry type: {gdf.geometry.type[0]}\")\nprint(f\"Bounds (minx, miny, maxx, maxy): {gdf.total_bounds}\")\nprint(f\"\\nColumns:\\n{list(gdf.columns)}\")\nSummary statistics:\ngdf.info()\nUnderstand geometry details:\n# First geometry\nfirst_geom = gdf.geometry.iloc[0]\nprint(f\"Geometry type: {first_geom.geom_type}\")\nprint(f\"Is valid: {first_geom.is_valid}\")\nprint(f\"Coordinate count: {len(first_geom.exterior.coords)}\")  # For Polygon\n\n\nWorking with Coordinate Reference Systems\nCheck current CRS:\nprint(\"Original CRS:\", gdf.crs)\nReproject to UTM Zone 51N (for Manila area):\n# Transform to UTM 51N for accurate distance/area calculations\ngdf_utm = gdf.to_crs('EPSG:32651')\nprint(\"Reprojected CRS:\", gdf_utm.crs)\n\n# Compare bounds in degrees vs meters\nprint(\"\\nOriginal bounds (degrees):\", gdf.total_bounds)\nprint(\"UTM bounds (meters):\", gdf_utm.total_bounds)\n\n\n\n\n\n\nTipWhen to Use Which CRS?\n\n\n\nUse Geographic CRS (EPSG:4326) for: - Loading data (most sources provide data in WGS84) - Web mapping and visualization - Storing data for sharing\nUse Projected CRS (UTM or PRS92) for: - Distance calculations: gdf.geometry.length - Area calculations: gdf.geometry.area - Buffer operations: gdf.geometry.buffer(1000) for 1 km buffer - Spatial analysis requiring metric units\nWorkflow: 1. Load data (usually in EPSG:4326) 2. Reproject to appropriate UTM zone 3. Perform spatial operations 4. Optionally reproject back to EPSG:4326 for web display\n\n\n\n\nCalculating Geometric Properties\n# Calculate area in square kilometers\ngdf_utm['area_km2'] = gdf_utm.geometry.area / 1_000_000  # m² to km²\nprint(f\"Province area: {gdf_utm['area_km2'].iloc[0]:.2f} km²\")\n\n# Calculate perimeter\ngdf_utm['perimeter_km'] = gdf_utm.geometry.length / 1000  # m to km\nprint(f\"Province perimeter: {gdf_utm['perimeter_km'].iloc[0]:.2f} km\")\n\n# Calculate centroid (in original CRS)\ngdf['centroid'] = gdf.geometry.centroid\nprint(f\"Centroid coordinates: {gdf['centroid'].iloc[0]}\")\n\n# Display updated GeoDataFrame\ngdf_utm[['area_km2', 'perimeter_km']].head()\n\n\nFiltering and Querying\nFilter by attribute:\n# If using multi-province dataset:\n# luzon_provinces = gdf[gdf['ISLAND'] == 'LUZON']\n\n# Filter by area (if area column exists or was calculated)\nlarge_provinces = gdf_utm[gdf_utm['area_km2'] &gt; 5000]\nprint(f\"Provinces larger than 5000 km²: {len(large_provinces)}\")\n\n# Example: Select specific province by name\ntarget = gdf[gdf['NAME'].str.contains('Bohol', case=False)]\nprint(f\"Selected: {target['NAME'].values}\")\n\n# Multiple conditions\n# coastal_large = gdf[(gdf['area_km2'] &gt; 3000) & (gdf['coastal'] == True)]\nSpatial filtering:\n# Check if geometries are valid\nprint(f\"Valid geometries: {gdf.geometry.is_valid.all()}\")\n\n# If invalid, fix them\nif not gdf.geometry.is_valid.all():\n    gdf['geometry'] = gdf.geometry.buffer(0)  # Common fix for invalid geometries\n\n# Spatial predicate example: find features within a bounding box\nbbox = (123.5, 9.5, 125.0, 11.0)  # (minx, miny, maxx, maxy)\nfrom shapely.geometry import box\nbbox_geom = box(*bbox)\nwithin_bbox = gdf[gdf.geometry.within(bbox_geom)]\nprint(f\"Features within bounding box: {len(within_bbox)}\")\n\n\nSpatial Operations\nBuffer (create zone around feature):\n# Create 10 km buffer around province (must use projected CRS!)\nbuffer_10km = gdf_utm.geometry.buffer(10000)  # 10,000 meters\n\n# Visualize original and buffer\nfig, ax = plt.subplots(figsize=(10, 8))\ngdf_utm.plot(ax=ax, facecolor='lightblue', edgecolor='black', label='Province')\nbuffer_10km.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=2, label='10 km buffer')\nplt.title(\"Province with 10 km Buffer\")\nplt.legend()\nplt.show()\nDissolve (merge features):\n# If you have multiple provinces, dissolve by region\n# dissolved = gdf.dissolve(by='REGION', aggfunc='sum')  # Aggregates numeric columns\nSpatial joins:\n# Example: Join point data to polygons (e.g., field sites to provinces)\n# points_gdf = gpd.read_file('field_sites.shp')\n# joined = gpd.sjoin(points_gdf, gdf, how='left', predicate='within')\n# This adds province attributes to each point based on which polygon contains it\n\n\nVisualizing Vector Data\nSimple plot:\ngdf.plot(figsize=(8, 8), edgecolor='black', facecolor='lightblue')\nplt.title(\"Bohol Province, Philippines\")\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.show()\nStyled plot with colors:\n# If you have multiple provinces with a classification column:\n# gdf.plot(column='REGION', legend=True, figsize=(10, 10),\n#          cmap='Set3', edgecolor='black', linewidth=0.5)\n# plt.title(\"Philippines Provinces by Region\")\n\n# For single feature, style it:\nfig, ax = plt.subplots(figsize=(10, 10))\ngdf.plot(ax=ax, facecolor='#90EE90', edgecolor='darkgreen', linewidth=2, alpha=0.7)\ngdf['centroid'].plot(ax=ax, color='red', markersize=50, label='Centroid')\nplt.title(\"Bohol Province with Centroid\", fontsize=16)\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\nInteractive map with Folium:\n# Install folium if not already available\n!pip install folium -q\n\nimport folium\n\n# Get centroid for map center\ncentroid = gdf.geometry.centroid.iloc[0]\ncenter = [centroid.y, centroid.x]  # folium expects [lat, lon]\n\n# Create map\nm = folium.Map(location=center, zoom_start=9, tiles='OpenStreetMap')\n\n# Add GeoDataFrame to map\nfolium.GeoJson(\n    gdf,\n    name='Province Boundary',\n    style_function=lambda x: {'fillColor': 'lightblue', 'color': 'darkblue', 'weight': 2}\n).add_to(m)\n\n# Add marker at centroid\nfolium.Marker(\n    location=center,\n    popup='Province Centroid',\n    icon=folium.Icon(color='red', icon='info-sign')\n).add_to(m)\n\n# Display map\nm\n\n\nCreating Area of Interest (AOI) Geometries\nfrom shapely.geometry import Polygon, Point, LineString\n\n# Create a simple polygon AOI (example coordinates)\naoi_coords = [\n    (123.5, 9.5),   # Southwest corner\n    (125.0, 9.5),   # Southeast corner\n    (125.0, 11.0),  # Northeast corner\n    (123.5, 11.0),  # Northwest corner\n    (123.5, 9.5)    # Close polygon (first point repeated)\n]\n\naoi_polygon = Polygon(aoi_coords)\naoi_gdf = gpd.GeoDataFrame([1], geometry=[aoi_polygon], crs='EPSG:4326')\naoi_gdf.columns = ['id', 'geometry']\n\n# Visualize AOI with province\nfig, ax = plt.subplots(figsize=(10, 8))\ngdf.plot(ax=ax, facecolor='lightgray', edgecolor='black', label='Province')\naoi_gdf.plot(ax=ax, facecolor='none', edgecolor='red', linewidth=3, label='AOI')\nplt.title(\"Province with Area of Interest (Red Box)\")\nplt.legend()\nplt.show()\n\nprint(\"AOI created successfully! ✓\")\nprint(f\"AOI bounds: {aoi_gdf.total_bounds}\")\nSave AOI for later use:\n# Save as shapefile\naoi_gdf.to_file('/content/drive/MyDrive/CoPhil_Training/data/vector/my_aoi.shp')\n\n# Or save as GeoJSON (more portable)\naoi_gdf.to_file('/content/drive/MyDrive/CoPhil_Training/data/vector/my_aoi.geojson', driver='GeoJSON')",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#part-6-raster-data-with-rasterio",
    "href": "day1/sessions/session3.html#part-6-raster-data-with-rasterio",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 6: Raster Data with Rasterio",
    "text": "Part 6: Raster Data with Rasterio\n\nWhat is Raster Data?\nRaster data is a grid of pixels (cells), each with a value:\n\nSatellite imagery: Each pixel = reflectance values (0-10000 for Sentinel-2)\nDEMs (Digital Elevation Models): Each pixel = elevation in meters\nTemperature maps: Each pixel = temperature value\nClassification maps: Each pixel = land cover class ID\n\n\n\n\n\n\nflowchart TD\n    A[Sentinel-2 GeoTIFF&lt;br/&gt;Multi-band raster&lt;br/&gt;13 spectral bands] --&gt; B[Open with Rasterio&lt;br/&gt;rasterio.open]\n\n    B --&gt; C[Read Metadata&lt;br/&gt;CRS, Transform&lt;br/&gt;Bounds, Resolution&lt;br/&gt;Band count]\n\n    C --&gt; D[Read Band Data&lt;br/&gt;src.read&lt;br/&gt;NumPy arrays]\n\n    D --&gt; E{Processing&lt;br/&gt;Goal?}\n\n    E --&gt;|Calculate Index| F[NDVI Calculation&lt;br/&gt;NIR - Red / NIR + Red&lt;br/&gt;NumPy operations]\n\n    E --&gt;|Create Composite| G[RGB Composite&lt;br/&gt;Stack bands&lt;br/&gt;Normalize values]\n\n    E --&gt;|Clip to AOI| H[Vector Mask&lt;br/&gt;GeoDataFrame boundary&lt;br/&gt;rasterio.mask.mask]\n\n    E --&gt;|Resample| I[Change Resolution&lt;br/&gt;Upsample/Downsample&lt;br/&gt;Resampling method]\n\n    F --&gt; J[Write Output&lt;br/&gt;rasterio.open 'w'&lt;br/&gt;Copy metadata&lt;br/&gt;Write array]\n\n    G --&gt; J\n    H --&gt; J\n    I --&gt; J\n\n    J --&gt; K[New GeoTIFF&lt;br/&gt;Processed raster&lt;br/&gt;Ready for ML]\n\n    subgraph RasterOps[\"RASTER OPERATIONS\"]\n        R1[Arithmetic&lt;br/&gt;Add, Subtract&lt;br/&gt;Multiply, Divide]\n        R2[Logical&lt;br/&gt;Masks, Filters&lt;br/&gt;Thresholds]\n        R3[Statistics&lt;br/&gt;Mean, Std, Min&lt;br/&gt;Max, Percentiles]\n        R4[Spatial&lt;br/&gt;Clip, Mosaic&lt;br/&gt;Reproject]\n    end\n\n    D --&gt; RasterOps\n\n    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px\n    style C fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style D fill:#e6ffe6,stroke:#00aa44,stroke-width:2px\n    style J fill:#ffe6ff,stroke:#cc00cc,stroke-width:2px\n    style K fill:#ccffcc,stroke:#00aa44,stroke-width:2px\n    style RasterOps fill:#ffe6e6,stroke:#cc0044,stroke-width:2px\n\n\n Raster Data Processing Workflow with Rasterio \n\n\n\nCommon formats: GeoTIFF (.tif, .tiff), NetCDF (.nc), HDF (.hdf, .h5), JPEG2000 (.jp2)\nRaster structure:\n┌─────┬─────┬─────┬─────┐\n│ 120 │ 130 │ 125 │ 118 │  ← Row 1 (top)\n├─────┼─────┼─────┼─────┤\n│ 115 │ 140 │ 135 │ 122 │  ← Row 2\n├─────┼─────┼─────┼─────┤\n│ 110 │ 125 │ 130 │ 119 │  ← Row 3 (bottom)\n└─────┴─────┴─────┴─────┘\n  ↑     ↑     ↑     ↑\n Col1  Col2  Col3  Col4\n (left)           (right)\nEach cell has: - Value: Reflectance, elevation, class, etc. - Location: Defined by geotransform + CRS - Size: Spatial resolution (e.g., 10m means 10m × 10m pixel)\nGeotransform: 6 coefficients that map pixel coordinates to geographic coordinates - [x_origin, x_pixel_size, x_rotation, y_origin, y_rotation, y_pixel_size] - Example: [400000, 10, 0, 5000000, 0, -10] → Origin at (400000, 5000000) UTM, 10m pixels\n\n\nUnderstanding Sentinel-2 Bands and Resolution\nBefore opening rasters, let’s review Sentinel-2’s band structure (from Session 1):\n\n\n\nBand\nName\nWavelength (nm)\nResolution\nPurpose\n\n\n\n\nB1\nCoastal Aerosol\n443\n60m\nAtmospheric correction\n\n\nB2\nBlue\n490\n10m\nWater body mapping, aerosol detection\n\n\nB3\nGreen\n560\n10m\nVegetation vigor, water clarity\n\n\nB4\nRed\n665\n10m\nChlorophyll absorption\n\n\nB5\nRed Edge 1\n705\n20m\nVegetation classification\n\n\nB6\nRed Edge 2\n740\n20m\nVegetation stress\n\n\nB7\nRed Edge 3\n783\n20m\nVegetation monitoring\n\n\nB8\nNIR\n842\n10m\nBiomass, vegetation health\n\n\nB8A\nNarrow NIR\n865\n20m\nAtmospheric correction\n\n\nB9\nWater Vapor\n945\n60m\nAtmospheric correction\n\n\nB10\nCirrus\n1373\n60m\nCirrus cloud detection\n\n\nB11\nSWIR 1\n1610\n20m\nSnow/ice/cloud discrimination\n\n\nB12\nSWIR 2\n2190\n20m\nVegetation moisture\n\n\n\nKey bands for this session: - 10m bands (B2, B3, B4, B8): Best for detailed analysis, vegetation indices - 20m bands (B5-B7, B8A, B11-B12): Useful for spectral indices requiring SWIR - 60m bands (B1, B9, B10): Mostly for atmospheric correction, rarely used in analysis\n\n\nOpening a Raster File\nExample: Sentinel-2 imagery subset\n# Sample raster path (adjust to your data location)\n# For demo, we'll show the workflow with a hypothetical path\nraster_path = '/content/drive/MyDrive/CoPhil_Training/data/raster/sentinel2_bohol_subset.tif'\n\n# In real workflow, you'd download Sentinel-2 from Copernicus or GEE\n# For now, let's demonstrate with metadata inspection\n\n# Open raster and view metadata\ntry:\n    src = rasterio.open(raster_path)\n\n    # View metadata\n    print(\"Raster Metadata:\")\n    print(f\"  Driver: {src.driver}\")\n    print(f\"  Width (pixels): {src.width}\")\n    print(f\"  Height (pixels): {src.height}\")\n    print(f\"  Number of bands: {src.count}\")\n    print(f\"  Data type: {src.dtypes[0]}\")\n    print(f\"  CRS: {src.crs}\")\n    print(f\"  Bounds: {src.bounds}\")\n    print(f\"  Transform:\\n{src.transform}\")\n    print(f\"  Resolution: {src.res}\")  # (x_res, y_res) in CRS units\n    print(f\"  Nodata value: {src.nodata}\")\n\n    src.close()\nexcept FileNotFoundError:\n    print(\"Sample raster file not found. We'll use a demonstration dataset.\")\n    print(\"In practice, you'd download Sentinel-2 data from Copernicus Data Space Ecosystem or Google Earth Engine.\")\n\n\n\n\n\n\nNoteUnderstanding Raster Metadata\n\n\n\nFor a Sentinel-2 10m band over Bohol (hypothetical):\n\nWidth/Height: 5490 x 5490 pixels → 54.9km x 54.9km area (standard Sentinel-2 tile)\nBands: 1 (if single band file) or 4 (if RGB+NIR stack)\nData type: uint16 (unsigned 16-bit integer, range 0-65535)\n\nSentinel-2 L2A: Reflectance scaled by 10000 (value 5000 = 50% reflectance)\n\nCRS: EPSG:32651 (UTM Zone 51N for western Philippines)\nResolution: (10.0, -10.0) meters\n\nPositive x-resolution: columns go west to east\nNegative y-resolution: rows go north to south (standard in GeoTIFF)\n\nNodata: 0 or 65535 (no valid data, masked areas)\n\nTransform (Affine):\n| x_pixel_size  0.0            x_origin |\n| 0.0          -y_pixel_size   y_origin |\n| 0.0           0.0            1.0      |\nMaps pixel coordinates (row, col) to real-world coordinates (x, y).\n\n\nBetter pattern: Context manager (auto-closes file):\n# Context manager ensures file is closed even if error occurs\nwith rasterio.open(raster_path) as src:\n    print(f\"Opened: {src.name}\")\n    print(f\"Bands: {src.count}\")\n    print(f\"CRS: {src.crs}\")\n# File automatically closed after 'with' block - prevents memory leaks!\n\n\nReading Raster Data into Arrays\nRead a single band:\nwith rasterio.open(raster_path) as src:\n    # Read band 1 (Rasterio uses 1-indexing for bands!)\n    band1 = src.read(1)\n\n    # Store metadata for later use\n    profile = src.profile\n\nprint(f\"Band 1 shape: {band1.shape}\")  # (height, width)\nprint(f\"Data type: {band1.dtype}\")\nprint(f\"Min value: {band1.min()}, Max value: {band1.max()}\")\nprint(f\"Mean value: {band1.mean():.2f}\")\n\n# Mask nodata values\nif profile['nodata'] is not None:\n    band1_masked = np.ma.masked_equal(band1, profile['nodata'])\n    print(f\"Mean (excluding nodata): {band1_masked.mean():.2f}\")\nRead multiple bands:\nwith rasterio.open(raster_path) as src:\n    # Read all bands as 3D array (bands, height, width)\n    all_bands = src.read()\n\n    # Or read specific bands\n    blue = src.read(1)   # Band 1: Blue (B2 for Sentinel-2)\n    green = src.read(2)  # Band 2: Green (B3)\n    red = src.read(3)    # Band 3: Red (B4)\n    nir = src.read(4)    # Band 4: NIR (B8)\n\nprint(f\"All bands shape: {all_bands.shape}\")  # (4, height, width)\nprint(f\"Individual band shape: {blue.shape}\")  # (height, width)\nWindowed reading (memory-efficient for large files):\nfrom rasterio.windows import Window\n\nwith rasterio.open(raster_path) as src:\n    # Read a 1000x1000 pixel window starting at row 500, col 500\n    window = Window(col_off=500, row_off=500, width=1000, height=1000)\n    subset = src.read(1, window=window)\n\nprint(f\"Subset shape: {subset.shape}\")  # (1000, 1000)\nprint(\"Memory saved by reading only needed area!\")\n\n\nCalculating Spectral Indices\nSpectral indices combine bands to highlight specific features. Let’s implement the most common indices.\n\nNDVI (Normalized Difference Vegetation Index)\nFormula: NDVI = (NIR - Red) / (NIR + Red)\nFor Sentinel-2: NDVI = (B8 - B4) / (B8 + B4)\nwith rasterio.open(raster_path) as src:\n    # Read bands as float to avoid integer overflow\n    red = src.read(3).astype(float)   # B4\n    nir = src.read(4).astype(float)   # B8\n\n    # Store metadata for saving later\n    profile = src.profile.copy()\n\n# Calculate NDVI\n# Add small value to prevent division by zero\nndvi = (nir - red) / (nir + red + 1e-10)\n\nprint(f\"NDVI range: {ndvi.min():.3f} to {ndvi.max():.3f}\")\nprint(f\"NDVI mean: {ndvi.mean():.3f}\")\n\n# Visualize NDVI\nplt.figure(figsize=(10, 8))\nplt.imshow(ndvi, cmap='RdYlGn', vmin=-1, vmax=1)\nplt.colorbar(label='NDVI', shrink=0.8)\nplt.title(\"NDVI - Vegetation Index\")\nplt.xlabel(\"Column (pixels)\")\nplt.ylabel(\"Row (pixels)\")\nplt.show()\nNDVI interpretation:\n\n\n\nNDVI Range\nSurface Type\n\n\n\n\n&lt; 0\nWater, clouds, snow\n\n\n0 - 0.2\nBare soil, rock, sand, urban areas\n\n\n0.2 - 0.4\nSparse vegetation, grassland, shrubland\n\n\n0.4 - 0.7\nModerate vegetation, cropland, forest\n\n\n&gt; 0.7\nDense vegetation, healthy forest, mature crops\n\n\n\n\n\n\nPhilippine Application: Rice Paddy Monitoring\nRice paddies show characteristic NDVI patterns during growing season:\n\nFlooding/transplanting: Low NDVI (0.1-0.3) - water and sparse seedlings\nVegetative growth: Rising NDVI (0.4-0.6) - canopy development\nPeak growth: High NDVI (0.6-0.8) - maximum biomass\nSenescence/harvest: Declining NDVI (0.3-0.5) - yellowing and harvest\n\n# Identify rice paddies (moderate-high NDVI during growing season)\nrice_mask = (ndvi &gt; 0.5) & (ndvi &lt; 0.85)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(rice_mask, cmap='Greens')\nplt.title(\"Potential Rice Paddies (NDVI 0.5-0.85)\")\nplt.colorbar()\nplt.show()\n\nrice_pixels = rice_mask.sum()\ntotal_pixels = rice_mask.size\npixel_area_m2 = 10 * 10  # 10m resolution\nrice_area_ha = (rice_pixels * pixel_area_m2) / 10000  # m² to hectares\n\nprint(f\"Potential rice paddies: {rice_pixels:,} pixels ({rice_pixels/total_pixels*100:.1f}%)\")\nprint(f\"Estimated rice area: {rice_area_ha:.1f} hectares\")\nOperational use: PRiSM uses similar SAR-based approaches for nationwide rice monitoring.\n\n\nEVI (Enhanced Vegetation Index)\nFormula: EVI = 2.5 × ((NIR - Red) / (NIR + 6×Red - 7.5×Blue + 1))\nAdvantages over NDVI: - Less saturated in dense vegetation - Better atmospheric correction - Reduces soil background effects\nwith rasterio.open(raster_path) as src:\n    blue = src.read(1).astype(float)  # B2\n    red = src.read(3).astype(float)   # B4\n    nir = src.read(4).astype(float)   # B8\n\n# Calculate EVI\nevi = 2.5 * ((nir - red) / (nir + 6*red - 7.5*blue + 1))\n\n# Typical EVI range is -1 to 1, but often 0 to 1 for vegetation\nevi = np.clip(evi, -1, 1)  # Clip extreme values\n\nplt.figure(figsize=(10, 8))\nplt.imshow(evi, cmap='YlGn', vmin=0, vmax=1)\nplt.colorbar(label='EVI', shrink=0.8)\nplt.title(\"EVI - Enhanced Vegetation Index\")\nplt.show()\n\nprint(f\"EVI range: {evi.min():.3f} to {evi.max():.3f}\")\n\n\nNDWI (Normalized Difference Water Index)\nFormula: NDWI = (Green - NIR) / (Green + NIR)\nFor Sentinel-2: NDWI = (B3 - B8) / (B3 + B8)\nwith rasterio.open(raster_path) as src:\n    green = src.read(2).astype(float)  # B3\n    nir = src.read(4).astype(float)    # B8\n\n# Calculate NDWI\nndwi = (green - nir) / (green + nir + 1e-10)\n\nplt.figure(figsize=(10, 8))\nplt.imshow(ndwi, cmap='Blues', vmin=-1, vmax=1)\nplt.colorbar(label='NDWI', shrink=0.8)\nplt.title(\"NDWI - Water Index\")\nplt.show()\n\n# Extract water bodies (NDWI &gt; 0.3 typically indicates water)\nwater_mask = ndwi &gt; 0.3\nwater_pixels = water_mask.sum()\nwater_area_ha = (water_pixels * 100) / 10000  # 10m pixels, convert to hectares\n\nprint(f\"Water pixels: {water_pixels:,}\")\nprint(f\"Water area: {water_area_ha:.1f} hectares\")\nNDWI interpretation: - &gt; 0.3: Open water (rivers, lakes, reservoirs) - 0.0 - 0.3: Wet soil, flooded vegetation - &lt; 0: Dry soil, vegetation\n\n\nCreating RGB and False Color Composites\n\nTrue Color Composite (Red, Green, Blue)\nwith rasterio.open(raster_path) as src:\n    red = src.read(3)    # B4\n    green = src.read(2)  # B3\n    blue = src.read(1)   # B2\n\n# Stack bands into RGB array (height, width, 3)\nrgb = np.stack([red, green, blue], axis=2)\n\n# Scale to 0-1 range for display\n# Sentinel-2 L2A surface reflectance: 0-10000\nrgb_scaled = rgb.astype(float) / 10000.0\nrgb_scaled = np.clip(rgb_scaled, 0, 1)  # Clip any values outside 0-1\n\n# Enhance contrast using percentile stretch\nfrom scipy.stats import scoreatpercentile\ndef percentile_stretch(image, lower=2, upper=98):\n    \"\"\"Stretch image values between percentiles for better visualization.\"\"\"\n    out = np.zeros_like(image, dtype=float)\n    for i in range(image.shape[2]):\n        band = image[:,:,i]\n        low, high = scoreatpercentile(band, (lower, upper))\n        out[:,:,i] = np.clip((band - low) / (high - low), 0, 1)\n    return out\n\nrgb_enhanced = percentile_stretch(rgb_scaled, lower=2, upper=98)\n\n# Display\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\naxes[0].imshow(rgb_scaled)\naxes[0].set_title(\"True Color (Linear Stretch)\")\naxes[0].axis('off')\n\naxes[1].imshow(rgb_enhanced)\naxes[1].set_title(\"True Color (Enhanced Contrast)\")\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\nFalse Color Composite (NIR, Red, Green)\nPurpose: Highlights vegetation (appears red) and makes it easier to distinguish land cover types.\nwith rasterio.open(raster_path) as src:\n    nir = src.read(4)    # B8\n    red = src.read(3)    # B4\n    green = src.read(2)  # B3\n\n# NIR-R-G composite (standard false color)\nfalse_color = np.stack([nir, red, green], axis=2)\nfalse_color_scaled = false_color.astype(float) / 10000.0\nfalse_color_scaled = np.clip(false_color_scaled, 0, 1)\n\n# Apply percentile stretch\nfalse_color_enhanced = percentile_stretch(false_color_scaled)\n\nplt.figure(figsize=(12, 10))\nplt.imshow(false_color_enhanced)\nplt.title(\"False Color Composite (NIR-Red-Green)\\nVegetation appears red\")\nplt.axis('off')\nplt.show()\nFalse color interpretation:\n\n\n\nColor\nSurface Type\n\n\n\n\nBright red\nDense, healthy vegetation (high NIR)\n\n\nPink/magenta\nModerate vegetation\n\n\nDark blue/black\nWater (absorbs NIR)\n\n\nCyan/gray\nUrban areas, bare soil\n\n\nBrown/tan\nSparse vegetation, agricultural fields\n\n\n\nWhy false color is useful: - Vegetation is much more distinguishable (red tones) - Water bodies are very dark (low NIR) - Urban areas are clearly visible (cyan/gray) - Useful for rapid visual interpretation before quantitative analysis\n\n\n\nSaving Processed Rasters\n# Save NDVI as GeoTIFF\noutput_path = '/content/drive/MyDrive/CoPhil_Training/outputs/ndvi.tif'\n\n# Update profile for single-band float output\nprofile.update(\n    dtype=rasterio.float32,\n    count=1,\n    compress='lzw',  # Compress to save space\n    nodata=-9999     # Define nodata value\n)\n\nwith rasterio.open(output_path, 'w', **profile) as dst:\n    dst.write(ndvi.astype(rasterio.float32), 1)\n\nprint(f\"✓ Saved NDVI to: {output_path}\")",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#part-7-combining-vector-and-raster-operations",
    "href": "day1/sessions/session3.html#part-7-combining-vector-and-raster-operations",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 7: Combining Vector and Raster Operations",
    "text": "Part 7: Combining Vector and Raster Operations\nReal-world EO workflows often require integrating vector and raster data. Let’s explore common operations.\n\nClipping Raster to Vector Boundary\nUse case: Extract satellite data only within your area of interest (e.g., a province, protected area, or farm boundary).\nfrom rasterio.mask import mask\n\n# Load AOI polygon (from earlier GeoPandas section)\n# aoi_gdf = gpd.read_file('my_aoi.geojson')\n\nwith rasterio.open(raster_path) as src:\n    # Ensure CRS match - CRITICAL for accurate clipping!\n    if aoi_gdf.crs != src.crs:\n        print(f\"Reprojecting AOI from {aoi_gdf.crs} to {src.crs}\")\n        aoi_gdf = aoi_gdf.to_crs(src.crs)\n\n    # Get geometries in proper format (list of shapely geometries)\n    shapes = aoi_gdf.geometry.values\n\n    # Clip raster to vector boundary\n    out_image, out_transform = mask(src, shapes, crop=True, nodata=src.nodata)\n    out_meta = src.meta.copy()\n\n# Update metadata for clipped raster\nout_meta.update({\n    \"driver\": \"GTiff\",\n    \"height\": out_image.shape[1],\n    \"width\": out_image.shape[2],\n    \"transform\": out_transform\n})\n\nprint(f\"Original raster: {src.width} x {src.height} pixels\")\nprint(f\"Clipped raster: {out_meta['width']} x {out_meta['height']} pixels\")\nprint(f\"Size reduction: {(1 - (out_meta['width']*out_meta['height'])/(src.width*src.height))*100:.1f}%\")\n\n# Visualize clipped area\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Original\nwith rasterio.open(raster_path) as src:\n    axes[0].imshow(src.read(3), cmap='gray')\n    axes[0].set_title(\"Original Raster (Full Extent)\")\n\n# Clipped\naxes[1].imshow(out_image[2], cmap='gray')  # Band 3 (Red channel)\naxes[1].set_title(\"Clipped to AOI\")\n\nplt.tight_layout()\nplt.show()\nSave clipped raster:\nclipped_path = '/content/drive/MyDrive/CoPhil_Training/outputs/clipped_raster.tif'\n\nwith rasterio.open(clipped_path, 'w', **out_meta) as dest:\n    dest.write(out_image)\n\nprint(f\"✓ Saved clipped raster to: {clipped_path}\")\nprint(f\"File size reduced for faster processing and storage!\")\n\n\n\n\n\n\nTipWhen to Clip Rasters\n\n\n\nAdvantages: - Reduced file size: Smaller files load and process faster - Focused analysis: Only relevant area is analyzed - Lower memory use: Important for large Sentinel-2 tiles (100 km² at 10m = 100 million pixels!) - Easier visualization: Zoomed to area of interest\nBest practice: Clip rasters early in your workflow to avoid processing unnecessary pixels.\n\n\n\n\nSampling Raster Values at Point Locations\nUse case: Extract satellite data at field survey locations for validation or model training.\n# Example: Field survey locations (GPS coordinates)\nsurvey_points = gpd.GeoDataFrame({\n    'site_id': ['Site_A', 'Site_B', 'Site_C'],\n    'land_cover': ['Forest', 'Cropland', 'Urban'],\n    'geometry': gpd.points_from_xy(\n        [124.0, 124.5, 123.8],  # Longitudes\n        [10.0, 10.3, 9.8]       # Latitudes\n    )\n}, crs='EPSG:4326')\n\n# Sample raster at points\nwith rasterio.open(raster_path) as src:\n    # Reproject points if needed\n    if survey_points.crs != src.crs:\n        survey_points = survey_points.to_crs(src.crs)\n\n    # Extract coordinates as (x, y) tuples\n    coords = [(x, y) for x, y in zip(survey_points.geometry.x, survey_points.geometry.y)]\n\n    # Sample all bands at each point\n    sampled_values = [x for x in src.sample(coords)]\n\n# Add sampled values to GeoDataFrame\nsurvey_points['blue'] = [v[0] for v in sampled_values]\nsurvey_points['green'] = [v[1] for v in sampled_values]\nsurvey_points['red'] = [v[2] for v in sampled_values]\nsurvey_points['nir'] = [v[3] for v in sampled_values]\n\n# Calculate NDVI at points\nsurvey_points['ndvi'] = (survey_points['nir'] - survey_points['red']) / \\\n                        (survey_points['nir'] + survey_points['red'] + 1e-10)\n\n# Convert DN to reflectance (Sentinel-2 scaling)\nfor band in ['blue', 'green', 'red', 'nir']:\n    survey_points[f'{band}_refl'] = survey_points[band] / 10000.0\n\nprint(\"\\nSampled Values at Survey Points:\")\nprint(survey_points[['site_id', 'land_cover', 'ndvi']])\nExport results for validation:\n# Save as CSV for Excel/analysis\nsurvey_points.drop('geometry', axis=1).to_csv(\n    '/content/drive/MyDrive/CoPhil_Training/outputs/field_validation.csv',\n    index=False\n)\n\n# Or save as shapefile with geometry\nsurvey_points.to_file(\n    '/content/drive/MyDrive/CoPhil_Training/outputs/field_validation.shp'\n)\n\nprint(\"✓ Validation data exported!\")\n\n\nZonal Statistics\nUse case: Calculate average NDVI (or other metrics) for each administrative unit, farm, or land parcel.\n# Example: Calculate mean NDVI per municipality\n# Requires rasterstats library\n!pip install rasterstats -q\n\nfrom rasterstats import zonal_stats\n\n# Load municipality boundaries\nmunicipalities = gpd.read_file('municipalities.shp')\n\n# Calculate zonal statistics\nstats = zonal_stats(\n    municipalities,\n    ndvi,\n    affine=profile['transform'],\n    stats=['min', 'max', 'mean', 'std', 'count'],\n    nodata=-9999\n)\n\n# Add statistics to GeoDataFrame\nmunicipalities['ndvi_mean'] = [s['mean'] for s in stats]\nmunicipalities['ndvi_std'] = [s['std'] for s in stats]\n\n# Visualize municipalities by mean NDVI\nfig, ax = plt.subplots(figsize=(12, 10))\nmunicipalities.plot(\n    column='ndvi_mean',\n    cmap='YlGn',\n    legend=True,\n    ax=ax,\n    edgecolor='black',\n    linewidth=0.5\n)\nplt.title(\"Mean NDVI by Municipality\")\nplt.show()\n\nprint(municipalities[['NAME', 'ndvi_mean', 'ndvi_std']].head())",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#part-8-preprocessing-workflows-for-machine-learning",
    "href": "day1/sessions/session3.html#part-8-preprocessing-workflows-for-machine-learning",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 8: Preprocessing Workflows for Machine Learning",
    "text": "Part 8: Preprocessing Workflows for Machine Learning\nNow let’s connect everything to prepare analysis-ready data for ML (Sessions 5-7).\n\nWorkflow 1: Preparing Training Samples for Classification\nGoal: Create a dataset of labeled pixels for land cover classification.\n# 1. Load training polygons (digitized by expert)\ntraining_polygons = gpd.read_file('training_areas.shp')\n# Columns: 'class_id', 'class_name', 'geometry'\n\n# 2. Rasterize polygons to match satellite image grid\nfrom rasterio.features import rasterize\n\nwith rasterio.open(raster_path) as src:\n    # Create empty array\n    training_mask = rasterize(\n        [(geom, value) for geom, value in zip(training_polygons.geometry, training_polygons['class_id'])],\n        out_shape=(src.height, src.width),\n        transform=src.transform,\n        fill=0,  # Background class\n        dtype='uint8'\n    )\n\n# 3. Extract pixel values at training locations\nwith rasterio.open(raster_path) as src:\n    bands = src.read()  # (n_bands, height, width)\n\n# Reshape to (n_pixels, n_bands)\nn_bands, height, width = bands.shape\npixels = bands.reshape(n_bands, -1).T  # (n_pixels, n_bands)\nlabels = training_mask.flatten()       # (n_pixels,)\n\n# Filter to labeled pixels only (exclude background class 0)\nlabeled_mask = labels &gt; 0\nX = pixels[labeled_mask]  # Features\ny = labels[labeled_mask]  # Labels\n\nprint(f\"Training samples: {X.shape[0]:,} pixels\")\nprint(f\"Features: {X.shape[1]} bands\")\nprint(f\"Classes: {np.unique(y)}\")\n\n# 4. Normalize features (important for ML!)\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(\"\\nData ready for ML model training!\")\nprint(\"Next steps: Split into train/val/test, train Random Forest or CNN\")\n\n\nWorkflow 2: Creating Multi-Temporal Stack\nGoal: Combine images from different dates to capture phenology (crop growth cycles, seasonal changes).\n# List of raster paths from different dates\ndate_paths = [\n    'sentinel2_2024_01_15.tif',  # Dry season\n    'sentinel2_2024_04_15.tif',  # Transition\n    'sentinel2_2024_07_15.tif',  # Wet season\n    'sentinel2_2024_10_15.tif'   # Harvest\n]\n\n# Read all dates and stack\nstacked_bands = []\ndates = []\n\nfor path in date_paths:\n    with rasterio.open(path) as src:\n        stacked_bands.append(src.read())  # (n_bands, height, width)\n        dates.append(path.split('_')[1])  # Extract date from filename\n        profile = src.profile.copy()  # Use last profile as template\n\n# Stack along band dimension: (n_dates * n_bands, height, width)\nmulti_temporal = np.concatenate(stacked_bands, axis=0)\n\nprint(f\"Multi-temporal stack shape: {multi_temporal.shape}\")\nprint(f\"Total bands: {multi_temporal.shape[0]} (4 dates × {stacked_bands[0].shape[0]} bands)\")\nprint(f\"Dates: {dates}\")\n\n# Save multi-temporal stack\nprofile.update(count=multi_temporal.shape[0])\n\nwith rasterio.open('multi_temporal_stack.tif', 'w', **profile) as dst:\n    dst.write(multi_temporal)\n\nprint(\"✓ Multi-temporal stack created!\")\nWhy multi-temporal? - Better classification: Captures seasonal patterns (e.g., crops have distinct phenology) - Change detection: Compare dates to detect deforestation, urban growth, flooding - Improved accuracy: Machine learning models benefit from temporal features - PRiSM approach: Uses SAR time series to detect rice planting/harvesting\n\n\nWorkflow 3: Resampling and Reprojection\nGoal: Make rasters from different sensors compatible (same resolution, CRS, extent).\nfrom rasterio.warp import reproject, Resampling\n\n# Example: Resample 20m Sentinel-2 bands to 10m\nwith rasterio.open('sentinel2_B11_20m.tif') as src:\n    # Define target resolution\n    scale_factor = 2  # 20m → 10m\n\n    # Calculate new dimensions\n    new_height = src.height * scale_factor\n    new_width = src.width * scale_factor\n\n    # Update transform\n    new_transform = src.transform * src.transform.scale(\n        (src.width / new_width),\n        (src.height / new_height)\n    )\n\n    # Create output array\n    upsampled = np.empty((src.count, new_height, new_width), dtype=src.dtypes[0])\n\n    # Reproject\n    reproject(\n        source=rasterio.band(src, 1),\n        destination=upsampled[0],\n        src_transform=src.transform,\n        src_crs=src.crs,\n        dst_transform=new_transform,\n        dst_crs=src.crs,\n        resampling=Resampling.bilinear  # or cubic, nearest\n    )\n\n    # Update profile\n    profile = src.profile.copy()\n    profile.update(\n        width=new_width,\n        height=new_height,\n        transform=new_transform\n    )\n\n# Save resampled raster\nwith rasterio.open('sentinel2_B11_10m.tif', 'w', **profile) as dst:\n    dst.write(upsampled)\n\nprint(f\"✓ Resampled from {src.height}x{src.width} to {new_height}x{new_width}\")\nResampling methods: - Nearest: Fast, preserves values (good for classification maps) - Bilinear: Smooth, averages neighbors (good for continuous data) - Cubic: Smoothest, best for visualization (slower)\n\n\nWorkflow 4: Data Augmentation for Deep Learning\nGoal: Artificially increase training data diversity to prevent overfitting.\n# Example: Augment image chips for CNN training\nimport cv2\n\ndef augment_chip(image, label=None):\n    \"\"\"\n    Apply random augmentations to image chip.\n\n    Parameters:\n    -----------\n    image : ndarray (H, W, C)\n        Image chip\n    label : ndarray (H, W), optional\n        Corresponding label mask\n\n    Returns:\n    --------\n    augmented_image, augmented_label (if label provided)\n    \"\"\"\n    # Random rotation (0, 90, 180, 270 degrees)\n    k = np.random.randint(0, 4)\n    image = np.rot90(image, k)\n    if label is not None:\n        label = np.rot90(label, k)\n\n    # Random flip (horizontal and/or vertical)\n    if np.random.rand() &gt; 0.5:\n        image = np.fliplr(image)\n        if label is not None:\n            label = np.fliplr(label)\n    if np.random.rand() &gt; 0.5:\n        image = np.flipud(image)\n        if label is not None:\n            label = np.flipud(label)\n\n    # Random brightness adjustment (simulate atmospheric variations)\n    if np.random.rand() &gt; 0.5:\n        factor = np.random.uniform(0.8, 1.2)\n        image = np.clip(image * factor, 0, 1)\n\n    if label is not None:\n        return image, label\n    return image\n\n# Example usage\n# chip = rgb_chip  # (256, 256, 3)\n# label_chip = land_cover_chip  # (256, 256)\n# aug_chip, aug_label = augment_chip(chip, label_chip)\n\nprint(\"Data augmentation functions ready!\")\nprint(\"Use during training to increase dataset size 4-8x without new data collection\")",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#part-9-best-practices-and-common-pitfalls",
    "href": "day1/sessions/session3.html#part-9-best-practices-and-common-pitfalls",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 9: Best Practices and Common Pitfalls",
    "text": "Part 9: Best Practices and Common Pitfalls\n\nMemory Management for Large Rasters\nProblem: Sentinel-2 tiles are huge (100km² at 10m = 100 million pixels × 4 bands = 400 MB per tile).\nSolutions:\n1. Windowed Reading:\n# Process in chunks instead of loading entire raster\nwith rasterio.open(large_raster) as src:\n    for window in src.block_windows():\n        data = src.read(window=window)\n        # Process chunk\n        result = process_data(data)\n        # Write chunk to output\n2. Downsampling:\n# Read at lower resolution for exploratory analysis\nwith rasterio.open(raster_path) as src:\n    # Read every 10th pixel\n    data = src.read(out_shape=(src.count, src.height // 10, src.width // 10))\n3. Cloud-Optimized GeoTIFF (COG):\n# Convert to COG for efficient cloud access\n!gdal_translate input.tif output_cog.tif -of COG -co COMPRESS=DEFLATE\n\n\nVectorization vs. Loops\nSlow (loop over pixels):\n# DON'T DO THIS\nfor i in range(height):\n    for j in range(width):\n        ndvi[i, j] = (nir[i, j] - red[i, j]) / (nir[i, j] + red[i, j])\nFast (vectorized with NumPy):\n# DO THIS\nndvi = (nir - red) / (nir + red + 1e-10)  # Single operation on entire array\nWhy? NumPy operations are implemented in C and optimized for array operations. Can be 100-1000x faster than Python loops!\n\n\nError Handling for Geospatial Operations\nimport warnings\n\n# Suppress common warnings that are usually harmless\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n# Safe division with error handling\ndef safe_divide(numerator, denominator):\n    \"\"\"Safely divide arrays, handling division by zero.\"\"\"\n    with np.errstate(divide='ignore', invalid='ignore'):\n        result = numerator / denominator\n        result[~np.isfinite(result)] = np.nan  # Replace inf/nan with nan\n    return result\n\n# Safe CRS transformation\ndef safe_reproject(gdf, target_crs):\n    \"\"\"Safely reproject GeoDataFrame with error handling.\"\"\"\n    try:\n        return gdf.to_crs(target_crs)\n    except Exception as e:\n        print(f\"Warning: Reprojection failed: {e}\")\n        print(\"Returning original GeoDataFrame\")\n        return gdf\n\n\nTroubleshooting Common Issues\nIssue 1: CRS Mismatch\n# Problem: \"CRS mismatch\" error when overlaying vector and raster\n# Solution: Check and align CRS\nwith rasterio.open(raster_path) as src:\n    raster_crs = src.crs\n\nif gdf.crs != raster_crs:\n    print(f\"Reprojecting vector from {gdf.crs} to {raster_crs}\")\n    gdf = gdf.to_crs(raster_crs)\nIssue 2: NoData Handling\n# Problem: Nodata values (0, -9999) skew statistics\n# Solution: Mask nodata before calculations\nimport numpy.ma as ma\n\n# Create masked array\nraster_masked = ma.masked_equal(raster, nodata_value)\n\n# Statistics now ignore nodata\nmean = raster_masked.mean()\nstd = raster_masked.std()\nIssue 3: Memory Error\n# Problem: MemoryError when loading large raster\n# Solution 1: Use windowed reading (shown above)\n# Solution 2: Work with lower resolution\n# Solution 3: Use Dask for out-of-core computation\n\nimport dask.array as da\nimport rioxarray\n\n# Open with rioxarray (uses Dask for lazy loading)\nraster_da = rioxarray.open_rasterio(raster_path, chunks='auto')\nndvi = (raster_da[3] - raster_da[2]) / (raster_da[3] + raster_da[2])\n# Computation happens only when .compute() is called\n\n\nCode Organization for Reproducibility\nGood practices:\n\nClear imports at top:\n\n# Standard library\nimport os\nfrom pathlib import Path\n\n# Third party\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport rasterio\nimport matplotlib.pyplot as plt\n\n# Project-specific\nfrom utils import safe_divide, calculate_ndvi\n\nDefine paths clearly:\n\n# Define all paths at the beginning\nBASE_DIR = Path('/content/drive/MyDrive/CoPhil_Training')\nDATA_DIR = BASE_DIR / 'data'\nRASTER_DIR = DATA_DIR / 'raster'\nVECTOR_DIR = DATA_DIR / 'vector'\nOUTPUT_DIR = BASE_DIR / 'outputs'\n\n# Create output directory if it doesn't exist\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nUse functions:\n\ndef preprocess_sentinel2(raster_path, aoi_gdf=None, scale_factor=10000):\n    \"\"\"\n    Load and preprocess Sentinel-2 imagery.\n\n    Parameters:\n    -----------\n    raster_path : str\n        Path to Sentinel-2 GeoTIFF\n    aoi_gdf : GeoDataFrame, optional\n        Area of interest for clipping\n    scale_factor : int\n        Scaling factor for reflectance conversion\n\n    Returns:\n    --------\n    bands : dict\n        Dictionary of band arrays\n    profile : dict\n        Raster metadata\n    \"\"\"\n    with rasterio.open(raster_path) as src:\n        if aoi_gdf is not None:\n            # Clip to AOI\n            from rasterio.mask import mask\n            bands_array, transform = mask(src, aoi_gdf.geometry, crop=True)\n        else:\n            bands_array = src.read()\n            transform = src.transform\n\n        profile = src.profile.copy()\n        profile.update(transform=transform)\n\n    # Convert to reflectance\n    bands = {\n        'blue': bands_array[0] / scale_factor,\n        'green': bands_array[1] / scale_factor,\n        'red': bands_array[2] / scale_factor,\n        'nir': bands_array[3] / scale_factor\n    }\n\n    return bands, profile\n\n# Usage\nbands, profile = preprocess_sentinel2(raster_path, aoi_gdf)\nndvi = calculate_ndvi(bands['nir'], bands['red'])",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#part-10-complete-workflow-example",
    "href": "day1/sessions/session3.html#part-10-complete-workflow-example",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Part 10: Complete Workflow Example",
    "text": "Part 10: Complete Workflow Example\nLet’s put everything together in a realistic scenario.\n\nScenario: Agricultural Monitoring for Palawan Rice Area\nGoal: Map rice cultivation areas in Palawan using Sentinel-2 and prepare training data for ML classification.\nimport numpy as np\nimport geopandas as gpd\nimport rasterio\nfrom rasterio.mask import mask\nfrom rasterio.plot import show\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# ============================================================\n# 1. SETUP\n# ============================================================\n\n# Define paths\nBASE_DIR = Path('/content/drive/MyDrive/CoPhil_Training')\npalawan_boundary = BASE_DIR / 'data/vector/palawan_province.shp'\nsentinel2_tile = BASE_DIR / 'data/raster/sentinel2_palawan_20240615.tif'\ntraining_areas = BASE_DIR / 'data/vector/rice_training_polygons.shp'\noutput_dir = BASE_DIR / 'outputs/palawan_rice_analysis'\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nprint(\"Step 1: Setup complete ✓\")\n\n# ============================================================\n# 2. LOAD AND INSPECT DATA\n# ============================================================\n\n# Load Palawan boundary\npalawan = gpd.read_file(palawan_boundary)\nprint(f\"Loaded Palawan boundary: {palawan.crs}\")\n\n# Load training areas\ntraining = gpd.read_file(training_areas)\nprint(f\"Loaded training areas: {len(training)} polygons\")\nprint(f\"Classes: {training['class_name'].unique()}\")\n\n# Inspect Sentinel-2 metadata\nwith rasterio.open(sentinel2_tile) as src:\n    print(f\"\\nSentinel-2 metadata:\")\n    print(f\"  Resolution: {src.res[0]}m\")\n    print(f\"  CRS: {src.crs}\")\n    print(f\"  Bounds: {src.bounds}\")\n    print(f\"  Bands: {src.count}\")\n\nprint(\"\\nStep 2: Data loaded ✓\")\n\n# ============================================================\n# 3. CLIP RASTER TO PALAWAN\n# ============================================================\n\nwith rasterio.open(sentinel2_tile) as src:\n    # Ensure CRS match\n    if palawan.crs != src.crs:\n        palawan = palawan.to_crs(src.crs)\n\n    # Clip to Palawan boundary\n    clipped, transform = mask(src, palawan.geometry, crop=True)\n\n    # Update metadata\n    out_meta = src.meta.copy()\n    out_meta.update({\n        \"height\": clipped.shape[1],\n        \"width\": clipped.shape[2],\n        \"transform\": transform\n    })\n\n# Save clipped raster\nclipped_path = output_dir / 'palawan_sentinel2_clipped.tif'\nwith rasterio.open(clipped_path, 'w', **out_meta) as dst:\n    dst.write(clipped)\n\nprint(f\"Step 3: Clipped raster to Palawan ✓\")\nprint(f\"Size: {clipped.shape[2]} x {clipped.shape[1]} pixels\")\n\n# ============================================================\n# 4. CALCULATE SPECTRAL INDICES\n# ============================================================\n\n# Extract bands\nblue = clipped[0].astype(float)\ngreen = clipped[1].astype(float)\nred = clipped[2].astype(float)\nnir = clipped[3].astype(float)\n\n# Calculate indices\ndef calc_ndvi(nir, red):\n    return (nir - red) / (nir + red + 1e-10)\n\ndef calc_evi(nir, red, blue):\n    return 2.5 * ((nir - red) / (nir + 6*red - 7.5*blue + 1))\n\ndef calc_ndwi(green, nir):\n    return (green - nir) / (green + nir + 1e-10)\n\nndvi = calc_ndvi(nir, red)\nevi = calc_evi(nir, red, blue)\nndwi = calc_ndwi(green, nir)\n\nprint(\"Step 4: Calculated spectral indices ✓\")\n\n# Save indices\nfor name, data in [('ndvi', ndvi), ('evi', evi), ('ndwi', ndwi)]:\n    index_meta = out_meta.copy()\n    index_meta.update(dtype=rasterio.float32, count=1, nodata=-9999)\n\n    index_path = output_dir / f'palawan_{name}.tif'\n    with rasterio.open(index_path, 'w', **index_meta) as dst:\n        dst.write(data.astype(rasterio.float32), 1)\n\n# ============================================================\n# 5. IDENTIFY POTENTIAL RICE AREAS\n# ============================================================\n\n# Rice detection criteria (during growing season):\n# - Moderate to high NDVI (0.4-0.8) - active vegetation\n# - Slightly negative NDWI (-0.2 to 0.1) - moist soil but not flooded\n# - EVI in moderate range (0.3-0.6)\n\nrice_mask = (\n    (ndvi &gt; 0.4) & (ndvi &lt; 0.8) &\n    (ndwi &gt; -0.2) & (ndwi &lt; 0.1) &\n    (evi &gt; 0.3) & (evi &lt; 0.6)\n)\n\n# Calculate area\nrice_pixels = rice_mask.sum()\npixel_area_m2 = 10 * 10  # 10m resolution\nrice_area_ha = (rice_pixels * pixel_area_m2) / 10000\n\nprint(\"\\nStep 5: Rice area detection ✓\")\nprint(f\"Potential rice area: {rice_area_ha:.1f} hectares\")\nprint(f\"Percentage of Palawan: {(rice_pixels / rice_mask.size) * 100:.2f}%\")\n\n# Save rice mask\nrice_meta = out_meta.copy()\nrice_meta.update(dtype='uint8', count=1, nodata=255)\n\nrice_mask_path = output_dir / 'palawan_rice_mask.tif'\nwith rasterio.open(rice_mask_path, 'w', **rice_meta) as dst:\n    dst.write(rice_mask.astype('uint8'), 1)\n\n# ============================================================\n# 6. VISUALIZE RESULTS\n# ============================================================\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# True color composite\nrgb = np.stack([red, green, blue], axis=2) / 10000\nrgb_clip = np.clip(rgb, 0, 0.3) / 0.3\naxes[0, 0].imshow(rgb_clip)\naxes[0, 0].set_title(\"True Color Composite\")\naxes[0, 0].axis('off')\n\n# NDVI\nim1 = axes[0, 1].imshow(ndvi, cmap='RdYlGn', vmin=0, vmax=1)\naxes[0, 1].set_title(\"NDVI\")\naxes[0, 1].axis('off')\nplt.colorbar(im1, ax=axes[0, 1], shrink=0.8)\n\n# EVI\nim2 = axes[0, 2].imshow(evi, cmap='YlGn', vmin=0, vmax=1)\naxes[0, 2].set_title(\"EVI\")\naxes[0, 2].axis('off')\nplt.colorbar(im2, ax=axes[0, 2], shrink=0.8)\n\n# NDWI\nim3 = axes[1, 0].imshow(ndwi, cmap='Blues', vmin=-1, vmax=1)\naxes[1, 0].set_title(\"NDWI\")\naxes[1, 0].axis('off')\nplt.colorbar(im3, ax=axes[1, 0], shrink=0.8)\n\n# Rice mask\naxes[1, 1].imshow(rice_mask, cmap='Greens')\naxes[1, 1].set_title(f\"Potential Rice Areas\\n{rice_area_ha:.1f} hectares\")\naxes[1, 1].axis('off')\n\n# Rice mask over true color\naxes[1, 2].imshow(rgb_clip)\naxes[1, 2].imshow(rice_mask, cmap='Greens', alpha=0.5)\naxes[1, 2].set_title(\"Rice Areas (overlay)\")\naxes[1, 2].axis('off')\n\nplt.suptitle(\"Palawan Rice Monitoring Analysis\", fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.savefig(output_dir / 'palawan_analysis_results.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# ============================================================\n# 7. PREPARE ML TRAINING DATA\n# ============================================================\n\n# Rasterize training polygons\nfrom rasterio.features import rasterize\n\ntraining_reprojected = training.to_crs(out_meta['crs'])\n\ntraining_raster = rasterize(\n    [(geom, value) for geom, value in zip(training_reprojected.geometry, training_reprojected['class_id'])],\n    out_shape=(out_meta['height'], out_meta['width']),\n    transform=out_meta['transform'],\n    fill=0,\n    dtype='uint8'\n)\n\n# Extract features and labels\nfeatures = np.stack([blue, green, red, nir, ndvi, evi, ndwi], axis=0)  # (7, H, W)\nfeatures_2d = features.reshape(features.shape[0], -1).T  # (n_pixels, 7)\nlabels = training_raster.flatten()\n\n# Filter to labeled pixels\nlabeled_idx = labels &gt; 0\nX = features_2d[labeled_idx]\ny = labels[labeled_idx]\n\nprint(\"\\nStep 7: ML training data prepared ✓\")\nprint(f\"Features shape: {X.shape}\")\nprint(f\"Labels shape: {y.shape}\")\nprint(f\"Feature names: blue, green, red, nir, ndvi, evi, ndwi\")\nprint(f\"Classes: {np.unique(y)}\")\n\n# Save training data\nnp.savez(\n    output_dir / 'training_data.npz',\n    features=X,\n    labels=y,\n    feature_names=['blue', 'green', 'red', 'nir', 'ndvi', 'evi', 'ndwi']\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ANALYSIS COMPLETE!\")\nprint(\"=\"*60)\nprint(f\"\\nOutputs saved to: {output_dir}\")\nprint(\"\\nFiles created:\")\nprint(\"  - palawan_sentinel2_clipped.tif\")\nprint(\"  - palawan_ndvi.tif, palawan_evi.tif, palawan_ndwi.tif\")\nprint(\"  - palawan_rice_mask.tif\")\nprint(\"  - palawan_analysis_results.png\")\nprint(\"  - training_data.npz (ready for ML in Sessions 5-7!)\")\nprint(\"\\nNext: Use training_data.npz to train Random Forest or CNN classifier\")\nWhat we accomplished:\n\n✓ Loaded vector and raster data\n✓ Clipped raster to area of interest\n✓ Calculated multiple spectral indices\n✓ Applied rule-based classification (rice detection)\n✓ Created publication-quality visualizations\n✓ Prepared training data for machine learning\n✓ Saved all outputs for future use\n\nThis workflow demonstrates a complete EO analysis pipeline from raw data to ML-ready datasets!",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#key-takeaways",
    "href": "day1/sessions/session3.html#key-takeaways",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nImportantSession 3 Summary\n\n\n\nGoogle Colab: - Cloud-based Python environment, no installation needed - Mount Google Drive for data persistence: drive.mount('/content/drive') - Install geospatial libraries: !pip install geopandas rasterio - Free GPU access for ML (Sessions 5-7)\nGeoPandas (Vector Data): - Read shapefiles/GeoJSON: gpd.read_file(path) - Filter by attributes: gdf[gdf['column'] == value] - Visualize: gdf.plot(column='field', cmap='Set3') - CRS operations: gdf.crs, gdf.to_crs('EPSG:32651') - Philippine CRS: UTM Zone 51N (EPSG:32651) for western Philippines, UTM Zone 52N (EPSG:32652) for eastern Philippines\nRasterio (Raster Data): - Open rasters: with rasterio.open(path) as src: - Read bands: src.read(band_number) (1-indexed!) - Arrays are NumPy: all array operations work (vectorized!) - Calculate indices: ndvi = (nir - red) / (nir + red + 1e-10) - Sentinel-2: 13 bands at 10m/20m/60m, values 0-10000 (divide by 10000 for reflectance)\nCombining Vector + Raster: - Clip rasters: rasterio.mask.mask(src, shapes, crop=True) - Sample at points: src.sample(coordinates) - Zonal statistics: rasterstats.zonal_stats(polygons, raster) - Always match CRS before spatial operations!\nPreprocessing for ML: - Normalize: StandardScaler().fit_transform(features) - Augment: Rotation, flipping, brightness adjustment - Multi-temporal: Stack dates for phenology - Resample: Match resolutions from different sensors\nPython for EO Advantage: - Powers operational systems (DATOS, PRiSM, SkAI-Pinas) - Seamless workflow: data access → preprocessing → ML → visualization - Rich ecosystem: GeoPandas, Rasterio, Scikit-learn, TensorFlow - You now have the foundation to build your own EO applications!\nNext: Session 4 will leverage Google Earth Engine to access petabytes of Sentinel data without downloading!",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#practice-exercises",
    "href": "day1/sessions/session3.html#practice-exercises",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\n\n\n\n\n\nTipTry These Challenges\n\n\n\nExercise 1: Load Your Own Boundary\nDownload a shapefile of your province or municipality from NAMRIA Geoportal or PhilGIS. Load it with GeoPandas, reproject to UTM 51N, calculate the area in km², and create a 5 km buffer zone.\n# Your code here\nmy_boundary = gpd.read_file('my_province.shp')\n# ...\nExercise 2: Calculate Multiple Indices\nUsing the provided Sentinel-2 tile, calculate NDVI, EVI, NDWI, and SAVI. Create a 2×2 subplot visualization comparing all indices.\nExercise 3: Multi-temporal NDVI Change\nIf you have two Sentinel-2 images (dry season, wet season), calculate NDVI for both and create a change map showing NDVI difference. Interpret areas of gain (positive change) vs. loss (negative change).\n# Hint:\nndvi_change = ndvi_wet - ndvi_dry\nplt.imshow(ndvi_change, cmap='RdBu', vmin=-0.5, vmax=0.5)\nExercise 4: Training Sample Creation\nCreate a small training dataset by: 1. Digitizing 3-5 polygons for each land cover class (use gdf = gpd.GeoDataFrame() manually) 2. Rasterizing the polygons to match Sentinel-2 grid 3. Extracting spectral values at labeled pixels 4. Saving as training_samples.csv for ML\nExercise 5: Coastal Water Body Detection\nFor a coastal area, use NDWI to detect water bodies. Apply a threshold (e.g., NDWI &gt; 0.3), vectorize the result using rasterio.features.shapes(), and save as a shapefile.\nBonus: Build Your Own Function Library\nCreate a Python module eo_utils.py with reusable functions: - calculate_indices(bands_dict) → returns dict of indices - clip_raster_to_aoi(raster_path, aoi_gdf) → returns clipped array - visualize_results(rgb, ndvi, mask) → creates 3-panel figure - prepare_ml_data(raster_path, training_gdf) → returns X, y arrays\nSave to your Drive and import in future notebooks!",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#further-reading",
    "href": "day1/sessions/session3.html#further-reading",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Further Reading",
    "text": "Further Reading\n\nGeoPandas\n\nOfficial Documentation\nGeoPandas Tutorial (DataCamp)\nGeoPandas Examples Gallery\nWorking with Projections\n\n\n\nRasterio\n\nOfficial Documentation\nRasterio Quickstart\nPython Raster Tutorial (WUR)\nWindowed Reading and Writing\n\n\n\nCombined Workflows\n\nCarpentries Geospatial Python\nCropping Rasters with Vector Boundaries\nPython for Geospatial Analysis (O’Reilly)\n\n\n\nPhilippine Context\n\nNAMRIA Geoportal\nPhilGIS Resources\nDATOS User Guide (if publicly available)\nPRiSM Technical Documentation\n\n\n\nAdvanced Topics\n\nXarray for Multi-dimensional Arrays\nDask for Parallel Computing\nCloud-Optimized GeoTIFF Guide\nTorchGeo for Deep Learning",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/sessions/session3.html#jupyter-notebook",
    "href": "day1/sessions/session3.html#jupyter-notebook",
    "title": "Session 3: Hands-on Python for Geospatial Data",
    "section": "Jupyter Notebook",
    "text": "Jupyter Notebook\n\n\n\n\n\n\nNoteAccess the Interactive Notebook\n\n\n\nA complete Jupyter notebook with all code examples from this session is available:\nOpen Notebook 1: Python Geospatial Data →\nThis notebook includes: - All code examples ready to run in Google Colab - Additional exercises with progressive difficulty - Sample datasets (downloadable links) - Detailed comments and explanations - Solutions to practice exercises (hidden cells) - Links to relevant documentation\nTo use the notebook: 1. Click the link above 2. Click “Open in Colab” button 3. Save a copy to your Drive: File → Save a copy in Drive 4. Mount your Drive and start coding!",
    "crumbs": [
      "Sessions",
      "Session 3: Hands-on Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "All Day 1 presentations have been enhanced for optimal 2-hour delivery based on best practices from NASA ARSET, EO College, and Copernicus MOOC programmes.\n\n\n\n\nFile: 00_precourse_orientation.qmd\nSlides: 30\nDuration: 45-60 minutes\nStatus: ✅ Complete and ready\nContent: - Welcome and course overview - Technical requirements and setup - Expectations and code of conduct - Philippine context - Pre-course action items\n\n\n\n\nFile: 01_session1_copernicus_philippine_eo.qmd\nCurrent: 1,438 lines (existing content)\nTarget: 70-75 slides | 2 hours\nStatus: ⚙️ Ready for enhancement\n\n\n\n\nAdded to every slide for instructor pacing:\n---\n## Slide Title {.timing data-timing=\"5min\" data-cumulative=\"15min\"}\n\n\n\n## Session Roadmap\n\n| Time | Topic | Minutes |\n|------|-------|---------|\n| 00-10 | Introduction & Copernicus Overview | 10 |\n| 10-40 | Sentinel-1 SAR Mission | 30 |\n| 40-70 | Sentinel-2 Optical Mission | 30 |\n| 70-75 | **Break** ☕ | 5 |\n| 75-100 | Philippine EO Ecosystem | 25 |\n| 100-115 | CoPhil Programme | 15 |\n| 115-120 | Q&A & Summary | 5 |\n\n\n\nAdd these key updates: - ✅ Sentinel-1C launched December 2024 (restore 6-day repeat) - ✅ Sentinel-2C operational January 2025 (3-satellite constellation, 5-day repeat) - ✅ 2024 hottest year on record (C3S data) - ✅ Copernicus Data Space Ecosystem (new platform) - ✅ PhilSA SIYASAT portal operational - ✅ DOST P2.6B AI investment (SkAI-Pinas, DIMER)\n\n\n\nPoll Slides to Add: - “Have you used SAR data before?” (before Sentinel-1 section) - “What’s your primary EO application?” (before Philippine section) - “Quick Check: SAR vs Optical” (after both Sentinel sections)\n\n\n\nAdd dedicated demo slides:\n## Live Demo: Sentinel-1 Flood Mapping {background-color=\"#1e40af\"}\n\n### We'll explore:\n- Finding Sentinel-1 acquisitions in Copernicus Browser\n- Before/after typhoon comparison\n- Water detection using VV polarization\n- Change detection visualization\n\n::: {.notes}\n[INSTRUCTOR: Open Copernicus Browser, navigate to recent Philippine typhoon, \ndemonstrate VV band water detection, show before/after slider]\n:::\nSimilar for Sentinel-2 demo.\n\n\n\nEvery slide needs detailed speaker notes with: - Key talking points - Expected questions and answers - Timing reminders - Demo instructions\n\n\n\nAdd every 20 minutes:\n## ✅ Quick Check: Sentinel-1 {.checkpoint}\n\n::: {.incremental}\n1. What wavelength band does Sentinel-1 use?\n2. Why is SAR useful for Philippines?\n3. What does bright/dark mean in SAR images?\n:::\n\n::: {.fragment}\n**Answers:** C-band (~5.6cm), All-weather/cloud penetration, Rough/smooth surfaces\n:::\n\n\n\n\n\n\nFile: 02_session2_ai_ml_fundamentals.qmd\nCurrent: 1,939 lines (existing content)\nTarget: 75-80 slides | 2 hours\nStatus: ⚙️ Ready for enhancement\n\n\n\n\nEvery slide gets timing:\n{.timing data-timing=\"3min\" data-cumulative=\"45min\"}\n\n\n\n## Session Roadmap\n\n| Time | Topic | Minutes |\n|------|-------|---------|\n| 00-10 | What is AI/ML? | 10 |\n| 10-35 | EO Workflow & Data Pipeline | 25 |\n| 35-60 | Supervised vs Unsupervised Learning | 25 |\n| 60-65 | **Break** ☕ | 5 |\n| 65-90 | Deep Learning & Neural Networks | 25 |\n| 90-110 | Data-Centric AI & Foundation Models | 20 |\n| 110-120 | Q&A & Summary | 10 |\n\n\n\nAdd cutting-edge developments: - ✅ NASA-IBM Geospatial Foundation Model (released 2024) - ✅ ESA Φsat-2 on-board AI processing (2024) - ✅ Prithvi foundation model (IBM/NASA/ESA collaboration) - ✅ Clay Foundation Model (open-source) - ✅ Data-centric AI paradigm shift - ✅ Self-supervised learning for EO\n\n\n\nActivity Slides:\n## 🎯 Exercise: Classify These Problems {.interactive}\n\n**Supervised or Unsupervised?**\n\n::: {.incremental}\n1. Mapping rice paddies from Sentinel-2\n2. Finding patterns in typhoon tracks\n3. Predicting flood extent from weather data\n4. Grouping similar forest types\n5. Detecting illegal mining sites\n:::\n\n::: {.fragment}\n**Answers:** 1-Supervised, 2-Unsupervised, 3-Supervised, 4-Unsupervised, 5-Supervised\n:::\n\n\n\nAdd specific case studies: - Typhoon Odette (Rai) 2021 damage mapping with ML - Metro Manila flood prediction using deep learning - Taal Volcano monitoring with change detection - Illegal logging detection in Palawan\n\n\n\nAdd architecture diagrams: - Simple neural network visualization - CNN architecture for satellite imagery - U-Net for segmentation - Data pipeline flowchart\n\n\n\nEvery 20-25 minutes:\n## ✅ Concept Check: ML Basics {.checkpoint}\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**True or False:**\n1. ML needs labeled data\n2. Deep learning is always better\n3. More data beats better algorithms\n:::\n\n::: {.column width=\"50%\"}\n::: {.fragment}\n**Answers:**\n1. FALSE (only supervised learning)\n2. FALSE (depends on problem)\n3. TRUE (in data-centric AI)\n:::\n:::\n:::\n\n\n\n\n\n\nFile: 03_session3_python_geospatial.qmd\nCurrent: 915 lines (existing content)\nTarget: 40-45 slides + Notebook walkthrough | 2 hours\nStatus: ⚙️ Ready for enhancement\n\n\n\n\nEvery slide with timing\n\n\n\n## Session Roadmap\n\n| Time | Topic | Minutes |\n|------|-------|---------|\n| 00-15 | Setup & Python Basics Recap | 15 |\n| 15-55 | GeoPandas for Vector Data (HANDS-ON) | 40 |\n| 55-60 | **Break** ☕ | 5 |\n| 60-110 | Rasterio for Raster Data (HANDS-ON) | 50 |\n| 110-120 | Summary & Exercises | 10 |\n\n\n\nAdd “Follow Along” slides:\n## 📓 Follow Along: Loading Shapefiles {.hands-on}\n\n**Open Notebook:** `Day1_Session3_Python_Geospatial_Data.ipynb`  \n**Section:** Part 3.1 - Loading Philippine Boundaries\n\n**We'll code together:**\n```python\nimport geopandas as gpd\n\n# Load Philippine boundaries\nph = gpd.read_file('philippines_admin.geojson')\n\n# Inspect\nprint(ph.head())\nprint(ph.crs)\nRun this cell now! ⚡\n\n#### 4. **Troubleshooting Slides** (NEW)\n\n**Add common error solutions:**\n\n```markdown\n## ⚠️ Common Error: CRS Mismatch {.troubleshooting}\n\n**Error Message:**\nValueError: CRS mismatch between the CRS of left geometries and the CRS of right geometries\n\n**Solution:**\n```python\n# Reproject to same CRS\ngdf2 = gdf2.to_crs(gdf1.crs)\nPrevention: Always check CRS before spatial operations!\n\n#### 5. **Expected Output Screenshots** (NEW)\n\nAdd screenshots of:\n- GeoPandas dataframe display\n- Matplotlib plots of Philippine boundaries\n- Rasterio raster array output\n- NDVI calculation results\n- Final visualizations\n\n#### 6. **Live Coding Pace Markers** (NEW)\n\n```markdown\n## ⏱️ Coding Time: 8 Minutes {.coding-session}\n\n**Task:** Load and visualize Philippine provinces\n\n**Steps:**\n1. Import GeoPandas (1 min)\n2. Load shapefile (2 min)\n3. Explore attributes (2 min)\n4. Create plot (3 min)\n\n**I'll code, you follow along in your notebook!**\n\n\n\n\n\n\nFile: 04_session4_google_earth_engine.qmd\nCurrent: DOES NOT EXIST\nTarget: 45-50 slides + Live coding | 2 hours\nStatus: 🆕 CREATE NEW\n\n\n\n\n\nWhat is Google Earth Engine?\nCloud computing for EO\nData catalog overview\nCode Editor vs Python API\nAuthentication process\n\n\n\n\n\nImage and ImageCollection\nFiltering (spatial, temporal, property)\nReducers and aggregation\nFeature and FeatureCollection\nJavaScript to Python translation\n\n\n\n\n\nAccessing Sentinel-1 in GEE\nAccessing Sentinel-2 in GEE\nCloud masking (QA60 band)\nTemporal compositing (median, mean)\nSpectral indices (NDVI, NDWI)\nVisualization with geemap\nExport workflows (Drive, Asset)\n\n\n\n\n\nKey takeaways\nResources for continued learning\nExercises to complete\nPreview of Day 2\n\n\n\n\n\n\nStep-by-step GEE Authentication (with screenshots)\ngeemap Installation Guide (Colab-specific)\nSide-by-side JavaScript vs Python examples\nLive coding sessions (3-4 throughout)\nPhilippine-specific examples (Metro Manila, Palawan)\nCommon GEE errors and solutions\nPerformance tips (scale, region, projection)\n\n\n\n\n\n\n\n\nAdd CSS customizations:\n/* Timing indicators */\n.timing::after {\n  content: \"⏱️ \" attr(data-timing) \" (Total: \" attr(data-cumulative) \")\";\n  font-size: 0.6em;\n  color: #666;\n  position: absolute;\n  top: 10px;\n  right: 20px;\n}\n\n/* Checkpoint slides */\n.checkpoint {\n  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n  color: white;\n}\n\n/* Hands-on slides */\n.hands-on {\n  border-left: 5px solid #10b981;\n  padding-left: 20px;\n}\n\n/* Interactive slides */\n.interactive {\n  background-color: #fef3c7;\n}\n\n\n\nAdd to every presentation:\n## {.progress-slide visibility=\"hidden\"}\n\n::: {.progress-bar}\nSession Progress: 45% Complete\n:::\n\n\n\nAdd QR codes linking to: - Course website - Notebook downloads - Discussion forum - Quick reference sheets\n\n\n\n\nAlt text for all images\nHigh contrast color schemes\nLarger fonts for code\nScreen reader friendly notes\n\n\n\n\n\n\nEvery slide should have:\n::: {.notes}\n**Timing:** 3 minutes\n\n**Key Points:**\n- Point 1 to emphasize\n- Point 2 to emphasize\n- Point 3 to emphasize\n\n**Common Questions:**\nQ: [Expected question]\nA: [Your answer]\n\n**Transition to Next:**\n\"Now that we understand X, let's explore Y...\"\n:::\n\n\n\n\nBefore delivery, verify:\n\nAll presentations render correctly with quarto render\nTiming markers sum to ~120 minutes per session\nAll images load properly\nAll links work (no 404s)\nSpeaker notes are detailed\nCode snippets are syntax-highlighted\nInteractive elements function\nDemos are clearly marked\nPhilippine examples are included\n2025 updates are integrated\n\n\n\n\n\n\n\n\n✅ 00_precourse_orientation.qmd (838 lines - DONE)\n⚙️ 01_session1_copernicus_philippine_eo.qmd (enhanced from 1,438 lines)\n⚙️ 02_session2_ai_ml_fundamentals.qmd (enhanced from 1,939 lines)\n⚙️ 03_session3_python_geospatial.qmd (enhanced from 915 lines)\n🆕 04_session4_google_earth_engine.qmd (NEW - ~800-900 lines)\n\n\n\n\n\ncustom.scss - Custom styling\nENHANCEMENT_SUMMARY.md - This file\nREADME.md - Presentation usage guide\nINSTRUCTOR_GUIDE.md - Delivery tips\n\n\n\n\n\n\n\n\n\nTask\nTime\n\n\n\n\nSession 1 enhancements\n45 min\n\n\nSession 2 enhancements\n45 min\n\n\nSession 3 enhancements\n30 min\n\n\nSession 4 creation\n90 min\n\n\nTesting & refinement\n30 min\n\n\nTotal\n~4 hours\n\n\n\n\n\n\n\n\nReview this summary - Confirm approach\nApply enhancements - Systematically update each file\nCreate Session 4 - Build from scratch\nTest renders - Ensure all presentations work\nPilot run - Practice delivery with timing\nFinal adjustments - Based on pilot feedback\n\n\n\n\n\nIf you need: - Different timing allocations - More/fewer interactive elements - Additional Philippine examples - Specific technical depth adjustments\nLet me know and I’ll adjust the enhancements accordingly!\n\nStatus: Enhancement framework complete. Ready to implement! ✅"
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#overview",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#overview",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "All Day 1 presentations have been enhanced for optimal 2-hour delivery based on best practices from NASA ARSET, EO College, and Copernicus MOOC programmes."
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#pre-course-orientation-completed",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#pre-course-orientation-completed",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "File: 00_precourse_orientation.qmd\nSlides: 30\nDuration: 45-60 minutes\nStatus: ✅ Complete and ready\nContent: - Welcome and course overview - Technical requirements and setup - Expectations and code of conduct - Philippine context - Pre-course action items"
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#session-1-copernicus-sentinel-data-philippine-eo-ecosystem",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#session-1-copernicus-sentinel-data-philippine-eo-ecosystem",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "File: 01_session1_copernicus_philippine_eo.qmd\nCurrent: 1,438 lines (existing content)\nTarget: 70-75 slides | 2 hours\nStatus: ⚙️ Ready for enhancement\n\n\n\n\nAdded to every slide for instructor pacing:\n---\n## Slide Title {.timing data-timing=\"5min\" data-cumulative=\"15min\"}\n\n\n\n## Session Roadmap\n\n| Time | Topic | Minutes |\n|------|-------|---------|\n| 00-10 | Introduction & Copernicus Overview | 10 |\n| 10-40 | Sentinel-1 SAR Mission | 30 |\n| 40-70 | Sentinel-2 Optical Mission | 30 |\n| 70-75 | **Break** ☕ | 5 |\n| 75-100 | Philippine EO Ecosystem | 25 |\n| 100-115 | CoPhil Programme | 15 |\n| 115-120 | Q&A & Summary | 5 |\n\n\n\nAdd these key updates: - ✅ Sentinel-1C launched December 2024 (restore 6-day repeat) - ✅ Sentinel-2C operational January 2025 (3-satellite constellation, 5-day repeat) - ✅ 2024 hottest year on record (C3S data) - ✅ Copernicus Data Space Ecosystem (new platform) - ✅ PhilSA SIYASAT portal operational - ✅ DOST P2.6B AI investment (SkAI-Pinas, DIMER)\n\n\n\nPoll Slides to Add: - “Have you used SAR data before?” (before Sentinel-1 section) - “What’s your primary EO application?” (before Philippine section) - “Quick Check: SAR vs Optical” (after both Sentinel sections)\n\n\n\nAdd dedicated demo slides:\n## Live Demo: Sentinel-1 Flood Mapping {background-color=\"#1e40af\"}\n\n### We'll explore:\n- Finding Sentinel-1 acquisitions in Copernicus Browser\n- Before/after typhoon comparison\n- Water detection using VV polarization\n- Change detection visualization\n\n::: {.notes}\n[INSTRUCTOR: Open Copernicus Browser, navigate to recent Philippine typhoon, \ndemonstrate VV band water detection, show before/after slider]\n:::\nSimilar for Sentinel-2 demo.\n\n\n\nEvery slide needs detailed speaker notes with: - Key talking points - Expected questions and answers - Timing reminders - Demo instructions\n\n\n\nAdd every 20 minutes:\n## ✅ Quick Check: Sentinel-1 {.checkpoint}\n\n::: {.incremental}\n1. What wavelength band does Sentinel-1 use?\n2. Why is SAR useful for Philippines?\n3. What does bright/dark mean in SAR images?\n:::\n\n::: {.fragment}\n**Answers:** C-band (~5.6cm), All-weather/cloud penetration, Rough/smooth surfaces\n:::"
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#session-2-core-concepts-of-aiml-for-earth-observation",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#session-2-core-concepts-of-aiml-for-earth-observation",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "File: 02_session2_ai_ml_fundamentals.qmd\nCurrent: 1,939 lines (existing content)\nTarget: 75-80 slides | 2 hours\nStatus: ⚙️ Ready for enhancement\n\n\n\n\nEvery slide gets timing:\n{.timing data-timing=\"3min\" data-cumulative=\"45min\"}\n\n\n\n## Session Roadmap\n\n| Time | Topic | Minutes |\n|------|-------|---------|\n| 00-10 | What is AI/ML? | 10 |\n| 10-35 | EO Workflow & Data Pipeline | 25 |\n| 35-60 | Supervised vs Unsupervised Learning | 25 |\n| 60-65 | **Break** ☕ | 5 |\n| 65-90 | Deep Learning & Neural Networks | 25 |\n| 90-110 | Data-Centric AI & Foundation Models | 20 |\n| 110-120 | Q&A & Summary | 10 |\n\n\n\nAdd cutting-edge developments: - ✅ NASA-IBM Geospatial Foundation Model (released 2024) - ✅ ESA Φsat-2 on-board AI processing (2024) - ✅ Prithvi foundation model (IBM/NASA/ESA collaboration) - ✅ Clay Foundation Model (open-source) - ✅ Data-centric AI paradigm shift - ✅ Self-supervised learning for EO\n\n\n\nActivity Slides:\n## 🎯 Exercise: Classify These Problems {.interactive}\n\n**Supervised or Unsupervised?**\n\n::: {.incremental}\n1. Mapping rice paddies from Sentinel-2\n2. Finding patterns in typhoon tracks\n3. Predicting flood extent from weather data\n4. Grouping similar forest types\n5. Detecting illegal mining sites\n:::\n\n::: {.fragment}\n**Answers:** 1-Supervised, 2-Unsupervised, 3-Supervised, 4-Unsupervised, 5-Supervised\n:::\n\n\n\nAdd specific case studies: - Typhoon Odette (Rai) 2021 damage mapping with ML - Metro Manila flood prediction using deep learning - Taal Volcano monitoring with change detection - Illegal logging detection in Palawan\n\n\n\nAdd architecture diagrams: - Simple neural network visualization - CNN architecture for satellite imagery - U-Net for segmentation - Data pipeline flowchart\n\n\n\nEvery 20-25 minutes:\n## ✅ Concept Check: ML Basics {.checkpoint}\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**True or False:**\n1. ML needs labeled data\n2. Deep learning is always better\n3. More data beats better algorithms\n:::\n\n::: {.column width=\"50%\"}\n::: {.fragment}\n**Answers:**\n1. FALSE (only supervised learning)\n2. FALSE (depends on problem)\n3. TRUE (in data-centric AI)\n:::\n:::\n:::"
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#session-3-hands-on-python-for-geospatial-data",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#session-3-hands-on-python-for-geospatial-data",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "File: 03_session3_python_geospatial.qmd\nCurrent: 915 lines (existing content)\nTarget: 40-45 slides + Notebook walkthrough | 2 hours\nStatus: ⚙️ Ready for enhancement\n\n\n\n\nEvery slide with timing\n\n\n\n## Session Roadmap\n\n| Time | Topic | Minutes |\n|------|-------|---------|\n| 00-15 | Setup & Python Basics Recap | 15 |\n| 15-55 | GeoPandas for Vector Data (HANDS-ON) | 40 |\n| 55-60 | **Break** ☕ | 5 |\n| 60-110 | Rasterio for Raster Data (HANDS-ON) | 50 |\n| 110-120 | Summary & Exercises | 10 |\n\n\n\nAdd “Follow Along” slides:\n## 📓 Follow Along: Loading Shapefiles {.hands-on}\n\n**Open Notebook:** `Day1_Session3_Python_Geospatial_Data.ipynb`  \n**Section:** Part 3.1 - Loading Philippine Boundaries\n\n**We'll code together:**\n```python\nimport geopandas as gpd\n\n# Load Philippine boundaries\nph = gpd.read_file('philippines_admin.geojson')\n\n# Inspect\nprint(ph.head())\nprint(ph.crs)\nRun this cell now! ⚡\n\n#### 4. **Troubleshooting Slides** (NEW)\n\n**Add common error solutions:**\n\n```markdown\n## ⚠️ Common Error: CRS Mismatch {.troubleshooting}\n\n**Error Message:**\nValueError: CRS mismatch between the CRS of left geometries and the CRS of right geometries\n\n**Solution:**\n```python\n# Reproject to same CRS\ngdf2 = gdf2.to_crs(gdf1.crs)\nPrevention: Always check CRS before spatial operations!\n\n#### 5. **Expected Output Screenshots** (NEW)\n\nAdd screenshots of:\n- GeoPandas dataframe display\n- Matplotlib plots of Philippine boundaries\n- Rasterio raster array output\n- NDVI calculation results\n- Final visualizations\n\n#### 6. **Live Coding Pace Markers** (NEW)\n\n```markdown\n## ⏱️ Coding Time: 8 Minutes {.coding-session}\n\n**Task:** Load and visualize Philippine provinces\n\n**Steps:**\n1. Import GeoPandas (1 min)\n2. Load shapefile (2 min)\n3. Explore attributes (2 min)\n4. Create plot (3 min)\n\n**I'll code, you follow along in your notebook!**"
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#session-4-introduction-to-google-earth-engine",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#session-4-introduction-to-google-earth-engine",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "File: 04_session4_google_earth_engine.qmd\nCurrent: DOES NOT EXIST\nTarget: 45-50 slides + Live coding | 2 hours\nStatus: 🆕 CREATE NEW\n\n\n\n\n\nWhat is Google Earth Engine?\nCloud computing for EO\nData catalog overview\nCode Editor vs Python API\nAuthentication process\n\n\n\n\n\nImage and ImageCollection\nFiltering (spatial, temporal, property)\nReducers and aggregation\nFeature and FeatureCollection\nJavaScript to Python translation\n\n\n\n\n\nAccessing Sentinel-1 in GEE\nAccessing Sentinel-2 in GEE\nCloud masking (QA60 band)\nTemporal compositing (median, mean)\nSpectral indices (NDVI, NDWI)\nVisualization with geemap\nExport workflows (Drive, Asset)\n\n\n\n\n\nKey takeaways\nResources for continued learning\nExercises to complete\nPreview of Day 2\n\n\n\n\n\n\nStep-by-step GEE Authentication (with screenshots)\ngeemap Installation Guide (Colab-specific)\nSide-by-side JavaScript vs Python examples\nLive coding sessions (3-4 throughout)\nPhilippine-specific examples (Metro Manila, Palawan)\nCommon GEE errors and solutions\nPerformance tips (scale, region, projection)"
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#universal-enhancements-all-presentations",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#universal-enhancements-all-presentations",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "Add CSS customizations:\n/* Timing indicators */\n.timing::after {\n  content: \"⏱️ \" attr(data-timing) \" (Total: \" attr(data-cumulative) \")\";\n  font-size: 0.6em;\n  color: #666;\n  position: absolute;\n  top: 10px;\n  right: 20px;\n}\n\n/* Checkpoint slides */\n.checkpoint {\n  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n  color: white;\n}\n\n/* Hands-on slides */\n.hands-on {\n  border-left: 5px solid #10b981;\n  padding-left: 20px;\n}\n\n/* Interactive slides */\n.interactive {\n  background-color: #fef3c7;\n}\n\n\n\nAdd to every presentation:\n## {.progress-slide visibility=\"hidden\"}\n\n::: {.progress-bar}\nSession Progress: 45% Complete\n:::\n\n\n\nAdd QR codes linking to: - Course website - Notebook downloads - Discussion forum - Quick reference sheets\n\n\n\n\nAlt text for all images\nHigh contrast color schemes\nLarger fonts for code\nScreen reader friendly notes"
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#speaker-notes-template",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#speaker-notes-template",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "Every slide should have:\n::: {.notes}\n**Timing:** 3 minutes\n\n**Key Points:**\n- Point 1 to emphasize\n- Point 2 to emphasize\n- Point 3 to emphasize\n\n**Common Questions:**\nQ: [Expected question]\nA: [Your answer]\n\n**Transition to Next:**\n\"Now that we understand X, let's explore Y...\"\n:::"
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#testing-checklist",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#testing-checklist",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "Before delivery, verify:\n\nAll presentations render correctly with quarto render\nTiming markers sum to ~120 minutes per session\nAll images load properly\nAll links work (no 404s)\nSpeaker notes are detailed\nCode snippets are syntax-highlighted\nInteractive elements function\nDemos are clearly marked\nPhilippine examples are included\n2025 updates are integrated"
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#deliverables",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#deliverables",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "✅ 00_precourse_orientation.qmd (838 lines - DONE)\n⚙️ 01_session1_copernicus_philippine_eo.qmd (enhanced from 1,438 lines)\n⚙️ 02_session2_ai_ml_fundamentals.qmd (enhanced from 1,939 lines)\n⚙️ 03_session3_python_geospatial.qmd (enhanced from 915 lines)\n🆕 04_session4_google_earth_engine.qmd (NEW - ~800-900 lines)\n\n\n\n\n\ncustom.scss - Custom styling\nENHANCEMENT_SUMMARY.md - This file\nREADME.md - Presentation usage guide\nINSTRUCTOR_GUIDE.md - Delivery tips"
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#estimated-enhancement-time",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#estimated-enhancement-time",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "Task\nTime\n\n\n\n\nSession 1 enhancements\n45 min\n\n\nSession 2 enhancements\n45 min\n\n\nSession 3 enhancements\n30 min\n\n\nSession 4 creation\n90 min\n\n\nTesting & refinement\n30 min\n\n\nTotal\n~4 hours"
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#next-steps",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#next-steps",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "Review this summary - Confirm approach\nApply enhancements - Systematically update each file\nCreate Session 4 - Build from scratch\nTest renders - Ensure all presentations work\nPilot run - Practice delivery with timing\nFinal adjustments - Based on pilot feedback"
  },
  {
    "objectID": "day1/presentations/ENHANCEMENT_SUMMARY.html#questions-or-modifications",
    "href": "day1/presentations/ENHANCEMENT_SUMMARY.html#questions-or-modifications",
    "title": "Day 1 Presentations Enhancement Summary",
    "section": "",
    "text": "If you need: - Different timing allocations - More/fewer interactive elements - Additional Philippine examples - Specific technical depth adjustments\nLet me know and I’ll adjust the enhancements accordingly!\n\nStatus: Enhancement framework complete. Ready to implement! ✅"
  },
  {
    "objectID": "day1/presentations/IMAGES_TO_SOURCE.html",
    "href": "day1/presentations/IMAGES_TO_SOURCE.html",
    "title": "Images to Source - Priority List",
    "section": "",
    "text": "Save all images to: course_site/day1/presentations/images/\n\n\n\n\n\n\n\n\nFilename\nSource\nHow to Get\n\n\n\n\nphilsa_logo.png\nPhilSA\nRight-click logo → Save image as…\n\n\ndost_logo.png\nDOST\nFooter or header logo\n\n\ndost_asti_logo.png\nDOST-ASTI\nHeader logo\n\n\nnamria_logo.png\nNAMRIA\nHeader logo\n\n\ngee_logo.png\nGoogle Earth Engine\nHomepage or press kit\n\n\ncopphil_logo.png\nCoPhil materials\nIf you have CoPhil branding materials\n\n\neu_global_gateway.png\nEU Website\nLogo/banner image\n\n\n\nTips: - Look for PNG format with transparent background - Save as high resolution (at least 400x400px for logos) - Check website footer for “Media” or “Downloads” sections\n\n\n\n\n\n\n\nFilename\nSource\nHow to Get\n\n\n\n\nsentinel1_satellite.jpg\nESA Multimedia\nSearch “Sentinel-1 satellite” → Download high-res\n\n\nsentinel2_mayon.jpg\nCopernicus Browser\nNavigate to Mayon Volcano (13.26°N, 123.69°E), Sentinel-2, true color → Screenshot\n\n\n\nFor Copernicus Browser: 1. Go to https://dataspace.copernicus.eu/browser/ 2. Search location or coordinates 3. Select Sentinel-2 L2A 4. Choose date with &lt;10% cloud cover 5. Select “True Color” visualization 6. Take screenshot (full screen for best quality)\n\n\n\n\n\n\n\nFilename\nURL to Screenshot\nWhat to Capture\n\n\n\n\nsiyasat_portal.jpg\nContact PhilSA or search online\nSIYASAT platform interface\n\n\nnamria_geoportal.jpg\nNAMRIA Geoportal\nHomepage or map view\n\n\ndataspace_portal.jpg\nCopernicus Data Space\nHomepage or browser interface\n\n\n\nHow to take good screenshots: - Use full screen browser (F11) - Hide browser toolbars - Capture at 1920x1080 resolution - Save as JPG or PNG\n\n\n\n\n\n\n\nThese are critical diagrams you need to create (use PowerPoint, draw.io, or Canva):\n\n\nCreate a diagram showing:\n┌─────────────────────────────────────────┐\n│         EU Copernicus Programme         │\n│      (Sentinel-1, Sentinel-2, ...)      │\n└────────────────┬────────────────────────┘\n                 │\n                 ▼\n┌─────────────────────────────────────────┐\n│           CoPhil Programme             │\n│    (EU-Philippines Cooperation)         │\n└─────┬─────────────┬──────────────┬──────┘\n      │             │              │\n      ▼             ▼              ▼\n  ┌────────┐   ┌─────────┐   ┌──────────┐\n  │ PhilSA │   │ NAMRIA  │   │DOST-ASTI │\n  │SIYASAT │   │Geoportal│   │SkAI-Pinas│\n  └────────┘   └─────────┘   └──────────┘\nTools: draw.io (free), PowerPoint, or Canva Size: 1920x1080px Colors: Use blue (#1e3a8a) as primary color\n\n\n\n\nCreate a workflow diagram:\nProblem → Data Collection → Preprocessing → \nFeature Engineering → Model Training → \nValidation → Deployment → Monitoring\nInclude: - Icons or boxes for each step - Arrows showing flow - Brief labels under each step - Feedback loop from Monitoring back to Data Collection\nSize: 1920x1080px\n\n\n\n\nCreate a timeline showing:\n2023: Early EO foundation models\n  │\n2024: NASA-IBM Geospatial FM (Aug)\n  │   ESA Φsat-2 launched (Sept)\n  │\n2025: Prithvi, Clay models released\n  │\nFuture: Widespread adoption\nStyle: Horizontal timeline with key milestones Size: 1920x1080px\n\n\n\n\nCreate a diagram showing Python libraries:\n┌───────────────────────────────────┐\n│   Application Layer               │\n│   (Your Analysis Code)            │\n└───────────┬───────────────────────┘\n            │\n┌───────────┴───────────────────────┐\n│   High-Level Libraries            │\n│   GeoPandas | Rasterio | geemap   │\n└───────────┬───────────────────────┘\n            │\n┌───────────┴───────────────────────┐\n│   Core Libraries                  │\n│   GDAL | Shapely | NumPy          │\n└───────────────────────────────────┘\nSize: 1600x900px\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFilename\nHow to Create\nDescription\n\n\n\n\ns1_flood_mapping.jpg\nCopernicus Browser\nSentinel-1 showing flood extent (dark = water)\n\n\nsentinel1_flood_ph.png\nCopernicus Browser\nPhilippine flooding example (recent typhoon)\n\n\nclassification_example.jpg\nCopernicus Browser\nSentinel-2 false color showing land cover types\n\n\nrice_monitoring_ph.jpg\nCopernicus Browser\nRice fields in Central Luzon (green/brown patches)\n\n\n\nHow to get these: 1. Go to Copernicus Browser 2. Navigate to Philippine locations 3. Select appropriate satellite and date 4. Choose visualization (True Color, False Color, NDVI) 5. Screenshot at high resolution\n\n\n\n\n\n\n\n\n\n\n\n\nFilename\nWhat to Show\nTools\n\n\n\n\nai_ml_dl_venn.png\nThree overlapping circles: AI (largest), ML (medium), DL (smallest)\nPowerPoint circles\n\n\nsupervised_learning_concept.png\nInput data + Labels → Model → Predictions\nSimple flowchart\n\n\ncnn_architecture.png\nBoxes showing: Input Image → Conv → Pool → Conv → Pool → Dense → Output\nPowerPoint shapes\n\n\nrandom_forest_diagram.png\nMultiple decision trees → Combined prediction\nSimple tree diagrams\n\n\n\nKeep these simple - don’t need to be fancy, just clear!\n\n\n\n\n\nThese are nice-to-have but not essential:\n\nsar_principle.png - SAR imaging principle (can explain verbally)\npolarization_comparison.jpg - VV vs VH (can show in live demo)\nphilsa_building.jpg - PhilSA headquarters (not critical)\nconfusion_matrix.png - Example matrix (can draw on slide)\nVarious platform interfaces (can show live instead)\n\n\n\n\n\n\n\n\nVisit ESA Multimedia Gallery: https://www.esa.int/ESA_Multimedia/Images\nSearch terms to use:\n\n“Sentinel-1 satellite”\n“Sentinel-2 satellite”\n“Copernicus programme”\n\nDownload:\n\nClick image → “Download” button\nChoose high resolution\nSave to images/ folder\n\nRename:\n\nRename downloaded file to match required filename\nExample: ESA_Sentinel1.jpg → sentinel1_satellite.jpg\n\n\n\n\n\n\n\nOpen Copernicus Browser: https://dataspace.copernicus.eu/browser/\nFor Mayon Volcano example:\n\nSearch: “Mayon Volcano” or coordinates (13.26°N, 123.69°E)\nSelect: Sentinel-2 L2A\nDate: Any recent date with &lt;20% cloud\nVisualization: True Color (B4-B3-B2)\nZoom in to fill frame\nTake screenshot\n\nFor Flood example:\n\nNavigate to recently flooded area in Philippines\nSelect: Sentinel-1 GRD IW\nDate: During flood event\nVisualization: VV polarization\nDark areas = water\nTake screenshot showing contrast\n\nFor Rice monitoring:\n\nNavigate to Central Luzon (Nueva Ecija area)\nSelect: Sentinel-2 L2A\nDate: Growing season (June-October)\nVisualization: False Color (B8-B4-B3) or NDVI\nRice fields appear bright red (healthy) or brown (harvested)\nTake screenshot\n\n\n\n\n\n\n\n\n\n\nSet slide size to 1920x1080:\n\nFile → Page Setup → Custom (1920 x 1080)\n\nUse simple shapes:\n\nInsert → Shapes → Rectangles, circles, arrows\nAdd text boxes for labels\n\nColor scheme:\n\nPrimary: Blue #1e3a8a\nBackground: White or light gray #f3f4f6\nText: Dark gray #1f2937\n\nExport:\n\nFile → Save As → PNG or JPG\nMaximum resolution\n\n\n\n\n\n\nGo to: https://app.diagrams.net\nCreate new diagram:\n\nChoose blank diagram\nSet canvas size: File → Page Setup → Custom (1920x1080)\n\nDrag and drop:\n\nShapes from left sidebar\nConnect with arrows\nAdd text labels\n\nExport:\n\nFile → Export As → PNG\nSelect high resolution\nTransparent background: optional\n\n\n\n\n\n\n\nAs you source images, check them off:\n\n\n\nphilsa_logo.png\ndost_logo.png\ndost_asti_logo.png\nnamria_logo.png\ngee_logo.png\nsentinel1_satellite.jpg\nsentinel2_mayon.jpg (or any S2 image)\ndataspace_portal.jpg (screenshot)\n\n\n\n\n\nph_eo_ecosystem.png (CREATE diagram)\neo_ml_workflow.png (CREATE diagram)\nfoundation_models_timeline.png (CREATE diagram)\npython_geospatial_stack.png (CREATE diagram)\n\n\n\n\n\nsiyasat_portal.jpg (if accessible)\nnamria_geoportal.jpg (screenshot)\ns1_flood_mapping.jpg (Copernicus Browser)\nclassification_example.jpg (Copernicus Browser)\nai_ml_dl_venn.png (CREATE simple)\ncnn_architecture.png (CREATE simple)\n\n\n\n\n\nEverything else can wait or be skipped\n\n\n\n\n\n\n\n\n\nTask\nTime\nFiles\n\n\n\n\nDownload logos\n30 min\n7 logos\n\n\nESA satellite images\n15 min\n2 images\n\n\nPlatform screenshots\n15 min\n3 screenshots\n\n\nCreate 4 key diagrams\n2-3 hours\n4 diagrams\n\n\nCreate example EO images\n1 hour\n4 images\n\n\nCreate simple concept diagrams\n1-2 hours\n4 diagrams\n\n\nTOTAL (Essential)\n3-4 hours\n20 images\n\n\nTOTAL (All high priority)\n4-6 hours\n24 images\n\n\n\n\n\n\n\nToday (1 hour): 1. Download all logos (30 min) 2. Download ESA satellite images (15 min) 3. Screenshot key platforms (15 min) 4. Result: Presentations 50% better visually\nTomorrow (2-3 hours): 1. Create Philippine EO ecosystem diagram (45 min) 2. Create EO ML workflow diagram (45 min) 3. Create foundation models timeline (30 min) 4. Create Python stack diagram (30 min) 5. Result: Presentations 80% complete visually\nLater (optional, 2-4 hours): 1. Create example EO images from Copernicus Browser 2. Create simple concept diagrams 3. Source remaining platform screenshots 4. Result: Presentations 100% complete\n\n\n\n\nCan’t find a specific image? - Skip it and use text-based slide instead - Show live demo during presentation - Use placeholder and explain verbally\nDon’t have time for diagrams? - Use text bullet points instead - Draw on screen during presentation - Use whiteboard/annotation tools\nQuestions about which images matter most? - Logos are most important (credibility) - Diagrams are second (understanding) - Examples are third (engagement) - Everything else is optional\n\n\n\n\nAfter sourcing images:\ncd course_site/day1/presentations/images\n\n# Check you have the critical files:\nls -lh philsa_logo.png dost_logo.png gee_logo.png \\\n       sentinel1_satellite.jpg ph_eo_ecosystem.png \\\n       eo_ml_workflow.png\n\n# Re-render presentations:\ncd ..\nquarto render 01_session1_copernicus_philippine_eo.qmd\nquarto render 02_session2_ai_ml_fundamentals.qmd\n\n# Preview to check:\nquarto preview 01_session1_copernicus_philippine_eo.qmd\nGood luck! Start with the critical items and you’ll have great-looking presentations! 🚀"
  },
  {
    "objectID": "day1/presentations/IMAGES_TO_SOURCE.html#critical---source-these-first-30-minutes",
    "href": "day1/presentations/IMAGES_TO_SOURCE.html#critical---source-these-first-30-minutes",
    "title": "Images to Source - Priority List",
    "section": "",
    "text": "Filename\nSource\nHow to Get\n\n\n\n\nphilsa_logo.png\nPhilSA\nRight-click logo → Save image as…\n\n\ndost_logo.png\nDOST\nFooter or header logo\n\n\ndost_asti_logo.png\nDOST-ASTI\nHeader logo\n\n\nnamria_logo.png\nNAMRIA\nHeader logo\n\n\ngee_logo.png\nGoogle Earth Engine\nHomepage or press kit\n\n\ncopphil_logo.png\nCoPhil materials\nIf you have CoPhil branding materials\n\n\neu_global_gateway.png\nEU Website\nLogo/banner image\n\n\n\nTips: - Look for PNG format with transparent background - Save as high resolution (at least 400x400px for logos) - Check website footer for “Media” or “Downloads” sections\n\n\n\n\n\n\n\nFilename\nSource\nHow to Get\n\n\n\n\nsentinel1_satellite.jpg\nESA Multimedia\nSearch “Sentinel-1 satellite” → Download high-res\n\n\nsentinel2_mayon.jpg\nCopernicus Browser\nNavigate to Mayon Volcano (13.26°N, 123.69°E), Sentinel-2, true color → Screenshot\n\n\n\nFor Copernicus Browser: 1. Go to https://dataspace.copernicus.eu/browser/ 2. Search location or coordinates 3. Select Sentinel-2 L2A 4. Choose date with &lt;10% cloud cover 5. Select “True Color” visualization 6. Take screenshot (full screen for best quality)\n\n\n\n\n\n\n\nFilename\nURL to Screenshot\nWhat to Capture\n\n\n\n\nsiyasat_portal.jpg\nContact PhilSA or search online\nSIYASAT platform interface\n\n\nnamria_geoportal.jpg\nNAMRIA Geoportal\nHomepage or map view\n\n\ndataspace_portal.jpg\nCopernicus Data Space\nHomepage or browser interface\n\n\n\nHow to take good screenshots: - Use full screen browser (F11) - Hide browser toolbars - Capture at 1920x1080 resolution - Save as JPG or PNG"
  },
  {
    "objectID": "day1/presentations/IMAGES_TO_SOURCE.html#high-priority---create-these-diagrams-2-3-hours",
    "href": "day1/presentations/IMAGES_TO_SOURCE.html#high-priority---create-these-diagrams-2-3-hours",
    "title": "Images to Source - Priority List",
    "section": "",
    "text": "These are critical diagrams you need to create (use PowerPoint, draw.io, or Canva):\n\n\nCreate a diagram showing:\n┌─────────────────────────────────────────┐\n│         EU Copernicus Programme         │\n│      (Sentinel-1, Sentinel-2, ...)      │\n└────────────────┬────────────────────────┘\n                 │\n                 ▼\n┌─────────────────────────────────────────┐\n│           CoPhil Programme             │\n│    (EU-Philippines Cooperation)         │\n└─────┬─────────────┬──────────────┬──────┘\n      │             │              │\n      ▼             ▼              ▼\n  ┌────────┐   ┌─────────┐   ┌──────────┐\n  │ PhilSA │   │ NAMRIA  │   │DOST-ASTI │\n  │SIYASAT │   │Geoportal│   │SkAI-Pinas│\n  └────────┘   └─────────┘   └──────────┘\nTools: draw.io (free), PowerPoint, or Canva Size: 1920x1080px Colors: Use blue (#1e3a8a) as primary color\n\n\n\n\nCreate a workflow diagram:\nProblem → Data Collection → Preprocessing → \nFeature Engineering → Model Training → \nValidation → Deployment → Monitoring\nInclude: - Icons or boxes for each step - Arrows showing flow - Brief labels under each step - Feedback loop from Monitoring back to Data Collection\nSize: 1920x1080px\n\n\n\n\nCreate a timeline showing:\n2023: Early EO foundation models\n  │\n2024: NASA-IBM Geospatial FM (Aug)\n  │   ESA Φsat-2 launched (Sept)\n  │\n2025: Prithvi, Clay models released\n  │\nFuture: Widespread adoption\nStyle: Horizontal timeline with key milestones Size: 1920x1080px\n\n\n\n\nCreate a diagram showing Python libraries:\n┌───────────────────────────────────┐\n│   Application Layer               │\n│   (Your Analysis Code)            │\n└───────────┬───────────────────────┘\n            │\n┌───────────┴───────────────────────┐\n│   High-Level Libraries            │\n│   GeoPandas | Rasterio | geemap   │\n└───────────┬───────────────────────┘\n            │\n┌───────────┴───────────────────────┐\n│   Core Libraries                  │\n│   GDAL | Shapely | NumPy          │\n└───────────────────────────────────┘\nSize: 1600x900px"
  },
  {
    "objectID": "day1/presentations/IMAGES_TO_SOURCE.html#medium-priority---good-to-have-2-4-hours",
    "href": "day1/presentations/IMAGES_TO_SOURCE.html#medium-priority---good-to-have-2-4-hours",
    "title": "Images to Source - Priority List",
    "section": "",
    "text": "Filename\nHow to Create\nDescription\n\n\n\n\ns1_flood_mapping.jpg\nCopernicus Browser\nSentinel-1 showing flood extent (dark = water)\n\n\nsentinel1_flood_ph.png\nCopernicus Browser\nPhilippine flooding example (recent typhoon)\n\n\nclassification_example.jpg\nCopernicus Browser\nSentinel-2 false color showing land cover types\n\n\nrice_monitoring_ph.jpg\nCopernicus Browser\nRice fields in Central Luzon (green/brown patches)\n\n\n\nHow to get these: 1. Go to Copernicus Browser 2. Navigate to Philippine locations 3. Select appropriate satellite and date 4. Choose visualization (True Color, False Color, NDVI) 5. Screenshot at high resolution\n\n\n\n\n\n\n\n\n\n\n\n\nFilename\nWhat to Show\nTools\n\n\n\n\nai_ml_dl_venn.png\nThree overlapping circles: AI (largest), ML (medium), DL (smallest)\nPowerPoint circles\n\n\nsupervised_learning_concept.png\nInput data + Labels → Model → Predictions\nSimple flowchart\n\n\ncnn_architecture.png\nBoxes showing: Input Image → Conv → Pool → Conv → Pool → Dense → Output\nPowerPoint shapes\n\n\nrandom_forest_diagram.png\nMultiple decision trees → Combined prediction\nSimple tree diagrams\n\n\n\nKeep these simple - don’t need to be fancy, just clear!"
  },
  {
    "objectID": "day1/presentations/IMAGES_TO_SOURCE.html#low-priority---can-skip-or-use-text-instead",
    "href": "day1/presentations/IMAGES_TO_SOURCE.html#low-priority---can-skip-or-use-text-instead",
    "title": "Images to Source - Priority List",
    "section": "",
    "text": "These are nice-to-have but not essential:\n\nsar_principle.png - SAR imaging principle (can explain verbally)\npolarization_comparison.jpg - VV vs VH (can show in live demo)\nphilsa_building.jpg - PhilSA headquarters (not critical)\nconfusion_matrix.png - Example matrix (can draw on slide)\nVarious platform interfaces (can show live instead)"
  },
  {
    "objectID": "day1/presentations/IMAGES_TO_SOURCE.html#quick-download-guide",
    "href": "day1/presentations/IMAGES_TO_SOURCE.html#quick-download-guide",
    "title": "Images to Source - Priority List",
    "section": "",
    "text": "Visit ESA Multimedia Gallery: https://www.esa.int/ESA_Multimedia/Images\nSearch terms to use:\n\n“Sentinel-1 satellite”\n“Sentinel-2 satellite”\n“Copernicus programme”\n\nDownload:\n\nClick image → “Download” button\nChoose high resolution\nSave to images/ folder\n\nRename:\n\nRename downloaded file to match required filename\nExample: ESA_Sentinel1.jpg → sentinel1_satellite.jpg\n\n\n\n\n\n\n\nOpen Copernicus Browser: https://dataspace.copernicus.eu/browser/\nFor Mayon Volcano example:\n\nSearch: “Mayon Volcano” or coordinates (13.26°N, 123.69°E)\nSelect: Sentinel-2 L2A\nDate: Any recent date with &lt;20% cloud\nVisualization: True Color (B4-B3-B2)\nZoom in to fill frame\nTake screenshot\n\nFor Flood example:\n\nNavigate to recently flooded area in Philippines\nSelect: Sentinel-1 GRD IW\nDate: During flood event\nVisualization: VV polarization\nDark areas = water\nTake screenshot showing contrast\n\nFor Rice monitoring:\n\nNavigate to Central Luzon (Nueva Ecija area)\nSelect: Sentinel-2 L2A\nDate: Growing season (June-October)\nVisualization: False Color (B8-B4-B3) or NDVI\nRice fields appear bright red (healthy) or brown (harvested)\nTake screenshot"
  },
  {
    "objectID": "day1/presentations/IMAGES_TO_SOURCE.html#creating-diagrams---quick-tips",
    "href": "day1/presentations/IMAGES_TO_SOURCE.html#creating-diagrams---quick-tips",
    "title": "Images to Source - Priority List",
    "section": "",
    "text": "Set slide size to 1920x1080:\n\nFile → Page Setup → Custom (1920 x 1080)\n\nUse simple shapes:\n\nInsert → Shapes → Rectangles, circles, arrows\nAdd text boxes for labels\n\nColor scheme:\n\nPrimary: Blue #1e3a8a\nBackground: White or light gray #f3f4f6\nText: Dark gray #1f2937\n\nExport:\n\nFile → Save As → PNG or JPG\nMaximum resolution\n\n\n\n\n\n\nGo to: https://app.diagrams.net\nCreate new diagram:\n\nChoose blank diagram\nSet canvas size: File → Page Setup → Custom (1920x1080)\n\nDrag and drop:\n\nShapes from left sidebar\nConnect with arrows\nAdd text labels\n\nExport:\n\nFile → Export As → PNG\nSelect high resolution\nTransparent background: optional"
  },
  {
    "objectID": "day1/presentations/IMAGES_TO_SOURCE.html#checklist---track-your-progress",
    "href": "day1/presentations/IMAGES_TO_SOURCE.html#checklist---track-your-progress",
    "title": "Images to Source - Priority List",
    "section": "",
    "text": "As you source images, check them off:\n\n\n\nphilsa_logo.png\ndost_logo.png\ndost_asti_logo.png\nnamria_logo.png\ngee_logo.png\nsentinel1_satellite.jpg\nsentinel2_mayon.jpg (or any S2 image)\ndataspace_portal.jpg (screenshot)\n\n\n\n\n\nph_eo_ecosystem.png (CREATE diagram)\neo_ml_workflow.png (CREATE diagram)\nfoundation_models_timeline.png (CREATE diagram)\npython_geospatial_stack.png (CREATE diagram)\n\n\n\n\n\nsiyasat_portal.jpg (if accessible)\nnamria_geoportal.jpg (screenshot)\ns1_flood_mapping.jpg (Copernicus Browser)\nclassification_example.jpg (Copernicus Browser)\nai_ml_dl_venn.png (CREATE simple)\ncnn_architecture.png (CREATE simple)\n\n\n\n\n\nEverything else can wait or be skipped"
  },
  {
    "objectID": "day1/presentations/IMAGES_TO_SOURCE.html#time-estimate",
    "href": "day1/presentations/IMAGES_TO_SOURCE.html#time-estimate",
    "title": "Images to Source - Priority List",
    "section": "",
    "text": "Task\nTime\nFiles\n\n\n\n\nDownload logos\n30 min\n7 logos\n\n\nESA satellite images\n15 min\n2 images\n\n\nPlatform screenshots\n15 min\n3 screenshots\n\n\nCreate 4 key diagrams\n2-3 hours\n4 diagrams\n\n\nCreate example EO images\n1 hour\n4 images\n\n\nCreate simple concept diagrams\n1-2 hours\n4 diagrams\n\n\nTOTAL (Essential)\n3-4 hours\n20 images\n\n\nTOTAL (All high priority)\n4-6 hours\n24 images"
  },
  {
    "objectID": "day1/presentations/IMAGES_TO_SOURCE.html#recommended-workflow",
    "href": "day1/presentations/IMAGES_TO_SOURCE.html#recommended-workflow",
    "title": "Images to Source - Priority List",
    "section": "",
    "text": "Today (1 hour): 1. Download all logos (30 min) 2. Download ESA satellite images (15 min) 3. Screenshot key platforms (15 min) 4. Result: Presentations 50% better visually\nTomorrow (2-3 hours): 1. Create Philippine EO ecosystem diagram (45 min) 2. Create EO ML workflow diagram (45 min) 3. Create foundation models timeline (30 min) 4. Create Python stack diagram (30 min) 5. Result: Presentations 80% complete visually\nLater (optional, 2-4 hours): 1. Create example EO images from Copernicus Browser 2. Create simple concept diagrams 3. Source remaining platform screenshots 4. Result: Presentations 100% complete"
  },
  {
    "objectID": "day1/presentations/IMAGES_TO_SOURCE.html#need-help",
    "href": "day1/presentations/IMAGES_TO_SOURCE.html#need-help",
    "title": "Images to Source - Priority List",
    "section": "",
    "text": "Can’t find a specific image? - Skip it and use text-based slide instead - Show live demo during presentation - Use placeholder and explain verbally\nDon’t have time for diagrams? - Use text bullet points instead - Draw on screen during presentation - Use whiteboard/annotation tools\nQuestions about which images matter most? - Logos are most important (credibility) - Diagrams are second (understanding) - Examples are third (engagement) - Everything else is optional"
  },
  {
    "objectID": "day1/presentations/IMAGES_TO_SOURCE.html#final-check",
    "href": "day1/presentations/IMAGES_TO_SOURCE.html#final-check",
    "title": "Images to Source - Priority List",
    "section": "",
    "text": "After sourcing images:\ncd course_site/day1/presentations/images\n\n# Check you have the critical files:\nls -lh philsa_logo.png dost_logo.png gee_logo.png \\\n       sentinel1_satellite.jpg ph_eo_ecosystem.png \\\n       eo_ml_workflow.png\n\n# Re-render presentations:\ncd ..\nquarto render 01_session1_copernicus_philippine_eo.qmd\nquarto render 02_session2_ai_ml_fundamentals.qmd\n\n# Preview to check:\nquarto preview 01_session1_copernicus_philippine_eo.qmd\nGood luck! Start with the critical items and you’ll have great-looking presentations! 🚀"
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html",
    "href": "day1/presentations/00_precourse_orientation.html",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "",
    "text": "4-Day Advanced Programme\n\nAI/ML for Earth Observation\nOnline delivery\nHands-on focused\nPhilippine context\n\n\nFocus Areas\n\nDisaster Risk Reduction (DRR)\nClimate Change Adaptation (CCA)\nNatural Resource Management (NRM)\n\n\n\n\nWelcome to the CoPhil 4-Day Advanced Training on AI/ML for Earth Observation. This is an intensive, hands-on programme designed specifically for Philippine EO professionals.\n\n\n\n\n\nDay 1: EO Data & AI/ML Fundamentals\nDay 2: Machine Learning (Random Forest & CNNs)\nDay 3: Advanced Deep Learning\nDay 4: Applications & Project Work\n\nTotal: 32 hours of training + self-paced exercises\n\n\nThe course is structured to progress from fundamentals to advanced applications, with each day building on previous knowledge."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#about-this-training",
    "href": "day1/presentations/00_precourse_orientation.html#about-this-training",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "",
    "text": "4-Day Advanced Programme\n\nAI/ML for Earth Observation\nOnline delivery\nHands-on focused\nPhilippine context\n\n\nFocus Areas\n\nDisaster Risk Reduction (DRR)\nClimate Change Adaptation (CCA)\nNatural Resource Management (NRM)\n\n\n\n\nWelcome to the CoPhil 4-Day Advanced Training on AI/ML for Earth Observation. This is an intensive, hands-on programme designed specifically for Philippine EO professionals."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#course-overview",
    "href": "day1/presentations/00_precourse_orientation.html#course-overview",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "",
    "text": "Day 1: EO Data & AI/ML Fundamentals\nDay 2: Machine Learning (Random Forest & CNNs)\nDay 3: Advanced Deep Learning\nDay 4: Applications & Project Work\n\nTotal: 32 hours of training + self-paced exercises\n\n\nThe course is structured to progress from fundamentals to advanced applications, with each day building on previous knowledge."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#partnership-for-progress",
    "href": "day1/presentations/00_precourse_orientation.html#partnership-for-progress",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Partnership for Progress",
    "text": "Partnership for Progress\n\n\nEU-Philippines space cooperation flagship\nBuilding sustainable digital infrastructure\nStrengthening research and education systems\nOpen data for societal benefit\n\n\n\nThis training is part of the EU Global Gateway Initiative, representing Europe’s commitment to building smart, clean, and secure partnerships worldwide."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#cophil-programme",
    "href": "day1/presentations/00_precourse_orientation.html#cophil-programme",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "CoPhil Programme",
    "text": "CoPhil Programme\n\n\nMission\nSupport PhilSA and DOST to enhance use of Earth Observation data for disaster response, climate adaptation, and resource management.\n\nKey Outputs\n\nCopernicus Mirror Site\nDigital Space Campus\nCapacity building programmes\nPilot services development\n\n\n\n\nCoPhil positions the Philippines as a pioneer in the EU’s international cooperation on Copernicus Earth Observation."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#day-1-foundations",
    "href": "day1/presentations/00_precourse_orientation.html#day-1-foundations",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Day 1: Foundations",
    "text": "Day 1: Foundations\n\n\nSession 1 (2h): Copernicus Sentinel Data & Philippine EO Ecosystem\nSession 2 (2h): Core Concepts of AI/ML for Earth Observation\nSession 3 (2h): Hands-on Python for Geospatial Data\nSession 4 (2h): Introduction to Google Earth Engine\n\n\n\nDay 1 establishes the foundation you’ll need for the entire programme."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#day-2-machine-learning",
    "href": "day1/presentations/00_precourse_orientation.html#day-2-machine-learning",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Day 2: Machine Learning",
    "text": "Day 2: Machine Learning\n\n\nSession 1 (3h): Random Forest Theory & Practice\nSession 2 (2h): Palawan Land Cover Classification Lab\nSession 3 (2.5h): Introduction to Deep Learning & CNNs\nSession 4 (2.5h): CNN Hands-on Lab with TensorFlow\n\n\n\nDay 2 covers both traditional machine learning and introduces deep learning fundamentals."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#day-3-advanced-deep-learning",
    "href": "day1/presentations/00_precourse_orientation.html#day-3-advanced-deep-learning",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Day 3: Advanced Deep Learning",
    "text": "Day 3: Advanced Deep Learning\n\n\nSession 1: U-Net for Semantic Segmentation\nSession 2: Object Detection for EO\nSession 3: Time Series Analysis with RNNs\nSession 4: Multi-modal Data Fusion\n\n\n\nDay 3 explores advanced deep learning architectures for specialized EO tasks."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#day-4-applications-projects",
    "href": "day1/presentations/00_precourse_orientation.html#day-4-applications-projects",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Day 4: Applications & Projects",
    "text": "Day 4: Applications & Projects\n\n\nSession 1: Model Deployment & MLOps\nSession 2: Real-world Case Studies\nSession 3: Capstone Project Work\nSession 4: Presentations & Wrap-up\n\n\n\nDay 4 brings everything together with practical applications and your own project work."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#what-you-need",
    "href": "day1/presentations/00_precourse_orientation.html#what-you-need",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "What You Need",
    "text": "What You Need\n\n✅ Required (FREE)\n\n\nGoogle Account (Gmail) for Colab access\nGoogle Earth Engine Account (sign up at earthengine.google.com)\nStable Internet Connection (2 Mbps+ recommended)\nModern Web Browser (Chrome, Firefox, Edge - latest version)\nComputer/Laptop (4GB+ RAM recommended)\n\n\n\nAll tools we use are cloud-based and free. No software installation required!"
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#optional-but-helpful",
    "href": "day1/presentations/00_precourse_orientation.html#optional-but-helpful",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Optional but Helpful",
    "text": "Optional but Helpful\n\n\nGoogle Drive with 5-10 GB free space (for saving outputs)\nDual monitors or large screen (easier for following along)\nHeadphones with microphone (better audio quality)\nNote-taking app (OneNote, Notion, Evernote, paper!)\n\n\n\nThese aren’t strictly required but will enhance your learning experience."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#step-1-google-account",
    "href": "day1/presentations/00_precourse_orientation.html#step-1-google-account",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Step 1: Google Account",
    "text": "Step 1: Google Account\n\n\nIf you have Gmail:\n✅ You’re all set!\nJust make sure you can access colab.research.google.com\n\nIf you don’t have Gmail:\n\nGo to accounts.google.com\nClick “Create account”\nFollow the prompts\nVerify your email\n\n\n\n\nMost participants already have a Google account, but if not, it takes just 5 minutes to create one."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#step-2-google-earth-engine",
    "href": "day1/presentations/00_precourse_orientation.html#step-2-google-earth-engine",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Step 2: Google Earth Engine",
    "text": "Step 2: Google Earth Engine\n\nSign Up Process\n\nVisit signup.earthengine.google.com\nSign in with your Google account\nFill out registration form:\n\n✅ Select “Non-commercial” use\n✅ Project type: “Education/Research”\n✅ Organization: Your agency (PhilSA, NAMRIA, DOST, University, etc.)\n\nSubmit and wait for approval email (usually 1-2 days)\n\n\n⚠️ Do this NOW if you haven’t already!\n\n\nGEE approval can take 24-48 hours, so sign up immediately. We’ll need it for Sessions 4, Day 2, and beyond."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#step-3-test-google-colab",
    "href": "day1/presentations/00_precourse_orientation.html#step-3-test-google-colab",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Step 3: Test Google Colab",
    "text": "Step 3: Test Google Colab\n\nQuick Test\n\nGo to colab.research.google.com\nClick “New Notebook”\nIn the first cell, type:\n\nprint(\"Hello, CoPhil Training!\")\n\nPress Shift+Enter to run\nYou should see the output below the cell\n\n\n✅ If you see “Hello, CoPhil Training!” — you’re ready!\n\n\nColab is where we’ll do all our hands-on work. Make sure it works before Day 1."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#step-4-join-course-platforms",
    "href": "day1/presentations/00_precourse_orientation.html#step-4-join-course-platforms",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Step 4: Join Course Platforms",
    "text": "Step 4: Join Course Platforms\n\nAccess the Course Site\n\nZoom/Meeting Link: [Will be sent before each session]\n\n\n📧 Check your email for access links and calendar invites\n\n\nWe’ll share all access information via email before the course begins."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#daily-schedule",
    "href": "day1/presentations/00_precourse_orientation.html#daily-schedule",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Daily Schedule",
    "text": "Daily Schedule\n\n\n\nTime\nActivity\nFormat\n\n\n\n\n09:00-11:00\nSession 1\nLecture + Demo\n\n\n11:00-11:15\n☕ Break\n-\n\n\n11:15-13:00\nSession 2\nLecture + Demo\n\n\n13:00-14:00\n🍱 Lunch\n-\n\n\n14:00-16:00\nSession 3\nHands-on Lab\n\n\n16:00-16:15\n☕ Break\n-\n\n\n16:15-18:00\nSession 4\nHands-on Lab\n\n\n\n\nEach day follows this consistent schedule. Plan your day accordingly and protect this time."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#session-structure",
    "href": "day1/presentations/00_precourse_orientation.html#session-structure",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Session Structure",
    "text": "Session Structure\n\nTypical 2-Hour Session\n\n\n10 min: Introduction & objectives\n60 min: Core content\n40 min: Hands-on exercises OR Q&A\n10 min: Summary & next steps\n\n\n\nWe balance theory with practice. You’ll spend roughly 50% of time doing hands-on work."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#learning-methods",
    "href": "day1/presentations/00_precourse_orientation.html#learning-methods",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Learning Methods",
    "text": "Learning Methods\n\n\nInstructor-Led\n\nPresentations\nCode examples\nGuided exercises\nReal-time Q&A\n\n\nSelf-Paced\n\nJupyter notebooks\nPractice exercises\nOptional challenges\nTake-home assignments\n\n\n\n\nBlended approach: Learn together, practice at your own pace\n\n\nWe combine live instruction with self-paced practice for maximum flexibility and learning."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#during-live-sessions",
    "href": "day1/presentations/00_precourse_orientation.html#during-live-sessions",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "During Live Sessions",
    "text": "During Live Sessions\n\n✅ Please DO:\n\n\nKeep your camera on (when possible) — builds community\nAsk questions via chat or unmute\nShare your screen if you need help\nParticipate in polls and exercises\nTake breaks when scheduled\n\n\n\nActive participation makes the training more engaging for everyone."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#during-live-sessions-1",
    "href": "day1/presentations/00_precourse_orientation.html#during-live-sessions-1",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "During Live Sessions",
    "text": "During Live Sessions\n\n⛔ Please AVOID:\n\n\nMultitasking (emails, other work) — you’ll miss key concepts\nEating during lectures (breaks are scheduled)\nSide conversations on unmuted mics\nRecording without permission\nSharing course materials publicly (check license)\n\n\n\nRespect everyone’s learning experience and focus on the material."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#code-of-conduct",
    "href": "day1/presentations/00_precourse_orientation.html#code-of-conduct",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Code of Conduct",
    "text": "Code of Conduct\n\n\nRespectful: All questions are welcome\nCollaborative: Help your fellow participants\nProfessional: Maintain work-appropriate language and behavior\nInclusive: Value diverse perspectives and experiences\nCurious: Embrace mistakes as learning opportunities\n\n\n\nWe’re all here to learn together! 🌟\n\n\nThis is a supportive learning environment. Everyone starts somewhere, and questions help everyone learn."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#during-sessions",
    "href": "day1/presentations/00_precourse_orientation.html#during-sessions",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "During Sessions",
    "text": "During Sessions\n\n\nTechnical Issues\n\nShare your screen\nDescribe the error\nCheck chat for solutions\nUse breakout rooms\n\n\nConceptual Questions\n\nAsk in chat anytime\nUnmute and ask verbally\nUse Q&A sessions\n\n\n\n\nDon’t suffer in silence! We’re here to help you succeed."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#outside-of-sessions",
    "href": "day1/presentations/00_precourse_orientation.html#outside-of-sessions",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Outside of Sessions",
    "text": "Outside of Sessions\n\nSupport Resources\n\nEmail Support: skotsopoulos@neuralio.ai\nOffice Hours: [Schedule TBA]\nPeer Study Groups: Connect with other participants\n\n\nResponse time: Usually within 24 hours\n\n\nWe provide multiple channels for support. Use whatever works best for you."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#what-youll-receive",
    "href": "day1/presentations/00_precourse_orientation.html#what-youll-receive",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "What You’ll Receive",
    "text": "What You’ll Receive\n\n\n📊 Presentation Slides (all sessions)\n📓 Jupyter Notebooks (with exercises and solutions)\n📚 Handouts & Cheat Sheets\n📁 Sample Datasets (Philippine examples)\n🔗 Resource Links (documentation, tutorials, papers)\n🎓 Certificate (upon completion)\n\n\n\nAll materials are yours to keep and use after the course."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#accessing-materials",
    "href": "day1/presentations/00_precourse_orientation.html#accessing-materials",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Accessing Materials",
    "text": "Accessing Materials\n\nDuring the Course\n\nCourse Website: Browse all content\nGitHub Repository: Download notebooks and code\nGoogle Drive: Shared datasets and resources\n\n\n\nAfter the Course\n\nMaterials remain accessible for 6 months\nFork/clone GitHub repository for permanent access\nDownload everything you need before access expires\n\n\nMake sure to download materials you want to keep permanently."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#completion-requirements",
    "href": "day1/presentations/00_precourse_orientation.html#completion-requirements",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Completion Requirements",
    "text": "Completion Requirements\nTo receive your certificate, you must:\n\n\n✅ Attend at least 75% of live sessions (24/32 hours)\n✅ Complete hands-on exercises (Day 1-3)\n✅ Submit capstone project (Day 4)\n✅ Fill out feedback survey (end of course)\n\n\n\nThe certificate documents your participation and skill development."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#capstone-project",
    "href": "day1/presentations/00_precourse_orientation.html#capstone-project",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Capstone Project",
    "text": "Capstone Project\n\nFinal Day Project\n\nIndividual or Team: Your choice (teams of 2-3)\nTopic: Philippine DRR/CCA/NRM application\nDuration: 3-4 hours work time\nDeliverable: 10-min presentation + Jupyter notebook\nAssessment: Peer review + instructor feedback\n\n\nGoal: Apply everything you’ve learned to a real problem\n\n\nThe capstone lets you demonstrate your new skills on a problem relevant to your work."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#what-you-should-know",
    "href": "day1/presentations/00_precourse_orientation.html#what-you-should-know",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "What You Should Know",
    "text": "What You Should Know\n\n✅ Required Knowledge\n\n\nPython Basics: Variables, lists, loops, functions\nRemote Sensing Concepts: Bands, resolution, spectral signatures\nBasic Statistics: Mean, standard deviation, correlation\n\n\n\nDon’t worry! We’ll review Python basics in Day 1, Session 3\n\n\nWe assume you have basic Python knowledge. If you’re rusty, review before the course."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#what-you-dont-need-to-know",
    "href": "day1/presentations/00_precourse_orientation.html#what-you-dont-need-to-know",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "What You DON’T Need to Know",
    "text": "What You DON’T Need to Know\n\n❌ NOT Required\n\n\nAdvanced math (linear algebra, calculus) — we explain intuitively\nDeep learning experience — we teach from scratch\nGoogle Earth Engine — we start with basics\nSpecific ML libraries — we’ll install and introduce everything\n\n\n\nWe teach what you need when you need it!\n\n\nThis is an introductory course. We don’t expect prior ML or GEE experience."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#recommended-pre-work",
    "href": "day1/presentations/00_precourse_orientation.html#recommended-pre-work",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Recommended Pre-Work",
    "text": "Recommended Pre-Work\n\n📚 Optional Reading\nIf you want a head start:\n\nPython Refresh: Python for Beginners\nRemote Sensing: Review Sentinel-1 and Sentinel-2 basics\nColab Tutorial: Welcome to Colaboratory\nGEE Intro: Earth Engine guides\n\n\n⏱️ Time: 2-3 hours total (optional!)\n\n\nThese are optional but can make Day 1 easier if you have time."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#maximizing-your-learning",
    "href": "day1/presentations/00_precourse_orientation.html#maximizing-your-learning",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Maximizing Your Learning",
    "text": "Maximizing Your Learning\n\n\nDedicate Time: Block your calendar, minimize distractions\nTake Notes: Write down key concepts and “aha” moments\nPractice Immediately: Run code as we demo it\nAsk Questions: No question is too basic\nConnect Concepts: Relate to your own work and data\nExperiment: Modify code, try variations, break things!\nNetwork: Connect with other participants\nFollow Up: Review materials after each session\n\n\n\nActive learning beats passive watching every time."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#common-pitfalls-to-avoid",
    "href": "day1/presentations/00_precourse_orientation.html#common-pitfalls-to-avoid",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Common Pitfalls to Avoid",
    "text": "Common Pitfalls to Avoid\n\n⚠️ Watch Out For:\n\n\nRushing ahead: Follow along step-by-step\nCopy-pasting without understanding: Read and modify code\nFalling behind: Ask for help immediately\nSkipping breaks: Your brain needs rest\nWorking in isolation: Collaborate and discuss\n\n\n\nLearn from others’ mistakes. These are the most common issues we see."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#why-this-training-matters",
    "href": "day1/presentations/00_precourse_orientation.html#why-this-training-matters",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Why This Training Matters",
    "text": "Why This Training Matters\n\n\nChallenges\n\n20+ typhoons per year\nClimate change impacts\nDeforestation & biodiversity loss\nRapid urbanization\nFood security\n\n\nSolutions\n\nEarly warning systems\nLand cover monitoring\nDisaster response mapping\nAgricultural monitoring\nCoastal zone management\n\n\n\n\nEO + AI/ML = Better decisions, faster response, lives saved\n\n\nThis isn’t just academic - these skills directly support Philippine resilience and development."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#philippine-eo-ecosystem",
    "href": "day1/presentations/00_precourse_orientation.html#philippine-eo-ecosystem",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Philippine EO Ecosystem",
    "text": "Philippine EO Ecosystem\n\nKey Agencies\n\n\nPhilSA: Philippine Space Agency (SIYASAT portal, Diwata missions)\nNAMRIA: National mapping and geoportal\nDOST-ASTI: AI platforms (SkAI-Pinas, PANDA, DIMER)\nPAGASA: Weather forecasting and climate monitoring\nDENR: Environmental management and forest monitoring\nDA: Agricultural applications\n\n\n\nYou’ll learn to work with data and tools from these agencies throughout the course."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#course-examples-feature",
    "href": "day1/presentations/00_precourse_orientation.html#course-examples-feature",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Course Examples Feature",
    "text": "Course Examples Feature\n\n\nPalawan land cover classification\nMetro Manila flood mapping\nTyphoon damage assessment\nCoral reef health monitoring\nRice paddy mapping\nForest fire detection\n\n\n\nReal data. Real problems. Real impact. 🇵🇭\n\n\nEvery example is chosen to be relevant to Philippine EO professionals."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#before-day-1",
    "href": "day1/presentations/00_precourse_orientation.html#before-day-1",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Before Day 1",
    "text": "Before Day 1\n\n✅ Action Items\n\n\nNOW: Sign up for Google Earth Engine (takes 1-2 days for approval)\n1 Week Before: Test Colab, check equipment\n3 Days Before: Review Python basics (if needed)\n1 Day Before: Join course platforms, note Zoom links\nMorning Of: Join 10 minutes early for tech check\n\n\n\nFollowing this timeline ensures a smooth start on Day 1."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#course-timeline",
    "href": "day1/presentations/00_precourse_orientation.html#course-timeline",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Course Timeline",
    "text": "Course Timeline\n\n\n\nDate\nDay\nFocus\n\n\n\n\n[TBA]\nDay 1\nFoundations\n\n\n[TBA]\nDay 2\nMachine Learning\n\n\n[TBA]\nDay 3\nDeep Learning\n\n\n[TBA]\nDay 4\nApplications & Projects\n\n\n\n\n📧 Calendar invites sent to your registered email\n\n\nExact dates will be in your calendar invite. Add them to your calendar now!"
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#contact-information",
    "href": "day1/presentations/00_precourse_orientation.html#contact-information",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "Contact Information",
    "text": "Contact Information\nEmail: skotsopoulos@neuralio.ai\n\n\nCan’t wait to start?\n\nConnect with other participants\nReview setup instructions again\n\n\n\n\nWe’re here to help. Reach out if you have any questions or concerns."
  },
  {
    "objectID": "day1/presentations/00_precourse_orientation.html#whats-next",
    "href": "day1/presentations/00_precourse_orientation.html#whats-next",
    "title": "Welcome to CoPhil EO AI/ML Training",
    "section": "What’s Next",
    "text": "What’s Next\n\n\nComplete setup steps (GEE signup, Colab test)\nMark your calendar for all 4 days\nPrepare your workspace (quiet, distraction-free)\nReview any optional pre-work\nGet excited — this will be amazing! 🚀\n\n\n\n\nSee you on Day 1, Session 1!\nCopernicus Sentinel Data & Philippine EO Ecosystem\n\n\n\nWe’re thrilled to have you join us. This is going to be an incredible learning experience!"
  },
  {
    "objectID": "resources/downloads.html",
    "href": "resources/downloads.html",
    "title": "Downloads",
    "section": "",
    "text": "Download training materials for the CoPhil EO AI/ML Training Programme. All materials are provided under open licenses for educational use.",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#overview",
    "href": "resources/downloads.html#overview",
    "title": "Downloads",
    "section": "",
    "text": "Download training materials for the CoPhil EO AI/ML Training Programme. All materials are provided under open licenses for educational use.",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#complete-day-1-package",
    "href": "resources/downloads.html#complete-day-1-package",
    "title": "Downloads",
    "section": "Complete Day 1 Package",
    "text": "Complete Day 1 Package\n\nAll-in-One Download\nPackage Contents: - All 4 session notebooks (Jupyter .ipynb) - Presentation slides (PDF) - Cheat sheets (PDF) - Sample datasets - Session handouts\nSize: ~150 MB\nDownload Complete Package (ZIP)\nLast updated: 2025-01-15",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#jupyter-notebooks",
    "href": "resources/downloads.html#jupyter-notebooks",
    "title": "Downloads",
    "section": "Jupyter Notebooks",
    "text": "Jupyter Notebooks\nDownload interactive notebooks for hands-on practice:\n\nSession 3: Python for Geospatial Data\n\nDay1_Session3_Python_Geospatial_Data.ipynb\nLearn to work with vector data (GeoPandas) and raster data (Rasterio) in Python.\nTopics Covered: - GeoPandas for vector data - Reading shapefiles, GeoJSON, and GeoPackages - Coordinate reference systems and reprojection - Rasterio for raster data - Reading GeoTIFF files - Band operations and visualization - Philippine case study: Palawan land cover\nRequirements: - Google Colab (recommended) - Or local Jupyter with geopandas, rasterio installed\nDownload Notebook Open in Colab\n\n\n\nSession 4: Google Earth Engine\n\nDay1_Session4_Google_Earth_Engine.ipynb\nMaster Google Earth Engine for accessing and processing Copernicus data.\nTopics Covered: - Earth Engine authentication and initialization - ImageCollection filtering (spatial, temporal, metadata) - Sentinel-1 SAR data access - Sentinel-2 optical data access - Cloud masking techniques - Temporal compositing - Data export workflows - Philippine case study: Metro Manila monitoring\nRequirements: - Google Earth Engine account (register at earthengine.google.com) - Google Colab\nDownload Notebook Open in Colab",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#presentation-slides",
    "href": "resources/downloads.html#presentation-slides",
    "title": "Downloads",
    "section": "Presentation Slides",
    "text": "Presentation Slides\nDownload presentation slides for reference:\n\nSession 1: Copernicus Sentinel Data & Philippine EO Ecosystem\n\nTopics: - Copernicus Programme overview - Sentinel-1 SAR characteristics - Sentinel-2 optical specifications - 2025 updates (Sentinel-2C, Sentinel-1C) - Philippine EO agencies (PhilSA, NAMRIA, DOST-ASTI) - CoPhil Mirror Site introduction\nFormats: RevealJS HTML Presentation | PDF\nView Online Download PDF\n\n\n\nSession 2: AI/ML Fundamentals for Earth Observation\n\nTopics: - What is AI/ML and the EO workflow - Supervised vs. Unsupervised learning - Classification and regression examples - Introduction to neural networks - Convolutional Neural Networks (CNNs) - Data-centric AI paradigm - EO-specific considerations\nFormats: RevealJS HTML Presentation | PDF\nView Online Download PDF\n\n\n\nSession 3: Python for Geospatial Data (Intro)\n\nTopics: - Google Colab setup - Python environment for geospatial - Vector data concepts - Raster data concepts - Coordinate reference systems\nFormats: RevealJS HTML Presentation | PDF\nView Online Download PDF\n\n\n\nSession 4: Introduction to Google Earth Engine\n\nTopics: - Earth Engine architecture - Python API overview - Data catalog navigation - Philippine EO applications\nFormats: RevealJS HTML Presentation | PDF\nView Online Download PDF",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#cheat-sheets-pdf",
    "href": "resources/downloads.html#cheat-sheets-pdf",
    "title": "Downloads",
    "section": "Cheat Sheets (PDF)",
    "text": "Cheat Sheets (PDF)\nQuick reference guides for common operations:\n\n\nPython Basics\nEssential Python syntax and operations\nDownload PDF\n\n\nGeoPandas Reference\nVector data operations\nDownload PDF\n\n\nRasterio Commands\nRaster data handling\nDownload PDF\n\n\nEarth Engine API\nGEE Python commands\nDownload PDF\n\n\nSentinel Missions\nBand specifications\nDownload PDF\n\n\nSpectral Indices\nCommon formulas (NDVI, NDWI, etc.)\nDownload PDF",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#sample-datasets",
    "href": "resources/downloads.html#sample-datasets",
    "title": "Downloads",
    "section": "Sample Datasets",
    "text": "Sample Datasets\nPractice datasets for offline work:\n\nPhilippine Administrative Boundaries\n\nDescription: Provincial and municipal boundaries for the Philippines\nFormat: GeoPackage (.gpkg)\nSize: ~5 MB\nSource: PhilGIS / NAMRIA\nLicense: Open Data\nDownload Dataset\nContents: - Regions (17) - Provinces (81) - Municipalities and Cities - Attribute table with population data\n\n\n\nPalawan Land Cover Sample\n\nDescription: Sentinel-2 true color composite and land cover classification\nFormat: GeoTIFF (.tif)\nSize: ~80 MB\nArea: Palawan Province subset\nDate: 2024 dry season composite\nDownload Dataset\nFiles: - palawan_s2_composite.tif (RGB composite) - palawan_landcover.tif (Classification) - palawan_boundary.gpkg (Vector boundary) - README.txt (Metadata)\n\n\n\nMetro Manila Sentinel-1 Time Series (Sample)\n\nDescription: Sentinel-1 VV/VH polarization samples\nFormat: GeoTIFF (.tif)\nSize: ~40 MB\nArea: Metro Manila subset\nDates: Monthly composites (Jan-Dec 2024)\nDownload Dataset\nUse Cases: - Urban monitoring - Flood detection practice - Time series analysis",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#session-handouts",
    "href": "resources/downloads.html#session-handouts",
    "title": "Downloads",
    "section": "Session Handouts",
    "text": "Session Handouts\nPrinted materials for participants:\n\nDay 1 Participant Handbook\nContents: - Session summaries - Learning objectives - Key concepts - Exercise instructions - Note-taking space\nFormat: PDF (A4, printable)\nSize: ~3 MB\nDownload PDF\n\n\nGlossary of Terms\nContents: - EO terminology - AI/ML concepts - Acronyms and abbreviations - Philippine EO agencies\nFormat: PDF (booklet style)\nSize: ~2 MB\nDownload PDF",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#code-examples-repository",
    "href": "resources/downloads.html#code-examples-repository",
    "title": "Downloads",
    "section": "Code Examples Repository",
    "text": "Code Examples Repository\nAccess all code examples and utilities:\n\nGitHub Repository\nRepository: copphil-training/day1-materials\nContents: - Jupyter notebooks (latest versions) - Python utility functions - Data processing scripts - Example workflows - Troubleshooting guides\nVisit Repository Clone via Git\n# Clone repository\ngit clone https://github.com/copphil-training/day1-materials.git\n\n# Or download ZIP\nwget https://github.com/copphil-training/day1-materials/archive/main.zip",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#google-colab-links",
    "href": "resources/downloads.html#google-colab-links",
    "title": "Downloads",
    "section": "Google Colab Links",
    "text": "Google Colab Links\nDirect links to open notebooks in Google Colab:\n\nSession 3: Python for Geospatial Data\n\n\n\nOpen In Colab\n\n\n\nSession 4: Google Earth Engine\n\n\n\nOpen In Colab\n\n\n\n\n\n\n\n\n\nTipWorking in Colab\n\n\n\nOpening notebooks in Colab requires no local installation. All code runs in the cloud with access to GPUs and pre-installed libraries.\nFirst Time? See the Setup Guide for authentication instructions.",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#additional-resources",
    "href": "resources/downloads.html#additional-resources",
    "title": "Downloads",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nRecommended Reading\nEarth Observation: - Copernicus Open Access Hub User Guide - Sentinel-1 Toolbox Documentation - Sentinel-2 User Handbook\nPython for Geospatial: - GeoPandas User Guide - Rasterio Quickstart - Python Geospatial Development\nGoogle Earth Engine: - Earth Engine Python API Guide - Earth Engine Community Tutorials - Awesome Earth Engine\nAI/ML for EO: - Deep Learning for Earth Observation (Springer) - Fundamentals of Machine Learning for Predictive Data Analytics - Data-Centric AI Resource Hub",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#installation-scripts",
    "href": "resources/downloads.html#installation-scripts",
    "title": "Downloads",
    "section": "Installation Scripts",
    "text": "Installation Scripts\n\nLocal Python Environment Setup\nFor participants who want to work locally (not in Colab):\n\nrequirements.txt\nnumpy&gt;=1.24.0\npandas&gt;=2.0.0\nmatplotlib&gt;=3.7.0\ngeopandas&gt;=0.13.0\nrasterio&gt;=1.3.0\nearthengine-api&gt;=0.1.350\nfolium&gt;=0.14.0\ngeemap&gt;=0.30.0\njupyter&gt;=1.0.0\nDownload requirements.txt\nInstallation:\n# Create virtual environment\npython -m venv eo-env\nsource eo-env/bin/activate  # On Windows: eo-env\\Scripts\\activate\n\n# Install packages\npip install -r requirements.txt\n\n# Verify installation\npython -c \"import geopandas; print('GeoPandas installed successfully!')\"\n\n\n\nConda Environment (Alternative)\n\nenvironment.yml\nname: copphil-day1\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.10\n  - numpy\n  - pandas\n  - matplotlib\n  - geopandas\n  - rasterio\n  - jupyter\n  - pip\n  - pip:\n    - earthengine-api\n    - geemap\nDownload environment.yml\nInstallation:\nconda env create -f environment.yml\nconda activate copphil-day1",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#license-attribution",
    "href": "resources/downloads.html#license-attribution",
    "title": "Downloads",
    "section": "License & Attribution",
    "text": "License & Attribution\n\n\n\n\n\n\nImportantUsage Terms\n\n\n\nTraining Materials License: - Licensed under Creative Commons BY-SA 4.0 - Free to use for educational purposes - Attribution required: “CoPhil EO AI/ML Training Programme”\nCode Examples: - Licensed under MIT License - Free to use, modify, and distribute\nSample Datasets: - Copernicus Sentinel data: Free and open (Copernicus Data Policy) - Philippine administrative boundaries: Open Data (government sources) - Processed derivatives: CC BY-SA 4.0\nCitation:\nCoPhil Training Programme (2025). Day 1: EO Data, AI/ML Fundamentals &\nGeospatial Python. EU-Philippines Copernicus Capacity Support Programme.",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#support-feedback",
    "href": "resources/downloads.html#support-feedback",
    "title": "Downloads",
    "section": "Support & Feedback",
    "text": "Support & Feedback\n\nHaving Issues?\n\nCheck the FAQ for common problems\nReview the Setup Guide for installation help\nContact training coordinators: skotsopoulos@neuralio.ai\n\n\n\nSuggest Improvements\nWe welcome your feedback to improve the training materials:\n\nReport broken links or errors\nRequest additional materials\nShare feedback on content\n\nContact: skotsopoulos@neuralio.ai\n\nAll materials are regularly updated. Check back for new resources and improved versions.",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#download-statistics",
    "href": "resources/downloads.html#download-statistics",
    "title": "Downloads",
    "section": "Download Statistics",
    "text": "Download Statistics\n\nMost Downloaded: 1. Complete Day 1 Package (ZIP) 2. Session 3 Jupyter Notebook 3. GeoPandas Cheat Sheet 4. Session 4 Jupyter Notebook 5. Palawan Land Cover Dataset\nTotal Downloads This Month: 1,247\n\n\nAll materials are regularly updated. Check back for new resources and improved versions.",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "Status: All image references are currently placeholders\nAction Required: Source and add images before delivery\n\n\n\nThe Day 1 presentations reference 87 images that need to be sourced and placed in the images/ directory. This guide provides: - Complete list of missing images by session - Description and purpose of each image - Recommended sources - Licensing requirements - Alternative solutions if images unavailable\n\n\n\n\ncourse_site/day1/presentations/\n├── images/                    ← CREATE THIS DIRECTORY\n│   ├── Logos/\n│   ├── Satellites/\n│   ├── Platforms/\n│   ├── Organizations/\n│   └── Concepts/\n└── *.qmd files\n\n\n\n\n\n\nSource images from official websites and open repositories. See detailed list below.\n\n\n\nReplace image slides with text-based content describing what would be shown.\n\n\n\nComment out image slides and rely on speaker descriptions.\n\n\n\nCreate simple placeholder images with text labels (e.g., “Sentinel-1 Satellite”).\n\n\n\n\n\n\n\n✅ No images required\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\neu_global_gateway.png\nEU Global Gateway logo\nEU Website\nFair use\n\n\ncopphil_logo.png\nCoPhil Programme logo\nCoPhil Programme\nOfficial\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nphilsa_logo.png\nPhilSA official logo\nPhilSA Website\nOfficial\n\n\nphilsa_building.jpg\nPhilSA headquarters\nPhilSA or Google Earth\nFair use\n\n\ndost_logo.png\nDOST logo\nDOST Website\nOfficial\n\n\ndost_asti_logo.png\nDOST-ASTI logo\nASTI Website\nOfficial\n\n\nnamria_logo.png\nNAMRIA logo\nNAMRIA Website\nOfficial\n\n\npagasa_logo.png\nPAGASA logo\nPAGASA Website\nOfficial\n\n\ndatos_logo.png\nDATOS platform logo\nDOST-ASTI\nOfficial\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\ncopernicus_overview.jpg\nProgramme overview infographic\nCopernicus.eu\nCC BY 4.0\n\n\ncopernicus_applications.png\nService applications diagram\nCopernicus website\nCC BY 4.0\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nsentinel1_satellite.jpg\nSentinel-1 satellite image\nESA Multimedia\nESA Standard\n\n\nsentinel2_satellite.jpg\nSentinel-2 satellite image\nESA Multimedia\nESA Standard\n\n\nsar_principle.png\nSAR imaging principle diagram\nCreate or ESA Education\nFair use\n\n\npolarization_comparison.jpg\nVV/VH polarization comparison\nESA or academic papers\nFair use\n\n\nsentinel2_mayon.jpg\nSentinel-2 image of Mayon Volcano\nCopernicus Browser\nFree & Open\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\ns1_flood_mapping.jpg\nFlood detection example\nCopernicus Browser or papers\nFair use\n\n\ns1_insar.jpg\nInSAR ground deformation\nESA or academic\nFair use\n\n\nsentinel1_flood_ph.png\nPhilippine flood case study\nCreate from Copernicus data\nOwn creation\n\n\ns1_s2_synergy.jpg\nCombined SAR & Optical\nESA or papers\nFair use\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\ndataspace_portal.jpg\nCopernicus Data Space Ecosystem\nScreenshot from dataspace.copernicus.eu\nFair use\n\n\nsentiboard.jpg\nSentiBoard interface\nScreenshot\nFair use\n\n\ngee_logo.png\nGoogle Earth Engine logo\nGEE Website\nGoogle\n\n\nsiyasat_portal.jpg\nSIYASAT platform interface\nPhilSA (with permission)\nOfficial\n\n\nspace_plus_dashboard.jpg\nSpace+ Dashboard\nPhilSA (with permission)\nOfficial\n\n\nnamria_geoportal.jpg\nNAMRIA Geoportal screenshot\nScreenshot from geoportal.gov.ph\nFair use\n\n\nnamria_landcover.jpg\nLand cover map example\nNAMRIA products\nOfficial\n\n\nhazardhunter.jpg\nHazardHunter interface\nNAMRIA\nOfficial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nskai_pinas.jpg\nSkAI-Pinas logo/interface\nDOST-ASTI\nOfficial\n\n\ndimer_interface.jpg\nDIMER platform screenshot\nDOST-ASTI\nOfficial\n\n\naipi_workflow.png\nAIPI workflow diagram\nCreate or DOST-ASTI\nFair use\n\n\nasti_ecosystem.png\nASTI platform ecosystem\nCreate diagram\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\ncoare_infrastructure.jpg\nCOARE HPC facility\nDOST-ASTI\nOfficial\n\n\nmirror_site_concept.jpg\nMirror site architecture\nCreate diagram\nOwn\n\n\ndigital_campus.jpg\nDigital campus concept\nDOST-ASTI or create\nFair use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nph_eo_ecosystem.png\nPhilippine EO ecosystem diagram\nCREATE THIS\nOwn\n\n\nintegration_diagram.png\nPlatform integration flow\nCREATE THIS\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nai_ml_dl_venn.png\nAI/ML/DL relationship diagram\nCREATE or Wikipedia\nCC/Own\n\n\nml_definition.png\nML definition visual\nCreate\nOwn\n\n\ntraditional_vs_ml.png\nTraditional vs ML comparison\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\neo_ml_workflow.png\nEnd-to-end workflow diagram\nCREATE THIS\nOwn\n\n\nproblem_formulation.png\nProblem definition visual\nCreate\nOwn\n\n\ndata_collection_eo.png\nEO data sources diagram\nCreate\nOwn\n\n\npreprocessing_pipeline.png\nPreprocessing steps\nCreate\nOwn\n\n\nfeature_engineering.jpg\nFeature extraction examples\nCreate/papers\nFair use\n\n\nmodel_training.png\nTraining process diagram\nCreate\nOwn\n\n\nvalidation_workflow.png\nValidation methodology\nCreate\nOwn\n\n\ndeployment_architecture.png\nDeployment diagram\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nsupervised_learning_concept.png\nSupervised learning visual\nCreate/Wikipedia\nCC/Own\n\n\nclassification_example.jpg\nLand cover classification\nCreate from S2 data\nOwn\n\n\nregression_example.png\nRegression plot example\nCreate\nOwn\n\n\ndecision_boundary.png\nDecision boundary visual\nCreate\nOwn\n\n\nrandom_forest_diagram.png\nRandom Forest structure\nWikipedia/Create\nCC/Own\n\n\nconfusion_matrix.png\nConfusion matrix example\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nunsupervised_concept.png\nUnsupervised learning visual\nCreate\nOwn\n\n\nclustering_example.jpg\nK-means clustering on imagery\nCreate\nOwn\n\n\nanomaly_detection.jpg\nAnomaly detection example\nCreate/papers\nFair use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nneural_network_diagram.png\nSimple neural network\nWikipedia/Create\nCC/Own\n\n\ncnn_architecture.png\nCNN structure diagram\nCreate/papers\nFair use\n\n\nconvolution_operation.gif\nConvolution animation\nCreate/GitHub\nOpen\n\n\npooling_operation.png\nPooling visualization\nCreate\nOwn\n\n\nunet_architecture.png\nU-Net architecture\nPapers/Create\nAcademic\n\n\nsemantic_segmentation.jpg\nSegmentation example\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nmodel_centric_vs_data_centric.png\nParadigm comparison\nCreate/Andrew Ng\nFair use\n\n\ndata_quality_impact.png\nQuality vs quantity chart\nCreate\nOwn\n\n\nlabeling_quality.jpg\nGood vs bad labels\nCreate\nOwn\n\n\nactive_learning_loop.png\nActive learning workflow\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nfoundation_models_timeline.png\nFM development timeline\nCREATE THIS\nOwn\n\n\nprithvi_architecture.png\nPrithvi model architecture\nIBM/NASA papers\nAcademic\n\n\nclay_model_overview.png\nClay model visual\nClay GitHub/papers\nOpen\n\n\nnasa_ibm_logo.png\nNASA-IBM partnership logo\nOfficial sources\nFair use\n\n\nfoundation_model_benefits.png\nFM advantages diagram\nCreate\nOwn\n\n\nfine_tuning_workflow.png\nFine-tuning process\nCreate\nOwn\n\n\nphisat2_satellite.jpg\nΦsat-2 satellite image\nESA\nOfficial\n\n\nonboard_ai_concept.png\nOn-board processing diagram\nCreate\nOwn\n\n\nedge_computing_diagram.png\nEdge AI architecture\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nph_flood_ml.jpg\nPhilippine flood ML example\nCreate\nOwn\n\n\ntyphoon_damage_assessment.jpg\nDamage detection case\nCreate/papers\nFair use\n\n\nrice_monitoring_ph.jpg\nRice field monitoring\nCreate\nOwn\n\n\nmangrove_mapping.jpg\nMangrove classification\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\ngeopandas_logo.png\nGeoPandas logo\nGeoPandas GitHub\nBSD\n\n\nrasterio_logo.png\nRasterio logo\nRasterio GitHub\nBSD\n\n\npython_geospatial_stack.png\nPython GIS ecosystem\nCREATE THIS\nOwn\n\n\ncolab_interface.jpg\nGoogle Colab screenshot\nScreenshot\nFair use\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\ngee_architecture.png\nGEE system architecture\nCreate from docs\nOwn\n\n\ngeemap_logo.png\ngeemap library logo\ngeemap GitHub\nMIT\n\n\n\n\n\n\n\n\n\n\nPriority diagrams to create (can use tools like draw.io, Canva, PowerPoint):\n\neo_ml_workflow.png - End-to-end ML workflow for EO\nph_eo_ecosystem.png - Philippine EO infrastructure diagram\nfoundation_models_timeline.png - 2025 FM developments\npython_geospatial_stack.png - Python GIS libraries\nintegration_diagram.png - Platform integration\n\n\n\n\nFor slides with missing images, you can convert to text-based slides. Example:\nInstead of:\n![](images/copernicus_overview.jpg)\nUse:\n**Copernicus Programme Overview**\n\n::: {.columns}\n::: {.column width=\"50%\"}\n- Launched 1998 (GMES)\n- Renamed Copernicus 2012\n- EU flagship programme\n- €6.7B (2021-2027)\n:::\n::: {.column width=\"50%\"}\n- 6 Sentinel families\n- Free & Open data policy\n- Petabytes of EO data\n- Global coverage\n:::\n:::\n\n\n\nCreate simple placeholder images with Python:\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\n\ndef create_placeholder(filename, text, size=(1920, 1080)):\n    img = Image.new('RGB', size, color='#1e3a8a')\n    d = ImageDraw.Draw(img)\n    \n    # Try to use a nice font, fallback to default\n    try:\n        font = ImageFont.truetype(\"Arial.ttf\", 80)\n    except:\n        font = ImageFont.load_default()\n    \n    # Calculate text position (center)\n    bbox = d.textbbox((0, 0), text, font=font)\n    text_width = bbox[2] - bbox[0]\n    text_height = bbox[3] - bbox[1]\n    position = ((size[0] - text_width) / 2, (size[1] - text_height) / 2)\n    \n    d.text(position, text, fill='white', font=font)\n    \n    os.makedirs('images', exist_ok=True)\n    img.save(f'images/{filename}')\n    print(f'Created: images/{filename}')\n\n# Create placeholders for key images\nplaceholders = [\n    ('copernicus_overview.jpg', 'Copernicus Programme'),\n    ('sentinel1_satellite.jpg', 'Sentinel-1 SAR'),\n    ('sentinel2_mayon.jpg', 'Sentinel-2 Optical'),\n    ('ph_eo_ecosystem.png', 'Philippine EO Ecosystem'),\n    ('eo_ml_workflow.png', 'EO ML Workflow'),\n    ('foundation_models_timeline.png', 'Foundation Models 2025'),\n]\n\nfor filename, text in placeholders:\n    create_placeholder(filename, text)\n\n\n\n\n\n\n\n\nVisit ESA Multimedia Gallery\nSearch for “Sentinel-1” or “Sentinel-2”\nDownload high-resolution images\nAttribution: “ESA” or “Contains modified Copernicus data”\n\n\n\n\n\nOpen Copernicus Browser\nNavigate to Philippine areas of interest\nTake screenshots or export images\nFree to use (Copernicus data policy)\n\n\n\n\nPhilSA: - Email: info@philsa.gov.ph - Request official logos and platform screenshots - Explain educational use for CoPhil training\nDOST-ASTI: - Visit: https://asti.dost.gov.ph - Contact for SkAI-Pinas, DIMER, PANDA materials - Request permission for screenshots\nNAMRIA: - Visit: https://www.namria.gov.ph - Download publicly available products - Screenshot Geoportal interface\n\n\n\nMost logos can be found on official websites: - Right-click → “Save image as…” - Look for “Media Kit” or “Press Resources” - Use PNG format with transparent background when available\n\n\n\n\n\n\n\nCredit: European Space Agency (ESA)\n\n\n\nContains modified Copernicus Sentinel data [year]\n\n\n\nSource: [Agency Name], Republic of the Philippines\n\n\n\nAdapted from [Author et al., Year]\n\n\n\n\n\nBefore final delivery, ensure:\n\nAll critical diagrams created (workflow, ecosystem, integration)\nOfficial logos obtained (PhilSA, DOST, NAMRIA, ESA)\nKey satellite images sourced (Sentinel-1, Sentinel-2)\nPlatform screenshots captured (Copernicus Browser, Geoportal)\nPhilippine example images prepared\nProper attribution added to all images\nImages optimized for web (&lt; 500KB each)\nImage dimensions appropriate (1920x1080 or smaller)\nAll images placed in course_site/day1/presentations/images/\nPresentations re-rendered and checked\n\n\n\n\n\n\n\n\nDraw.io (diagrams.net) - Excellent for technical diagrams\nCanva - Good for infographics\nPowerPoint/Google Slides - Export slides as PNG\nInkscape - Vector graphics (open-source)\nGIMP - Image editing (open-source)\n\n\n\n\n\nDiagrams: 1920x1080 or 1600x900\nLogos: 500x500 (PNG with transparency)\nScreenshots: 1920x1080 cropped as needed\nConcept images: 1200x800 minimum\n\n\n\n\n\nPrimary: #1e3a8a (Blue)\nSecondary: #7c3aed (Purple)\nAccent: #10b981 (Green)\nDark: #1f2937 (Gray)\nLight: #f3f4f6 (Light Gray)\n\n\n\n\n\n\nRun this to create placeholder images for testing:\ncd course_site/day1/presentations\nmkdir -p images\n\n# Create simple text placeholders\necho \"IMAGE PLACEHOLDER\" &gt; images/README.txt\nOr use the Python script above to create actual placeholder images.\n\n\n\n\nFor Image Permissions: - PhilSA: info@philsa.gov.ph - DOST-ASTI: info@asti.dost.gov.ph - NAMRIA: namria@namria.gov.ph - ESA: Contact via ESA website\nFor Technical Issues: - Check image file paths in .qmd files - Ensure images are in course_site/day1/presentations/images/ - Verify image filenames match exactly (case-sensitive)\n\n\n\n\n\n\n\nOfficial logos (PhilSA, DOST, ESA, Copernicus)\nPhilippine EO ecosystem diagram\nEO ML workflow diagram\nSentinel satellite images\n\n\n\n\n\nPlatform screenshots (SIYASAT, Geoportal)\nFoundation models timeline\nPython geospatial stack diagram\nData processing examples\n\n\n\n\n\nConcept diagrams\nIntegration architectures\nPhilippine application examples\n\n\n\n\n\nDecorative images\nBackground visuals\nAdditional examples\n\n\n\n\n\n\n\n\n\nSource all images\nCreate custom diagrams\nProfessional quality\nTime: 2-3 days\n\n\n\n\n\nCritical logos and diagrams\nText-based alternatives for others\nTime: 4-6 hours\n\n\n\n\n\nUse placeholder images\nEnhanced text descriptions\nSpeaker explains visually\nTime: 1 hour\n\n\n\n\n\nComment out image-heavy slides\nRely on speaker notes\nQuickest solution\nTime: 30 minutes\n\n\n\n\n\n\n\nPresentations are fully functional without images\nSpeaker notes provide context for missing visuals\nImages enhance but are not essential for learning\nCan deliver with placeholders and add images later\nStudents can access online resources for visuals\n\n\nCurrent Status: ⚠️ 87 images missing\nRecommended Action: Start with Option B (Essential Images Only)\nEstimated Time: 4-6 hours for core images\nQuestions? See INSTRUCTOR_GUIDE.md or README.md"
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#overview",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#overview",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "The Day 1 presentations reference 87 images that need to be sourced and placed in the images/ directory. This guide provides: - Complete list of missing images by session - Description and purpose of each image - Recommended sources - Licensing requirements - Alternative solutions if images unavailable"
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#directory-structure",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#directory-structure",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "course_site/day1/presentations/\n├── images/                    ← CREATE THIS DIRECTORY\n│   ├── Logos/\n│   ├── Satellites/\n│   ├── Platforms/\n│   ├── Organizations/\n│   └── Concepts/\n└── *.qmd files"
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#quick-solutions",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#quick-solutions",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "Source images from official websites and open repositories. See detailed list below.\n\n\n\nReplace image slides with text-based content describing what would be shown.\n\n\n\nComment out image slides and rely on speaker descriptions.\n\n\n\nCreate simple placeholder images with text labels (e.g., “Sentinel-1 Satellite”)."
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#missing-images-by-session",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#missing-images-by-session",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "✅ No images required\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\neu_global_gateway.png\nEU Global Gateway logo\nEU Website\nFair use\n\n\ncopphil_logo.png\nCoPhil Programme logo\nCoPhil Programme\nOfficial\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nphilsa_logo.png\nPhilSA official logo\nPhilSA Website\nOfficial\n\n\nphilsa_building.jpg\nPhilSA headquarters\nPhilSA or Google Earth\nFair use\n\n\ndost_logo.png\nDOST logo\nDOST Website\nOfficial\n\n\ndost_asti_logo.png\nDOST-ASTI logo\nASTI Website\nOfficial\n\n\nnamria_logo.png\nNAMRIA logo\nNAMRIA Website\nOfficial\n\n\npagasa_logo.png\nPAGASA logo\nPAGASA Website\nOfficial\n\n\ndatos_logo.png\nDATOS platform logo\nDOST-ASTI\nOfficial\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\ncopernicus_overview.jpg\nProgramme overview infographic\nCopernicus.eu\nCC BY 4.0\n\n\ncopernicus_applications.png\nService applications diagram\nCopernicus website\nCC BY 4.0\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nsentinel1_satellite.jpg\nSentinel-1 satellite image\nESA Multimedia\nESA Standard\n\n\nsentinel2_satellite.jpg\nSentinel-2 satellite image\nESA Multimedia\nESA Standard\n\n\nsar_principle.png\nSAR imaging principle diagram\nCreate or ESA Education\nFair use\n\n\npolarization_comparison.jpg\nVV/VH polarization comparison\nESA or academic papers\nFair use\n\n\nsentinel2_mayon.jpg\nSentinel-2 image of Mayon Volcano\nCopernicus Browser\nFree & Open\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\ns1_flood_mapping.jpg\nFlood detection example\nCopernicus Browser or papers\nFair use\n\n\ns1_insar.jpg\nInSAR ground deformation\nESA or academic\nFair use\n\n\nsentinel1_flood_ph.png\nPhilippine flood case study\nCreate from Copernicus data\nOwn creation\n\n\ns1_s2_synergy.jpg\nCombined SAR & Optical\nESA or papers\nFair use\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\ndataspace_portal.jpg\nCopernicus Data Space Ecosystem\nScreenshot from dataspace.copernicus.eu\nFair use\n\n\nsentiboard.jpg\nSentiBoard interface\nScreenshot\nFair use\n\n\ngee_logo.png\nGoogle Earth Engine logo\nGEE Website\nGoogle\n\n\nsiyasat_portal.jpg\nSIYASAT platform interface\nPhilSA (with permission)\nOfficial\n\n\nspace_plus_dashboard.jpg\nSpace+ Dashboard\nPhilSA (with permission)\nOfficial\n\n\nnamria_geoportal.jpg\nNAMRIA Geoportal screenshot\nScreenshot from geoportal.gov.ph\nFair use\n\n\nnamria_landcover.jpg\nLand cover map example\nNAMRIA products\nOfficial\n\n\nhazardhunter.jpg\nHazardHunter interface\nNAMRIA\nOfficial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nskai_pinas.jpg\nSkAI-Pinas logo/interface\nDOST-ASTI\nOfficial\n\n\ndimer_interface.jpg\nDIMER platform screenshot\nDOST-ASTI\nOfficial\n\n\naipi_workflow.png\nAIPI workflow diagram\nCreate or DOST-ASTI\nFair use\n\n\nasti_ecosystem.png\nASTI platform ecosystem\nCreate diagram\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\ncoare_infrastructure.jpg\nCOARE HPC facility\nDOST-ASTI\nOfficial\n\n\nmirror_site_concept.jpg\nMirror site architecture\nCreate diagram\nOwn\n\n\ndigital_campus.jpg\nDigital campus concept\nDOST-ASTI or create\nFair use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nph_eo_ecosystem.png\nPhilippine EO ecosystem diagram\nCREATE THIS\nOwn\n\n\nintegration_diagram.png\nPlatform integration flow\nCREATE THIS\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nai_ml_dl_venn.png\nAI/ML/DL relationship diagram\nCREATE or Wikipedia\nCC/Own\n\n\nml_definition.png\nML definition visual\nCreate\nOwn\n\n\ntraditional_vs_ml.png\nTraditional vs ML comparison\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\neo_ml_workflow.png\nEnd-to-end workflow diagram\nCREATE THIS\nOwn\n\n\nproblem_formulation.png\nProblem definition visual\nCreate\nOwn\n\n\ndata_collection_eo.png\nEO data sources diagram\nCreate\nOwn\n\n\npreprocessing_pipeline.png\nPreprocessing steps\nCreate\nOwn\n\n\nfeature_engineering.jpg\nFeature extraction examples\nCreate/papers\nFair use\n\n\nmodel_training.png\nTraining process diagram\nCreate\nOwn\n\n\nvalidation_workflow.png\nValidation methodology\nCreate\nOwn\n\n\ndeployment_architecture.png\nDeployment diagram\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nsupervised_learning_concept.png\nSupervised learning visual\nCreate/Wikipedia\nCC/Own\n\n\nclassification_example.jpg\nLand cover classification\nCreate from S2 data\nOwn\n\n\nregression_example.png\nRegression plot example\nCreate\nOwn\n\n\ndecision_boundary.png\nDecision boundary visual\nCreate\nOwn\n\n\nrandom_forest_diagram.png\nRandom Forest structure\nWikipedia/Create\nCC/Own\n\n\nconfusion_matrix.png\nConfusion matrix example\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nunsupervised_concept.png\nUnsupervised learning visual\nCreate\nOwn\n\n\nclustering_example.jpg\nK-means clustering on imagery\nCreate\nOwn\n\n\nanomaly_detection.jpg\nAnomaly detection example\nCreate/papers\nFair use\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nneural_network_diagram.png\nSimple neural network\nWikipedia/Create\nCC/Own\n\n\ncnn_architecture.png\nCNN structure diagram\nCreate/papers\nFair use\n\n\nconvolution_operation.gif\nConvolution animation\nCreate/GitHub\nOpen\n\n\npooling_operation.png\nPooling visualization\nCreate\nOwn\n\n\nunet_architecture.png\nU-Net architecture\nPapers/Create\nAcademic\n\n\nsemantic_segmentation.jpg\nSegmentation example\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nmodel_centric_vs_data_centric.png\nParadigm comparison\nCreate/Andrew Ng\nFair use\n\n\ndata_quality_impact.png\nQuality vs quantity chart\nCreate\nOwn\n\n\nlabeling_quality.jpg\nGood vs bad labels\nCreate\nOwn\n\n\nactive_learning_loop.png\nActive learning workflow\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nfoundation_models_timeline.png\nFM development timeline\nCREATE THIS\nOwn\n\n\nprithvi_architecture.png\nPrithvi model architecture\nIBM/NASA papers\nAcademic\n\n\nclay_model_overview.png\nClay model visual\nClay GitHub/papers\nOpen\n\n\nnasa_ibm_logo.png\nNASA-IBM partnership logo\nOfficial sources\nFair use\n\n\nfoundation_model_benefits.png\nFM advantages diagram\nCreate\nOwn\n\n\nfine_tuning_workflow.png\nFine-tuning process\nCreate\nOwn\n\n\nphisat2_satellite.jpg\nΦsat-2 satellite image\nESA\nOfficial\n\n\nonboard_ai_concept.png\nOn-board processing diagram\nCreate\nOwn\n\n\nedge_computing_diagram.png\nEdge AI architecture\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\nph_flood_ml.jpg\nPhilippine flood ML example\nCreate\nOwn\n\n\ntyphoon_damage_assessment.jpg\nDamage detection case\nCreate/papers\nFair use\n\n\nrice_monitoring_ph.jpg\nRice field monitoring\nCreate\nOwn\n\n\nmangrove_mapping.jpg\nMangrove classification\nCreate\nOwn\n\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\ngeopandas_logo.png\nGeoPandas logo\nGeoPandas GitHub\nBSD\n\n\nrasterio_logo.png\nRasterio logo\nRasterio GitHub\nBSD\n\n\npython_geospatial_stack.png\nPython GIS ecosystem\nCREATE THIS\nOwn\n\n\ncolab_interface.jpg\nGoogle Colab screenshot\nScreenshot\nFair use\n\n\n\n\n\n\n\n\n\n\nImage\nPurpose\nSource\nLicense\n\n\n\n\ngee_architecture.png\nGEE system architecture\nCreate from docs\nOwn\n\n\ngeemap_logo.png\ngeemap library logo\ngeemap GitHub\nMIT"
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#solutions-workarounds",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#solutions-workarounds",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "Priority diagrams to create (can use tools like draw.io, Canva, PowerPoint):\n\neo_ml_workflow.png - End-to-end ML workflow for EO\nph_eo_ecosystem.png - Philippine EO infrastructure diagram\nfoundation_models_timeline.png - 2025 FM developments\npython_geospatial_stack.png - Python GIS libraries\nintegration_diagram.png - Platform integration\n\n\n\n\nFor slides with missing images, you can convert to text-based slides. Example:\nInstead of:\n![](images/copernicus_overview.jpg)\nUse:\n**Copernicus Programme Overview**\n\n::: {.columns}\n::: {.column width=\"50%\"}\n- Launched 1998 (GMES)\n- Renamed Copernicus 2012\n- EU flagship programme\n- €6.7B (2021-2027)\n:::\n::: {.column width=\"50%\"}\n- 6 Sentinel families\n- Free & Open data policy\n- Petabytes of EO data\n- Global coverage\n:::\n:::\n\n\n\nCreate simple placeholder images with Python:\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\n\ndef create_placeholder(filename, text, size=(1920, 1080)):\n    img = Image.new('RGB', size, color='#1e3a8a')\n    d = ImageDraw.Draw(img)\n    \n    # Try to use a nice font, fallback to default\n    try:\n        font = ImageFont.truetype(\"Arial.ttf\", 80)\n    except:\n        font = ImageFont.load_default()\n    \n    # Calculate text position (center)\n    bbox = d.textbbox((0, 0), text, font=font)\n    text_width = bbox[2] - bbox[0]\n    text_height = bbox[3] - bbox[1]\n    position = ((size[0] - text_width) / 2, (size[1] - text_height) / 2)\n    \n    d.text(position, text, fill='white', font=font)\n    \n    os.makedirs('images', exist_ok=True)\n    img.save(f'images/{filename}')\n    print(f'Created: images/{filename}')\n\n# Create placeholders for key images\nplaceholders = [\n    ('copernicus_overview.jpg', 'Copernicus Programme'),\n    ('sentinel1_satellite.jpg', 'Sentinel-1 SAR'),\n    ('sentinel2_mayon.jpg', 'Sentinel-2 Optical'),\n    ('ph_eo_ecosystem.png', 'Philippine EO Ecosystem'),\n    ('eo_ml_workflow.png', 'EO ML Workflow'),\n    ('foundation_models_timeline.png', 'Foundation Models 2025'),\n]\n\nfor filename, text in placeholders:\n    create_placeholder(filename, text)"
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#sourcing-official-images",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#sourcing-official-images",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "Visit ESA Multimedia Gallery\nSearch for “Sentinel-1” or “Sentinel-2”\nDownload high-resolution images\nAttribution: “ESA” or “Contains modified Copernicus data”\n\n\n\n\n\nOpen Copernicus Browser\nNavigate to Philippine areas of interest\nTake screenshots or export images\nFree to use (Copernicus data policy)\n\n\n\n\nPhilSA: - Email: info@philsa.gov.ph - Request official logos and platform screenshots - Explain educational use for CoPhil training\nDOST-ASTI: - Visit: https://asti.dost.gov.ph - Contact for SkAI-Pinas, DIMER, PANDA materials - Request permission for screenshots\nNAMRIA: - Visit: https://www.namria.gov.ph - Download publicly available products - Screenshot Geoportal interface\n\n\n\nMost logos can be found on official websites: - Right-click → “Save image as…” - Look for “Media Kit” or “Press Resources” - Use PNG format with transparent background when available"
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#licensing-attribution",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#licensing-attribution",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "Credit: European Space Agency (ESA)\n\n\n\nContains modified Copernicus Sentinel data [year]\n\n\n\nSource: [Agency Name], Republic of the Philippines\n\n\n\nAdapted from [Author et al., Year]"
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#pre-delivery-checklist",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#pre-delivery-checklist",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "Before final delivery, ensure:\n\nAll critical diagrams created (workflow, ecosystem, integration)\nOfficial logos obtained (PhilSA, DOST, NAMRIA, ESA)\nKey satellite images sourced (Sentinel-1, Sentinel-2)\nPlatform screenshots captured (Copernicus Browser, Geoportal)\nPhilippine example images prepared\nProper attribution added to all images\nImages optimized for web (&lt; 500KB each)\nImage dimensions appropriate (1920x1080 or smaller)\nAll images placed in course_site/day1/presentations/images/\nPresentations re-rendered and checked"
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#creating-custom-diagrams",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#creating-custom-diagrams",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "Draw.io (diagrams.net) - Excellent for technical diagrams\nCanva - Good for infographics\nPowerPoint/Google Slides - Export slides as PNG\nInkscape - Vector graphics (open-source)\nGIMP - Image editing (open-source)\n\n\n\n\n\nDiagrams: 1920x1080 or 1600x900\nLogos: 500x500 (PNG with transparency)\nScreenshots: 1920x1080 cropped as needed\nConcept images: 1200x800 minimum\n\n\n\n\n\nPrimary: #1e3a8a (Blue)\nSecondary: #7c3aed (Purple)\nAccent: #10b981 (Green)\nDark: #1f2937 (Gray)\nLight: #f3f4f6 (Light Gray)"
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#quick-start-script",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#quick-start-script",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "Run this to create placeholder images for testing:\ncd course_site/day1/presentations\nmkdir -p images\n\n# Create simple text placeholders\necho \"IMAGE PLACEHOLDER\" &gt; images/README.txt\nOr use the Python script above to create actual placeholder images."
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#support-contacts",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#support-contacts",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "For Image Permissions: - PhilSA: info@philsa.gov.ph - DOST-ASTI: info@asti.dost.gov.ph - NAMRIA: namria@namria.gov.ph - ESA: Contact via ESA website\nFor Technical Issues: - Check image file paths in .qmd files - Ensure images are in course_site/day1/presentations/images/ - Verify image filenames match exactly (case-sensitive)"
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#priority-levels",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#priority-levels",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "Official logos (PhilSA, DOST, ESA, Copernicus)\nPhilippine EO ecosystem diagram\nEO ML workflow diagram\nSentinel satellite images\n\n\n\n\n\nPlatform screenshots (SIYASAT, Geoportal)\nFoundation models timeline\nPython geospatial stack diagram\nData processing examples\n\n\n\n\n\nConcept diagrams\nIntegration architectures\nPhilippine application examples\n\n\n\n\n\nDecorative images\nBackground visuals\nAdditional examples"
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#delivery-options",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#delivery-options",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "Source all images\nCreate custom diagrams\nProfessional quality\nTime: 2-3 days\n\n\n\n\n\nCritical logos and diagrams\nText-based alternatives for others\nTime: 4-6 hours\n\n\n\n\n\nUse placeholder images\nEnhanced text descriptions\nSpeaker explains visually\nTime: 1 hour\n\n\n\n\n\nComment out image-heavy slides\nRely on speaker notes\nQuickest solution\nTime: 30 minutes"
  },
  {
    "objectID": "day1/presentations/IMAGE_PLACEHOLDERS.html#notes",
    "href": "day1/presentations/IMAGE_PLACEHOLDERS.html#notes",
    "title": "Day 1 Presentations - Image Placeholders Guide",
    "section": "",
    "text": "Presentations are fully functional without images\nSpeaker notes provide context for missing visuals\nImages enhance but are not essential for learning\nCan deliver with placeholders and add images later\nStudents can access online resources for visuals\n\n\nCurrent Status: ⚠️ 87 images missing\nRecommended Action: Start with Option B (Essential Images Only)\nEstimated Time: 4-6 hours for core images\nQuestions? See INSTRUCTOR_GUIDE.md or README.md"
  },
  {
    "objectID": "day1/presentations/DIAGRAMS_CREATED.html",
    "href": "day1/presentations/DIAGRAMS_CREATED.html",
    "title": "✅ Diagrams Created Successfully!",
    "section": "",
    "text": "I’ve created the 4 most critical diagrams as SVG files for your Day 1 presentations.\n\n\n\n\n\nFile: images/ph_eo_ecosystem.svg\nUsed in: Session 1 (Slide: Overview of Philippine EO Landscape)\nShows: - EU Copernicus Programme at top - CoPhil Programme in middle - Three main agencies: PhilSA, NAMRIA, DOST-ASTI - Their respective platforms (SIYASAT, Geoportal, SkAI-Pinas) - Data flow from EU to Philippines\n\n\n\n\nFile: images/eo_ml_workflow.svg\nUsed in: Session 2 (AI/ML workflow slides)\nShows: - 8-step workflow: Problem → Data → Preprocessing → Features → Training → Validation → Deployment → Monitoring - Feedback loop from Monitoring back to Data Collection - Color-coded steps for easy understanding - Key principles at bottom\n\n\n\n\nFile: images/foundation_models_timeline.svg\nUsed in: Session 2 (2025 AI updates section)\nShows: - 2023: Early EO foundation models - Aug 2024: NASA-IBM Geospatial FM - Sept 2024: ESA Φsat-2 (on-board AI) - 2025: Prithvi & Clay models - Future: Widespread adoption - Benefits for Philippines listed\n\n\n\n\nFile: images/python_geospatial_stack.svg\nUsed in: Session 3 (Python tools overview)\nShows: - Application layer (your code) - High-level libraries (GeoPandas, Rasterio, geemap) - Core libraries (GDAL, Shapely, NumPy, Pandas, Matplotlib) - Platform layer (Google Colab/Jupyter)\n\n\n\n\n\nThe diagrams are created as SVG (Scalable Vector Graphics) files, which have advantages:\n✅ Perfect quality at any size - scale without pixelation\n✅ Small file size - 4-9 KB each\n✅ Work in modern browsers - all Reveal.js presentations support SVG\n✅ Crisp text - always readable\nHowever, your presentations reference .png files. Here are your options:\n\n\nChange image references from .png to .svg:\n# Old\n![](images/ph_eo_ecosystem.png)\n\n# New\n![](images/ph_eo_ecosystem.svg)\n\n\n\nBrowsers will render SVG content even with .png extension:\ncd course_site/day1/presentations/images\ncp ph_eo_ecosystem.svg ph_eo_ecosystem.png\ncp eo_ml_workflow.svg eo_ml_workflow.png\ncp foundation_models_timeline.svg foundation_models_timeline.png\ncp python_geospatial_stack.svg python_geospatial_stack.png\n\n\n\nIf SVG doesn’t work, use a tool to convert: - Online: https://cloudconvert.com/svg-to-png - Command line (if ImageMagick installed): convert diagram.svg diagram.png\n\n\n\n\n\n\n\n\n87 missing images (404 errors)\n4 critical diagrams needed\n\n\n\n\n\n✅ Philippine ecosystem now visualized\n✅ ML workflow clearly explained\n✅ Foundation models timeline shows 2025 updates\n✅ Python stack architecture documented\nRemaining: 83 images (mostly logos and screenshots)\n\n\n\n\n\nSession 1: ~30% better (1 critical diagram added)\nSession 2: ~50% better (2 critical diagrams added)\nSession 3: ~40% better (1 critical diagram added)\nOverall: Presentations now have professional custom diagrams!\n\n\n\n\n\n\nAll diagrams use your presentation color scheme: - Primary Blue: #1e3a8a (headers, accents) - Purple: #7c3aed (CoPhil programme) - Green: #10b981 (PhilSA, core libraries) - Orange: #f59e0b (NAMRIA) - Pink: #ec4899 (DOST-ASTI)\nProfessional styling: - Clean, modern design - Readable fonts (Arial) - Proper spacing - Clear hierarchy - Accessible colors\n\n\n\n\n\n\n\nUpdate file references OR copy files:\n\nOption A - Update references (Recommended):\ncd course_site/day1/presentations\n\n# Update Session 1\n# Find: images/ph_eo_ecosystem.png\n# Replace: images/ph_eo_ecosystem.svg\n\n# Or use sed:\nsed -i.bak 's/ph_eo_ecosystem\\.png/ph_eo_ecosystem.svg/g' 01_session1_*.qmd\nsed -i.bak 's/eo_ml_workflow\\.png/eo_ml_workflow.svg/g' 02_session2_*.qmd\nsed -i.bak 's/foundation_models_timeline\\.png/foundation_models_timeline.svg/g' 02_session2_*.qmd\nsed -i.bak 's/python_geospatial_stack\\.png/python_geospatial_stack.svg/g' 03_session3_*.qmd\nOption B - Copy to PNG extension:\ncd course_site/day1/presentations/images\ncp ph_eo_ecosystem.svg ph_eo_ecosystem.png\ncp eo_ml_workflow.svg eo_ml_workflow.png\ncp foundation_models_timeline.svg foundation_models_timeline.png\ncp python_geospatial_stack.svg python_geospatial_stack.png\n\nRe-render presentations:\n\ncd course_site/day1/presentations\nquarto render 01_session1_copernicus_philippine_eo.qmd\nquarto render 02_session2_ai_ml_fundamentals.qmd\nquarto render 03_session3_python_geospatial.qmd\n\nPreview to verify:\n\nquarto preview 01_session1_copernicus_philippine_eo.qmd\n\n\n\nDownload the critical logos from IMAGES_TO_SOURCE.md: - PhilSA logo - DOST logo - DOST-ASTI logo - NAMRIA logo - GEE logo\nThis will eliminate the most visible missing images.\n\n\n\nSource remaining high-priority images per IMAGES_TO_SOURCE.md guide.\n\n\n\n\n\nTest the diagrams by: 1. Opening presentations in browser 2. Navigating to slides with diagrams 3. Verifying they display correctly 4. Checking they scale properly 5. Confirming text is readable\nIf diagrams don’t show: - Check file paths are correct - Verify browser supports SVG (all modern browsers do) - Try Option B (copy to .png extension) - Check browser console for errors\n\n\n\n\ncourse_site/day1/presentations/images/\n├── ph_eo_ecosystem.svg              (4.8 KB) ✅\n├── eo_ml_workflow.svg                (8.9 KB) ✅\n├── foundation_models_timeline.svg    (4.1 KB) ✅\n└── python_geospatial_stack.svg       (4.5 KB) ✅\n\nTotal: 22.3 KB of professional diagrams\n\n\n\n\nCritical (download from websites): - Official logos (7 files, 30 minutes) - Sentinel satellite images (2 files, 15 minutes)\nOptional: - Platform screenshots (can show live instead) - Example EO images (can create later) - Concept diagrams (text descriptions work)\nSee IMAGES_TO_SOURCE.md for complete guide.\n\n\n\n\nFor logos: - Visit organization websites - Look for “Media Kit” or “Downloads” - Right-click logo → Save image - PNG format with transparency preferred\nFor testing: - Use quarto preview to see live changes - Check both desktop and mobile views - Verify diagrams are legible when projected\nFor future: - Keep SVG source files (easy to edit) - Can regenerate PNGs at any resolution - Share SVG files with co-instructors for customization\n\n\n\n\n✅ You now have: - 4 professional custom diagrams - Matching your presentation theme - Ready to use (just rename/update references) - Small file sizes, perfect quality - Easy to modify if needed\n📊 Presentation status: - Before: 0% images complete - After: Critical diagrams 100% complete - Remaining: Logos and screenshots - Estimated improvement: 40-50% better visually\n🚀 Ready to deliver!\nThe most important diagrams are done. You can deliver presentations with these and source logos separately. The diagrams alone make a huge difference in understanding!\n\nQuestions? Check IMAGES_TO_SOURCE.md for remaining images or README.md for general guidance."
  },
  {
    "objectID": "day1/presentations/DIAGRAMS_CREATED.html#created-diagrams",
    "href": "day1/presentations/DIAGRAMS_CREATED.html#created-diagrams",
    "title": "✅ Diagrams Created Successfully!",
    "section": "",
    "text": "File: images/ph_eo_ecosystem.svg\nUsed in: Session 1 (Slide: Overview of Philippine EO Landscape)\nShows: - EU Copernicus Programme at top - CoPhil Programme in middle - Three main agencies: PhilSA, NAMRIA, DOST-ASTI - Their respective platforms (SIYASAT, Geoportal, SkAI-Pinas) - Data flow from EU to Philippines\n\n\n\n\nFile: images/eo_ml_workflow.svg\nUsed in: Session 2 (AI/ML workflow slides)\nShows: - 8-step workflow: Problem → Data → Preprocessing → Features → Training → Validation → Deployment → Monitoring - Feedback loop from Monitoring back to Data Collection - Color-coded steps for easy understanding - Key principles at bottom\n\n\n\n\nFile: images/foundation_models_timeline.svg\nUsed in: Session 2 (2025 AI updates section)\nShows: - 2023: Early EO foundation models - Aug 2024: NASA-IBM Geospatial FM - Sept 2024: ESA Φsat-2 (on-board AI) - 2025: Prithvi & Clay models - Future: Widespread adoption - Benefits for Philippines listed\n\n\n\n\nFile: images/python_geospatial_stack.svg\nUsed in: Session 3 (Python tools overview)\nShows: - Application layer (your code) - High-level libraries (GeoPandas, Rasterio, geemap) - Core libraries (GDAL, Shapely, NumPy, Pandas, Matplotlib) - Platform layer (Google Colab/Jupyter)"
  },
  {
    "objectID": "day1/presentations/DIAGRAMS_CREATED.html#technical-note-svg-vs-png",
    "href": "day1/presentations/DIAGRAMS_CREATED.html#technical-note-svg-vs-png",
    "title": "✅ Diagrams Created Successfully!",
    "section": "",
    "text": "The diagrams are created as SVG (Scalable Vector Graphics) files, which have advantages:\n✅ Perfect quality at any size - scale without pixelation\n✅ Small file size - 4-9 KB each\n✅ Work in modern browsers - all Reveal.js presentations support SVG\n✅ Crisp text - always readable\nHowever, your presentations reference .png files. Here are your options:\n\n\nChange image references from .png to .svg:\n# Old\n![](images/ph_eo_ecosystem.png)\n\n# New\n![](images/ph_eo_ecosystem.svg)\n\n\n\nBrowsers will render SVG content even with .png extension:\ncd course_site/day1/presentations/images\ncp ph_eo_ecosystem.svg ph_eo_ecosystem.png\ncp eo_ml_workflow.svg eo_ml_workflow.png\ncp foundation_models_timeline.svg foundation_models_timeline.png\ncp python_geospatial_stack.svg python_geospatial_stack.png\n\n\n\nIf SVG doesn’t work, use a tool to convert: - Online: https://cloudconvert.com/svg-to-png - Command line (if ImageMagick installed): convert diagram.svg diagram.png"
  },
  {
    "objectID": "day1/presentations/DIAGRAMS_CREATED.html#impact-summary",
    "href": "day1/presentations/DIAGRAMS_CREATED.html#impact-summary",
    "title": "✅ Diagrams Created Successfully!",
    "section": "",
    "text": "87 missing images (404 errors)\n4 critical diagrams needed\n\n\n\n\n\n✅ Philippine ecosystem now visualized\n✅ ML workflow clearly explained\n✅ Foundation models timeline shows 2025 updates\n✅ Python stack architecture documented\nRemaining: 83 images (mostly logos and screenshots)\n\n\n\n\n\nSession 1: ~30% better (1 critical diagram added)\nSession 2: ~50% better (2 critical diagrams added)\nSession 3: ~40% better (1 critical diagram added)\nOverall: Presentations now have professional custom diagrams!"
  },
  {
    "objectID": "day1/presentations/DIAGRAMS_CREATED.html#design-features",
    "href": "day1/presentations/DIAGRAMS_CREATED.html#design-features",
    "title": "✅ Diagrams Created Successfully!",
    "section": "",
    "text": "All diagrams use your presentation color scheme: - Primary Blue: #1e3a8a (headers, accents) - Purple: #7c3aed (CoPhil programme) - Green: #10b981 (PhilSA, core libraries) - Orange: #f59e0b (NAMRIA) - Pink: #ec4899 (DOST-ASTI)\nProfessional styling: - Clean, modern design - Readable fonts (Arial) - Proper spacing - Clear hierarchy - Accessible colors"
  },
  {
    "objectID": "day1/presentations/DIAGRAMS_CREATED.html#next-steps",
    "href": "day1/presentations/DIAGRAMS_CREATED.html#next-steps",
    "title": "✅ Diagrams Created Successfully!",
    "section": "",
    "text": "Update file references OR copy files:\n\nOption A - Update references (Recommended):\ncd course_site/day1/presentations\n\n# Update Session 1\n# Find: images/ph_eo_ecosystem.png\n# Replace: images/ph_eo_ecosystem.svg\n\n# Or use sed:\nsed -i.bak 's/ph_eo_ecosystem\\.png/ph_eo_ecosystem.svg/g' 01_session1_*.qmd\nsed -i.bak 's/eo_ml_workflow\\.png/eo_ml_workflow.svg/g' 02_session2_*.qmd\nsed -i.bak 's/foundation_models_timeline\\.png/foundation_models_timeline.svg/g' 02_session2_*.qmd\nsed -i.bak 's/python_geospatial_stack\\.png/python_geospatial_stack.svg/g' 03_session3_*.qmd\nOption B - Copy to PNG extension:\ncd course_site/day1/presentations/images\ncp ph_eo_ecosystem.svg ph_eo_ecosystem.png\ncp eo_ml_workflow.svg eo_ml_workflow.png\ncp foundation_models_timeline.svg foundation_models_timeline.png\ncp python_geospatial_stack.svg python_geospatial_stack.png\n\nRe-render presentations:\n\ncd course_site/day1/presentations\nquarto render 01_session1_copernicus_philippine_eo.qmd\nquarto render 02_session2_ai_ml_fundamentals.qmd\nquarto render 03_session3_python_geospatial.qmd\n\nPreview to verify:\n\nquarto preview 01_session1_copernicus_philippine_eo.qmd\n\n\n\nDownload the critical logos from IMAGES_TO_SOURCE.md: - PhilSA logo - DOST logo - DOST-ASTI logo - NAMRIA logo - GEE logo\nThis will eliminate the most visible missing images.\n\n\n\nSource remaining high-priority images per IMAGES_TO_SOURCE.md guide."
  },
  {
    "objectID": "day1/presentations/DIAGRAMS_CREATED.html#quality-check",
    "href": "day1/presentations/DIAGRAMS_CREATED.html#quality-check",
    "title": "✅ Diagrams Created Successfully!",
    "section": "",
    "text": "Test the diagrams by: 1. Opening presentations in browser 2. Navigating to slides with diagrams 3. Verifying they display correctly 4. Checking they scale properly 5. Confirming text is readable\nIf diagrams don’t show: - Check file paths are correct - Verify browser supports SVG (all modern browsers do) - Try Option B (copy to .png extension) - Check browser console for errors"
  },
  {
    "objectID": "day1/presentations/DIAGRAMS_CREATED.html#files-created",
    "href": "day1/presentations/DIAGRAMS_CREATED.html#files-created",
    "title": "✅ Diagrams Created Successfully!",
    "section": "",
    "text": "course_site/day1/presentations/images/\n├── ph_eo_ecosystem.svg              (4.8 KB) ✅\n├── eo_ml_workflow.svg                (8.9 KB) ✅\n├── foundation_models_timeline.svg    (4.1 KB) ✅\n└── python_geospatial_stack.svg       (4.5 KB) ✅\n\nTotal: 22.3 KB of professional diagrams"
  },
  {
    "objectID": "day1/presentations/DIAGRAMS_CREATED.html#what-you-still-need",
    "href": "day1/presentations/DIAGRAMS_CREATED.html#what-you-still-need",
    "title": "✅ Diagrams Created Successfully!",
    "section": "",
    "text": "Critical (download from websites): - Official logos (7 files, 30 minutes) - Sentinel satellite images (2 files, 15 minutes)\nOptional: - Platform screenshots (can show live instead) - Example EO images (can create later) - Concept diagrams (text descriptions work)\nSee IMAGES_TO_SOURCE.md for complete guide."
  },
  {
    "objectID": "day1/presentations/DIAGRAMS_CREATED.html#pro-tips",
    "href": "day1/presentations/DIAGRAMS_CREATED.html#pro-tips",
    "title": "✅ Diagrams Created Successfully!",
    "section": "",
    "text": "For logos: - Visit organization websites - Look for “Media Kit” or “Downloads” - Right-click logo → Save image - PNG format with transparency preferred\nFor testing: - Use quarto preview to see live changes - Check both desktop and mobile views - Verify diagrams are legible when projected\nFor future: - Keep SVG source files (easy to edit) - Can regenerate PNGs at any resolution - Share SVG files with co-instructors for customization"
  },
  {
    "objectID": "day1/presentations/DIAGRAMS_CREATED.html#summary",
    "href": "day1/presentations/DIAGRAMS_CREATED.html#summary",
    "title": "✅ Diagrams Created Successfully!",
    "section": "",
    "text": "✅ You now have: - 4 professional custom diagrams - Matching your presentation theme - Ready to use (just rename/update references) - Small file sizes, perfect quality - Easy to modify if needed\n📊 Presentation status: - Before: 0% images complete - After: Critical diagrams 100% complete - Remaining: Logos and screenshots - Estimated improvement: 40-50% better visually\n🚀 Ready to deliver!\nThe most important diagrams are done. You can deliver presentations with these and source logos separately. The diagrams alone make a huge difference in understanding!\n\nQuestions? Check IMAGES_TO_SOURCE.md for remaining images or README.md for general guidance."
  },
  {
    "objectID": "day1/sessions/session4.html",
    "href": "day1/sessions/session4.html",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "",
    "text": "Home › Day 1 › Session 4",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#session-overview",
    "href": "day1/sessions/session4.html#session-overview",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Session Overview",
    "text": "Session Overview\nGoogle Earth Engine (GEE) is a planetary-scale platform for Earth science data and analysis. This session introduces you to GEE’s Python API, enabling you to access Sentinel-1 and Sentinel-2 data, filter massive image collections, perform cloud masking, create temporal composites, and export processed data - all without downloading terabytes of imagery. You’ll learn core GEE concepts and apply them to Philippine use cases.\n\n\nLearning Objectives\nBy the end of this session, you will be able to:\n\nExplain what Google Earth Engine is and its advantages for EO\nAuthenticate and initialize the Earth Engine Python API\nDefine core GEE concepts: Image, ImageCollection, Feature, FeatureCollection\nApply filters (spatial, temporal, metadata) to image collections\nAccess Sentinel-1 GRD and Sentinel-2 SR data catalogs\nImplement cloud masking using QA bands\nCreate temporal composites (median, mean) to reduce cloud cover\nCalculate spectral indices (NDVI, NDWI) at scale\nExport processed imagery to Google Drive\nUnderstand GEE’s capabilities and limitations for AI/ML workflows",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#presentation-slides",
    "href": "day1/sessions/session4.html#presentation-slides",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Presentation Slides",
    "text": "Presentation Slides",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#part-1-what-is-google-earth-engine",
    "href": "day1/sessions/session4.html#part-1-what-is-google-earth-engine",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 1: What is Google Earth Engine?",
    "text": "Part 1: What is Google Earth Engine?\n\nOverview\nGoogle Earth Engine is a cloud-based platform combining:\n\nMulti-petabyte catalog of satellite imagery and geospatial datasets\nPlanetary-scale analysis capabilities via Google’s computational infrastructure\nCode Editor (JavaScript) and Python API for programmatic access\n\n\n\n\n\n\nflowchart TB\n    subgraph User[\"USER INTERFACE\"]\n        U1[Code Editor&lt;br/&gt;JavaScript&lt;br/&gt;Web-based IDE]\n        U2[Python API&lt;br/&gt;Local/Colab&lt;br/&gt;ee library]\n        U3[Apps&lt;br/&gt;Earth Engine Apps&lt;br/&gt;Custom web apps]\n    end\n\n    subgraph GEECloud[\"GOOGLE EARTH ENGINE CLOUD\"]\n        subgraph DataCatalog[\"DATA CATALOG (70+ PB)\"]\n            DC1[Landsat&lt;br/&gt;1972-present&lt;br/&gt;30m resolution]\n            DC2[Sentinel-1/2&lt;br/&gt;2014-present&lt;br/&gt;10m resolution]\n            DC3[MODIS&lt;br/&gt;2000-present&lt;br/&gt;250m-1km]\n            DC4[Climate&lt;br/&gt;ERA5, CHIRPS&lt;br/&gt;Weather data]\n            DC5[Terrain&lt;br/&gt;SRTM, ALOS&lt;br/&gt;DEMs]\n        end\n\n        subgraph Processing[\"PROCESSING ENGINE\"]\n            P1[Parallel&lt;br/&gt;Computation&lt;br/&gt;Distributed]\n            P2[Server-side&lt;br/&gt;Operations&lt;br/&gt;ee.Image, ee.ImageCollection]\n            P3[Optimized&lt;br/&gt;Algorithms&lt;br/&gt;Reducers, Filters]\n        end\n\n        subgraph Operations[\"COMMON OPERATIONS\"]\n            O1[Filtering&lt;br/&gt;filterBounds&lt;br/&gt;filterDate&lt;br/&gt;filterMetadata]\n            O2[Compositing&lt;br/&gt;median, mean&lt;br/&gt;mosaic, reduce]\n            O3[Indices&lt;br/&gt;NDVI, EVI&lt;br/&gt;normalizedDifference]\n            O4[Classification&lt;br/&gt;Random Forest&lt;br/&gt;CART, SVM]\n        end\n    end\n\n    subgraph Output[\"OUTPUT\"]\n        OUT1[Interactive Maps&lt;br/&gt;Visualization&lt;br/&gt;Map.addLayer]\n        OUT2[Exports&lt;br/&gt;Drive, Asset&lt;br/&gt;Cloud Storage]\n        OUT3[Charts&lt;br/&gt;Time series&lt;br/&gt;Statistics]\n        OUT4[Training Data&lt;br/&gt;For external ML]\n    end\n\n    U1 --&gt; P2\n    U2 --&gt; P2\n    U3 --&gt; P2\n\n    DataCatalog --&gt; P1\n    P1 --&gt; P2\n    P2 --&gt; O1\n    P2 --&gt; O2\n    P2 --&gt; O3\n    P2 --&gt; O4\n\n    Operations --&gt; OUT1\n    Operations --&gt; OUT2\n    Operations --&gt; OUT3\n    Operations --&gt; OUT4\n\n    style User fill:#e6f3ff,stroke:#0066cc,stroke-width:2px\n    style DataCatalog fill:#e6ffe6,stroke:#00aa44,stroke-width:2px\n    style Processing fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style Operations fill:#ffe6e6,stroke:#cc0044,stroke-width:2px\n    style Output fill:#f0e6ff,stroke:#6666cc,stroke-width:2px\n\n\n Google Earth Engine Architecture and Workflow \n\n\n\n\n\n\n\n\n\nNoteEarth Engine by the Numbers\n\n\n\n\n40+ years of historical imagery\n70+ petabytes of data\n700+ datasets including Landsat, Sentinel, MODIS, climate, terrain\nGlobal coverage updated daily\nFree for research, education, and non-profit use\n\n\n\n\n\nWhy Use Google Earth Engine?\nTraditional workflow problems:\n\nDownloading terabytes of satellite data\nStoring data locally (expensive storage)\nPre-processing each scene individually (time-consuming)\nLimited computational resources for large-area analysis\n\nEarth Engine solution:\n┌─────────────────────────────────────┐\n│   Your Computer                     │\n│   ┌──────────────┐                  │\n│   │ Write Code   │                  │\n│   │ (Python/JS)  │                  │\n│   └──────┬───────┘                  │\n│          │                           │\n│   ┌──────▼───────────────────────┐  │\n│   │ Send to Cloud                │  │\n│   └──────────────────────────────┘  │\n└──────────────┬──────────────────────┘\n               │\n               ▼\n┌──────────────────────────────────────────────┐\n│   Google Earth Engine Cloud                 │\n│   ┌──────────────┐  ┌──────────────┐        │\n│   │ Petabyte     │  │ Massive      │        │\n│   │ Data Catalog │  │ Computation  │        │\n│   └──────────────┘  └──────────────┘        │\n│                                              │\n│   Process → Results → Send back to you      │\n└──────────────────────────────────────────────┘\nKey advantages:\n\nNo downloading: Data stays in Google’s cloud\nParallel processing: Distributed computation across many machines\nPre-processed data: Analysis-ready collections (e.g., Sentinel-2 SR)\nTemporal analysis: Easily work with time series\nReproducible: Share code, not gigabytes of data\n\n\n\nUse Cases for Earth Observation\nIdeal for:\n\nLarge-area mapping (country/continent scale)\nMulti-temporal analysis (time series, change detection)\nRapid prototyping and exploration\nCloud-based pre-processing\nTeaching and learning (no infrastructure needed)\n\nLess ideal for:\n\nTraining custom deep learning models (CNNs, U-Net) - limited GPU support\nReal-time processing requiring millisecond latency\nWorkflows requiring full control over hardware\nProprietary/restricted datasets not in GEE catalog\n\n\nPhilippine Applications:\n\nNational land cover mapping: Process all of Philippines (~300,000 km²) at 10m resolution\nMulti-year deforestation monitoring: Annual forest loss detection 2015-2025\nTyphoon impact assessment: Before/after composites for disaster response\nRice paddy monitoring: Track planting/harvest cycles using SAR time series\nCoastal change detection: Erosion and accretion mapping using optical+SAR",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#part-2-setting-up-earth-engine-in-python",
    "href": "day1/sessions/session4.html#part-2-setting-up-earth-engine-in-python",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 2: Setting Up Earth Engine in Python",
    "text": "Part 2: Setting Up Earth Engine in Python\n\nPrerequisites\n\n\n\n\n\n\nWarningRequired Setup\n\n\n\nBefore using Earth Engine, you must:\n\nGoogle account (Gmail or G Suite)\nEarth Engine account - Register at earthengine.google.com\nCloud project - Create or select a project after registration\n\nRegistration is FREE and typically approved within 1-2 days.\n\n\n\n\nInstallation in Google Colab\nThe earthengine-api library is pre-installed in Colab, but let’s ensure it’s up-to-date:\n# Update Earth Engine API\n!pip install earthengine-api --upgrade -q\n\nprint(\"Earth Engine API updated! ✓\")\n\n\nAuthentication & Initialization\nFirst-time setup (one-time per environment):\nimport ee\n\n# Authenticate (opens browser window for authorization)\nee.Authenticate()\nThis will: 1. Open a new browser tab 2. Ask you to select your Google account 3. Request permission to access Earth Engine 4. Provide an authorization code 5. Automatically apply the code\n\n\n\n\n\n\nTipAuthentication Troubleshooting\n\n\n\nIf authentication fails:\n\nEnsure you’ve registered at earthengine.google.com\nCheck that you’re using the same Google account\nClear browser cookies and try again\nUse an incognito/private browsing window\n\n\n\nInitialize Earth Engine (required every session):\n# Initialize with your cloud project\n# Replace with your actual project ID\nee.Initialize(project='your-project-id')\n\nprint(\"Earth Engine initialized! ✓\")\nTo find your project ID: 1. Go to console.cloud.google.com 2. Select your project from the dropdown at the top 3. Copy the Project ID (not Project Name)\nComplete setup code:\nimport ee\nimport geemap  # Interactive mapping library\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Authenticate (first time only - comment out after first use)\n# ee.Authenticate()\n\n# Initialize\nee.Initialize(project='your-project-id')\n\n# Test that it works\nimage = ee.Image('USGS/SRTMGL1_003')\nprint(\"✓ Successfully connected to Earth Engine!\")\nprint(f\"  Test image bands: {image.bandNames().getInfo()}\")",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#part-3-core-earth-engine-concepts",
    "href": "day1/sessions/session4.html#part-3-core-earth-engine-concepts",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 3: Core Earth Engine Concepts",
    "text": "Part 3: Core Earth Engine Concepts\n\nThe Building Blocks\nEarth Engine has a unique data model optimized for planetary-scale analysis:\n\n\n\n\n\n\n\n\nConcept\nDescription\nExample\n\n\n\n\nImage\nSingle raster with multiple bands\nOne Sentinel-2 scene\n\n\nImageCollection\nStack of images (time series)\nAll Sentinel-2 over Philippines in 2024\n\n\nGeometry\nVector shapes\nPoint, polygon, line\n\n\nFeature\nGeometry + properties (attributes)\nProvince polygon with name, population\n\n\nFeatureCollection\nMultiple features\nAll Philippine provinces\n\n\n\n\n\n1. Geometry\nDefine locations and areas of interest:\n# Point (longitude, latitude)\nmanila = ee.Geometry.Point([121.0244, 14.5995])\n\n# Rectangle (min_lon, min_lat, max_lon, max_lat)\nbohol_bbox = ee.Geometry.Rectangle([123.8, 9.6, 124.6, 10.2])\n\n# Polygon (list of coordinate pairs)\ncustom_aoi = ee.Geometry.Polygon([\n    [[123.5, 9.5], [125.0, 9.5], [125.0, 11.0], [123.5, 11.0], [123.5, 9.5]]\n])\n\nprint(\"Geometries created! ✓\")\nprint(f\"Manila coordinates: {manila.coordinates().getInfo()}\")\nprint(f\"Bohol bbox area: {bohol_bbox.area().divide(1e6).getInfo():.2f} km²\")\n\n\n2. Image\nSingle raster image with one or more bands:\n# Load a Sentinel-2 image by ID\nsentinel2_image = ee.Image('COPERNICUS/S2_SR/20240315T015701_20240315T015659_T51PWN')\n\n# Examine properties\nprint(\"Image ID:\", sentinel2_image.id().getInfo())\nprint(\"Band names:\", sentinel2_image.bandNames().getInfo())\nprint(\"Cloud cover:\", sentinel2_image.get('CLOUDY_PIXEL_PERCENTAGE').getInfo(), \"%\")\n\n# Select specific bands\nrgb_bands = sentinel2_image.select(['B4', 'B3', 'B2'])  # Red, Green, Blue\nImage operations:\n# Calculate NDVI for single image\nndvi = sentinel2_image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n\n# Add as band to original image\nimage_with_ndvi = sentinel2_image.addBands(ndvi)\n\nprint(\"NDVI band added! ✓\")\n\n\n3. ImageCollection\nStack of images (time series):\n# Load Sentinel-2 collection\ns2_collection = ee.ImageCollection('COPERNICUS/S2_SR')\n\n# Check collection size (can be huge!)\nprint(\"Total Sentinel-2 images in catalog:\", s2_collection.size().getInfo())\n# This will be millions!\nImageCollection operations:\n# Filter by date, location, and cloud cover\nfiltered = (s2_collection\n    .filterBounds(bohol_bbox)\n    .filterDate('2024-01-01', '2024-12-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30)))\n\nprint(f\"Filtered to {filtered.size().getInfo()} images over Bohol in 2024 with &lt;30% clouds\")\n\n# Get image at specific index\nfirst_image = ee.Image(filtered.first())\nprint(\"First image date:\", first_image.date().format('YYYY-MM-dd').getInfo())\n\n\n4. Feature & FeatureCollection\nVector data with attributes:\n# Single feature (geometry + properties)\nmanila_feature = ee.Feature(\n    ee.Geometry.Point([121.0244, 14.5995]),\n    {'name': 'Manila', 'population': 1780148, 'country': 'Philippines'}\n)\n\nprint(\"Feature:\", manila_feature.getInfo())\n\n# Load feature collection (e.g., GADM administrative boundaries)\nphilippines = ee.FeatureCollection('FAO/GAUL/2015/level1').filter(\n    ee.Filter.eq('ADM0_NAME', 'Philippines')\n)\n\nprint(f\"Philippine provinces: {philippines.size().getInfo()}\")",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#part-4-filtering-and-querying-data",
    "href": "day1/sessions/session4.html#part-4-filtering-and-querying-data",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 4: Filtering and Querying Data",
    "text": "Part 4: Filtering and Querying Data\n\nFilter Types\nEarth Engine provides powerful filtering to reduce massive collections to relevant data.\n\n\n\n\n\nflowchart TD\n    A[Full Sentinel-2 Collection&lt;br/&gt;COPERNICUS/S2_SR&lt;br/&gt;Global, All dates&lt;br/&gt;~Millions of images] --&gt; B[filterBounds&lt;br/&gt;Spatial Filter&lt;br/&gt;AOI: Bohol Province]\n\n    B --&gt; C[Bohol Coverage&lt;br/&gt;~10,000s images&lt;br/&gt;Since 2015]\n\n    C --&gt; D[filterDate&lt;br/&gt;Temporal Filter&lt;br/&gt;2024-03-01 to 2024-05-31]\n\n    D --&gt; E[Date Range&lt;br/&gt;~100 images&lt;br/&gt;3 months]\n\n    E --&gt; F{filterMetadata&lt;br/&gt;CLOUDY_PIXEL_PERCENTAGE&lt;br/&gt;&lt; 20%}\n\n    F --&gt;|Pass| G[Clear Images&lt;br/&gt;~30-40 images&lt;br/&gt;Usable quality]\n\n    F --&gt;|Fail&lt;br/&gt;Too cloudy| H[Excluded&lt;br/&gt;High cloud cover]\n\n    G --&gt; I[select&lt;br/&gt;Band Selection&lt;br/&gt;B2, B3, B4, B8, B11, B12]\n\n    I --&gt; J[Final Collection&lt;br/&gt;30-40 images&lt;br/&gt;6 bands each&lt;br/&gt;Ready for analysis]\n\n    J --&gt; K1[median&lt;br/&gt;Temporal composite]\n    J --&gt; K2[mean&lt;br/&gt;Average]\n    J --&gt; K3[mosaic&lt;br/&gt;Spatial mosaic]\n    J --&gt; K4[map&lt;br/&gt;Apply function]\n\n    K1 --&gt; L[Single Image&lt;br/&gt;Cloud-free composite&lt;br/&gt;Analysis-ready]\n\n    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px\n    style C fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style E fill:#ffe6ff,stroke:#cc00cc,stroke-width:2px\n    style G fill:#e6ffe6,stroke:#00aa44,stroke-width:2px\n    style H fill:#ffe6e6,stroke:#cc0044,stroke-width:2px\n    style J fill:#ccffcc,stroke:#00aa44,stroke-width:3px\n    style L fill:#ccffcc,stroke:#00aa44,stroke-width:3px\n\n\n ImageCollection Filtering Pipeline \n\n\n\n\n1. Spatial Filtering\nfilterBounds() - Keep images intersecting a geometry:\n# Define AOI\npalawan = ee.Geometry.Rectangle([117.5, 8.0, 119.5, 12.0])\n\n# Filter Sentinel-2 to Palawan\ns2_palawan = ee.ImageCollection('COPERNICUS/S2_SR').filterBounds(palawan)\n\nprint(f\"Total Sentinel-2 images over Palawan: {s2_palawan.size().getInfo()}\")\n\n\n2. Temporal Filtering\nfilterDate() - Keep images within date range:\n# Dry season 2024\ndry_season = s2_palawan.filterDate('2024-01-01', '2024-05-31')\nprint(f\"Dry season images: {dry_season.size().getInfo()}\")\n\n# Wet season 2024\nwet_season = s2_palawan.filterDate('2024-06-01', '2024-11-30')\nprint(f\"Wet season images: {wet_season.size().getInfo()}\")\n\n\n3. Metadata Filtering\nfilter() - Custom filters on image properties:\n# Low cloud cover\nclear_images = dry_season.filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\nprint(f\"Clear images (&lt;10% clouds): {clear_images.size().getInfo()}\")\n\n# Specific satellite\ns2a_only = dry_season.filter(ee.Filter.eq('SPACECRAFT_NAME', 'Sentinel-2A'))\nprint(f\"Sentinel-2A only: {s2a_only.size().getInfo()}\")\n\n# Multiple conditions\nbest_images = (dry_season\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 5))\n    .filter(ee.Filter.eq('SPACECRAFT_NAME', 'Sentinel-2B'))\n)\nprint(f\"Best images (S-2B, &lt;5% clouds): {best_images.size().getInfo()}\")\n\n\n\nChain Filters for Precision\nCombine multiple filters:\n# Define AOI for Bohol\nbohol_aoi = ee.Geometry.Rectangle([123.8, 9.6, 124.6, 10.2])\n\n# Multi-filter pipeline\nbohol_images = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(bohol_aoi)\n    .filterDate('2024-03-01', '2024-05-31')  # Dry season\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n    .select(['B2', 'B3', 'B4', 'B8', 'B11', 'B12', 'QA60']))  # Relevant bands\n\nprint(\"=\" * 50)\nprint(f\"Bohol Image Collection (Dry Season 2024)\")\nprint(\"=\" * 50)\nprint(f\"  Total images: {bohol_images.size().getInfo()}\")\nprint(f\"  Date range: {bohol_images.aggregate_min('system:time_start').getInfo()} to {bohol_images.aggregate_max('system:time_start').getInfo()}\")\nprint(\"=\" * 50)",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#part-5-working-with-sentinel-data-in-gee",
    "href": "day1/sessions/session4.html#part-5-working-with-sentinel-data-in-gee",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 5: Working with Sentinel Data in GEE",
    "text": "Part 5: Working with Sentinel Data in GEE\n\nSentinel-2 Surface Reflectance\nCollection ID: COPERNICUS/S2_SR\nKey information:\n\nLevel: Level-2A (atmospherically corrected surface reflectance)\nBands: 13 spectral bands (B1-B12, plus QA60)\nResolution: 10m (B2-B4, B8), 20m (B5-B7, B8A, B11-B12), 60m (B1, B9, B10)\nRevisit: 5 days (constellation)\nUpdates: Near real-time (within days of acquisition)\n\nBand naming in GEE:\n\n\n\nBand\nName\nWavelength\nResolution\nUse\n\n\n\n\nB2\nBlue\n490 nm\n10m\nAtmospheric, water\n\n\nB3\nGreen\n560 nm\n10m\nVegetation, water\n\n\nB4\nRed\n665 nm\n10m\nVegetation discrimination\n\n\nB8\nNIR\n842 nm\n10m\nBiomass, vegetation\n\n\nB11\nSWIR1\n1610 nm\n20m\nMoisture, soil/vegetation\n\n\nB12\nSWIR2\n2190 nm\n20m\nMoisture, geology\n\n\nQA60\nQuality\n-\n60m\nCloud mask\n\n\n\nLoad and visualize:\n# Load collection\ns2 = ee.ImageCollection('COPERNICUS/S2_SR')\n\n# Filter to area and time\nimage = (s2\n    .filterBounds(ee.Geometry.Point([124.0, 10.0]))  # Bohol\n    .filterDate('2024-03-01', '2024-03-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10))\n    .first())\n\n# Visualize with geemap (interactive mapping)\nimport geemap\n\nMap = geemap.Map(center=[10.0, 124.0], zoom=9)\nvis_params = {\n    'bands': ['B4', 'B3', 'B2'],\n    'min': 0,\n    'max': 3000,\n    'gamma': 1.4\n}\nMap.addLayer(image, vis_params, 'Sentinel-2 True Color')\nMap\n\n\nSentinel-1 SAR\nCollection ID: COPERNICUS/S1_GRD\nKey information:\n\nLevel: Level-1 Ground Range Detected (GRD)\nPolarization: VV, VH (or HH, HV depending on mode)\nResolution: 10m (IW mode)\nRevisit: 6-12 days\nAdvantages: All-weather, day-night imaging\n\nLoad Sentinel-1:\n# Load Sentinel-1 collection\ns1 = ee.ImageCollection('COPERNICUS/S1_GRD')\n\n# Filter for ascending pass, IW mode, VV+VH polarization\ns1_filtered = (s1\n    .filterBounds(bohol_aoi)\n    .filterDate('2024-06-01', '2024-08-31')  # Wet season\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n    .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING'))\n    .select(['VV', 'VH']))\n\nprint(f\"Sentinel-1 images: {s1_filtered.size().getInfo()}\")\n\n# Visualize\ns1_image = s1_filtered.median()  # Median composite\nMap = geemap.Map(center=[10.0, 124.0], zoom=9)\nMap.addLayer(s1_image, {'bands': ['VV'], 'min': -25, 'max': 0}, 'S1 VV')\nMap.addLayer(s1_image, {'bands': ['VH'], 'min': -30, 'max': -5}, 'S1 VH')\nMap",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#part-6-cloud-masking-and-preprocessing",
    "href": "day1/sessions/session4.html#part-6-cloud-masking-and-preprocessing",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 6: Cloud Masking and Preprocessing",
    "text": "Part 6: Cloud Masking and Preprocessing\n\nWhy Cloud Masking?\nProblems with clouds:\n\nObscure ground features\nContaminate spectral indices\nReduce classification accuracy\nCreate artifacts in composites\n\nSolution: Use quality assessment (QA) bands to identify and mask clouds.\n\n\nSentinel-2 Cloud Masking\nSentinel-2 includes QA60 band:\n\nBit 10: Opaque clouds\nBit 11: Cirrus clouds\n\nCloud masking function:\ndef mask_s2_clouds(image):\n    \"\"\"Mask clouds using QA60 band.\"\"\"\n    qa = image.select('QA60')\n\n    # Bits 10 and 11 are clouds and cirrus\n    cloud_bit_mask = 1 &lt;&lt; 10\n    cirrus_bit_mask = 1 &lt;&lt; 11\n\n    # Both flags should be zero = clear\n    mask = (qa.bitwiseAnd(cloud_bit_mask).eq(0)\n            .And(qa.bitwiseAnd(cirrus_bit_mask).eq(0)))\n\n    # Return masked image, scaled to reflectance (0-1)\n    return image.updateMask(mask).divide(10000)\n\n# Apply to collection\ns2_masked = bohol_images.map(mask_s2_clouds)\n\nprint(\"Cloud masking applied! ✓\")\n\n\n\n\n\n\nNoteUnderstanding Bitwise Operations\n\n\n\nQA60 band stores flags as bits:\nQA60 = 1024 (binary: 10000000000)\n         Bit 10 is set → Cloud present\n\nBit mask:\ncloud_bit_mask = 1 &lt;&lt; 10 = 1024\nqa.bitwiseAnd(cloud_bit_mask) extracts bit 10\n.eq(0) checks if bit is 0 (no cloud)\nThis efficient encoding allows multiple flags in one band!\n\n\n\n\nAdvanced Cloud Masking with SCL\nScene Classification Layer (SCL) band provides detailed classification:\ndef mask_s2_clouds_scl(image):\n    \"\"\"Advanced cloud masking using SCL band.\"\"\"\n    scl = image.select('SCL')\n\n    # SCL values:\n    # 3 = cloud shadows\n    # 8 = cloud medium probability\n    # 9 = cloud high probability\n    # 10 = thin cirrus\n    # 11 = snow/ice\n\n    # Keep only vegetation (4), bare soil (5), water (6)\n    mask = scl.eq(4).Or(scl.eq(5)).Or(scl.eq(6))\n\n    return image.updateMask(mask).divide(10000)\n\n# Note: Need to load SCL band\ns2_with_scl = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(bohol_aoi)\n    .filterDate('2024-03-01', '2024-05-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n    .select(['B2', 'B3', 'B4', 'B8', 'SCL']))\n\ns2_masked_scl = s2_with_scl.map(mask_s2_clouds_scl)",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#part-7-creating-temporal-composites",
    "href": "day1/sessions/session4.html#part-7-creating-temporal-composites",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 7: Creating Temporal Composites",
    "text": "Part 7: Creating Temporal Composites\n\nWhy Composites?\nChallenges in tropical regions (like Philippines):\n\nFrequent cloud cover (&gt;60% annual average)\nDifficult to find single cloud-free scene\nMonsoon seasons worsen problem\n\nSolution: Temporal compositing\nCombine multiple images over time to create cloud-free mosaic.\n\n\nComposite Methods\n\n1. Median Composite\nMost common - robust to outliers:\n# Create median composite (dry season 2024)\ncomposite_median = s2_masked.median()\n\nprint(\"Median composite created! ✓\")\n\n# Visualize\nMap = geemap.Map(center=[10.0, 124.0], zoom=9)\nvis_params = {\n    'bands': ['B4', 'B3', 'B2'],\n    'min': 0,\n    'max': 0.3,\n    'gamma': 1.4\n}\nMap.addLayer(composite_median, vis_params, 'Median Composite (Dry Season)')\nMap\nWhy median?\n\nMiddle value of sorted pixel values over time\nRemoves clouds (typically brightest values)\nRemoves shadows (typically darkest values)\nPreserves realistic surface reflectance\n\n\n\n2. Mean Composite\nAverage of all pixels:\ncomposite_mean = s2_masked.mean()\n\n# Smoother than median, but sensitive to remaining clouds\n\n\n3. Greenest Pixel Composite\nSelect pixel with highest NDVI (most vegetated):\ndef add_ndvi(image):\n    \"\"\"Add NDVI band to image.\"\"\"\n    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n    return image.addBands(ndvi)\n\n# Add NDVI to all images\ns2_with_ndvi = s2_masked.map(add_ndvi)\n\n# Get maximum NDVI composite (greenest pixel)\ncomposite_max_ndvi = s2_with_ndvi.qualityMosaic('NDVI')\n\nMap.addLayer(composite_max_ndvi, vis_params, 'Greenest Pixel Composite')\n\n\n\nMulti-temporal Analysis\nCompare dry vs. wet season:\n# Dry season composite (Jan-May)\ndry_season = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(bohol_aoi)\n    .filterDate('2024-01-01', '2024-05-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .map(mask_s2_clouds)\n    .median())\n\n# Wet season composite (Jun-Nov)\nwet_season = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(bohol_aoi)\n    .filterDate('2024-06-01', '2024-11-30')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .map(mask_s2_clouds)\n    .median())\n\n# Calculate NDVI for both\ndry_ndvi = dry_season.normalizedDifference(['B8', 'B4'])\nwet_ndvi = wet_season.normalizedDifference(['B8', 'B4'])\n\n# NDVI difference (wet - dry)\nndvi_change = wet_ndvi.subtract(dry_ndvi)\n\n# Visualize\nMap = geemap.Map(center=[10.0, 124.0], zoom=9)\nMap.addLayer(dry_season, vis_params, 'Dry Season')\nMap.addLayer(wet_season, vis_params, 'Wet Season')\nMap.addLayer(ndvi_change, {'min': -0.3, 'max': 0.3, 'palette': ['red', 'white', 'green']},\n             'NDVI Change (Wet-Dry)')\nMap\n\nInterpretation for Philippines:\n\nGreen areas (positive change): Rice paddies planted during wet season, increased vegetation vigor\nRed areas (negative change): Areas with less vegetation in wet season (possibly fallow, harvested, or flooded)\nWhite areas (no change): Stable land cover (evergreen forest, urban)",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#part-8-calculating-spectral-indices-at-scale",
    "href": "day1/sessions/session4.html#part-8-calculating-spectral-indices-at-scale",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 8: Calculating Spectral Indices at Scale",
    "text": "Part 8: Calculating Spectral Indices at Scale\n\nNDVI (Vegetation Health)\ndef calculate_ndvi(image):\n    \"\"\"Calculate NDVI for an image.\"\"\"\n    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n    return image.addBands(ndvi)\n\n# Apply to collection\ns2_with_ndvi = s2_masked.map(calculate_ndvi)\n\n# Get NDVI composite\nndvi_composite = s2_with_ndvi.select('NDVI').median()\n\n# Visualize\nndvi_params = {\n    'min': 0,\n    'max': 1,\n    'palette': ['red', 'yellow', 'green']\n}\nMap = geemap.Map(center=[10.0, 124.0], zoom=9)\nMap.addLayer(ndvi_composite, ndvi_params, 'NDVI Composite')\nMap\n\n\nNDWI (Water Bodies)\ndef calculate_ndwi(image):\n    \"\"\"Calculate NDWI for water detection.\"\"\"\n    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\n    return image.addBands(ndwi)\n\ns2_with_ndwi = s2_masked.map(calculate_ndwi)\nndwi_composite = s2_with_ndwi.select('NDWI').median()\n\n# Extract water bodies (NDWI &gt; 0.3)\nwater_mask = ndwi_composite.gt(0.3)\n\nMap.addLayer(water_mask.selfMask(), {'palette': 'blue'}, 'Water Bodies')\n\n\nMultiple Indices\ndef add_indices(image):\n    \"\"\"Add multiple spectral indices.\"\"\"\n    # NDVI\n    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n\n    # NDWI\n    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\n\n    # NDBI (Built-up)\n    ndbi = image.normalizedDifference(['B11', 'B8']).rename('NDBI')\n\n    # EVI (Enhanced Vegetation Index)\n    evi = image.expression(\n        '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))',\n        {\n            'NIR': image.select('B8'),\n            'RED': image.select('B4'),\n            'BLUE': image.select('B2')\n        }\n    ).rename('EVI')\n\n    return image.addBands([ndvi, ndwi, ndbi, evi])\n\n# Apply to collection\ns2_with_indices = s2_masked.map(add_indices)\n\n# Create composite with all indices\nmulti_index_composite = s2_with_indices.median()\n\nprint(\"Bands in composite:\", multi_index_composite.bandNames().getInfo())",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#part-9-exporting-data-from-earth-engine",
    "href": "day1/sessions/session4.html#part-9-exporting-data-from-earth-engine",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 9: Exporting Data from Earth Engine",
    "text": "Part 9: Exporting Data from Earth Engine\n\nExport to Google Drive\nWhy export?\n\nUse data outside Earth Engine\nTrain ML models locally or in Colab\nShare processed results\nCreate high-resolution figures\n\n\n\n\n\n\nflowchart LR\n    A[Processed Data&lt;br/&gt;in GEE] --&gt; B{Export&lt;br/&gt;Type?}\n\n    B --&gt;|Image| C1[ee.batch.Export&lt;br/&gt;.image.toDrive]\n    B --&gt;|Table| C2[ee.batch.Export&lt;br/&gt;.table.toDrive]\n    B --&gt;|Video| C3[ee.batch.Export&lt;br/&gt;.video.toDrive]\n\n    C1 --&gt; D1[Configure&lt;br/&gt;Image Export]\n    C2 --&gt; D2[Configure&lt;br/&gt;Table Export]\n\n    D1 --&gt; E1[Parameters:&lt;br/&gt;region, scale&lt;br/&gt;crs, bands&lt;br/&gt;fileFormat]\n\n    D2 --&gt; E2[Parameters:&lt;br/&gt;columns&lt;br/&gt;fileFormat&lt;br/&gt;CSV/SHP/GeoJSON]\n\n    E1 --&gt; F[task.start&lt;br/&gt;Submit to Queue]\n    E2 --&gt; F\n\n    F --&gt; G[GEE Processing&lt;br/&gt;Server-side&lt;br/&gt;computation]\n\n    G --&gt; H{Status?}\n\n    H --&gt;|READY| I[In Queue&lt;br/&gt;Waiting]\n    H --&gt;|RUNNING| J[Processing...&lt;br/&gt;Monitor progress]\n    H --&gt;|COMPLETED| K[Success!&lt;br/&gt;Check Drive]\n    H --&gt;|FAILED| L[Error&lt;br/&gt;Check logs]\n\n    K --&gt; M[Google Drive&lt;br/&gt;CoPhil_Training/&lt;br/&gt;filename.tif]\n\n    M --&gt; N1[Download&lt;br/&gt;Local use]\n    M --&gt; N2[Colab Access&lt;br/&gt;mount Drive]\n    M --&gt; N3[Share&lt;br/&gt;Collaborators]\n\n    N1 --&gt; O[ML Training&lt;br/&gt;PyTorch/TensorFlow&lt;br/&gt;Scikit-learn]\n    N2 --&gt; O\n\n    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px\n    style F fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style G fill:#ffe6ff,stroke:#cc00cc,stroke-width:2px\n    style K fill:#e6ffe6,stroke:#00aa44,stroke-width:2px\n    style L fill:#ffe6e6,stroke:#cc0044,stroke-width:2px\n    style M fill:#ccffcc,stroke:#00aa44,stroke-width:2px\n    style O fill:#cce6ff,stroke:#0066cc,stroke-width:2px\n\n\n Earth Engine Export Workflow \n\n\n\nExport image:\n# Define export region (use AOI)\nexport_region = bohol_aoi\n\n# Export composite to Drive\nexport_task = ee.batch.Export.image.toDrive(\n    image=composite_median.select(['B2', 'B3', 'B4', 'B8']),\n    description='Bohol_S2_Median_Dry2024',\n    folder='CoPhil_Training',\n    fileNamePrefix='bohol_s2_composite',\n    region=export_region,\n    scale=10,  # Resolution in meters\n    crs='EPSG:32651',  # UTM Zone 51N\n    maxPixels=1e9,\n    fileFormat='GeoTIFF'\n)\n\n# Start export task\nexport_task.start()\n\nprint(\"Export task started! ✓\")\nprint(\"Check status at: https://code.earthengine.google.com/tasks\")\nMonitor export:\nimport time\n\n# Check task status\ntask_id = export_task.id\nprint(f\"Task ID: {task_id}\")\n\n# Poll until complete (check every 30 seconds)\nwhile export_task.active():\n    print(f\"  Status: {export_task.status()['state']} ...\")\n    time.sleep(30)\n\nprint(f\"✓ Export complete! Status: {export_task.status()['state']}\")\n\n\nExport Options\n1. Export to Google Drive (easiest):\nee.batch.Export.image.toDrive()\n2. Export to Cloud Storage:\nee.batch.Export.image.toCloudStorage(\n    image=image,\n    bucket='your-gcs-bucket',\n    fileNamePrefix='path/to/file',\n    ...\n)\n3. Export to Asset (for reuse in GEE):\nee.batch.Export.image.toAsset(\n    image=image,\n    description='MyAsset',\n    assetId='users/your-username/your-asset-name',\n    ...\n)\n\n\nExport FeatureCollection (Vector)\nExport classification results or statistics:\n# Create sample points with NDVI values\nsample_points = ndvi_composite.sample(\n    region=bohol_aoi,\n    scale=100,\n    numPixels=1000,\n    geometries=True\n)\n\n# Export to Drive\nexport_vector = ee.batch.Export.table.toDrive(\n    collection=sample_points,\n    description='Bohol_NDVI_Samples',\n    folder='CoPhil_Training',\n    fileFormat='CSV'\n)\n\nexport_vector.start()\nprint(\"Vector export started! ✓\")",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#part-10-best-practices-and-limitations",
    "href": "day1/sessions/session4.html#part-10-best-practices-and-limitations",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 10: Best Practices and Limitations",
    "text": "Part 10: Best Practices and Limitations\n\nBest Practices\n\n\n\n\n\n\nTipGEE Workflow Tips\n\n\n\n\nFilter aggressively: Reduce collection size before processing\nUse Cloud masking: Always mask clouds for optical data\nScale matters: Choose appropriate scale for export (don’t over-sample)\nTest on small areas: Prototype with small AOI before scaling up\nMonitor quotas: Be aware of computation and storage limits\nReproducibility: Save scripts, document parameters\nLeverage built-in functions: Don’t reinvent the wheel\n\n\n\n\n\nComputational Limits\nEarth Engine has quotas (free tier):\n\nSimultaneous requests: Limited concurrent computations\nExport size: Max 100,000 pixels per dimension\nAsset storage: Limited space for uploaded/exported assets\nProcessing time: Long-running tasks may timeout\n\nSolutions:\n\nBreak large exports into tiles\nUse reduce() operations instead of getInfo() for large data\nExport to Asset for intermediate results\nConsider upgrading to commercial tier for production workflows\n\n\n\nLimitations for AI/ML\nWhat GEE does well:\n\nData access and pre-processing\nLarge-scale feature extraction\nRandom Forest / CART classification\nPixel-based analysis\n\nWhat GEE struggles with:\n\nTraining deep learning models (CNNs, U-Net, LSTMs)\nCustom loss functions and optimizers\nGPU-accelerated training\nComplex model architectures requiring TensorFlow/PyTorch\n\n\n\n\n\n\n\nNoteRecommended Workflow for Deep Learning\n\n\n\n\nUse GEE for: Data discovery, filtering, cloud masking, compositing, exporting training patches\nUse Python (Colab/local) for: Training CNNs/U-Nets with TensorFlow or PyTorch\nReturn to GEE for: Applying trained model at scale (if feasible) or export tiles for prediction",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#part-11-complete-example-philippine-land-cover-composite",
    "href": "day1/sessions/session4.html#part-11-complete-example-philippine-land-cover-composite",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 11: Complete Example: Philippine Land Cover Composite",
    "text": "Part 11: Complete Example: Philippine Land Cover Composite\nScenario: Create cloud-free RGB and NDVI composites for entire Palawan province.\n# 1. Define AOI (Palawan province)\npalawan = ee.Geometry.Rectangle([117.5, 8.0, 119.5, 12.0])\n\n# 2. Load and filter Sentinel-2\ns2_palawan = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(palawan)\n    .filterDate('2024-01-01', '2024-05-31')  # Dry season\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .select(['B2', 'B3', 'B4', 'B8', 'QA60']))\n\nprint(f\"Images in collection: {s2_palawan.size().getInfo()}\")\n\n# 3. Apply cloud masking\ndef mask_clouds(image):\n    qa = image.select('QA60')\n    cloud_mask = qa.bitwiseAnd(1 &lt;&lt; 10).eq(0).And(qa.bitwiseAnd(1 &lt;&lt; 11).eq(0))\n    return image.updateMask(cloud_mask).divide(10000)\n\ns2_masked = s2_palawan.map(mask_clouds)\n\n# 4. Create median composite\ncomposite = s2_masked.median()\n\n# 5. Calculate NDVI\nndvi = composite.normalizedDifference(['B8', 'B4']).rename('NDVI')\ncomposite_with_ndvi = composite.addBands(ndvi)\n\n# 6. Visualize\nMap = geemap.Map(center=[10.0, 118.5], zoom=8)\n\nrgb_vis = {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3, 'gamma': 1.4}\nndvi_vis = {'bands': ['NDVI'], 'min': 0, 'max': 1, 'palette': ['brown', 'yellow', 'green', 'darkgreen']}\n\nMap.addLayer(composite, rgb_vis, 'Palawan True Color')\nMap.addLayer(composite_with_ndvi, ndvi_vis, 'Palawan NDVI')\nMap\n\n# 7. Export to Drive\nexport_rgb = ee.batch.Export.image.toDrive(\n    image=composite.select(['B4', 'B3', 'B2']),\n    description='Palawan_RGB_DryS2024',\n    folder='CoPhil_Training',\n    region=palawan,\n    scale=10,\n    maxPixels=1e10,\n    fileFormat='GeoTIFF'\n)\n\nexport_ndvi = ee.batch.Export.image.toDrive(\n    image=ndvi,\n    description='Palawan_NDVI_DryS2024',\n    folder='CoPhil_Training',\n    region=palawan,\n    scale=10,\n    maxPixels=1e10,\n    fileFormat='GeoTIFF'\n)\n\nexport_rgb.start()\nexport_ndvi.start()\n\nprint(\"=\" * 60)\nprint(\"PALAWAN LAND COVER COMPOSITE - EXPORT STARTED\")\nprint(\"=\" * 60)\nprint(\"RGB Composite: Check Google Drive in ~10-30 minutes\")\nprint(\"NDVI Layer: Check Google Drive in ~10-30 minutes\")\nprint(\"Monitor at: https://code.earthengine.google.com/tasks\")\nprint(\"=\" * 60)",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#part-12-philippine-case-studies-and-applications",
    "href": "day1/sessions/session4.html#part-12-philippine-case-studies-and-applications",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Part 12: Philippine Case Studies and Applications",
    "text": "Part 12: Philippine Case Studies and Applications\n\nCase Study 1: Typhoon Impact Assessment\nScenario: Assess vegetation damage from Typhoon Odette (Rai) in December 2021 over Bohol and Cebu.\n# Define affected region\nvisayas_aoi = ee.Geometry.Rectangle([123.5, 9.5, 125.0, 11.0])\n\n# Pre-typhoon composite (November 2021)\npre_typhoon = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(visayas_aoi)\n    .filterDate('2021-11-01', '2021-11-30')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .map(mask_s2_clouds)\n    .median())\n\n# Post-typhoon composite (January 2022 - after clouds cleared)\npost_typhoon = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(visayas_aoi)\n    .filterDate('2022-01-15', '2022-02-15')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n    .map(mask_s2_clouds)\n    .median())\n\n# Calculate NDVI for both periods\npre_ndvi = pre_typhoon.normalizedDifference(['B8', 'B4']).rename('pre_NDVI')\npost_ndvi = post_typhoon.normalizedDifference(['B8', 'B4']).rename('post_NDVI')\n\n# Calculate NDVI difference (damage indicator)\nndvi_change = post_ndvi.subtract(pre_ndvi).rename('NDVI_change')\n\n# Identify severely damaged areas (NDVI drop &gt; 0.3)\nsevere_damage = ndvi_change.lt(-0.3).selfMask()\n\n# Calculate affected area\npixel_area = severe_damage.multiply(ee.Image.pixelArea())\naffected_area_m2 = pixel_area.reduceRegion(\n    reducer=ee.Reducer.sum(),\n    geometry=visayas_aoi,\n    scale=10,\n    maxPixels=1e10\n).getInfo()\n\naffected_hectares = affected_area_m2['NDVI_change'] / 10000\n\nprint(f\"Severely damaged vegetation: {affected_hectares:.0f} hectares\")\n\n# Visualize\nMap = geemap.Map(center=[10.2, 124.0], zoom=9)\nMap.addLayer(pre_typhoon, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3}, 'Pre-Typhoon')\nMap.addLayer(post_typhoon, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3}, 'Post-Typhoon')\nMap.addLayer(ndvi_change, {'min': -0.5, 'max': 0.2, 'palette': ['red', 'yellow', 'green']}, 'NDVI Change')\nMap.addLayer(severe_damage, {'palette': 'darkred'}, 'Severe Damage')\nMap\nApplications: - Rapid disaster assessment for relief planning - Insurance claims verification - Forest damage quantification - Agricultural loss estimation\n\n\nCase Study 2: Manila Bay Water Quality Monitoring\nScenario: Monitor water turbidity and suspended sediment in Manila Bay using Sentinel-2.\n# Define Manila Bay AOI\nmanila_bay = ee.Geometry.Polygon([\n    [[120.7, 14.4], [120.95, 14.4], [121.0, 14.65], [120.75, 14.75], [120.7, 14.4]]\n])\n\n# Load Sentinel-2 for dry season 2024\ns2_manila = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(manila_bay)\n    .filterDate('2024-02-01', '2024-04-30')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n    .map(mask_s2_clouds)\n    .median())\n\n# Calculate water indices\n# Turbidity proxy using Red band\nturbidity = s2_manila.select('B4').rename('turbidity')\n\n# Normalized Difference Turbidity Index (NDTI)\nndti = s2_manila.normalizedDifference(['B4', 'B3']).rename('NDTI')\n\n# Total Suspended Matter (TSM) estimation (simplified model)\n# TSM (mg/L) ≈ 3000 * Red_reflectance - 100\ntsm = s2_manila.select('B4').multiply(3000).subtract(100).rename('TSM_mgL')\n\n# Mask out land (keep only water bodies)\nndwi = s2_manila.normalizedDifference(['B3', 'B8'])\nwater_mask = ndwi.gt(0.0)\ntsm_water = tsm.updateMask(water_mask)\n\n# Visualize\nMap = geemap.Map(center=[14.55, 120.85], zoom=10)\nMap.addLayer(s2_manila, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.15}, 'True Color')\nMap.addLayer(tsm_water, {'min': 0, 'max': 200, 'palette': ['blue', 'cyan', 'yellow', 'red']},\n             'TSM (mg/L)')\nMap.addLayer(ndti, {'min': -0.2, 'max': 0.4, 'palette': ['blue', 'white', 'brown']}, 'Turbidity Index')\nMap\nApplications: - Coastal water quality monitoring - Rehabilitation program assessment - Pollution source identification - Seasonal variation analysis\n\n\nCase Study 3: Rice Paddy Phenology with Sentinel-1\nScenario: Track rice planting and harvesting cycles in Central Luzon using SAR.\n# Define Central Luzon rice area\ncentral_luzon = ee.Geometry.Rectangle([120.5, 15.0, 121.0, 15.5])\n\n# Load Sentinel-1 time series (wet season 2024)\ns1_series = (ee.ImageCollection('COPERNICUS/S1_GRD')\n    .filterBounds(central_luzon)\n    .filterDate('2024-06-01', '2024-11-30')\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n    .filter(ee.Filter.eq('instrumentMode', 'IW'))\n    .select('VH'))\n\n# Function to add date as band\ndef add_date_band(image):\n    date = ee.Date(image.get('system:time_start'))\n    day_of_year = date.getRelative('day', 'year')\n    return image.addBands(ee.Image.constant(day_of_year).rename('day_of_year'))\n\ns1_with_dates = s1_series.map(add_date_band)\n\n# Create monthly composites\nmonths = ee.List.sequence(6, 11)\n\ndef monthly_composite(month):\n    start = ee.Date.fromYMD(2024, month, 1)\n    end = start.advance(1, 'month')\n    return s1_series.filterDate(start, end).median().set('month', month)\n\nmonthly_composites = ee.ImageCollection.fromImages(months.map(monthly_composite))\n\n# Extract time series for a sample point\nrice_point = ee.Geometry.Point([120.75, 15.25])\n\ndef sample_vh(image):\n    value = image.reduceRegion(\n        reducer=ee.Reducer.mean(),\n        geometry=rice_point,\n        scale=10\n    )\n    return ee.Feature(None, {\n        'VH': value.get('VH'),\n        'month': image.get('month')\n    })\n\ntime_series = monthly_composites.map(sample_vh)\ndata = time_series.getInfo()\n\n# Print time series\nprint(\"VH Backscatter Time Series (dB):\")\nfor feat in data['features']:\n    month = feat['properties']['month']\n    vh = feat['properties'].get('VH', 'N/A')\n    if vh != 'N/A':\n        print(f\"  Month {month}: {vh:.2f} dB\")\n\n# Visualize VH for different months\nMap = geemap.Map(center=[15.25, 120.75], zoom=11)\nfor i, month in enumerate([6, 8, 10]):\n    composite = monthly_composites.filter(ee.Filter.eq('month', month)).first()\n    Map.addLayer(composite, {'min': -25, 'max': -5, 'palette': ['blue', 'white', 'green']},\n                 f'VH - Month {month}')\nMap.addLayer(rice_point, {'color': 'red'}, 'Sample Point')\nMap\nInterpretation: - High VH (&gt; -15 dB): Flooding/transplanting phase (water surface with sparse vegetation) - Decreasing VH (-15 to -20 dB): Vegetative growth (increasing biomass scatters radar) - Low VH (&lt; -20 dB): Peak growth/maturity - Increasing VH: Senescence and harvest\nApplications: - Crop calendar monitoring - Planting date estimation - Yield forecasting - Irrigation management\n\n\nCase Study 4: Mangrove Forest Monitoring in Palawan\nScenario: Map and monitor mangrove extent and health in Puerto Princesa.\n# Define Palawan coastal AOI\npalawan_coast = ee.Geometry.Rectangle([118.6, 9.6, 118.9, 10.0])\n\n# Load Sentinel-2 (dry season for best visibility)\ns2_palawan = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(palawan_coast)\n    .filterDate('2024-03-01', '2024-05-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n    .map(mask_s2_clouds)\n    .median())\n\n# Calculate indices for mangrove detection\nndvi = s2_palawan.normalizedDifference(['B8', 'B4']).rename('NDVI')\nndwi = s2_palawan.normalizedDifference(['B3', 'B8']).rename('NDWI')\n\n# Mangrove Vegetation Index (MVI) - uses SWIR\n# Mangroves have high NIR but moderate SWIR compared to other vegetation\nmvi = s2_palawan.normalizedDifference(['B8', 'B11']).rename('MVI')\n\n# Combined index for mangrove detection\n# Criteria: High NDVI (dense vegetation), slightly positive NDWI (coastal), high MVI\nmangrove_mask = (\n    ndvi.gt(0.5)\n    .And(ndwi.gt(-0.1))\n    .And(ndwi.lt(0.3))\n    .And(mvi.gt(0.4))\n)\n\n# Calculate mangrove area\nmangrove_area = mangrove_mask.multiply(ee.Image.pixelArea()).reduceRegion(\n    reducer=ee.Reducer.sum(),\n    geometry=palawan_coast,\n    scale=10,\n    maxPixels=1e9\n).getInfo()\n\nmangrove_hectares = mangrove_area['NDVI'] / 10000\n\nprint(f\"Estimated mangrove area: {mangrove_hectares:.1f} hectares\")\n\n# Visualize\nMap = geemap.Map(center=[9.8, 118.75], zoom=12)\nMap.addLayer(s2_palawan, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3}, 'True Color')\nMap.addLayer(ndvi, {'min': 0, 'max': 1, 'palette': ['white', 'yellow', 'green']}, 'NDVI')\nMap.addLayer(mvi, {'min': 0, 'max': 1, 'palette': ['white', 'lightgreen', 'darkgreen']}, 'MVI')\nMap.addLayer(mangrove_mask.selfMask(), {'palette': 'darkgreen'}, 'Mangrove Detection')\nMap\nMulti-temporal Change Detection:\n# Compare 2019 vs 2024 to detect changes\nmangrove_2019 = (ee.ImageCollection('COPERNICUS/S2_SR')\n    .filterBounds(palawan_coast)\n    .filterDate('2019-03-01', '2019-05-31')\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n    .map(mask_s2_clouds)\n    .median())\n\nndvi_2019 = mangrove_2019.normalizedDifference(['B8', 'B4'])\nmvi_2019 = mangrove_2019.normalizedDifference(['B8', 'B11'])\nmangrove_2019_mask = ndvi_2019.gt(0.5).And(mvi_2019.gt(0.4))\n\n# Change detection\nmangrove_loss = mangrove_2019_mask.And(mangrove_mask.Not()).selfMask()\nmangrove_gain = mangrove_2019_mask.Not().And(mangrove_mask).selfMask()\n\nMap.addLayer(mangrove_loss, {'palette': 'red'}, 'Mangrove Loss (2019-2024)')\nMap.addLayer(mangrove_gain, {'palette': 'lime'}, 'Mangrove Gain (2019-2024)')\nMap\nApplications: - Coastal protection assessment - Carbon stock estimation - Restoration monitoring - Policy compliance verification",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#key-takeaways",
    "href": "day1/sessions/session4.html#key-takeaways",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nImportantSession 4 Summary\n\n\n\nGoogle Earth Engine: - Cloud platform with petabytes of EO data - No downloading required - process in cloud - Free for research/education\nCore Concepts: - Image / ImageCollection for rasters - Feature / FeatureCollection for vectors - Geometry for locations and AOIs\nKey Operations: - Filter by bounds, date, metadata - Cloud masking using QA bands - Temporal composites (median, mean) - Spectral indices (NDVI, NDWI) - Export to Drive/Cloud Storage\nWorkflow: 1. Define AOI 2. Filter collection 3. Mask clouds 4. Create composite 5. Calculate indices 6. Visualize 7. Export\nBest for: Pre-processing, data access, Random Forest, large-area mapping\nUse Python/TensorFlow for: Deep learning model training\nNext steps: Apply these skills in Days 2-4 for land cover classification, flood mapping, and time series analysis!",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#practice-exercises",
    "href": "day1/sessions/session4.html#practice-exercises",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\n\n\n\n\n\nTipTry These Challenges\n\n\n\nExercise 1: Your Province Composite\nCreate a cloud-free Sentinel-2 composite for your home province using the complete workflow above.\nExercise 2: Multi-temporal NDVI Analysis\nCalculate NDVI composites for each month of 2024 over an agricultural area. Create a time series chart.\nExercise 3: Water Body Extraction\nUse NDWI to extract all water bodies in a coastal province. Export as vector (polygons).\nExercise 4: SAR Flood Detection\nCompare Sentinel-1 VV polarization before and after a typhoon to detect flooded areas.\nExercise 5: Export Training Data\nCreate stratified random samples of different land cover classes and export as CSV for ML training.\nBonus: Integrate with Session 3\nExport a GEE composite, then load it in Python (Session 3 techniques) to perform additional analysis with Rasterio.",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#further-reading",
    "href": "day1/sessions/session4.html#further-reading",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Further Reading",
    "text": "Further Reading\n\nOfficial Documentation\n\nEarth Engine Guides - Complete developer documentation\nPython API Introduction - Getting started with Python API\nPython Installation Guide - Setup instructions for various environments\nSentinel-2 in GEE - Sentinel-2 Surface Reflectance catalog\nSentinel-1 Algorithms - SAR processing workflows\nEarth Engine GitHub - Official repository with examples\n\n\n\nRecent Tutorials (2025)\n\nEnd-to-End GEE Course - Comprehensive training with Python modules\nGBIF GEE Python Primer (Jan 2025) - Latest Python API tutorial for environmental applications\nGEE Community Tutorials - User-contributed examples and workflows\nSentinel Data Downloader Tool - Automated dataset creation for AI applications\n\n\n\nTools and Libraries\n\ngeemap Library - Interactive mapping in Python with Jupyter integration\neemont - Extended functionality for GEE Python\nAwesome Earth Engine - Curated resources and examples\nGEE Community Forum - Help and discussions",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session4.html#jupyter-notebook",
    "href": "day1/sessions/session4.html#jupyter-notebook",
    "title": "Session 4: Introduction to Google Earth Engine",
    "section": "Jupyter Notebook",
    "text": "Jupyter Notebook\n\n\n\n\n\n\nNoteAccess the Interactive Notebook\n\n\n\nA complete Jupyter notebook with all Google Earth Engine examples from this session is available:\nOpen Notebook 2: Google Earth Engine →\nThis notebook includes: - All code examples ready to run - Philippine case studies - Additional exercises - Export workflows - Troubleshooting guide",
    "crumbs": [
      "Sessions",
      "Session 4: Introduction to Google Earth Engine"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html",
    "href": "day1/sessions/session2.html",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "",
    "text": "Home › Day 1 › Session 2",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html#session-overview",
    "href": "day1/sessions/session2.html#session-overview",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Session Overview",
    "text": "Session Overview\nThis session provides a comprehensive introduction to Artificial Intelligence and Machine Learning concepts specifically tailored for Earth Observation applications. You’ll learn the complete AI/ML workflow, understand different learning paradigms, explore deep learning architectures including CNNs, Vision Transformers, and temporal models, discover benchmark datasets, and understand why data quality matters more than model complexity in 2025’s data-centric AI paradigm.\n\nThe session integrates cutting-edge methodologies with concrete Philippine case studies, from DOST-ASTI’s 10-20 minute flood detection using AI to PhilSA’s mangrove mapping efforts, demonstrating operational AI/ML systems already serving disaster risk reduction and natural resource management needs.\n\nLearning Objectives\nBy the end of this session, you will be able to:\n\nDefine AI and ML in the context of Earth Observation\nDescribe the complete AI/ML workflow from problem definition to deployment\nDistinguish between supervised and unsupervised learning with EO examples\nExplain classification vs. regression tasks in satellite data analysis\nIdentify major deep learning architectures: CNNs, U-Net, Vision Transformers, RNNs/LSTMs, object detection networks\nUnderstand key components: neurons, layers, activation functions, loss functions, optimizers\nCompare different model architectures and when to apply each\nRecognize benchmark datasets used for training and evaluation\nArticulate the data-centric AI paradigm and its importance for EO\nApply best practices for data quality, quantity, diversity, and annotation\nExplain Explainable AI (XAI) and why it matters for operational systems",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html#presentation-slides",
    "href": "day1/sessions/session2.html#presentation-slides",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Presentation Slides",
    "text": "Presentation Slides",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html#part-1-what-is-aiml",
    "href": "day1/sessions/session2.html#part-1-what-is-aiml",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Part 1: What is AI/ML?",
    "text": "Part 1: What is AI/ML?\n\nDefining the Terms\nArtificial Intelligence (AI):\n\nBroad field focused on creating intelligent machines\nSystems that can perceive, reason, learn, and act\nIncludes everything from rule-based systems to machine learning\nIn EO: Enables automated interpretation of petabytes of satellite data\n\nMachine Learning (ML):\n\nSubset of AI focused on learning from data\nAlgorithms that improve performance through experience\nKey distinction: No explicit programming of rules\nDeep Learning: Subset of ML using multi-layered neural networks\n\n\n\n\n\n\ngraph TB\n    subgraph AI[\"ARTIFICIAL INTELLIGENCE&lt;br/&gt;Creating intelligent machines\"]\n        subgraph ML[\"MACHINE LEARNING&lt;br/&gt;Learning from data without explicit programming\"]\n            subgraph DL[\"DEEP LEARNING&lt;br/&gt;Multi-layered neural networks\"]\n                DL1[Convolutional&lt;br/&gt;Neural Networks&lt;br/&gt;CNNs]\n                DL2[Recurrent&lt;br/&gt;Neural Networks&lt;br/&gt;RNNs/LSTMs]\n                DL3[Transformers&lt;br/&gt;Vision Transformers]\n                DL4[GANs & VAEs&lt;br/&gt;Generative Models]\n            end\n            ML1[Random Forest]\n            ML2[Support Vector&lt;br/&gt;Machines]\n            ML3[Decision Trees]\n            ML4[K-Means&lt;br/&gt;Clustering]\n        end\n        AI1[Expert Systems&lt;br/&gt;Rule-based]\n        AI2[Fuzzy Logic]\n        AI3[Genetic&lt;br/&gt;Algorithms]\n    end\n\n    DL1 -.-&gt;|EO Apps| EO1[Land Cover&lt;br/&gt;Classification]\n    DL1 -.-&gt;|EO Apps| EO2[Semantic&lt;br/&gt;Segmentation]\n    DL2 -.-&gt;|EO Apps| EO3[Time Series&lt;br/&gt;Crop Monitoring]\n    DL3 -.-&gt;|EO Apps| EO4[Change&lt;br/&gt;Detection]\n\n    style AI fill:#e6f3ff,stroke:#0066cc,stroke-width:3px\n    style ML fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style DL fill:#e6ffe6,stroke:#00aa44,stroke-width:2px\n    style DL1 fill:#00cc66,stroke:#008844,stroke-width:1px,color:#fff\n    style DL2 fill:#00cc66,stroke:#008844,stroke-width:1px,color:#fff\n    style DL3 fill:#00cc66,stroke:#008844,stroke-width:1px,color:#fff\n\n\n AI, Machine Learning, and Deep Learning Relationship \n\n\n\n\n\n\n\n\n\nNoteThe ML Difference\n\n\n\nTraditional Programming:\nRules + Data → Output\nMachine Learning:\nData + Desired Output → Rules (Model)\nIn EO: Instead of coding “if NIR &gt; 0.6 and Red &lt; 0.3, then forest”, ML learns the pattern from labeled examples.\n\n\n\n\nWhy ML for Earth Observation?\nChallenges that ML addresses:\n\nScale: NASA’s Earth Science Data Systems exceeded 148 PB in 2023, projected 250 PB in 2025 - impossible to manually analyze\nComplexity: Multispectral, multi-temporal, spatial patterns humans can’t easily detect\nConsistency: Automated processing ensures reproducible results across time and space\nSpeed: Real-time disaster mapping requires immediate analysis (DOST-ASTI DATOS: 10-20 minute flood response)\nMulti-modal fusion: Integrating optical, SAR, LiDAR data for robust monitoring\n\nTraditional vs. ML approaches:\n\n\n\n\n\n\n\n\nTask\nTraditional\nML Approach\n\n\n\n\nWater detection\nManual NDWI threshold\nLearn optimal threshold + texture from examples\n\n\nLand cover\nRule-based classification\nRandom Forest or CNN with training samples\n\n\nFlood mapping\nExpert visual interpretation\nU-Net segmentation trained on labeled floods\n\n\nCrop monitoring\nFixed vegetation index thresholds\nLSTM time series model learning phenology\n\n\nBuilding detection\nManual digitization\nYOLO or Faster R-CNN object detection\n\n\nDeforestation\nVisual comparison of dates\nSiamese networks for change detection\n\n\n\n\nPhilippine Operational Systems:\nThe Philippines demonstrates successful AI/ML deployment:\n\nDATOS (DOST-ASTI): AI-powered flood mapping from Sentinel-1 SAR achieves 10-20 minute response time during typhoons\nPRiSM (PhilRice-IRRI): Operational since 2014, first satellite-based rice monitoring in Southeast Asia\nSkAI-Pinas (DOST): National AI framework addressing the gap between abundant remote sensing data and sustainable AI pipelines\nDIMER Model Repository (DOST-ASTI): Democratizing access to trained models for Philippine contexts",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html#part-2-the-aiml-workflow-for-earth-observation",
    "href": "day1/sessions/session2.html#part-2-the-aiml-workflow-for-earth-observation",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Part 2: The AI/ML Workflow for Earth Observation",
    "text": "Part 2: The AI/ML Workflow for Earth Observation\nUnderstanding the complete workflow is essential for successful EO projects. Each step matters, and according to 2024 research, most underperforming models suffer from data issues rather than algorithm deficiencies.\n\n\n\n\n\nflowchart TD\n    A[1. Problem Definition&lt;br/&gt;What question?&lt;br/&gt;What output?] --&gt; B[2. Data Acquisition&lt;br/&gt;Satellite imagery&lt;br/&gt;Ground truth&lt;br/&gt;Ancillary data]\n\n    B --&gt; C[3. Data Preprocessing&lt;br/&gt;Atmospheric correction&lt;br/&gt;Cloud masking&lt;br/&gt;Normalization]\n\n    C --&gt; D[4. Data Annotation&lt;br/&gt;Label training samples&lt;br/&gt;Quality control&lt;br/&gt;Class balancing]\n\n    D --&gt; E[5. Feature Engineering&lt;br/&gt;Spectral indices&lt;br/&gt;Texture features&lt;br/&gt;Temporal metrics]\n\n    E --&gt; F{6. Train/Val/Test&lt;br/&gt;Split}\n    F --&gt;|70%| G[Training Set]\n    F --&gt;|15%| H[Validation Set]\n    F --&gt;|15%| I[Test Set]\n\n    G --&gt; J[7. Model Training&lt;br/&gt;Select architecture&lt;br/&gt;Set hyperparameters&lt;br/&gt;Train on GPU]\n\n    H --&gt; K[8. Model Validation&lt;br/&gt;Tune hyperparameters&lt;br/&gt;Monitor overfitting&lt;br/&gt;Early stopping]\n\n    J --&gt; K\n    K --&gt;|Iterate| J\n\n    K --&gt; L{Performance&lt;br/&gt;Acceptable?}\n    L --&gt;|No| M[Improve Data&lt;br/&gt;More samples&lt;br/&gt;Better labels&lt;br/&gt;Data augmentation]\n    M --&gt; D\n\n    L --&gt;|Yes| N[9. Model Testing&lt;br/&gt;Final evaluation&lt;br/&gt;Unseen test set&lt;br/&gt;Confusion matrix]\n\n    I --&gt; N\n\n    N --&gt; O[10. Deployment&lt;br/&gt;Production system&lt;br/&gt;Monitoring&lt;br/&gt;Maintenance]\n\n    O --&gt; P{Model Drift?&lt;br/&gt;Performance&lt;br/&gt;degraded?}\n    P --&gt;|Yes| Q[Retrain with&lt;br/&gt;new data]\n    Q --&gt; D\n    P --&gt;|No| O\n\n    style A fill:#0066cc,stroke:#003d7a,stroke-width:2px,color:#fff\n    style B fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff\n    style C fill:#00aa44,stroke:#006622,stroke-width:2px,color:#fff\n    style D fill:#ff8800,stroke:#cc6600,stroke-width:2px,color:#fff\n    style J fill:#cc00cc,stroke:#880088,stroke-width:2px,color:#fff\n    style K fill:#cc00cc,stroke:#880088,stroke-width:2px,color:#fff\n    style O fill:#009999,stroke:#006666,stroke-width:2px,color:#fff\n\n\n Complete AI/ML Workflow for Earth Observation \n\n\n\n\nStep 1: Problem Definition\nDefine clearly what you want to achieve:\n\nWhat question are you answering? (e.g., “Where are mangroves declining?”)\nWhat output do you need? (map, time series, alert system?)\nWhat accuracy is acceptable?\nWhat constraints exist? (time, computational resources, data availability)\nWhat is the operational context and who will use the outputs?\n\n\nPhilippine Example: PRiSM Rice Monitoring\nProblem: Provide timely rice area and production estimates for food security planning and disaster response\nClear definition: - Multi-class: rice wet season, rice dry season, non-rice agriculture, non-agriculture - 10m spatial resolution (Sentinel-1 SAR + Sentinel-2 optical) - Temporal: Per-season mapping (wet: June-Nov, dry: Dec-May) - Accuracy target: &gt;90% for policy-level decisions - Operational: Automated processing, quarterly updates to DA/PCIC - Cloud-penetrating capability essential for monsoon season\n\n\n\nStep 2: Data Acquisition\nGather all necessary data:\n\nSatellite imagery: Sentinel-1/2, Landsat, commercial VHR, hyperspectral\nGround truth: Field surveys, high-res imagery interpretation, existing maps, crowdsourced data\nAncillary data: DEM, climate, administrative boundaries, road networks, socioeconomic data\n\nData volume considerations: - NASA’s Earth Science Data Systems: 148 PB (2023) → 205 PB (2024) → 250 PB (2025) - Sentinel constellation generates thousands of terabytes daily - 1,052 active EO satellites as of 2024\nData sources for Philippines:\n\nCoPhil Mirror Site (2025): Local, high-bandwidth access to Sentinel data covering entire archipelago\nCopernicus Data Space Ecosystem: STAC-compliant catalogues, API-driven access\nGoogle Earth Engine: Harmonized Sentinel-2 surface reflectance, Sentinel-1 GRD collections\nPhilSA SIYASAT: NovaSAR-1 X-band SAR data\nDiwata-1/2 microsatellites: Philippine-operated disaster monitoring\nNAMRIA Geoportal: Land cover basemaps, topographic data\nPAGASA: Climate and meteorological data\n\n\n\nStep 3: Data Pre-processing\nCritical step - “Garbage in, garbage out”\nData pre-processing is foundational and directly impacts downstream analysis accuracy. Proper preprocessing ensures data quality, consistency, and comparability across time and sensors.\nFor satellite imagery:\nAtmospheric Correction: - Purpose: Remove atmospheric effects (scattering, absorption) - Convert: Top-of-Atmosphere (TOA) reflectance → Surface reflectance (SR) - Sentinel-2: Use Level-2A products (Sen2Cor algorithm) - HLS Products: NASA’s Harmonized Landsat Sentinel-2 applies LaSRC + BRDF normalization - Essential for: Multi-temporal comparisons, quantitative biophysical parameter retrieval\nCloud Masking: - Sentinel-2 SCL: Scene Classification Layer (clouds, shadows, snow, water, vegetation) - Machine learning approaches: U-Net architectures for pixel-wise cloud segmentation - Multi-temporal approaches: Leverage temporal patterns to identify clouds - Gap filling: Temporal interpolation, spatial interpolation, deep learning reconstruction (Prithvi-EO-2.0)\nSAR-Specific Preprocessing (Sentinel-1): - Orbit file application: Precise geolocation - Radiometric calibration: Convert DN to sigma nought (σ⁰), beta nought (β⁰), or gamma nought (γ⁰) - De-bursting: Remove black boundaries between sub-swaths in TOPS mode - Speckle filtering: Lee, Frost, Gamma-MAP filters; CNN-based despecklers preserve edges - Terrain correction (RTC): Orthorectification using DEM (SRTM, Copernicus DEM) - Multi-temporal filtering: Leverage temporal stack to reduce speckle\nNormalization and Scaling: - Min-max normalization: Scale to [0, 1] range - Z-score standardization: Center to mean=0, std=1 (common for deep learning) - Percentile clipping: Reduce impact of outliers (e.g., 2nd and 98th percentiles) - Per-band normalization: Account for different dynamic ranges across spectral bands\nFor training labels:\n\nQuality control: Verify label accuracy through multiple reviewers\nCoordinate alignment: Ensure labels match imagery timing and location\nClass balancing: Ensure adequate samples per class\nFormat standardization: Convert to ML-ready format (GeoTIFF, TFRecord, COG)\n\n\n\n\n\n\n\nWarningPre-processing Pitfalls\n\n\n\nCommon errors that degrade model performance:\n\nUsing Top-of-Atmosphere instead of surface reflectance\nTemporal mismatch: 2020 imagery with 2018 labels\nIncomplete cloud masking leaving cloud shadows\nMixed pixels at boundaries (especially for validation)\nInconsistent band ordering across scenes\nIgnoring spatial autocorrelation (random train-test splits can lead to 28% overoptimistic performance)\nNot applying same preprocessing to training and deployment data\n\n\n\n\n\nStep 4: Feature Engineering\nDeriving informative variables from raw data\nFeature engineering transforms raw satellite data into informative representations that enhance model performance. This step is crucial for traditional ML algorithms and beneficial even for deep learning.\nFor traditional ML (Random Forest, SVM):\nSpectral Indices: - Vegetation: NDVI, EVI, SAVI, NDRE, GNDVI, LAI - Water: NDWI, MNDWI, NDMI - Built-up: NDBI, Bare Soil Index (BSI) - Burn: NBR, dNBR\nTextural Features (GLCM): - Contrast, Correlation, Energy, Homogeneity, Entropy, Dissimilarity, Variance - Window sizes: 3×3, 5×5, 7×7 - Multiple directions: 0°, 45°, 90°, 135°\nTemporal Features: - Statistical: Mean, median, std dev, min, max, percentiles (10th, 25th, 75th, 90th) - Coefficient of Variation (CV): Normalized variability measure - Amplitude: Difference between peak and minimum - Phenological: Start of season (SOS), Peak of season (POS), End of season (EOS) - Trends: Linear regression slopes, breakpoint detection\nMulti-Modal Features: - Optical-SAR fusion: Concatenate optical indices with SAR backscatter - Derived ratios: VV/VH polarization ratio, optical/SAR combinations - SAR texture: GLCM features from backscatter - Interferometric: Coherence from InSAR\nExample: Forest classification features\n# Spectral indices\nNDVI = (NIR - Red) / (NIR + Red)\nNDWI = (Green - NIR) / (Green + NIR)\nEVI = 2.5 * (NIR - Red) / (NIR + 6*Red - 7.5*Blue + 1)\n\n# SAR features\nVV_VH_ratio = VV_backscatter / VH_backscatter\nSAR_texture = GLCM_contrast(VH_backscatter)\n\n# Temporal\nNDVI_mean = mean(NDVI_time_series)\nNDVI_cv = std(NDVI_time_series) / NDVI_mean\n\n# Topographic\nElevation, Slope, Aspect\n\n# Result: Input feature vector per pixel\nX = [Red, Green, Blue, NIR, SWIR1, SWIR2, NDVI, EVI, NDWI,\n     VV, VH, VV_VH_ratio, SAR_texture, NDVI_mean, NDVI_cv,\n     Elevation, Slope, Aspect]\nFor deep learning (CNNs, U-Net, Vision Transformers):\n\nLess manual feature engineering needed\nNetworks automatically learn features from raw pixels\nStill benefit from good input data (cloud-free, calibrated, normalized)\nMulti-spectral bands as input channels\nConsider temporal stacking for multi-date analysis\n\n\n\n\n\n\n\nTip2024 Research Insight: Feature Selection\n\n\n\nSeven unsupervised dimensionality reduction algorithms tested on hyperspectral data from HYPSO-1 satellite showed that:\n\nCareful feature selection can achieve optimal accuracy with &lt;20% of temporal instances\nSingle band from single sensor can be sufficient for specific tasks\nImplication: Smart data selection &gt; brute force data collection\nUse PCA, MNF, or tree-based feature importance for efficient selection\n\n\n\n\n\nStep 5: Model Selection and Training\nChoose appropriate algorithm:\nConsider:\n\nTask type (classification, regression, segmentation, object detection)\nData size (deep learning needs more data; transfer learning reduces requirements)\nInterpretability requirements (operational systems often need explainability)\nComputational resources (edge devices vs. cloud platforms)\nDeployment constraints (inference speed, model size)\n\nCommon EO algorithms:\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nType\nBest For\nData Needs\nKey Strengths\n\n\n\n\nRandom Forest\nEnsemble\nClassification, feature importance, baseline\nMedium (100s-1000s)\nRobust, interpretable, handles high dimensions\n\n\nSVM\nKernel\nBinary classification, small data\nSmall-Medium\nEffective high-dimensional spaces\n\n\nXGBoost/LightGBM\nGradient Boosting\nTabular features, yield prediction\nMedium\nHigh performance on structured data\n\n\nCNN\nDeep Learning\nImage classification, automatic features\nLarge (10,000s+)\nSpatial awareness, hierarchical learning\n\n\nU-Net\nDeep Learning\nSemantic segmentation (pixel-wise)\nLarge\nSkip connections, works with limited data\n\n\nResNet\nDeep Learning\nVery deep networks, complex classification\nLarge\nResidual connections avoid vanishing gradients\n\n\nVision Transformer\nDeep Learning\nGlobal context, spatial relationships\nVery Large\nAttention mechanisms, long-range dependencies\n\n\nLSTM/GRU\nDeep Learning\nTime series prediction, phenology\nLarge\nTemporal pattern learning\n\n\nYOLO/Faster R-CNN\nDeep Learning\nObject detection (buildings, ships)\nLarge\nReal-time detection, bounding boxes\n\n\n\nTraining process:\n\nSplit data: 70-80% training, 10-15% validation, 10-15% testing\n\nSpatial cross-validation: Essential for EO to avoid spatial leakage\nSpatial k-fold or buffered leave-one-out\n\nFeed training data: Algorithm adjusts parameters to minimize error\nMonitor validation: Track performance on held-out validation set\nHyperparameter tuning: Optimize learning rate, batch size, architecture parameters\nEarly stopping: Stop when validation performance plateaus\nFinal evaluation: Test on completely independent test set\n\nTransfer Learning: - Pre-train on large dataset (ImageNet, SatViT, Prithvi) - Fine-tune on task-specific data - Reduces training data requirements by 10-100× - Faster convergence and better generalization\n\nPhilippine Example: Poverty Mapping with Transfer Learning\nStudy using satellite imagery, nighttime lights, and OpenStreetMap data: - Transfer learning from ImageNet improved performance by 14.1% for tropical cyclone impact areas - Requires careful hyperparameter tuning for generalization across regions - Cost-effective approach for limited labeled data scenarios\n\n\n\nStep 6: Validation and Evaluation\nRigorous testing on independent data\n\n\n\n\n\n\nImportantNever Test on Training Data!\n\n\n\nTesting on data the model has seen gives falsely optimistic results. Always use held-out test data. For EO, random train-test splits can overestimate performance by up to 28% due to spatial autocorrelation.\n\n\nClassification metrics:\n\nOverall Accuracy (OA): Percentage of correctly classified pixels\nConfusion Matrix: Shows which classes are confused with each other\nPer-Class Metrics:\n\nProducer’s Accuracy (Recall): How many ground truth samples were correctly classified\nUser’s Accuracy (Precision): How many predicted samples are actually correct\n\nF1-Score: Harmonic mean of precision and recall (2 × Precision × Recall / (Precision + Recall))\nKappa Coefficient: Agreement accounting for chance\nMatthews Correlation Coefficient (MCC): Balanced measure even for imbalanced classes\n\nSemantic Segmentation metrics:\n\nIoU (Intersection over Union): Area of overlap / Area of union\n\nIoU &gt; 0.5: Generally acceptable\nIoU &gt; 0.7: High-quality segmentation\nIoU &gt; 0.9: Excellent agreement\n\nMean IoU (mIoU): Average IoU across all classes\nDice Coefficient: 2 × IoU / (1 + IoU), less harsh penalty than IoU\nPixel Accuracy: Simple but biased toward majority class\nBoundary F1 Score: Precision/recall on boundary pixels\n\nRegression metrics:\n\nRMSE (Root Mean Squared Error): Average prediction error, penalizes large errors\nMAE (Mean Absolute Error): Average absolute deviation, less sensitive to outliers\nR² (Coefficient of Determination): Proportion of variance explained (0.88 = 88% explained)\n\nObject Detection metrics:\n\nPrecision/Recall: At various IoU thresholds (0.5, 0.75)\nAverage Precision (AP): Area under precision-recall curve\nMean Average Precision (mAP): Mean AP across classes\n\nPhilippine Example: Flood mapping evaluation\nConfusion Matrix (DOST-ASTI DATOS flood detection):\n                Predicted\n              | Flood | No Flood |\nActual Flood  |  450  |   50     |  Producer's Acc (Recall): 90%\nActual No Flood|  30   |  1470    |  Producer's Acc: 98%\n\nUser's Accuracy (Precision): 93.8%   96.7%\nOverall Accuracy: 96%\nF1-Score (Flood class): 91.8%\nSpatial Validation Best Practices:\n\nSpatial k-fold cross-validation: Divide data into spatially homogeneous clusters\nBuffered leave-one-out: Create buffer zones around test samples\nIndependent geographic testing: Test on completely different regions\nTemporal validation: Train on one time period, test on another\n\n\n\nStep 7: Deployment and Operationalization\nMaking the model operational:\nDeployment strategies:\n\nBatch processing: Apply model to large archives (entire countries, multi-year time series)\nNear real-time: Process new satellite acquisitions automatically (disaster response)\nOn-demand: User-triggered analysis (web portals, APIs)\nEdge processing: On-board satellite AI (ESA Φsat-2, launched 2024)\n\nOn-Board AI Processing (2024 Breakthrough): - ESA Φsat-2: 22cm CubeSat with on-board AI - Processes imagery directly in orbit - Cloud filtering: Only clear, usable images sent to Earth - Reduces data transmission costs, enables real-time event detection - Rationale: With 1,052 active EO satellites generating thousands of terabytes daily, traditional communication cannot relay this volume\nOperational considerations:\n\nScalability: Can it handle regional/national scale?\nAutomation: Minimize manual intervention\nPerformance monitoring: Track accuracy over time, detect distribution shifts\nModel retraining: Update as conditions change or new data becomes available\nModel versioning: Maintain reproducibility and track improvements\nIntegration: Connect to decision support systems, early warning platforms\nAPI development: Create accessible interfaces for inference\nCloud deployment: Google Earth Engine, AWS SageMaker, Azure ML, Vertex AI\n\nPhilippine context:\n\nDOST-ASTI AIPI platform: For model deployment and user-facing AI interfaces\nDIMER repository: For model sharing and democratizing access to trained models\nALaM (Automated Labeling Machine): Combining automated labeling with crowdsourcing for continuous data quality improvement\nIntegration with LGU disaster response protocols: DATOS outputs delivered to local government units\nDelivery via PhilSA Digital Space Campus: Training and capacity building\nCoPhil Data Centre (2025): Cloud-native distribution with API-driven access",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html#part-3-types-of-machine-learning",
    "href": "day1/sessions/session2.html#part-3-types-of-machine-learning",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Part 3: Types of Machine Learning",
    "text": "Part 3: Types of Machine Learning\n\n\n\n\n\ngraph TB\n    subgraph Supervised[\"SUPERVISED LEARNING&lt;br/&gt;Learning from labeled examples\"]\n        S1[Classification&lt;br/&gt;Discrete categories]\n        S2[Regression&lt;br/&gt;Continuous values]\n        S3[Object Detection&lt;br/&gt;Locate + classify]\n        S4[Semantic Segmentation&lt;br/&gt;Pixel-wise classification]\n\n        S1 --&gt; S1A[Land Cover&lt;br/&gt;Forest, Water, Urban]\n        S2 --&gt; S2A[Biomass Estimation&lt;br/&gt;Predict AGB in tons/ha]\n        S3 --&gt; S3A[Building Detection&lt;br/&gt;YOLO, Faster R-CNN]\n        S4 --&gt; S4A[Flood Mapping&lt;br/&gt;U-Net segmentation]\n    end\n\n    subgraph Unsupervised[\"UNSUPERVISED LEARNING&lt;br/&gt;Finding patterns without labels\"]\n        U1[Clustering&lt;br/&gt;Group similar pixels]\n        U2[Dimensionality&lt;br/&gt;Reduction]\n        U3[Anomaly&lt;br/&gt;Detection]\n\n        U1 --&gt; U1A[K-Means&lt;br/&gt;ISODATA&lt;br/&gt;Spectral clusters]\n        U2 --&gt; U2A[PCA&lt;br/&gt;MNF Transform&lt;br/&gt;Band reduction]\n        U3 --&gt; U3A[Outlier Detection&lt;br/&gt;Change hotspots]\n    end\n\n    subgraph SemiSupervised[\"SEMI-SUPERVISED&lt;br/&gt;Combine labeled + unlabeled\"]\n        SS1[Self-training&lt;br/&gt;Pseudo-labeling]\n        SS2[Co-training&lt;br/&gt;Multiple views]\n        SS1 --&gt; SS1A[Bootstrap from&lt;br/&gt;limited labels]\n    end\n\n    subgraph Reinforcement[\"REINFORCEMENT LEARNING&lt;br/&gt;Learn from interaction\"]\n        R1[Agent-Environment&lt;br/&gt;Interaction]\n        R1 --&gt; R1A[Drone Path&lt;br/&gt;Planning]\n    end\n\n    style Supervised fill:#e6ffe6,stroke:#00aa44,stroke-width:3px\n    style Unsupervised fill:#ffe6e6,stroke:#cc0044,stroke-width:3px\n    style SemiSupervised fill:#fff4e6,stroke:#ff8800,stroke-width:3px\n    style Reinforcement fill:#e6e6ff,stroke:#6666cc,stroke-width:3px\n\n\n Machine Learning Paradigms for Earth Observation \n\n\n\n\nSupervised Learning\nLearning from labeled data\nThe algorithm is given: - Input: Satellite image or features - Output: Known label (class or value) - Goal: Learn mapping from input to output\nSupervised learning is the predominant approach in Earth Observation, where models learn from labeled training data to make predictions on new, unseen data.\n\nClassification Tasks\nPredicting categorical labels\nCommon Algorithms: - Random Forest (RF): Ensemble method, best performance in object-based classification; robust to noise - Support Vector Machines (SVM): Effective for high-dimensional spaces; performs well with limited training samples - CNNs (Convolutional Neural Networks): Deep learning for automatic feature learning and complex patterns - Vision Transformers (ViT): Attention mechanisms for global context and long-range dependencies\nEO Examples:\n\nLand Cover Classification\n\nInput: Sentinel-2 pixel values (13 bands)\nOutput: Forest, Water, Urban, Agriculture, Bare soil, Wetlands\nAlgorithm: Random Forest, CNN, Vision Transformer\nDatasets: EuroSAT (27,000 images, 10 classes, 98.57% accuracy)\n\nCloud Detection\n\nInput: Multi-band imagery (blue, cirrus bands effective)\nOutput: Cloud vs. Clear, Cloud shadow, Cirrus\nAlgorithm: Threshold, Random Forest, U-Net for pixel-wise segmentation\nSentinel-2 SCL: Scene Classification Layer with 11 classes\n\nCrop Type Mapping\n\nInput: Multi-temporal NDVI, SAR backscatter\nOutput: Rice, Corn, Sugarcane, Coconut, Vegetables\nAlgorithm: Random Forest, LSTM (for temporal patterns)\nMulti-temporal data &gt; single-date imagery for capturing phenology\n\n\n\nPhilippine Case Study: PhilSA-DENR Mangrove Mapping\nTask: AI-based mangrove forest identification nationwide\nTechnology: - Google Earth Engine platform - Mangrove Vegetation Index (MVI) - Sentinel-1 and Sentinel-2 fusion - Multi-temporal Landsat 8 and Sentinel-2 data\nApproach: - U-Net deep learning architecture - Binary classification: mangrove vs. non-mangrove - SAR for cloud-penetrating capability during monsoon\nPerformance: - Accuracy: 99.73% (Myanmar study using similar approach) - Random Forest classifier also effective with high temporal resolution (5-day Sentinel-2) - Support Vector Machine shows high accuracy for mangrove discrimination\nApplications: - Blue carbon programs and carbon stock monitoring - Ecosystem service valuation - Conservation planning and restoration monitoring - Palawan multi-spatiotemporal analysis with Markov chain future trend prediction\nResult: Operational nationwide mangrove extent maps with regular updates\n\nObject Detection:\nUnlike pixel-level classification, object detection identifies and localizes specific objects of interest with bounding boxes.\nPopular Architectures: - YOLO (You Only Look Once): Real-time detection, single-stage architecture, fast inference - Faster R-CNN: Two-stage detector with region proposals, high accuracy - RetinaNet: Single-stage with focal loss for handling class imbalance - EfficientDet: Scalable architecture balancing accuracy and efficiency\nKey Considerations: - Scale variation: Objects appear at different sizes depending on altitude and resolution - Dense object detection: Multiple overlapping objects in urban or agricultural scenes - Small object detection: Challenging for standard architectures (e.g., individual trees, vehicles)\nApplications: - Building footprint extraction (SpaceNet, xView datasets) - Ship detection in maritime surveillance - Vehicle counting in traffic monitoring - Individual tree crown delineation\nBenchmark Dataset: xView - &gt;1 million objects - 60 classes - &gt;1,400 km² coverage - 0.3m resolution (WorldView-3) - Purpose: Disaster response, overhead imagery analysis\n\n\nRegression Tasks\nPredicting continuous values\nEO Examples:\n\nBiomass Estimation\n\nInput: Sentinel-1 SAR backscatter, Sentinel-2 vegetation indices, LiDAR (GEDI)\nOutput: Forest biomass (tons per hectare), Above-ground biomass (AGB)\nAlgorithm: Random Forest Regression (most popular: R² = 0.70, RMSE = 25.38 Mg C ha⁻¹)\nMulti-sensor integration: LiDAR + SAR + Optical improves accuracy\nNote: SAR saturates at high biomass levels; LiDAR methods achieve ~90% agreement with field data\n\nSoil Moisture Prediction\n\nInput: Sentinel-1 VV/VH polarization, temperature, NDVI\nOutput: Volumetric soil moisture (%)\nAlgorithm: Neural network regression, Random Forest\n\nCrop Yield Forecasting\n\nInput: NDVI time series, EVI, NDMI, weather data (temperature, precipitation), soil properties\nOutput: Expected yield (tons per hectare)\nAlgorithm: LSTM regression (preferred in &gt;40% of studies), Random Forest\nPerformance: R² &gt; 0.93 for corn and soybean, RMSE &lt; 0.075 for NDVI estimation\nTemporal considerations: Early-season (higher uncertainty) vs. end-of-season (more accurate, less actionable)\n\n\n\nPhilippine Operational System: PRiSM Rice Yield Prediction\nOverview: Philippine Rice Information System, operational since 2014\nTechnology: - Synthetic Aperture Radar (SAR): Day and night, cloud-penetrating - Crop modeling and cloud computing - UAV imagery and smartphone field surveys - Statistical data integration\nData Sources: - Remote sensing satellites (Sentinel-1, RADARSAT) - Vegetation indices (NDVI, EVI) - Weather data from PAGASA - Historical yield records\nModeling: - LSTM networks for temporal modeling of SAR backscatter and vegetation indices - Random Forest for integrating multi-source data - Phenology-based models\nPartners: - International Rice Research Institute (IRRI) - technology development - Philippine Rice Research Institute (PhilRice) - operations since 2018 - Department of Agriculture (DA) - policy formulation and planning\nApplications: - Food security planning and policy formulation - Disaster response (typhoon impact assessment) - Crop insurance (PCIC integration) - Per-season mapping: Wet season (June-Nov), Dry season (Dec-May)\nSignificance: First satellite-based rice monitoring system in Southeast Asia, model for regional applications\n\nKey difference from classification: - Output is a number on a continuous scale rather than discrete classes - Loss functions measure distance from true value (MSE, MAE, RMSE) - Evaluation uses regression metrics (R², RMSE, MAE) - Predictions can be interpolated and extrapolated\n\n\n\nUnsupervised Learning\nFinding patterns in unlabeled data\nThe algorithm receives: - Input: Satellite imagery or features - No labels provided - Goal: Discover inherent structure or groupings\nUnsupervised learning techniques do not require labeled training data, making them valuable for exploratory analysis, data reduction, and scenarios where ground-truth is unavailable or expensive to obtain.\n\nClustering\nGrouping similar pixels/regions together\nCommon algorithm: k-means\n\nSpecify number of clusters (k)\nAlgorithm iteratively groups pixels with similar spectral characteristics\nResult: Image segmented into k clusters\nHuman interpretation needed: “Cluster 1 looks like water, Cluster 2 like forest…”\n\nOther Clustering Methods:\nHierarchical Clustering: - Builds tree-like structure (dendrogram) of nested clusters - Agglomerative (bottom-up) or Divisive (top-down) - No need to specify number of clusters a priori - Applications: Multi-scale land cover analysis, ecological zone identification\nDBSCAN (Density-Based Spatial Clustering): - Groups points based on density - Identifies clusters of arbitrary shape - Robust to outliers - Applications: Urban area detection, anomaly detection in satellite imagery\nEO Applications:\n\nExploratory analysis: “How many distinct spectral classes in this region?”\nChange detection: Cluster before/after images to find anomalies\nImage segmentation: Group similar pixels for object-based analysis\nInSAR time series: K-means with PCA for identifying spatially and temporally coherent displacement phenomena\n\n\n\nDimensionality Reduction\nPrincipal Component Analysis (PCA): - Linear transformation projecting high-dimensional data onto orthogonal axes of maximum variance - Applications: Hyperspectral data compression, feature extraction, noise reduction, change detection - Workflow: Center data → Compute covariance → Calculate eigenvalues/eigenvectors → Select top K components - Benefits: Reduces computational requirements, removes redundancy, enhances signal-to-noise ratio - First few PCs capture most variance\nIndependent Component Analysis (ICA): - Separates multivariate signal into independent, non-Gaussian components - Applications: Mixed pixel decomposition, blind source separation, endmember extraction in hyperspectral imagery\nt-SNE and UMAP: - Non-linear dimensionality reduction for visualization and exploratory analysis - Preserves local structure, reveals clusters and patterns in 2D/3D - Applications: Visualization of high-dimensional feature spaces, exploration of spectral diversity - Limitations: Computationally expensive, hyperparameter sensitive, not suitable for new data projection (t-SNE)\n\n\n\n\n\n\nTipWhen to Use Unsupervised Learning\n\n\n\nAdvantages: - No need for expensive labeled data - Can discover unexpected patterns - Good for initial data exploration - Data reduction for preprocessing\nLimitations: - Results need interpretation - No guarantee clusters match desired classes - Often less accurate than supervised methods for specific tasks - Difficult to evaluate objectively without labels - Determining optimal number of clusters can be challenging\nBest Practice: Use unsupervised methods for exploration, then refine with supervised learning for operational applications\n\n\nComparison Example:\nSupervised (Land Cover Classification): - Provide 1000 labeled samples: forest, water, urban - Train Random Forest - Result: Every pixel assigned forest/water/urban - Evaluation: 90% accuracy against test labels\nUnsupervised (k-means Clustering): - No labels provided - Run k-means with k=3 - Result: Three clusters emerge - Interpretation: Cluster A=water, B=vegetation, C=mixed urban/bare - Evaluation: Subjective or requires labels anyway\nIntegration Strategy: 1. Use clustering to create initial training samples 2. Apply dimensionality reduction (PCA) before classification 3. Combine unsupervised pre-training with supervised fine-tuning 4. Use clustering for quality control of labeled data",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html#part-4-deep-learning-architectures-for-earth-observation",
    "href": "day1/sessions/session2.html#part-4-deep-learning-architectures-for-earth-observation",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Part 4: Deep Learning Architectures for Earth Observation",
    "text": "Part 4: Deep Learning Architectures for Earth Observation\n\nWhat is Deep Learning?\nDeep Learning = Neural Networks with Many Layers\n\nSubset of machine learning\nInspired by biological neurons\nMultiple processing layers extract progressively abstract features\nDominant approach for image analysis since ~2012\nRevolutionized Earth Observation, enabling automated feature learning and state-of-the-art performance\n\nWhy “deep”? - Refers to depth: many hidden layers - Modern networks: 10s to 100s of layers (ResNet-152 has 152 layers) - Enables learning complex, hierarchical representations - Lower layers: Edges, textures - Middle layers: Patterns, shapes - Higher layers: Semantic concepts, objects\nKey Advantages for EO: - Automatic feature extraction from raw pixels - Spatial awareness through convolutional operations - Multi-scale analysis capabilities - Handles large, complex datasets - Transfer learning reduces data requirements\n\n\nNeural Network Fundamentals\n\nThe Artificial Neuron\nBuilding block of neural networks:\nInputs (x1, x2, x3) → [Weighted Sum + Bias] → Activation Function → Output\nMathematical operation:\n\nWeighted sum: z = w1*x1 + w2*x2 + w3*x3 + b\nActivation function: output = activation(z)\n\nExample: Detecting bright pixels\nInputs: [Red=0.8, Green=0.7, NIR=0.9]\nWeights: [w1=1.0, w2=1.0, w3=1.0]\nBias: b = -2.0\n\nz = 1.0*0.8 + 1.0*0.7 + 1.0*0.9 - 2.0 = 0.4\noutput = ReLU(0.4) = 0.4  (indicates moderately bright)\n\n\nNetwork Architecture\nLayers of neurons:\n\nInput Layer: Receives raw data (e.g., pixel values from all spectral bands)\nHidden Layers: Process and transform data through learned representations\nOutput Layer: Produces final prediction (class probabilities or continuous values)\n\nFor a simple image classification:\nInput Layer (256 neurons = 16×16 image)\n   ↓\nHidden Layer 1 (128 neurons with ReLU)\n   ↓\nHidden Layer 2 (64 neurons with ReLU)\n   ↓\nOutput Layer (5 neurons = 5 classes, softmax activation)\nEach connection has a weight - the network learns optimal weights through training via backpropagation.\n\n\nActivation Functions\nIntroduce non-linearity - crucial for learning complex patterns\nCommon activation functions:\n\n\n\n\n\n\n\n\n\n\nFunction\nEquation\nRange\nUse Case\nProperties\n\n\n\n\nReLU\nmax(0, x)\n[0, ∞)\nHidden layers (most common)\nSimple, efficient, avoids vanishing gradient\n\n\nSigmoid\n1 / (1 + e^-x)\n(0, 1)\nBinary classification output\nSmooth, probabilistic interpretation\n\n\nSoftmax\ne^xi / Σe^xj\n(0, 1), sum=1\nMulti-class classification output\nConverts logits to probabilities\n\n\nTanh\n(e^x - e^-x) / (e^x + e^-x)\n(-1, 1)\nHidden layers (older networks)\nZero-centered, smooth\n\n\nLeakyReLU\nmax(αx, x), α=0.01\n(-∞, ∞)\nHidden layers\nAllows small negative gradient\n\n\nGELU\nx * Φ(x)\n(-∞, ∞)\nTransformers\nSmooth, stochastic regularization\n\n\n\nWhy activation functions matter:\nWithout non-linearity, multiple layers would collapse to a single linear transformation - no benefit from depth! Networks would only learn linear decision boundaries.\n\n\n\n\n\n\nNoteReLU: The Default Choice\n\n\n\nReLU (Rectified Linear Unit) has become standard for hidden layers because:\n\nSimple: f(x) = max(0, x)\nComputationally efficient\nAvoids vanishing gradient problem that plagued sigmoid/tanh\nEmpirically performs very well across diverse problems\nSparsity: ~50% of neurons set to zero, acting as automatic feature selection\n\n\n\n\n\nLoss Functions\nMeasure how wrong the model’s predictions are\nThe model’s objective: minimize the loss function through gradient descent optimization.\nFor classification:\nCategorical Cross-Entropy (Log Loss):\nLoss = -Σ(y_true * log(y_pred))\n\nPenalizes confident wrong predictions heavily\nEncourages high probability for correct class\nStandard for multi-class classification\n\nExample:\nTrue class: Forest (encoded as [1, 0, 0, 0, 0])\nPrediction: [0.7, 0.1, 0.1, 0.05, 0.05]  ← Good, 70% on forest\nLoss = -1*log(0.7) = 0.36\n\nPrediction: [0.2, 0.3, 0.4, 0.05, 0.05]  ← Bad, only 20% on forest\nLoss = -1*log(0.2) = 1.61  (much higher penalty)\nBinary Cross-Entropy: - For binary classification (e.g., flood vs. no-flood) - Similar principle, optimized for two classes\nFocal Loss: - Addresses class imbalance by down-weighting well-classified examples - Focuses training on hard examples - Used in RetinaNet object detection\nFor semantic segmentation:\nDice Loss: - Based on Dice coefficient: 2×IoU / (1 + IoU) - Differentiable, suitable for optimization - More balanced for small objects - Often used in medical imaging and EO segmentation\nIoU Loss: - Directly optimizes intersection over union - Less strict than Dice for small discrepancies\nFor regression:\nMean Squared Error (MSE):\nLoss = (1/n) * Σ(y_true - y_pred)²\nExample: Biomass prediction:\nTrue: 150 tons/ha\nPrediction: 140 tons/ha\nError: 10 tons/ha\nSquared Error: 100\nMSE = 100 (if single sample)\nMean Absolute Error (MAE): - Less sensitive to outliers than MSE - More robust when errors follow non-Gaussian distribution\nHuber Loss: - Combination of MSE (small errors) and MAE (large errors) - Robust to outliers while maintaining smooth gradient\n\n\nOptimizers\nAlgorithms that adjust weights to minimize loss\nThe process:\n\nCalculate loss on current batch of data\nCompute gradients (via backpropagation): how should each weight change?\nUpdate weights in direction that reduces loss\nRepeat thousands/millions of times across epochs\n\nCommon optimizers:\n\n\n\n\n\n\n\n\n\nOptimizer\nDescription\nLearning Rate\nWhen to Use\n\n\n\n\nSGD\nStochastic Gradient Descent\nConstant or scheduled\nSimple, well-understood, good for fine-tuning\n\n\nAdam\nAdaptive Moment Estimation\nAdaptive per parameter\nDefault choice, usually works well\n\n\nAdamW\nAdam with Weight Decay\nAdaptive\nImproved generalization, Transformers\n\n\nRMSprop\nRoot Mean Square Propagation\nAdaptive\nGood for RNNs/LSTMs\n\n\nAdaGrad\nAdaptive Gradient\nAdaptive\nFeatures vary in frequency\n\n\n\nAdam is most popular because: - Adapts learning rate per parameter - Combines benefits of momentum (accelerates convergence) and adaptive learning - Requires minimal tuning - Works well across diverse problems - Default hyperparameters (lr=0.001, β1=0.9, β2=0.999) often sufficient\nLearning Rate Scheduling: - Step Decay: Reduce learning rate at fixed intervals - Cosine Annealing: Smooth decay following cosine function - Warm-up: Gradually increase learning rate at beginning - ReduceLROnPlateau: Reduce when validation loss plateaus\n\n\n\n\n\n\nTipTraining Terminology\n\n\n\nEpoch: One complete pass through the entire training dataset\nBatch: Subset of training data processed together before updating weights\nIteration: One weight update (one batch processed)\nExample: - Training data: 10,000 samples - Batch size: 100 - 1 epoch = 100 iterations (10,000 / 100) - Training for 50 epochs = 5,000 iterations\nTypical batch sizes for EO: - Images: 16-64 (limited by GPU memory) - Patches: 32-128 - Time series samples: 64-256\n\n\n\n\nThe Training Process\nIterative improvement:\n1. Initialize weights (random or pre-trained)\n2. For each epoch:\n    For each batch:\n        a. Forward pass: Compute predictions\n        b. Calculate loss\n        c. Backward pass: Compute gradients (backpropagation)\n        d. Update weights using optimizer\n    e. Evaluate on validation set\n    f. Save checkpoint if best performance\n3. Stop when validation performance plateaus (early stopping)\nMonitoring training:\n\nTraining loss should decrease - model learning patterns in training data\nValidation loss should decrease - model generalizing to new data\nIf validation loss increases while training loss decreases: Overfitting! Apply regularization.\nIf both losses remain high: Underfitting. Need more capacity or better features.\n\nRegularization techniques: - Dropout: Randomly deactivate neurons during training - Weight Decay (L2): Penalize large weights - Data Augmentation: Artificially expand training data - Early Stopping: Stop training when validation loss stops improving - Batch Normalization: Normalize activations, improves stability\n\n\n\nConvolutional Neural Networks (CNNs)\nCNNs are the foundation of modern computer vision and EO analysis\n\nWhy CNNs Excel at EO\nTraditional ML: - Manual feature engineering needed (NDVI, GLCM textures) - Limited ability to capture spatial patterns - Each pixel treated somewhat independently - Features fixed before training\nCNNs: - Automatic feature extraction from raw pixels - Spatial awareness through convolutional filters - Hierarchical learning: edges → textures → objects → scenes - Translation invariance: Detects patterns anywhere in image - Parameter sharing: Same filters applied across entire image (efficiency) - Multi-scale analysis: Through pooling and different kernel sizes\n\n\nCNN Architecture Components\n\n\n\n\n\nflowchart LR\n    A[Input Image&lt;br/&gt;Sentinel-2&lt;br/&gt;64x64x13 bands] --&gt; B[Conv Layer 1&lt;br/&gt;32 filters 3x3&lt;br/&gt;ReLU activation]\n\n    B --&gt; C[Max Pooling&lt;br/&gt;2x2&lt;br/&gt;32x32x32]\n\n    C --&gt; D[Conv Layer 2&lt;br/&gt;64 filters 3x3&lt;br/&gt;ReLU activation]\n\n    D --&gt; E[Max Pooling&lt;br/&gt;2x2&lt;br/&gt;16x16x64]\n\n    E --&gt; F[Conv Layer 3&lt;br/&gt;128 filters 3x3&lt;br/&gt;ReLU activation]\n\n    F --&gt; G[Max Pooling&lt;br/&gt;2x2&lt;br/&gt;8x8x128]\n\n    G --&gt; H[Flatten&lt;br/&gt;8192 neurons]\n\n    H --&gt; I[Dense Layer&lt;br/&gt;256 neurons&lt;br/&gt;ReLU + Dropout]\n\n    I --&gt; J[Output Layer&lt;br/&gt;Softmax&lt;br/&gt;6 classes]\n\n    J --&gt; K[Predictions&lt;br/&gt;Forest: 0.85&lt;br/&gt;Water: 0.05&lt;br/&gt;Urban: 0.03&lt;br/&gt;Agriculture: 0.04&lt;br/&gt;Bare: 0.02&lt;br/&gt;Wetlands: 0.01]\n\n    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px\n    style B fill:#ffe6e6,stroke:#cc0044,stroke-width:2px\n    style C fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style D fill:#ffe6e6,stroke:#cc0044,stroke-width:2px\n    style E fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style F fill:#ffe6e6,stroke:#cc0044,stroke-width:2px\n    style G fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style I fill:#e6ffe6,stroke:#00aa44,stroke-width:2px\n    style J fill:#e6e6ff,stroke:#6666cc,stroke-width:2px\n    style K fill:#ccffcc,stroke:#00aa44,stroke-width:2px\n\n\n Convolutional Neural Network Architecture for Land Cover Classification \n\n\n\nConvolutional Layers: - Apply learnable filters (kernels) across image - Each filter detects specific patterns (edges, textures, shapes) - Example: 3×3 kernel slides across image, computing dot product - Multiple filters per layer (e.g., 64, 128, 256 filters) - Stride controls movement (stride=1: every pixel, stride=2: every other pixel) - Padding preserves spatial dimensions\nPooling Layers: - Reduce spatial dimensions - Max pooling: Take maximum value in window (common) - Average pooling: Take average value - Increases receptive field - Provides translation invariance - Reduces computational cost\nFully Connected Layers: - Traditional neural network layers at end - Flatten spatial features - Perform final classification - Often replaced by Global Average Pooling in modern architectures\n\n\nPopular CNN Architectures for EO\nVGG Networks (VGG16, VGG19): - Deep architecture with small (3×3) convolutional filters - Simple, uniform design - 16 or 19 layers - Large memory footprint - EO Application: VGG16 with instance normalization applied for LULC classification - Good for transfer learning from ImageNet\nResNet (Residual Networks): - Innovation: Skip connections address vanishing gradient problem - Enables very deep networks (50, 101, 152 layers) - Residual blocks: output = F(x) + x - Performance: ResNet-18 and ResNet-50 widely used as encoders in semantic segmentation - EO Success: U-Net with ResNet encoder achieved precision 0.943 and recall 0.954 for building extraction - Winner architecture in many EO competitions\nInception/GoogLeNet: - Multi-scale feature extraction - Parallel convolutions with different kernel sizes (1×1, 3×3, 5×5) - Computationally efficient through 1×1 bottleneck layers - EO Application: Multi-scale land cover classification\nEfficientNet: - Innovation: Compound scaling of depth, width, and resolution - Optimal balance between accuracy and computational efficiency - EfficientNet-B0 to B7 variants - EO Application: Increasingly popular for resource-constrained applications - Mobile deployment, edge computing\n\n\n\n\n\n\nNoteTransfer Learning for EO\n\n\n\nPre-trained CNNs (trained on ImageNet) are widely used in EO:\nAdvantages: - Reduce training time - Improve performance with limited data - Lower layers learn generic features (edges, textures) applicable across domains\nConsiderations: - ImageNet uses RGB images; EO often has more bands - Solutions: Use only RGB bands, or initialize additional channels with pre-trained weights - Fine-tune all layers or freeze early layers\nRecent Research (2024): Self-supervised pre-training on RS data (SatMAE, SatViT) offers modest improvements over ImageNet in few-shot settings, especially when pre-trained on domain-specific EO data.\n\n\n\n\n\nU-Net and Semantic Segmentation\n\nU-Net Architecture\nDescription: Encoder-decoder architecture with skip connections, originally designed for biomedical image segmentation but widely adopted for Earth Observation.\n\n\n\n\n\nflowchart TD\n    subgraph Encoder[\"ENCODER (Contracting Path)\"]\n        A[Input&lt;br/&gt;SAR Image&lt;br/&gt;256x256x2&lt;br/&gt;VV, VH] --&gt; B[Conv 3x3&lt;br/&gt;ReLU&lt;br/&gt;64 filters]\n        B --&gt; C[Conv 3x3&lt;br/&gt;ReLU&lt;br/&gt;64 filters]\n        C --&gt; D[Max Pool 2x2&lt;br/&gt;128x128x64]\n\n        D --&gt; E[Conv 3x3&lt;br/&gt;128 filters]\n        E --&gt; F[Conv 3x3&lt;br/&gt;128 filters]\n        F --&gt; G[Max Pool 2x2&lt;br/&gt;64x64x128]\n\n        G --&gt; H[Conv 3x3&lt;br/&gt;256 filters]\n        H --&gt; I[Conv 3x3&lt;br/&gt;256 filters]\n        I --&gt; J[Max Pool 2x2&lt;br/&gt;32x32x256]\n    end\n\n    subgraph Bottleneck[\"BOTTLENECK\"]\n        J --&gt; K[Conv 3x3&lt;br/&gt;512 filters]\n        K --&gt; L[Conv 3x3&lt;br/&gt;512 filters]\n    end\n\n    subgraph Decoder[\"DECODER (Expanding Path)\"]\n        L --&gt; M[Up-Conv 2x2&lt;br/&gt;256 filters&lt;br/&gt;64x64x256]\n        M --&gt; N[Concatenate&lt;br/&gt;with I]\n        N --&gt; O[Conv 3x3&lt;br/&gt;256 filters]\n        O --&gt; P[Conv 3x3&lt;br/&gt;256 filters]\n\n        P --&gt; Q[Up-Conv 2x2&lt;br/&gt;128 filters&lt;br/&gt;128x128x128]\n        Q --&gt; R[Concatenate&lt;br/&gt;with F]\n        R --&gt; S[Conv 3x3&lt;br/&gt;128 filters]\n        S --&gt; T[Conv 3x3&lt;br/&gt;128 filters]\n\n        T --&gt; U[Up-Conv 2x2&lt;br/&gt;64 filters&lt;br/&gt;256x256x64]\n        U --&gt; V[Concatenate&lt;br/&gt;with C]\n        V --&gt; W[Conv 3x3&lt;br/&gt;64 filters]\n        W --&gt; X[Conv 3x3&lt;br/&gt;64 filters]\n    end\n\n    X --&gt; Y[Conv 1x1&lt;br/&gt;2 classes&lt;br/&gt;Sigmoid]\n    Y --&gt; Z[Output&lt;br/&gt;Flood Mask&lt;br/&gt;256x256x2&lt;br/&gt;Water/No-Water]\n\n    I -.-&gt;|Skip Connection| N\n    F -.-&gt;|Skip Connection| R\n    C -.-&gt;|Skip Connection| V\n\n    style A fill:#e6f3ff,stroke:#0066cc,stroke-width:2px\n    style Encoder fill:#ffe6e6,stroke:#cc0044,stroke-width:2px\n    style Bottleneck fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style Decoder fill:#e6ffe6,stroke:#00aa44,stroke-width:2px\n    style Z fill:#ccffcc,stroke:#00aa44,stroke-width:3px\n\n\n U-Net Architecture for Semantic Segmentation (Flood Mapping Example) \n\n\n\nArchitecture: - Encoder (Contracting Path): Progressively downsamples input (convolutional + pooling), capturing context - Decoder (Expanding Path): Upsamples features (transpose convolution), enabling precise localization - Skip Connections: Concatenate encoder features with decoder at same resolution, preserving spatial information - Fully symmetric structure\nWhy U-Net Works Well: - Skip connections preserve fine-grained spatial information lost during downsampling - Works with relatively small training datasets (important for EO where labels are expensive) - End-to-end pixel-wise predictions - Multi-scale feature fusion\nApplications in EO: - Land cover semantic segmentation - Building footprint extraction - Road network mapping - Crop field delineation - Water body detection - Flood extent mapping\n\nPhilippine Case Study: Benguet Province Deforestation\nStudy Details: - Location: Benguet Province tropical montane forest - Time period: 2015 to early 2022 - Total deforestation detected: 417.93 km² - Significance: First deep learning application in Southeast Asian montane forests\nMethods: - Sentinel-1 SAR and Sentinel-2 optical fusion - U-Net deep learning architecture - Comparison with Random Forest and K-Nearest Neighbors\nPerformance: - Accuracy: 99.73% for binary forest/non-forest classification - Outperformed traditional ML methods - Validated effectiveness of multi-sensor data fusion - Demonstrated U-Net suitability for tropical conditions\nTechnology Advantages: - SAR cloud-penetrating capability fills observational gap in tropics - Multi-temporal analysis detects gradual and abrupt changes - Automated processing enables continuous monitoring - Scalable to national level\n\nU-Net Variants and Improvements:\nUNet++: - Nested skip connections for improved gradient flow - Dense skip pathways - Better feature aggregation\nAttention U-Net: - Incorporates attention mechanisms to focus on relevant features - Attention gates highlight salient features - Improved performance on complex scenes\n3D U-Net: - Extends to volumetric data or multi-temporal stacks - Temporal convolutions for time series - Applications: Crop monitoring, change detection\nU-Net with Advanced Encoders: - U-Net with ResNet encoder: Combines U-Net decoder with ResNet encoder - U-Net with SK-ResNeXt encoder: Integrates selective kernel and ResNeXt for enhanced feature extraction - UNetFormer: Hybrid CNN encoder + Transformer decoder (discussed below)\nPerformance Metrics: - Mean IoU (mIoU): Average IoU across all classes - F1-Score per class - Boundary accuracy for precise delineation\n\n\n\nDeepLab Family\nDeepLab: State-of-the-art semantic segmentation architecture family\nKey Innovations:\nAtrous (Dilated) Convolutions: - Enlarge receptive field without losing resolution - Insert “holes” in convolution kernel - Capture multi-scale context efficiently\nAtrous Spatial Pyramid Pooling (ASPP): - Parallel atrous convolutions with different rates - Captures features at multiple scales - Aggregates information from different receptive fields\nEncoder-Decoder Structure (DeepLabv3+): - Similar to U-Net philosophy - Combines ASPP with decoder for refined boundaries\nPerformance: - DeepLabv3+ shows superior performance compared to standard U-Net on many benchmarks - Improved Mean IoU - Better boundary delineation\nEO Applications: - Large-scale land cover mapping - Urban scene segmentation - Agricultural field boundaries\nComparison: U-Net vs. DeepLab - U-Net: Better with limited data, simpler architecture, faster training - DeepLab: Better overall performance, more complex, requires more data - Both: Widely used in EO, choice depends on data availability and computational resources\n\n\nVision Transformers (ViTs)\nParadigm shift from convolutions to self-attention mechanisms\n\nFundamentals\nArchitecture:\n\nPatch Embedding: Divide image into fixed-size patches (e.g., 16×16 pixels)\nLinear Projection: Flatten patches and project to embedding dimension (e.g., 768-D)\nPositional Encoding: Add learnable position information to preserve spatial relationships\nTransformer Encoder: Stack of multi-head self-attention and feed-forward layers\nClassification Head: MLP (Multi-Layer Perceptron) for final prediction\n\nSelf-Attention Mechanism: - Models relationships between all image patches - Each patch “attends to” all other patches - Learns which patches are relevant for prediction - Captures long-range dependencies - Adaptively focuses on informative regions\nMathematical Formulation:\nAttention(Q, K, V) = softmax(QK^T / √d_k) V\n\nQ = Query (what am I looking for?)\nK = Key (what do I contain?)\nV = Value (what information do I pass?)\nMulti-Head Attention: - Multiple attention mechanisms in parallel - Each head learns different relationships - Aggregate outputs for richer representation\n\n\nAdvantages for EO\nGlobal Context Modeling: - Attention mechanism captures relationships across entire image from early layers - CNNs build up receptive field gradually through layers - Particularly valuable for EO where context matters (e.g., urban vs. rural forest)\nLong-Range Dependencies: - Can relate distant image regions - Example: Recognizing rice paddy requires context of surrounding infrastructure, water bodies\nEffective for Large-Scale Imagery: - Scales well to high-resolution satellite images - Efficient self-attention variants reduce computational cost\nStrong Transfer Learning: - Pre-trained ViTs transfer well across tasks - SatViT: Pre-trained on 1.3 million satellite-derived RS images\nHandles Variable Input Sizes: - Flexible patch-based approach - Can adapt to different image resolutions\n\n\nVariants for Remote Sensing\nSwin Transformer: - Innovation: Hierarchical architecture with shifted windows - Local attention within windows (efficient computation) - Shifted window scheme enables cross-window connections - Multi-scale feature representation (like CNN feature pyramids) - State-of-the-art performance on many EO benchmarks\nViT with Spectral Adaptation: - Modified patch embedding for multi-spectral inputs - Handles variable number of spectral bands (not just RGB) - Pre-training on large satellite image datasets - Applications: Hyperspectral classification, multi-sensor fusion\nSatViT: - Pre-trained on 1.3 million satellite-derived RS images - Domain-specific Vision Transformer for remote sensing - Improved transfer learning performance over ImageNet pre-training - Publicly available for EO community\nMS-CLIP (IBM, 2024): - First vision-language model for multi-spectral Sentinel-2 data - Adapts CLIP dual-encoder architecture for 10+ spectral bands - Enables zero-shot classification and image-text retrieval - Example: “Show me images with dense vegetation” without explicit classification\n\n\nChallenges\nData Requirements: - ViTs require large training datasets (millions of samples) - Less effective with small datasets compared to CNNs - Solution: Transfer learning from pre-trained models (SatViT, ImageNet)\nComputational Cost: - Self-attention quadratic complexity in number of patches - Memory intensive for high-resolution images - Solutions: Swin Transformer (local attention), efficient attention mechanisms\nInterpretability: - Attention maps provide some interpretability - Can visualize which patches model focuses on - Still less intuitive than CNN filter visualizations\n\n\n\nHybrid Architectures: UNetFormer\nUNetFormer: Combines CNN encoders with Transformer decoders\nDescription: Hybrid architecture leveraging strengths of both CNNs and Transformers\nKey Features: - CNN Encoder: ResNet18 captures local spatial features efficiently - Transformer Decoder: Models global context and long-range dependencies - Hybrid Design: Balances computational efficiency with modeling capacity - Skip connections from encoder to decoder (like U-Net)\nPerformance: - State-of-the-art on remote sensing semantic segmentation benchmarks - Particularly effective for urban scene imagery - Outperforms pure CNN and pure Transformer approaches on many tasks\nRelated Architectures: - UNeXt: Efficient network optimizing depth, width, and resolution - UNetFormer with boundary enhancement: Multi-scale approach for improved edge detection - Segformer: Transformer encoder + lightweight decoder\nWhen to Use: - Complex EO scenes requiring both local detail and global context - Semantic segmentation tasks with diverse object scales - When computational resources allow (more expensive than standard U-Net)\n\n\nTemporal Models for Time Series\nMulti-temporal satellite data captures dynamic processes - temporal models extract these patterns\n\nLSTM and GRU\nLSTM (Long Short-Term Memory): - Purpose: Temporal pattern learning in sequential data - Architecture: Recurrent neural network with gating mechanisms - Gates: Input gate, forget gate, output gate, cell state - Advantage: Learns long-term dependencies, avoids vanishing gradient problem of vanilla RNNs\nApplications in EO: - Time series classification: Crop type mapping from multi-temporal NDVI - Phenology monitoring: Extracting growing season characteristics - Yield prediction: Forecasting crop yields from vegetation index time series - Change detection: Detecting disturbances in forest time series - Weather forecasting: Climate variables prediction\nGRU (Gated Recurrent Unit): - Simplified version of LSTM - Fewer parameters (faster training) - Often comparable performance to LSTM - Good choice when computational resources are limited\nPerformance: - Most Used: RNNs applied in &gt;22% of EO time series studies - LSTMs Preferred: Used in &gt;40% of crop yield prediction studies - Accuracy: R² &gt; 0.93 for corn and soybean yield prediction\n\nPhilippine Application: Crop Yield Forecasting with LSTM\nPRiSM Enhanced with Deep Learning:\nData: - Multi-temporal Sentinel-1 SAR backscatter (VV, VH polarizations) - Sentinel-2 NDVI time series - PAGASA weather data (rainfall, temperature) - Historical yield records from PhilRice\nApproach: - LSTM network processes time series sequentially - Captures phenological patterns (planting, vegetative growth, reproductive phase, maturity) - Integrates weather variables as auxiliary inputs - Trained on multi-year data across provinces\nPerformance: - Earlier and more accurate yield forecasts than statistical models - Mid-season prediction (2-3 months before harvest) with acceptable accuracy - Integration with PRiSM for operational deployment\nApplications: - Food security early warning - Crop insurance (PCIC) - Agricultural planning and market stabilization - Disaster impact assessment\n\n\n\nConvLSTM\nConvLSTM: Combines spatial convolutions with temporal LSTM\nArchitecture: - Replaces matrix multiplications in LSTM with convolutional operations - Preserves spatial structure throughout temporal modeling - Input: 3D tensor (time, height, width) - Output: Spatial predictions over time\nAdvantages: - Captures both spatial and temporal patterns simultaneously - More parameter efficient than separate spatial and temporal models - End-to-end learning\nApplications: - Weather forecasting and precipitation nowcasting - Flood prediction from time series of meteorological variables - Crop monitoring with spatial context - Spatiotemporal land cover change\n\n\nTemporal Attention\nLightweight Temporal Attention Encoder (L-TAE): - Innovation: Distributes channels among compact attention heads operating in parallel - Outperforms RNNs with fewer parameters and reduced computational complexity - Particularly effective for satellite image time series (SITS) classification\nMulti-Head Temporal Attention: - Each head attends to different temporal patterns - Learn complementary temporal representations - Aggregate outputs for final prediction\nAdvantages over LSTMs: - Parallelizable (faster training) - Direct access to all time steps (no sequential bottleneck) - Attention weights provide interpretability (which dates are important?)\nApplications: - Crop type classification from Sentinel-2 time series - Land cover change detection - Phenology extraction\n\n\nTemporal Transformers\nTransformer for Time Series: - Self-attention over temporal sequence - Positional encoding preserves temporal order - Can model arbitrarily long sequences\nTiMo (2025): - Description: Spatiotemporal vision transformer foundation model - Innovation: Hierarchical gyroscope attention mechanism - Captures evolving multi-scale patterns across time and space - Pre-trained on large satellite image time series datasets\nAdvantages: - Global temporal context from first layer - Handles variable-length sequences - State-of-the-art performance on temporal EO tasks\nChallenges: - Requires large amounts of training data - Computationally expensive - Best suited for long time series (many observations)\n\n\n\nObject Detection Architectures\nObject detection identifies and localizes specific objects with bounding boxes\n\nYOLO (You Only Look Once)\nCharacteristics: - Real-time detection: Single-pass architecture, processes entire image once - Fast inference: 30-60+ FPS depending on variant - Good accuracy-speed trade-off: Suitable for operational systems - Single-stage detector: Predicts bounding boxes and class probabilities directly\nVersions: - YOLOv3: Introduced multi-scale predictions - YOLOv4: Enhanced training techniques, better accuracy - YOLOv5: Popular, well-documented, easy to use (Ultralytics implementation) - YOLOv6-v8: Latest, best performance, improved small object detection - YOLO-NAS: Neural Architecture Search, state-of-the-art accuracy\nApplications in EO: - Building detection: Rapid mapping of structures for disaster damage assessment - Vehicle detection: Traffic monitoring, parking lot analysis - Ship detection: Maritime surveillance, illegal fishing monitoring - Small object detection: Improved in recent versions (important for vehicles, individual trees)\nAdvantages: - Fast training and inference - Good generalization - Easy to deploy - Active community and pre-trained models\n\n\nR-CNN Family\nFaster R-CNN: - Architecture: Two-stage detector - Stage 1: Region Proposal Network (RPN) generates candidate object locations - Stage 2: Classifies proposals and refines bounding boxes - Advantage: High accuracy, especially for diverse object sizes - Disadvantage: Slower than single-stage detectors like YOLO\nMask R-CNN: - Extension of Faster R-CNN: Adds instance segmentation branch - Output: Bounding box + pixel-level mask for each object - Applications: - Building footprints with precise boundaries - Individual tree crown delineation - Object-level change detection - Counting objects (vehicles, animals) with high precision\nPerformance: - Generally higher accuracy than YOLO - Better for complex scenes with occlusions - Preferred when accuracy is more important than speed\n\n\nRetinaNet\nKey Innovation: - Focal Loss: Addresses class imbalance by down-weighting well-classified examples - Focuses training on hard examples - Single-stage detector\nAdvantages: - Excellent for imbalanced datasets (common in EO: rare objects like ships, rare land cover classes) - Competitive accuracy with two-stage detectors - Faster than R-CNN family\nApplications: - Rare object detection (e.g., informal settlements, landslides) - Multi-class detection with imbalanced classes\n\n\nEfficientDet\nKey Innovation: - Compound scaling: Jointly scales resolution, depth, and width - BiFPN (Bi-directional Feature Pyramid Network): Efficient multi-scale feature fusion\nAdvantages: - Optimal balance between accuracy and efficiency - Scalable (EfficientDet-D0 to D7) - Suitable for deployment on resource-constrained devices\nApplications: - Edge computing and mobile deployment - Operational systems requiring fast inference\nComparison Table:\n\n\n\n\n\n\n\n\n\nArchitecture\nSpeed\nAccuracy\nBest For\n\n\n\n\nYOLO\nVery Fast\nGood\nReal-time applications, rapid mapping\n\n\nFaster R-CNN\nSlow\nHigh\nHigh-accuracy requirements, diverse object sizes\n\n\nMask R-CNN\nSlow\nHigh\nInstance segmentation, precise boundaries\n\n\nRetinaNet\nModerate\nHigh\nImbalanced datasets, rare objects\n\n\nEfficientDet\nFast-Moderate\nHigh\nBalanced accuracy/speed, deployment\n\n\n\n\n\n\nMulti-Modal Architectures\nIntegrating data from multiple sensors for robust monitoring\n\nOptical-SAR Fusion\nComplementary Information: - Optical: Rich spectral information (13 bands for Sentinel-2), sensitive to biochemical properties - SAR: Structural information (backscatter), penetrates clouds, sensitive to moisture and geometry\nFusion Strategies:\nEarly Fusion (Input-level): - Concatenate inputs at beginning - Example: Stack Sentinel-2 bands with Sentinel-1 VV/VH as additional channels - Simple, but assumes features align semantically - Advantage: Single model processes all modalities - Disadvantage: May not capture modality-specific patterns optimally\nLate Fusion (Decision-level): - Separate models for each modality - Combine predictions (average, weighted average, voting) - Advantage: Each modality processed optimally - Disadvantage: Doesn’t exploit inter-modality relationships\nIntermediate Fusion (Feature-level): - Merge features at middle layers - Learn joint representations - Advantage: Balances early and late fusion benefits - Disadvantage: More complex architecture design\nRecent Approaches:\nProgressive Fusion Learning: - Gradually integrates multimodal information - Addresses semantic misalignment between modalities - Applications: Building extraction with optical + SAR\nM2Caps (Multi-modal Capsule Networks, 2024): - Capsule networks for optical-SAR fusion - Applications: Land cover classification - Handles appearance disparities between modalities\nBi-modal Contrastive Learning: - Self-supervised approach for joint representation - Pre-training on unlabeled optical-SAR pairs - Fine-tune for specific tasks (crop classification, change detection)\nTransformer Temporal-Spatial Model (TTSM): - Synergizes SAR and optical time-series for vegetation monitoring - Performance: R² &gt; 0.88 for vegetation reconstruction - Handles missing data in one modality\n\nPhilippine Application: All-Weather Rice Monitoring\nPRiSM Multi-Sensor Approach:\nChallenge: Philippines has &gt;60% cloud cover during monsoon season (June-November)\nSolution: Sentinel-1 SAR + Sentinel-2 optical fusion\nMethodology: - Sentinel-1 SAR: Primary data source during wet season (cloud-penetrating) - Sentinel-2 optical: Complementary data during dry season and cloud-free periods - Feature-level fusion: Combine SAR backscatter (VV, VH) with optical indices (NDVI, EVI) - Random Forest classifier: Trained on fused features\nBenefits: - Year-round monitoring regardless of weather - Higher accuracy than single-sensor approach - Reduced data gaps - Continuous rice area tracking\nOperational Impact: - Reliable per-season mapping even during typhoons - Supports disaster damage assessment - Improved yield prediction with temporal SAR backscatter patterns\n\nChallenges: - Modality alignment: Different imaging mechanisms (reflectance vs. backscatter) - Semantic misalignment: Features may not correspond across modalities - Optimal fusion level depends on task and data availability - Increased computational cost\nApplications: - All-weather land cover classification - Crop monitoring during cloudy seasons - Building extraction (optical for spectral, SAR for structure) - Flood mapping (SAR for water extent, optical for pre-event land cover) - Forest biomass estimation (optical for species, SAR for structure)\n\n\n\nFoundation Models for Earth Observation\nFoundation models are large, pre-trained models adaptable to various downstream tasks\nEmerged as transformative trend in EO 2023-2025, dramatically reducing resources required for environmental monitoring.\n\nPrithvi Family (IBM-NASA)\nPrithvi-EO-1.0 (August 2023): - Scale: 100 million parameters - Training Data: NASA’s Harmonized Landsat Sentinel-2 (HLS) dataset - Pre-training Strategy: Masked autoencoder (MAE) - self-supervised learning on unlabeled imagery - Significance: World’s largest geospatial AI model at release - Availability: Open-source on Hugging Face\nPrithvi-EO-2.0 (December 2024): - Scale: 600 million parameters (6× larger than predecessor) - Training Data: 4.2 million global time series samples from HLS at 30m resolution - Architecture: Temporal transformer with location and temporal embeddings - Performance: 75.6% average score on GEO-bench framework (8% improvement over 1.0) - Availability: Hugging Face and IBM’s TerraTorch toolkit\nApplications Demonstrated: - Flood Mapping: Valencia, Spain floods (October 2024) using Sentinel-1 + Sentinel-2 - Burn Scar Detection: Wildfire impact assessment - Cloud Gap Reconstruction: Filling missing data in cloudy imagery - Multi-Temporal Crop Segmentation: Mapping crop types across United States\nFine-Tuning Workflow: 1. Load pre-trained Prithvi model 2. Replace classification head for specific task 3. Fine-tune on small labeled dataset (hundreds to thousands of samples) 4. Deploy for inference\nImpact: - Enables users with limited ML expertise to deploy state-of-the-art models - Reduces labeled data requirements by 10-100× - Democratizes access to advanced AI for EO - Foundation for operational systems in resource-constrained settings\nDeployment: - Integrated into IBM’s TerraTorch toolkit for easy fine-tuning - Model zoo with pre-trained variants - Tutorials and example notebooks\n\n\nOther Foundation Models\nSatMAE: - Masked autoencoding for satellite imagery - Self-supervised pre-training on unlabeled data - Transfer learning for downstream tasks - Competitive with ImageNet pre-training in few-shot scenarios\nSatViT: - Pre-trained Vision Transformer on 1.3 million satellite-derived RS images - Domain-specific for remote sensing - Improved transfer learning over ImageNet pre-training - Publicly available for EO community\nMS-CLIP (IBM, 2024): - First vision-language model for multi-spectral Sentinel-2 data - Dual encoder architecture adapted from CLIP - Handles 10+ spectral bands (not just RGB) - Capabilities: - Zero-shot classification: Classify without task-specific training - Image-text retrieval: “Find images with rice paddies” - Semantic search: Natural language queries over satellite archives\nTiMo (2025): - Spatiotemporal vision transformer foundation model for satellite image time series - Hierarchical gyroscope attention mechanism - Captures evolving multi-scale patterns across time and space - Pre-trained on large temporal satellite datasets\nWhy Foundation Models Matter:\n\nData Efficiency: Pre-training on massive unlabeled data, fine-tune with small labeled sets\nGeneralization: Learn robust representations applicable across tasks and regions\nDemocratization: Lower barrier to entry for EO AI applications\nRapid Deployment: Quickly adapt to new applications without training from scratch\nTransfer Across Domains: Models pre-trained globally applicable to local Philippine contexts\n\n\n\n\n\n\n\nNoteSelf-Supervised Learning\n\n\n\nFoundation models typically use self-supervised learning for pre-training:\nMasked Autoencoding (MAE): - Randomly mask patches of input image - Model learns to reconstruct masked patches - Forces model to learn semantic representations - No labels needed - learns from structure of data itself\nContrastive Learning (MoCo, SimCLR): - Learn representations by contrasting positive and negative pairs - Augmented views of same image are positive pairs - Different images are negative pairs - Model learns invariance to augmentations\nSSL4EO-S12 Dataset: - Large-scale, global, multimodal corpus from Sentinel-1 and Sentinel-2 - Supports self-supervised pre-training research - Multi-seasonal coverage - Enables research on contrastive learning for remote sensing\n\n\n\n\n\nTraining Strategies\nTransfer Learning: - Approach: Pre-train on large dataset (ImageNet, SatViT, Prithvi), fine-tune on task-specific data - Benefits: Reduces training time, improves performance with limited data - Best Practice: Freeze early layers (generic features), fine-tune later layers (task-specific features) - Recent Research (2024): Self-supervised pre-training on RS data offers modest improvements over ImageNet in few-shot settings\nData Augmentation: - Rotation and flipping: Particularly suitable for satellite imagery (no canonical orientation) - Color jittering: Simulate atmospheric variations - Random crops: Increase spatial diversity - Mixup and CutMix: Regularization techniques for classification - Caution: Ensure augmentations are realistic for EO (e.g., don’t vertically flip landscapes with clear sky/ground distinction)\nSelf-Supervised Learning: - Contrastive learning: MoCo, SimCLR for learning representations - Masked image modeling: MAE for learning to reconstruct images - Multi-modal alignment: CLIP-style vision-language pre-training - SSL4EO-2024 Summer School: First summer school on self-supervised learning for EO (July 2024, Copenhagen)\nFew-Shot Learning: - Motivation: Limited labeled data, expensive annotation - Methods: Metric learning, meta-learning, prototypical networks - Applications: Novel land cover classes, rare object detection, new geographic regions - Example: Gerry Roxas Foundation deforestation classification achieved 43% accuracy with only 8% training data\nActive Learning: - Strategy: Iteratively select most informative samples for labeling - Process: Train model → Find uncertain predictions → Label those → Retrain - Benefits: Reduced annotation cost (27% improvement in mIoU with only 2% labeled data) - WeakAL Framework: Combines active learning and weak supervision, computing &gt;90% of labels automatically while maintaining competitive performance\nBest Practices: - Start with pre-trained weights when available (Prithvi, SatViT, ImageNet) - Use appropriate learning rate schedules (cosine annealing with warm-up) - Apply batch normalization or layer normalization for training stability - Monitor overfitting through validation metrics (gap between train and validation loss) - Implement early stopping and model checkpointing - Use mixed-precision training (FP16) for faster training on modern GPUs",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html#part-5-benchmark-datasets-for-training-and-validation",
    "href": "day1/sessions/session2.html#part-5-benchmark-datasets-for-training-and-validation",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Part 5: Benchmark Datasets for Training and Validation",
    "text": "Part 5: Benchmark Datasets for Training and Validation\nBenchmark datasets enable standardized comparison of algorithms and serve as training resources\n\nPatch-Level Classification Datasets\n\nEuroSAT\nSpecifications: - Images: 27,000 labeled images - Classes: 10 land cover types - Size: 64×64 pixel patches - Bands: 13 (Sentinel-2 multispectral) - Coverage: Europe - Classification Accuracy: 98.57% achieved with CNNs\nClasses: Annual Crop, Forest, Herbaceous Vegetation, Highway, Industrial Buildings, Pasture, Permanent Crop, Residential Buildings, River, Sea/Lake\nAccess: - GitHub: https://github.com/phelber/EuroSAT - TensorFlow Datasets - PyTorch datasets - Commonly used for benchmarking deep learning architectures\n\n\nBigEarthNet v2.0\nSpecifications: - Patches: 549,488 paired Sentinel-1 and Sentinel-2 patches - Size: 1.2×1.2 km on ground - Classes: 19 (CORINE Land Cover nomenclature) - Type: Multi-label classification (multiple classes per patch) - Coverage: 10 European countries (Austria, Belgium, Finland, Ireland, Kosovo, Lithuania, Luxembourg, Portugal, Serbia, Switzerland)\nKey Features: - Multi-modal (optical + SAR) - Multi-label annotations (real-world complexity) - Large-scale (largest Sentinel dataset)\nAccess: - Website: https://bigearth.net/ - TensorFlow Datasets - Papers With Code\nApplications: - Multi-label land cover classification - Multi-modal fusion research - Benchmark for semantic segmentation\n\n\nLandCoverNet\nSpecifications: - Global coverage - Sentinel-2 based - Multi-temporal (annual) - Multiple continents\nApplications: - Global land cover mapping benchmark - Multi-temporal classification - Seasonal analysis and phenology\n\n\n\nObject Detection Datasets\n\nxView\nSpecifications: - Objects: &gt;1 million annotated objects - Classes: 60 - Area: &gt;1,400 km² - Resolution: 0.3m (WorldView-3 satellite) - Format: Bounding boxes\nPurpose: - Disaster response applications - Overhead imagery analysis - Object detection benchmarking - Small object detection\nAccess: - Website: http://xviewdataset.org/ - Papers With Code - Challenge competitions\n\n\nDOTA (Dataset for Object Detection in Aerial Images)\nSpecifications: - Instances: 1,793,658 annotated objects - Categories: 18 object types - Images: 11,268 - Annotation: Oriented bounding boxes (OBB) - Sources: Google Earth, GF-2 Satellite, aerial platforms\nKey Feature: - Oriented annotations: Captures object rotation (important for buildings, ships, aircraft) - Various object orientations and aspect ratios - Multiple sensors and resolutions\nAccess: - Website: https://captain-whu.github.io/DOTA/ - Papers With Code - GitHub repositories\n\n\n\nSemantic Segmentation Datasets\n\nOpenEarthMap\nSpecifications: - Global high-resolution land cover mapping benchmark - Multiple continents represented - Semantic segmentation annotations - High-resolution imagery\nPurpose: - Global mapping challenges - Multi-region training and generalization testing - Standardized semantic segmentation evaluation\n\n\nSpaceNet\nOverview: Foundation dataset for building footprints and road networks\nVersions: - SpaceNet 1-7: Multiple cities, different tasks - Building footprint extraction - Road network mapping - Flood impact assessment (SpaceNet 8) - Open competition with benchmark results\nApplications: - Building extraction algorithms - Road network detection - Multi-sensor fusion (optical + SAR for SpaceNet 6)\n\n\n\nScene Classification Datasets\n\nAID (Aerial Image Dataset)\nSpecifications: - Images: 10,000 - Categories: 30 scene categories - Size: 600×600 pixels - Resolution: 0.5-8m spatial resolution - Source: Google Earth imagery\nPurpose: - Scene classification benchmarking - Transfer learning evaluation - Feature extraction research\n\n\nNWPU-RESISC45\nSpecifications: - Categories: 45 scene types - Images: 31,500 (700 per class) - Size: 256×256 pixels - Source: High-resolution aerial images\nApplications: - Scene recognition - Transfer learning source - Benchmark comparisons\n\n\n\nTime Series Datasets\n\nTiSeLaC (Time Series Land Cover)\nPurpose: - Multi-temporal classification - Phenology analysis - Temporal pattern learning\nApplications: - Crop type mapping from time series - Vegetation dynamics - Seasonal change detection\n\n\nSatellite Image Time Series (SITS) Datasets\nVarious Sources: - MODIS time series (daily, 250m-1km) - Sentinel-2 time series (5-day, 10-20m) - Landsat time series (16-day, 30m)\nApplications: - LSTM and temporal attention training - Phenology extraction - Land cover trajectory analysis\n\n\n\nPhilippine-Specific Data Resources\nAvailable Operational Data:\nPRiSM Products: - Rice area maps (per season: wet and dry) - Seasonality information (planting dates, growth stages) - Yield estimates - Historical archive since 2014 - Website: https://prism.philrice.gov.ph/\nPhilSA Products: - Flood extent maps from DATOS system - Mangrove extent maps (PhilSA-DENR collaboration) - Land cover maps - Disaster damage assessment outputs - Website: https://philsa.gov.ph/\nDOST-ASTI: - DATOS disaster response maps - Hazard maps (flood, landslide susceptibility) - AI-powered rapid assessments - Website: https://hazardhunter.georisk.gov.ph/map\nNAMRIA Geoportal: - Topographic maps - Land cover basemaps - Administrative boundaries - Digital Elevation Models\nImportance of Benchmark Datasets: 1. Standardized Evaluation: Compare algorithms objectively 2. Training Resources: Pre-labeled data for model training 3. Transfer Learning: Pre-train on large datasets, fine-tune for specific applications 4. Research Reproducibility: Enable comparison across studies 5. Community Building: Shared resources accelerate progress",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html#part-6-data-centric-ai-in-earth-observation",
    "href": "day1/sessions/session2.html#part-6-data-centric-ai-in-earth-observation",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Part 6: Data-Centric AI in Earth Observation",
    "text": "Part 6: Data-Centric AI in Earth Observation\n\nThe Paradigm Shift (2025)\n\n\n\n\n\n\nImportantData &gt; Models\n\n\n\nOld paradigm (Model-Centric AI): - Focus on developing better algorithms - Keep data fixed, iterate on model architecture - “Our new model achieves 92% accuracy!” - Endless hyperparameter tuning\nNew paradigm (Data-Centric AI): - Focus on improving data quality and curation - Keep model fixed (use proven architectures), iterate on data - “Better data improved our model from 85% to 95% accuracy!” - Systematic data improvement\nVan der Schaar Lab’s DC-Check Framework: Argues that reliable ML hinges on characterizing, evaluating, and monitoring training data across the pipeline - not just model complexity.\n\n\nWhy the shift?\n\nModel architectures have matured: ResNet, U-Net, LSTM, Transformers are well-established and publicly available\nBiggest gains come from data: Research shows most underperforming models suffer from data issues, not algorithm deficiencies\nReal-world deployment: Data quality determines operational success and trustworthiness\nDiminishing returns: Incremental model improvements yield smaller gains than data improvements\nFoundation models: Pre-trained models (Prithvi, SatViT) reduce need for architecture innovation\n\nData-Centric Principles:\nFrom van der Schaar Lab’s DC-Check framework: - Characterizing: Understand training data distribution, coverage, biases - Evaluating: Assess data quality, label accuracy, representation - Monitoring: Track data drift, performance on subgroups, uncertainty - Stratification: Easy/Ambiguous/Hard samples require different treatment - Data-SUITE: Suitability, Usefulness, Insufficiency, Thoroughness, Expressiveness checks\n\n\n\n\n\nflowchart TB\n    subgraph ModelCentric[\"MODEL-CENTRIC AI (Old Paradigm)\"]\n        MC1[Fixed Data] --&gt; MC2[Iterate Models]\n        MC2 --&gt; MC3[Tune Hyperparameters]\n        MC3 --&gt; MC4[Try New Architectures]\n        MC4 --&gt; MC5[85% → 87% → 88%&lt;br/&gt;Diminishing Returns]\n    end\n\n    subgraph DataCentric[\"DATA-CENTRIC AI (2025 Paradigm)\"]\n        DC1[Proven Architecture&lt;br/&gt;ResNet, U-Net, ViT] --&gt; DC2[Improve Data Quality]\n        DC2 --&gt; DC3[Increase Data Quantity]\n        DC3 --&gt; DC4[Enhance Data Diversity]\n        DC4 --&gt; DC5[Refine Annotations]\n        DC5 --&gt; DC6[85% → 92% → 95%&lt;br/&gt;Significant Gains]\n    end\n\n    subgraph Pillars[\"FOUR PILLARS OF DATA-CENTRIC AI\"]\n        P1[1. Quality&lt;br/&gt;Accurate, consistent&lt;br/&gt;Cloud-free&lt;br/&gt;Atmospherically corrected]\n        P2[2. Quantity&lt;br/&gt;Sufficient samples&lt;br/&gt;Per class balance&lt;br/&gt;Training data scale]\n        P3[3. Diversity&lt;br/&gt;Geographic coverage&lt;br/&gt;Temporal variation&lt;br/&gt;Seasonal representation]\n        P4[4. Annotation&lt;br/&gt;Label accuracy&lt;br/&gt;Boundary precision&lt;br/&gt;Class consistency]\n    end\n\n    DC2 --&gt; P1\n    DC3 --&gt; P2\n    DC4 --&gt; P3\n    DC5 --&gt; P4\n\n    subgraph DCCheck[\"DC-CHECK FRAMEWORK\"]\n        DCC1[Characterize&lt;br/&gt;Data distribution&lt;br/&gt;Coverage, biases]\n        DCC2[Evaluate&lt;br/&gt;Label quality&lt;br/&gt;Representation]\n        DCC3[Monitor&lt;br/&gt;Data drift&lt;br/&gt;Performance tracking]\n    end\n\n    P1 --&gt; DCC1\n    P2 --&gt; DCC1\n    P3 --&gt; DCC2\n    P4 --&gt; DCC2\n    DCC1 --&gt; DCC3\n    DCC2 --&gt; DCC3\n\n    DCC3 --&gt; Result[Robust,&lt;br/&gt;Operational&lt;br/&gt;Models]\n\n    style ModelCentric fill:#ffe6e6,stroke:#cc0044,stroke-width:2px\n    style DataCentric fill:#e6ffe6,stroke:#00aa44,stroke-width:3px\n    style Pillars fill:#e6f3ff,stroke:#0066cc,stroke-width:2px\n    style DCCheck fill:#fff4e6,stroke:#ff8800,stroke-width:2px\n    style Result fill:#ccffcc,stroke:#00aa44,stroke-width:3px,color:#000\n\n\n Data-Centric AI Framework for Earth Observation \n\n\n\n\n\nPillar 1: Data Quality\nHigh-quality data is accurate, consistent, and properly processed\nFor satellite imagery:\nQuality issues to address:\n\nCloud contamination: Use Level-2A with SCL cloud masks, aggressive filtering\nAtmospheric effects: Always use atmospherically corrected data (surface reflectance, not TOA)\nSensor artifacts: Check for striping, banding, saturation, dead pixels\nGeometric accuracy: Ensure sub-pixel registration across time and sensors\nRadiometric consistency: Calibrate across sensors and acquisition times\nTemporal alignment: Match acquisition dates to ground conditions (phenology, seasonal changes)\n\n\nPhilippine Challenge: Cloud Cover\nPhilippines has one of highest cloud cover frequencies globally (&gt;60% during monsoon season).\nData quality solutions: - Multi-temporal compositing: Median over 3-6 months to reduce cloud impact - Multi-sensor fusion: Combine optical (Sentinel-2) + SAR (Sentinel-1) which penetrates clouds - Aggressive cloud masking: Accept fewer images for higher quality (quality &gt; quantity) - Leverage dry season: December-May for optical data acquisition - Deep learning reconstruction: Prithvi-EO-2.0 demonstrated cloud gap reconstruction - Temporal interpolation: Fill gaps using adjacent clear observations\nDATOS System Approach: - Prioritize Sentinel-1 SAR during typhoon season (cloud-independent) - Rapid processing (10-20 minutes) for disaster response - Multi-temporal composites for flood extent mapping - Integration with pre-event optical data for context\n\nFor training labels:\nQuality issues:\n\nPositional error: GPS drift (±5-10m common), georeferencing mismatch\nTemporal mismatch: 2018 labels with 2020 imagery (land cover changes)\nClass ambiguity: Unclear definitions (shrub vs. sparse forest? informal settlement vs. slum?)\nMixed pixels: Polygon boundaries include multiple classes (especially at coarse resolutions)\nLabeling inconsistency: Different interpreters apply different criteria\nEdge effects: Boundaries between classes often have high uncertainty\nScale mismatch: Labels created at different resolution than imagery\n\nBest practices:\n\nClear class definitions: Document what each class includes/excludes with examples\nConsistent methodology: Same interpreter(s), same time of year, same reference imagery\nQuality control: Multiple reviewers, consensus protocols, inter-annotator agreement metrics\nTemporal alignment: Labels contemporary with imagery (within months for dynamic classes)\nPositional accuracy: Use high-resolution reference imagery (VHR, Google Earth)\nBuffer boundaries: Consider excluding mixed pixels at class boundaries from training\nMetadata: Record labeling conditions, interpreter, date, confidence level\nIterative refinement: Use model predictions to identify and correct label errors\n\nTraining Data Errors Impact:\nResearch shows training data errors cause substantial errors in final predictions. Example scenarios: - Mislabeled rice paddies → Model confuses rice with other crops - Temporal mismatch → Model learns outdated patterns - Positional errors → Model learns from wrong pixels - Inconsistent labels → Model learns noise rather than signal\n\n\nPillar 2: Data Quantity\nMore data (usually) improves performance, but quality matters more!\nHow much data do you need?\n\n\n\n\n\n\n\n\nAlgorithm\nTypical Requirements\nWith Transfer Learning\n\n\n\n\nRandom Forest\n100s - 1000s samples per class\nSame\n\n\nSVM\n100s - 1000s samples\nSame\n\n\nSimple CNN\n1000s - 10,000s samples\n100s - 1000s\n\n\nDeep CNN (ResNet, U-Net)\n10,000s - 100,000s samples\n1000s - 10,000s\n\n\nVision Transformer\n100,000s - millions\n10,000s - 100,000s\n\n\nFoundation Models (pre-training)\nMillions - billions\nN/A (already pre-trained)\n\n\nFoundation Models (fine-tuning)\nN/A\n100s - 1000s\n\n\n\nStrategies when labeled data is limited:\n1. Data Augmentation - Geometric: Rotation, flipping, cropping, scaling, translation - Photometric: Brightness, contrast, saturation adjustments - Noise addition: Gaussian noise, salt-and-pepper - Spectral: Band dropout, mixup between spectral signatures - Caution: Ensure augmentations are realistic for EO (e.g., don’t flip images with clear up/down orientation)\n2. Transfer Learning - Use model pre-trained on large dataset (ImageNet, SatMAE, Prithvi) - Fine-tune on your small dataset - Leverages learned features from similar tasks - Reduces data requirements by 10-100× - Philippine poverty mapping example: 14.1% improvement using transfer learning\n3. Active Learning - Process: Iteratively train model → find uncertain predictions → label those → retrain - Efficiently focuses labeling effort where it matters most - Research shows 27% improvement in mIoU with only 2% labeled data - Prioritize samples near decision boundaries\n4. Few-Shot Learning - Methods: Metric learning, meta-learning, prototypical networks - Learn from very few examples per class - Gerry Roxas Foundation deforestation: 43% accuracy with only 8% training data - Useful for rare classes or novel geographic regions\n5. Weak Supervision - Leverage noisy or incomplete labels - WeakAL framework: Combines active learning and weak supervision - Computes &gt;90% of labels automatically while maintaining competitive performance - Trade-off: Lower individual label quality, but much larger quantity\n6. Synthetic Data - Generate training data via simulation or GANs - Example: Simulated SAR scenes for flood detection - Useful when real data is dangerous/expensive to collect - Caution: Domain gap between synthetic and real data\n7. Self-Supervised Pre-training - Pre-train on unlabeled data (masked autoencoding, contrastive learning) - Fine-tune on small labeled dataset - Foundation models (Prithvi) exemplify this approach - SSL4EO-S12: Large-scale dataset for self-supervised learning\n\n\n\n\n\n\nNote2024 Research: Data Efficiency\n\n\n\nFindings from “Data-Centric Machine Learning for Earth Observation” (arXiv 2024):\n\nSome EO tasks reach optimal accuracy with &lt;20% of temporal instances\nSingle band from single sensor can be sufficient for specific tasks\nImplication: Smart data selection &gt; brute force data collection\nFeature selection and dimensionality reduction crucial\nUse PCA, tree-based feature importance, or domain knowledge to identify essential features\n\nTakeaway: Focus on acquiring diverse, high-quality samples rather than maximizing quantity indiscriminately.\n\n\n\nPhilippine Solution: ALaM Project (DOST-ASTI)\nAutomated Labeling Machine (ALaM) addresses annotation bottleneck:\nApproach: - Automated labeling: ML models generate initial labels - Crowdsourcing: Distributed verification and correction - Human-in-the-loop quality control: Expert review of uncertain labels - Active learning integration: Prioritize samples for human review\nBenefits: - Significantly reduces labeling time and cost - Scales to national coverage - Integration with DIMER model repository for continuous improvement - Democratizes access to labeled training data\nIntegration with SkAI-Pinas: - Part of national AI framework - Addresses gap between abundant remote sensing data and sustainable AI pipelines - Supports operational systems like DATOS and PRiSM\n\n\n\nPillar 3: Data Diversity\nRepresentative data covers the full range of scenarios the model will encounter\nModels trained on narrow data distributions fail when deployed in diverse real-world conditions. Diversity ensures robustness and generalization.\nDimensions of diversity:\n1. Geographic diversity - Different regions (Luzon, Visayas, Mindanao) - Different ecosystems (lowland rainforest, montane cloud forest, mangrove, coral reef) - Different climate zones (Type I-IV Philippine climate classification) - Urban, peri-urban, rural contexts - Different topography (flat, hilly, mountainous)\n2. Temporal diversity - Different seasons (wet season: June-Nov, dry season: Dec-May) - Different years (inter-annual variability, El Niño vs. La Niña) - Different phenological stages (rice: planting, vegetative, reproductive, maturity) - Different times of day (for SAR: morning vs. evening passes) - Historical baselines and recent conditions\n3. Class diversity - Multiple examples per class capturing intra-class variability - Edge cases and rare types (e.g., burned forest, flooded agriculture) - Transitional zones (forest-agriculture boundary, urban-rural fringe) - Different sub-types (e.g., rice varieties, mangrove species, building materials)\n4. Sensor diversity - Different satellites (Sentinel-2A, 2B, 2C) - Different atmospheric conditions (clear, hazy, dusty) - Different viewing angles (SAR: ascending vs. descending) - Different processing baselines (if applicable) - Multi-sensor when relevant (optical + SAR)\n5. Socioeconomic diversity - Different development contexts (high-density urban, informal settlements, rural villages) - Different agricultural practices (mechanized, traditional, mixed) - Different infrastructure quality (paved roads, dirt tracks)\nExample: Urban classification\nPoor diversity: All training samples from Metro Manila CBD (Central Business District)\nResult: Model fails on: - Small provincial towns (different building density, height, materials) - Informal settlements (different patterns, materials, roof types) - Peri-urban areas (mixed land cover, agriculture near buildings) - Historical centers (older building styles)\nGood diversity: Samples from: - Large cities: Manila, Cebu, Davao (high-density, modern buildings) - Medium towns: Baguio, Iloilo, Cagayan de Oro (mixed density) - Small municipalities: Various provinces - Different building materials: Concrete, metal roofing, nipa huts, wood - Different periods: Capture urban growth and change - Informal settlements: Slums, squatter areas - Peri-urban: Transition zones\nResult: Model generalizes well across Philippines\nValidation of Diversity:\nTest model performance on stratified subsets: - Per-region accuracy (does it work in all islands?) - Per-season accuracy (dry vs. wet season) - Per-class accuracy (all classes represented equally well?) - Cross-region generalization (train on Luzon, test on Mindanao)\n\n\nPillar 4: Annotation Strategy\nHow you label data profoundly impacts model performance\nAnnotation is often the most expensive and time-consuming part of ML workflow. Strategic annotation maximizes value.\nAnnotation approaches:\n\nPoint sampling: Fast, but limited context, suitable for classification\nPolygon delineation: More information, more time-consuming, required for semantic segmentation\nPixel-level labeling: Maximum detail, most expensive, essential for precise segmentation\nImage-level labels: Easiest, suitable for scene classification, limited spatial information\nBounding boxes: For object detection, faster than pixel-level masks\n\nBest practices:\n1. Expert involvement - Use domain experts for complex classes (forest types, crop stages, mangrove species) - Train labelers thoroughly on class definitions with examples - Regular calibration sessions to maintain consistency - Document difficult cases and edge cases\n2. Quality over quantity - 500 high-quality labels &gt; 5000 noisy labels - Invest in review and correction processes - Document difficult cases and ambiguous examples - Use confidence scores to flag uncertain labels\n3. Class balance - Ensure adequate representation of minority classes - Stratified sampling by class (not just random) - Consider class weights in training if imbalanced - Oversampling rare classes or undersampling common classes - Imbalanced classes: Major challenge in EO (e.g., rare disasters, rare land cover types)\n4. Consensus protocols - Multiple labelers per sample (especially for ambiguous cases) - Majority vote or adjudication for disagreements - Measure inter-annotator agreement (Cohen’s Kappa, Krippendorff’s Alpha) - Establish minimum agreement threshold (e.g., 80%)\n5. Iterative refinement - Use model predictions to find label errors (disagreement between model and label) - Retrain after improving labels (data-centric iteration) - Focus effort on low-confidence predictions - Model-in-the-loop labeling: Model suggests labels, humans verify\n6. Annotation tools and platforms - Use efficient labeling tools (LabelMe, CVAT, Label Studio, Labelbox) - For EO: Tools supporting geospatial formats (GeoTIFF, shapefiles) - Integration with cloud platforms (Google Earth Engine, QGIS) - Export to ML-ready formats\n7. Crowdsourcing considerations - Clear instructions and examples - Quality control through redundancy and expert review - Gamification to maintain engagement - Examples: Humanitarian OpenStreetMap Team (HOT OSM) for disaster mapping\nEO-Specific Annotation Challenges:\nFrom Kili Technology’s Earth Observation Data Labeling Guide: - Sensor diversity: Different spectral bands, resolutions, formats - Massive data volumes: Petabyte-scale archives (“four Vs”: Volume, Velocity, Variety, Veracity) - Domain expertise requirements: Complex classes require specialized knowledge - Weak labeling approaches: Leverage noisy labels, distant supervision - Active learning integration: Prioritize informative samples - Stakeholder-friendly tooling: Tools accessible to non-ML experts\n\nPhilippine Annotation Ecosystem:\nALaM (Automated Labeling Machine - DOST-ASTI): - Combines automated labeling with crowdsourcing - Human-in-the-loop quality control - Integration with DIMER model repository - Reduces labeling time and cost significantly - Workflow: Automated labels → Crowdsourced verification → Expert review → Training data\nDATOS (DOST-ASTI): - Rapid disaster mapping (10-20 minute response) - On-the-fly labeling during disaster response - Iterative refinement based on ground validation - Integration with LGU feedback\nAcademic Partnerships: - University of the Philippines - remote sensing courses with labeling components - PhilRice - rice field delineation and crop stage labeling - DENR - forest and mangrove mapping with expert foresters\nInternational Support: - Humanitarian OpenStreetMap Team (HOT OSM) for disaster mapping - CoPhil training programs on labeling best practices - European Copernicus expertise transfer\n\n\n\n2025 Examples: Data-Centric Success Stories\n\nNASA-IBM Geospatial Foundation Model (Prithvi)\nOpen-source model trained on massive HLS dataset (Harmonized Landsat-Sentinel-2)\nData-centric approach: - Scale: Millions of satellite images from HLS (30m resolution, global coverage) - Self-supervised pre-training: Masked autoencoding (no labels needed) - Data quality: HLS provides analysis-ready data (atmospheric correction, BRDF normalization, co-registration) - Fine-tuned for specific tasks: With small labeled datasets (100s-1000s samples)\nResult: - State-of-the-art performance on multiple EO tasks (flood mapping, burn scar detection, crop segmentation) - Reduces labeled data requirements by 10-100× - Democratizes access to powerful EO AI - Foundation for operational systems worldwide\nKey Insight: Investment in massive, high-quality pre-training data enables downstream applications with minimal task-specific labels.\n\n\nESA Φsat-2 On-Board AI (Launched 2024)\n22cm CubeSat with on-board AI processing\nData-centric innovation: - Processes imagery directly on satellite - Data quality selection happens in space! - Only transmits actionable information (not raw data) - Cloud filtering: Only clear, usable images sent to Earth - Reduces bandwidth requirements by orders of magnitude - Enables real-time event detection (fires, ships, clouds)\nRationale: With 1,052 active EO satellites generating thousands of terabytes daily, traditional radio frequency communication cannot relay this volume. On-board AI filters data at source.\nImplication: Data quality and relevance prioritized over quantity. Shift from “collect everything” to “collect intelligently.”\n\n\nEarthDaily Constellation\n10-satellite constellation for daily global coverage at 5-10m resolution\nFocus on AI-ready data: - Scientific-grade calibration: Rigorous radiometric accuracy - Consistent, reliable acquisitions: Predictable revisit times - Optimized spectral bands for ML: Bands selected based on ML feature importance - Emphasis on data quality for algorithm performance: Analysis-ready data products\nPhilosophy: Data quality and consistency are first-class design criteria, not afterthoughts. Build satellites around AI needs.\n\n\nWeakAL Framework (Active Learning + Weak Supervision)\nResearch from remote sensing ML community\nApproach: - Combines active learning (select informative samples) with weak supervision (leverage noisy labels) - Computes &gt;90% of labels automatically while maintaining competitive performance - Human effort focused on most uncertain/informative samples\nResults: - 27% improvement in mIoU with only 2% manually labeled data - Demonstrates data-efficient learning - Practical for large-scale operational mapping\nKey Insight: Strategic data selection and semi-automated labeling can achieve strong performance with minimal human effort.",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html#part-7-explainable-ai-xai-for-earth-observation",
    "href": "day1/sessions/session2.html#part-7-explainable-ai-xai-for-earth-observation",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Part 7: Explainable AI (XAI) for Earth Observation",
    "text": "Part 7: Explainable AI (XAI) for Earth Observation\n\nWhy XAI Matters in EO\nThe Problem:\nDeep learning models are often “black boxes” - they produce accurate predictions, but we don’t understand why. For operational EO systems, this creates challenges:\n\nScientific Insights: Can’t extract physical understanding from model decisions\nBias Detection: Can’t identify if model relies on spurious correlations (e.g., cloud shadows, artifacts)\nTrust and Adoption: Stakeholders reluctant to use models they don’t understand\nDebugging: Difficult to diagnose errors and improve models\nRegulatory/Policy: Some applications require explainability (e.g., disaster fund allocation)\n\nRecent Efforts (2023-2025):\nDespite significant advances in deep learning for remote sensing, lack of explainability remains a major criticism. The community is increasingly exploring Explainable AI techniques:\n\nIncreasingly intensive exploration of XAI methods for EO\nIntegration of attention visualization in transformer architectures\nSaliency maps and feature attribution techniques\nTrade-off studies: accuracy vs. interpretability\n\n\n\nXAI Methods for EO\nGradient-Based Methods:\nGrad-CAM (Gradient-weighted Class Activation Mapping): - Process: Compute gradients of target class with respect to final convolutional layer - Output: Heatmap highlighting regions important for prediction - Advantages: Most interpretable method, computationally efficient, works with any CNN - Applications: Visualize which parts of satellite image model focuses on (e.g., “model detects water by focusing on blue spectral signature”)\nGuided Backpropagation: - Visualizes pixels contributing to prediction - Sharper visualizations than Grad-CAM - Highlights fine-grained features\nIntegrated Gradients: - Accumulates gradients along path from baseline to input - More robust attributions than simple gradients - Satisfies desirable axioms (sensitivity, implementation invariance)\nPerturbation-Based Methods:\nOcclusion: - Process: Block image regions and observe prediction change - Output: Sensitivity map showing which regions are critical - Advantages: High interpretability, intuitive - Disadvantages: Computationally expensive (must test many occlusions)\nLIME (Local Interpretable Model-agnostic Explanations): - Process: Train simple, interpretable model (e.g., linear) to approximate complex model locally - Output: Feature importances for specific prediction - Advantages: Model-agnostic, interpretable - Disadvantages: Expensive computation, local rather than global explanation\nModel-Based Methods:\nSHAP (SHapley Additive exPlanations): - Process: Game theory approach - compute contribution of each feature - Output: Feature importance values for prediction - Advantages: Theoretically grounded, consistent - Applications: Explain which spectral bands, indices, or temporal features drive predictions\nAttention Visualization (for Transformers): - Process: Visualize attention weights from self-attention mechanism - Output: Heatmap showing which patches/regions model attends to - Advantages: Built into architecture, interpretable - Applications: Vision Transformers (ViT), UNetFormer - see which spatial regions model focuses on\nFeature Importance (for Tree-Based Models): - Random Forest, XGBoost provide feature importance scores - Output: Ranking of features by contribution to predictions - Advantages: Simple, intuitive, built-in - Applications: Understand which spectral bands, indices, temporal features are most informative\n\n\nApplications in EO\n1. Understanding Model Decisions: - Visualize which spectral bands contribute most (e.g., does model rely on SWIR for burn detection?) - Identify spatial patterns model focuses on (e.g., texture vs. spectral signature) - Discover unexpected correlations (e.g., model using cloud shadows instead of actual land cover)\n2. Discovering Scientific Insights: - Identify which vegetation indices are most predictive for crop types - Understand temporal patterns in multi-date imagery (which dates are critical for classification?) - Extract biophysical relationships learned by model\n3. Detecting and Mitigating Biases: - Identify if model relies on artifacts (e.g., sensor striping, JPEG compression) - Detect geographic biases (model works in training region, fails elsewhere due to spurious features) - Ensure model uses physically meaningful features\n4. Building Trust with Stakeholders: - Demonstrate to policymakers that model decisions are reasonable - Show LGUs which features drive disaster risk predictions - Explain to farmers why certain fields are flagged for attention\n5. Debugging and Improving Models: - Identify when model makes errors (e.g., confuses rice with water due to flooding) - Guide data collection (which features need more training samples?) - Inform feature engineering (which derived features would help?)\n\n\nChallenges and Trade-Offs\nAccuracy vs. Interpretability: - Simple models (decision trees, linear regression) are interpretable but less accurate - Complex models (deep CNNs, transformers) are more accurate but less interpretable - Trade-off: Choose based on application criticality and stakeholder needs\nComputational Cost: - Post-hoc explanation methods (LIME, occlusion) can be expensive - Gradient-based methods (Grad-CAM) are fast - Consider explanation cost for operational systems\nFaithfulness: - Do explanations truly reflect model’s reasoning, or are they misleading? - Saliency maps can be noisy or highlight irrelevant features - Validation: Compare explanations against domain knowledge\nGlobal vs. Local: - Local explanations (single prediction) may not generalize - Global explanations (entire model behavior) are harder to compute and interpret - Need both perspectives for complete understanding\n\n\nBest Practices for XAI in EO\n\nUse Multiple Methods: Different XAI methods can reveal complementary insights\nValidate Explanations: Check against domain knowledge, physical understanding\nIntegrate into Workflow: Make XAI routine part of model development, not afterthought\nCommunicate Effectively: Visualize explanations clearly for stakeholders (heatmaps, feature importance plots)\nDocument Limitations: Be transparent about what explanations can and cannot tell us\nBalance Complexity: For operational systems, consider interpretable models when accuracy difference is small\n\n\n\n\n\n\n\nNoteXAI Resources for EO\n\n\n\nTools: - Captum (PyTorch): Library for model interpretability (Grad-CAM, Integrated Gradients, SHAP) - SHAP Library: SHapley Additive exPlanations for Python - Grad-CAM Implementations: Available for TensorFlow/Keras and PyTorch - Attention Visualization: Built into transformer implementations (HuggingFace Transformers)\nResearch: - “Explainable AI for Earth Observation: A Review” (ongoing research area) - SSL4EO-2024 Summer School included XAI sessions - Growing number of papers combining EO and XAI at IGARSS, ISPRS, ML4Earth conferences",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html#key-takeaways",
    "href": "day1/sessions/session2.html#key-takeaways",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\n\n\n\nImportantSession 2 Summary\n\n\n\n\nCore Concepts\n\nAI/ML learns patterns from data rather than explicit programming - enables automated analysis of massive satellite archives\nThe EO workflow spans problem definition → data acquisition → preprocessing → features → training → validation → deployment\nSupervised learning (classification & regression) is dominant for EO because we need specific outputs; unsupervised (clustering) useful for exploration\n\n\n\nDeep Learning Architectures\n\nCNNs are foundation of EO image analysis - automatic feature extraction, spatial awareness, hierarchical learning\nU-Net excels at semantic segmentation with encoder-decoder + skip connections (e.g., Benguet deforestation: 99.73% accuracy)\nVision Transformers capture global context and long-range dependencies via self-attention (SatViT, MS-CLIP for multi-spectral data)\nLSTMs/RNNs model temporal patterns in time series (PRiSM rice monitoring, crop yield prediction: R² &gt; 0.93)\nObject Detection (YOLO, Faster R-CNN) localize objects with bounding boxes (buildings, ships, vehicles)\nFoundation Models (Prithvi-EO-2.0: 600M parameters) enable fine-tuning with 10-100× less labeled data\n\n\n\nAdvanced Techniques\n\nMulti-modal fusion combines optical + SAR for all-weather monitoring (critical for Philippine monsoon season)\nTransfer learning dramatically reduces data requirements - pre-train on large dataset, fine-tune on small task-specific dataset\nSelf-supervised learning pre-trains on unlabeled data via masked autoencoding (Prithvi) or contrastive learning\n\n\n\nBenchmark Datasets\n\nEuroSAT (27,000 images, 10 classes, 98.57% accuracy), BigEarthNet (549,488 patches, multi-modal), xView (&gt;1M objects, 60 classes)\nBenchmarks enable standardized evaluation, provide training resources, support transfer learning\n\n\n\nData-Centric AI (2025 Paradigm)\n\nData quality &gt; model complexity: Improving data from 85% → 95% accuracy beats endless model tuning\nFour Pillars: Quality (accurate, consistent, properly processed), Quantity (sufficient samples, augmentation), Diversity (geographic, temporal, class, sensor), Annotation (strategic, high-quality labeling)\nPhilippine Solutions: DOST-ASTI ALaM (Automated Labeling Machine), DIMER model repository, active learning\n\n\n\nExplainable AI\n\nXAI crucial for operational systems: Builds trust, enables debugging, extracts scientific insights, detects biases\nMethods: Grad-CAM (heatmaps), SHAP (feature importance), Attention visualization (transformers)\n\n\n\nPhilippine Operational Context\n\nDATOS (DOST-ASTI): 10-20 minute AI-powered flood mapping from Sentinel-1 SAR\nPRiSM (PhilRice-IRRI): Operational since 2014, all-weather rice monitoring combining SAR + optical\nPhilSA-DENR: Nationwide mangrove mapping with U-Net (99.73% accuracy)\nCoPhil Data Centre (2025): Local, high-bandwidth access to Sentinel data, cloud-native distribution\nLeverage existing infrastructure: DIMER, AIPI, ALaM, CoPhil to operationalize AI/ML workflows\n\nNext steps: Hands-on Python for geospatial data (Session 3) and Google Earth Engine (Session 4) to put these concepts into practice!",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html#discussion-questions",
    "href": "day1/sessions/session2.html#discussion-questions",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Discussion Questions",
    "text": "Discussion Questions\n\n\n\n\n\n\nTipReflect & Discuss\n\n\n\n\nWhat EO problem in your work could benefit from ML? Is it classification, regression, segmentation, or object detection? Which architecture would you choose?\nData quality in Philippine context: How do you address cloud cover, temporal dynamics, and atmospheric effects in your satellite data?\nFoundation models: How could Prithvi-EO-2.0 or other pre-trained models reduce barriers for your organization? What Philippine-specific fine-tuning would be needed?\nMulti-modal fusion: When would you combine Sentinel-2 optical with Sentinel-1 SAR? What are practical challenges?\nData-centric approach: What are biggest data quality issues you face? How could ALaM or active learning help?\nBenchmark datasets: Which international datasets could you use for pre-training? How to ensure models generalize to Philippines?\nExplainable AI: For your application, why would explainability matter? Which XAI method would you use?\nDIMER and AIPI platforms: How might these reduce barriers to deploying ML in your organization? What models would you contribute or use?\nTemporal modeling: For what applications would LSTM or temporal attention be valuable? What data would you need?\nCoPhil opportunities: How can you leverage the upcoming Data Centre and training programs? What collaborations would be valuable?",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/sessions/session2.html#further-reading",
    "href": "day1/sessions/session2.html#further-reading",
    "title": "Session 2: Core Concepts of AI/ML for Earth Observation",
    "section": "Further Reading",
    "text": "Further Reading\n\nFoundational Concepts\n\nNASA ARSET: Fundamentals of Machine Learning for Earth Science\nData-Centric AI: Better, Not Just More\nVan der Schaar Lab: What is Data-Centric AI?\n\n\n\nDeep Learning Architectures\n\nDeep Learning Book (Goodfellow et al.) - Free online\nNeural Networks and Deep Learning (Nielsen) - Interactive tutorial\nSatellite Image Deep Learning Techniques - Comprehensive GitHub repository\n\n\n\nDeep Learning for EO\n\nDeep Learning for Land Use and Land Cover Classification - 2020 review\nDeep Learning for Remote Sensing Image Segmentation - 2024 review\nObject Detection and Image Segmentation with Deep Learning on EO Data\n\n\n\nFoundation Models\n\nIBM-NASA Prithvi Models on Hugging Face\nPrithvi-EO-2.0: A Versatile Multi-Temporal Foundation Model\nIBM Research: Prithvi-EO-2.0 Blog\n\n\n\nSelf-Supervised Learning\n\nSSL4EO-2024 Summer School Review\nMulti-Label Guided Soft Contrastive Learning for EO\n\n\n\nData-Centric AI\n\nData-Centric Machine Learning for Earth Observation\nKili Technology: Earth Observation Data Labeling Guide\n\n\n\nExplainable AI\n\nCaptum: Model Interpretability for PyTorch\nSHAP Library Documentation\n\n\n\nEO-Specific ML\n\nEO College: Introduction to Machine Learning for Earth Observation\nML4Earth Resources\nClimate Change AI: Earth Observation & Monitoring\nA Review of Practical AI for Remote Sensing in Earth Sciences - 2023\n\n\n\nBenchmark Datasets\n\nEuroSAT GitHub\nBigEarthNet Website\nxView Dataset\nDOTA: Dataset for Object Detection in Aerial Images\n\n\n\nPhilippine AI Initiatives\n\nDOST-ASTI: Remote Sensing and Data Science (DATOS) Help Desk\nPhilippine News Agency: DOST AI R&D Projects - SkAI-Pinas, DIMER, AIPI\nPRiSM: Philippine Rice Information System\nPhilSA: Philippine Space Agency\nCoPhil Centre\n\n\n\nRecent Advances\n\nArtificial Intelligence to Advance Earth Observation: A Review - 2023\nAdvancing Earth Observation with AI - 2025\nESA AI for Earth Observation\nAwesome Earth Observation Code",
    "crumbs": [
      "Sessions",
      "Session 2: Core Concepts of AI/ML for Earth Observation"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook1.html",
    "href": "day1/notebooks/notebook1.html",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "",
    "text": "This notebook accompanies Session 3: Python for Geospatial Data. You’ll learn to work with vector data using GeoPandas and raster data using Rasterio.\n\n\nBy completing this notebook, you will:\n\nLoad and visualize vector data with GeoPandas\nWork with different vector formats (Shapefiles, GeoJSON, GeoPackage)\nPerform coordinate reference system transformations\nRead and process raster data with Rasterio\nExtract band information and metadata\nVisualize multi-band imagery\nApply the concepts to Philippine data",
    "crumbs": [
      "Notebooks",
      "Notebook 1: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook1.html#session-3-hands-on-notebook",
    "href": "day1/notebooks/notebook1.html#session-3-hands-on-notebook",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "",
    "text": "This notebook accompanies Session 3: Python for Geospatial Data. You’ll learn to work with vector data using GeoPandas and raster data using Rasterio.\n\n\nBy completing this notebook, you will:\n\nLoad and visualize vector data with GeoPandas\nWork with different vector formats (Shapefiles, GeoJSON, GeoPackage)\nPerform coordinate reference system transformations\nRead and process raster data with Rasterio\nExtract band information and metadata\nVisualize multi-band imagery\nApply the concepts to Philippine data",
    "crumbs": [
      "Notebooks",
      "Notebook 1: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook1.html#getting-started",
    "href": "day1/notebooks/notebook1.html#getting-started",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Getting Started",
    "text": "Getting Started\n\nOption 1: Open in Google Colab (Recommended)\nClick the button below to open this notebook in Google Colab:\n\n\n\n\nOpen In Colab\n\n\n\n\n\n\n\n\n\nNoteFirst Time Using This Notebook?\n\n\n\nIf you get a “Not Found” error: 1. The notebook files need to be pushed to GitHub first 2. Alternative: Download the notebook below and upload to your own Google Drive 3. Then open from Drive in Colab\n\n\nAdvantages: - No installation required - Free GPU access - Auto-saves to Google Drive - Pre-configured environment\n\n\nOption 2: Download Notebook\nDownload the Jupyter notebook to run locally or upload to your own Colab:\n\nDownload .ipynb File\nRequirements for local use:\npip install numpy pandas matplotlib geopandas rasterio\n\n\n\nOption 3: View Online\nYou can also view the notebook content below without running any code.",
    "crumbs": [
      "Notebooks",
      "Notebook 1: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook1.html#notebook-preview",
    "href": "day1/notebooks/notebook1.html#notebook-preview",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Notebook Preview",
    "text": "Notebook Preview\n\n\n\n\n\n\nNoteInteractive Execution Required\n\n\n\nThis is a hands-on exercise notebook. For the best learning experience, open it in Google Colab or Jupyter to run the code cells interactively.\n\n\n\nTopics Covered\n\nIntroduction to GeoPandas\n\nReading vector data\nExploring GeoDataFrames\nCoordinate reference systems\nSpatial operations\n\nWorking with Philippine Data\n\nLoading administrative boundaries\nFiltering regions and provinces\nCalculating areas and centroids\nCreating maps\n\nIntroduction to Rasterio\n\nReading raster data\nUnderstanding raster metadata\nExtracting bands\nVisualizing imagery\n\nPalawan Case Study\n\nSentinel-2 imagery analysis\nLand cover visualization\nSpectral band combinations\nNDVI calculation",
    "crumbs": [
      "Notebooks",
      "Notebook 1: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook1.html#prerequisites",
    "href": "day1/notebooks/notebook1.html#prerequisites",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore starting this notebook, ensure you have:\n\nCompleted the Setup Guide\nGoogle account (for Colab)\nBasic Python knowledge\nUnderstanding of Session 3 concepts",
    "crumbs": [
      "Notebooks",
      "Notebook 1: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook1.html#notebook-contents",
    "href": "day1/notebooks/notebook1.html#notebook-contents",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Notebook Contents",
    "text": "Notebook Contents\nThe full interactive notebook includes:\n\n15+ code cells with detailed explanations\n10+ visualizations of vector and raster data\nExercises to test your understanding\nPhilippine case studies using real data\nTroubleshooting tips for common issues",
    "crumbs": [
      "Notebooks",
      "Notebook 1: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook1.html#support",
    "href": "day1/notebooks/notebook1.html#support",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Support",
    "text": "Support\n\nDuring the Training\n\nAsk questions in the live session\nConsult teaching assistants\nWork through exercises at your own pace\n\n\n\nAfter the Training\n\nReview the Cheat Sheets\nCheck the FAQ\nAccess the Glossary",
    "crumbs": [
      "Notebooks",
      "Notebook 1: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook1.html#related-resources",
    "href": "day1/notebooks/notebook1.html#related-resources",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Related Resources",
    "text": "Related Resources\n\nSession Materials: - Session 3: Python for Geospatial Data - Session 3 Presentation Slides\nQuick References: - GeoPandas Cheat Sheet - Rasterio Cheat Sheet\nDocumentation: - GeoPandas Documentation - Rasterio Documentation",
    "crumbs": [
      "Notebooks",
      "Notebook 1: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/notebook1.html#next-steps",
    "href": "day1/notebooks/notebook1.html#next-steps",
    "title": "Notebook 1: Python for Geospatial Data",
    "section": "Next Steps",
    "text": "Next Steps\nAfter completing this notebook:\n\n✅ Practice with your own Philippine data\n✅ Move on to Session 4: Google Earth Engine\n✅ Try Notebook 2: Google Earth Engine\n\n\nReady to code? Open the notebook in Colab and start learning!",
    "crumbs": [
      "Notebooks",
      "Notebook 1: Python for Geospatial Data"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#learning-objectives",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#learning-objectives",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this session, you will be able to:\n\nAuthenticate and initialize Google Earth Engine (GEE) in Python\nUnderstand GEE core concepts (Image, ImageCollection, Feature, FeatureCollection)\nAccess and filter Sentinel-1 and Sentinel-2 data collections\nApply cloud masking and create temporal composites\nCalculate spectral indices (NDVI) at scale\nExport processed data for use in AI/ML workflows\nApply GEE best practices for computational efficiency",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#why-google-earth-engine",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#why-google-earth-engine",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "Why Google Earth Engine?",
    "text": "Why Google Earth Engine?\nGoogle Earth Engine provides:\n\nPetabyte-scale data catalog: Sentinel-1, Sentinel-2, Landsat, MODIS, and more\nCloud computing: Process data without downloading\nPlanetary-scale analysis: Analyze entire countries or continents\nFree access: For research and education\n\nPerfect for preparing training data for AI/ML models!",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#prerequisites",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#prerequisites",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nCompletion of Session 3 (Python geospatial basics)\nGoogle Earth Engine account (sign up at https://earthengine.google.com/)\nBasic understanding of Sentinel-1 and Sentinel-2 missions",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#setup-and-authentication",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#setup-and-authentication",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "1. Setup and Authentication",
    "text": "1. Setup and Authentication\n\n1.1 Install Earth Engine API\n\n# Install Earth Engine Python API\n!pip install earthengine-api -q\n\nprint(\"Earth Engine API installed successfully!\")\n\n\n\n1.2 Authentication\nImportant: You need to authenticate once per environment. Follow the instructions that appear:\n\nClick the authentication link\nSelect your Google account\nGrant permissions\nCopy the authorization code\nPaste it back into the notebook\n\n\nimport ee\n\n# Authenticate Earth Engine\n# This will prompt you to click a link and authenticate\ntry:\n    ee.Authenticate()\n    print(\"✓ Authentication successful!\")\nexcept Exception as e:\n    print(f\"⚠ Authentication error: {e}\")\n    print(\"\\nPlease follow these steps:\")\n    print(\"1. Click the authentication link above\")\n    print(\"2. Sign in with your Google account\")\n    print(\"3. Grant Earth Engine permissions\")\n    print(\"4. Copy the authorization code\")\n\n\n\n1.3 Initialize Earth Engine\n\n# Initialize Earth Engine\ntry:\n    ee.Initialize()\n    print(\"✓ Earth Engine initialized successfully!\")\n    print(f\"  Earth Engine Python API version: {ee.__version__}\")\nexcept Exception as e:\n    print(f\"❌ Initialization error: {e}\")\n    print(\"\\nTroubleshooting:\")\n    print(\"1. Make sure you completed authentication in the previous cell\")\n    print(\"2. Run ee.Authenticate() again if needed\")\n    print(\"3. Check your internet connection\")\n    print(\"4. Verify you have an approved Earth Engine account at:\")\n    print(\"   https://earthengine.google.com/\")\n\n\n\n1.4 Import Additional Libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display\nimport datetime\n\n# Set visualization defaults\nplt.rcParams['figure.figsize'] = (12, 8)\n\nprint(\"All libraries imported successfully!\")\n\n\n\nTroubleshooting Authentication\nIf you encounter issues:\n\nNot signed up? Register at https://earthengine.google.com/\nPermission errors? Ensure you’re using the correct Google account\nAlready authenticated? Skip authentication and go directly to ee.Initialize()",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#gee-core-concepts",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#gee-core-concepts",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "2. GEE Core Concepts",
    "text": "2. GEE Core Concepts\n\n2.1 Geometry Objects\nEarth Engine uses geometries to define areas of interest (AOI).\n\n# Create a Point geometry (Metro Manila)\nmanila_point = ee.Geometry.Point([121.0, 14.6])\nprint(\"Manila Point:\", manila_point.getInfo())\n\n# Create a Rectangle (Palawan)\npalawan_bbox = ee.Geometry.Rectangle([117.5, 8.5, 119.5, 11.5])\nprint(\"\\nPalawan Bounding Box:\", palawan_bbox.getInfo())\n\n# Create a Polygon (custom AOI)\ncustom_polygon = ee.Geometry.Polygon([\n    [[120.8, 14.4], [121.2, 14.4], [121.2, 14.8], [120.8, 14.8], [120.8, 14.4]]\n])\nprint(\"\\nCustom Polygon:\", custom_polygon.getInfo())\n\n# Buffer around point (10 km)\nmanila_buffer = manila_point.buffer(10000)  # meters\nprint(\"\\nManila 10km Buffer area (km²):\", manila_buffer.area().divide(1e6).getInfo())\n\n\n\n2.2 Image and ImageCollection\n\nImage: A single raster image (one scene)\nImageCollection: A stack/time-series of images\n\n\n# Access Sentinel-2 Surface Reflectance collection\ns2_collection = ee.ImageCollection('COPERNICUS/S2_SR')\n\n# Get collection size (total images available - this will be very large!)\n# Note: This queries the entire global archive, so we'll filter first\n\n# Filter to Manila area for 2024\nmanila_s2 = s2_collection.filterBounds(manila_point) \\\n                         .filterDate('2024-01-01', '2024-12-31')\n\nprint(f\"Sentinel-2 images over Manila in 2024: {manila_s2.size().getInfo()}\")\n\n# Get the first image\nfirst_image = manila_s2.first()\nprint(f\"\\nFirst image ID: {first_image.get('system:id').getInfo()}\")\nprint(f\"Acquisition date: {ee.Date(first_image.get('system:time_start')).format('YYYY-MM-dd').getInfo()}\")\n\n\n\n2.3 Feature and FeatureCollection\nFeatures are vector data (points, lines, polygons with attributes).\n\n# Create a Feature (point with properties)\nmanila_feature = ee.Feature(\n    manila_point,\n    {'name': 'Metro Manila', 'population': 13000000, 'type': 'capital'}\n)\n\nprint(\"Manila Feature properties:\", manila_feature.getInfo()['properties'])\n\n# Create a FeatureCollection\ncities = ee.FeatureCollection([\n    ee.Feature(ee.Geometry.Point([121.0, 14.6]), {'name': 'Manila', 'pop': 13000000}),\n    ee.Feature(ee.Geometry.Point([125.6, 7.1]), {'name': 'Davao', 'pop': 1800000}),\n    ee.Feature(ee.Geometry.Point([123.9, 10.3]), {'name': 'Cebu', 'pop': 3000000})\n])\n\nprint(f\"\\nNumber of cities: {cities.size().getInfo()}\")\n\n\n\n2.4 Filters and Reducers\nFilters select subsets of collections. Reducers aggregate or summarize data.\n\n# Filtering examples\nfiltered = s2_collection \\\n    .filterBounds(palawan_bbox) \\\n    .filterDate('2024-06-01', '2024-08-31') \\\n    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 20)\n\nprint(f\"Filtered images (Palawan, Jun-Aug 2024, &lt;20% clouds): {filtered.size().getInfo()}\")\n\n# Reducer examples\n# Create a median composite (reduces time dimension)\nmedian_composite = filtered.median()\n\n# Calculate mean NDVI over region (reduces spatial dimension)\n# We'll do this in detail later\nprint(\"\\nReducers allow you to:\")\nprint(\"  - Temporal: mean(), median(), max(), min() across time\")\nprint(\"  - Spatial: reduceRegion() for statistics over an area\")",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#working-with-sentinel-2",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#working-with-sentinel-2",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "3. Working with Sentinel-2",
    "text": "3. Working with Sentinel-2\n\n3.1 Define Area of Interest (AOI)\nWe’ll focus on Palawan Province - important for Natural Resource Management.\n\n# Define Palawan AOI\naoi = ee.Geometry.Rectangle([117.8, 9.0, 119.2, 10.8])\n\n# Calculate AOI area\naoi_area_km2 = aoi.area().divide(1e6).getInfo()\nprint(f\"AOI Area: {aoi_area_km2:.2f} km²\")\n\n# Visualize AOI bounds\nbounds = aoi.bounds().getInfo()['coordinates'][0]\nprint(f\"AOI Bounds: {bounds}\")\n\n\n\n3.2 Access Sentinel-2 Collection\n\n# Define date range (2024 dry season - less clouds)\nstart_date = '2024-01-01'\nend_date = '2024-03-31'\n\n# Access Sentinel-2 Surface Reflectance\ns2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(aoi) \\\n    .filterDate(start_date, end_date) \\\n    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 30)\n\nprint(f\"Sentinel-2 images found: {s2.size().getInfo()}\")\n\n# List image dates and cloud cover\ndef get_image_info(image):\n    date = ee.Date(image.get('system:time_start')).format('YYYY-MM-dd')\n    clouds = image.get('CLOUDY_PIXEL_PERCENTAGE')\n    return ee.Feature(None, {'date': date, 'clouds': clouds})\n\nimage_info = s2.map(get_image_info).getInfo()['features']\n\nprint(\"\\nAvailable images:\")\nprint(f\"{'Date':&lt;15} {'Cloud %':&gt;10}\")\nprint(\"-\" * 25)\nfor info in image_info[:10]:  # Show first 10\n    props = info['properties']\n    print(f\"{props['date']:&lt;15} {props['clouds']:&gt;10.1f}\")\n\nif len(image_info) &gt; 10:\n    print(f\"... and {len(image_info) - 10} more images\")\n\n\n\n3.3 Cloud Masking Function\nSentinel-2 Level-2A includes quality bands for cloud masking.\n\ndef maskS2clouds(image):\n    \"\"\"\n    Mask clouds and cirrus in Sentinel-2 imagery using QA60 band.\n    \n    QA60 is a bitmask band:\n    - Bit 10: Opaque clouds\n    - Bit 11: Cirrus clouds\n    \n    Parameters:\n    -----------\n    image : ee.Image\n        Sentinel-2 Level-2A image\n    \n    Returns:\n    --------\n    ee.Image : Cloud-masked image\n    \"\"\"\n    qa = image.select('QA60')\n    \n    # Bits 10 and 11 are clouds and cirrus, respectively\n    cloudBitMask = 1 &lt;&lt; 10\n    cirrusBitMask = 1 &lt;&lt; 11\n    \n    # Both flags should be set to zero, indicating clear conditions\n    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(\n           qa.bitwiseAnd(cirrusBitMask).eq(0))\n    \n    return image.updateMask(mask).copyProperties(image, ['system:time_start'])\n\nprint(\"Cloud masking function defined!\")\nprint(\"This function will:\")\nprint(\"  1. Read the QA60 quality band\")\nprint(\"  2. Check bits 10 (clouds) and 11 (cirrus)\")\nprint(\"  3. Mask pixels where either bit is set\")\nprint(\"  4. Preserve image metadata\")\n\n\n\n3.4 Apply Cloud Masking and Create Composite\n\n# Apply cloud mask to all images\ns2_masked = s2.map(maskS2clouds)\n\nprint(f\"Cloud masking applied to {s2_masked.size().getInfo()} images\")\n\n# Create median composite\ncomposite = s2_masked.median().clip(aoi)\n\nprint(\"\\nMedian composite created!\")\nprint(\"Why median?\")\nprint(\"  - Robust to outliers (remaining clouds, shadows)\")\nprint(\"  - Better than mean for temporal composites\")\nprint(\"  - Produces clean, cloud-free images\")\n\n\n\n3.5 Visualize with Thumbnail\nEarth Engine can generate quick preview images.\n\n# Define visualization parameters for True Color (RGB)\nvis_params_rgb = {\n    'bands': ['B4', 'B3', 'B2'],  # Red, Green, Blue\n    'min': 0,\n    'max': 3000,\n    'gamma': 1.4  # Enhance contrast\n}\n\n# Get thumbnail URL\nthumbnail_url = composite.getThumbURL({\n    'region': aoi,\n    'dimensions': 512,\n    **vis_params_rgb\n})\n\nprint(\"True Color Composite (Sentinel-2 RGB):\")\ndisplay(Image(url=thumbnail_url))\n\n\n# False Color Composite (NIR, Red, Green) - highlights vegetation\nvis_params_false = {\n    'bands': ['B8', 'B4', 'B3'],  # NIR, Red, Green\n    'min': 0,\n    'max': 4000,\n    'gamma': 1.4\n}\n\nthumbnail_url_false = composite.getThumbURL({\n    'region': aoi,\n    'dimensions': 512,\n    **vis_params_false\n})\n\nprint(\"False Color Composite (NIR-R-G) - Vegetation appears RED:\")\ndisplay(Image(url=thumbnail_url_false))\n\n\n\n3.6 Calculate NDVI\nNDVI = (NIR - Red) / (NIR + Red)\n\n# Calculate NDVI using normalized difference\nndvi = composite.normalizedDifference(['B8', 'B4']).rename('NDVI')\n\nprint(\"NDVI calculated!\")\n\n# Get NDVI statistics over AOI\nndvi_stats = ndvi.reduceRegion(\n    reducer=ee.Reducer.mean().combine(\n        reducer2=ee.Reducer.minMax(),\n        sharedInputs=True\n    ),\n    geometry=aoi,\n    scale=10,  # 10m resolution\n    maxPixels=1e9\n).getInfo()\n\nprint(\"\\nNDVI Statistics:\")\nprint(f\"  Mean: {ndvi_stats['NDVI_mean']:.3f}\")\nprint(f\"  Min:  {ndvi_stats['NDVI_min']:.3f}\")\nprint(f\"  Max:  {ndvi_stats['NDVI_max']:.3f}\")\n\n\n# Visualize NDVI\nvis_params_ndvi = {\n    'bands': ['NDVI'],\n    'min': -0.2,\n    'max': 0.8,\n    'palette': ['blue', 'white', 'yellow', 'green', 'darkgreen']\n}\n\nthumbnail_url_ndvi = ndvi.getThumbURL({\n    'region': aoi,\n    'dimensions': 512,\n    **vis_params_ndvi\n})\n\nprint(\"NDVI (Normalized Difference Vegetation Index):\")\nprint(\"Blue/White: Water/Bare soil\")\nprint(\"Yellow: Sparse vegetation\")\nprint(\"Green: Moderate vegetation\")\nprint(\"Dark Green: Dense vegetation\\n\")\ndisplay(Image(url=thumbnail_url_ndvi))\n\n\n\nExercise 1: Change Location and Dates\nTask: Modify the code to analyze a different Philippine location and time period.\nSuggestions: - Metro Manila: [120.9, 14.4, 121.1, 14.7] - Mindanao (Davao): [125.3, 6.9, 125.7, 7.3] - Cebu: [123.7, 10.2, 124.0, 10.5]\nTry different seasons: - Dry season: January-May - Wet season: June-November\n\n# Your code here\n# Example: Metro Manila during wet season\n\n# Define new AOI\nmanila_aoi = ee.Geometry.Rectangle([120.9, 14.4, 121.1, 14.7])\n\n# New date range (wet season)\nnew_start = '2024-07-01'\nnew_end = '2024-09-30'\n\n# Query Sentinel-2\nmanila_s2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(manila_aoi) \\\n    .filterDate(new_start, new_end) \\\n    .filterMetadata('CLOUDY_PIXEL_PERCENTAGE', 'less_than', 30) \\\n    .map(maskS2clouds)\n\nprint(f\"Images found: {manila_s2.size().getInfo()}\")\n\n# Create composite\nmanila_composite = manila_s2.median().clip(manila_aoi)\n\n# Visualize\nmanila_thumb = manila_composite.getThumbURL({\n    'region': manila_aoi,\n    'dimensions': 512,\n    **vis_params_rgb\n})\n\nprint(\"\\nMetro Manila True Color Composite:\")\ndisplay(Image(url=manila_thumb))",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#working-with-sentinel-1-sar",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#working-with-sentinel-1-sar",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "4. Working with Sentinel-1 SAR",
    "text": "4. Working with Sentinel-1 SAR\nSentinel-1 provides all-weather, day-night radar imagery - essential for the Philippines’ cloudy tropical climate!\n\n4.1 Access Sentinel-1 Collection\n\n# Define parameters\nsar_aoi = palawan_bbox\nsar_start = '2024-01-01'\nsar_end = '2024-03-31'\n\n# Access Sentinel-1 GRD (Ground Range Detected)\ns1 = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n    .filterBounds(sar_aoi) \\\n    .filterDate(sar_start, sar_end) \\\n    .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n    .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING')) \\\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n    .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VH'))\n\nprint(f\"Sentinel-1 images found: {s1.size().getInfo()}\")\n\nprint(\"\\nFilters applied:\")\nprint(\"  - Instrument Mode: IW (Interferometric Wide swath)\")\nprint(\"  - Orbit: Descending (evening pass)\")\nprint(\"  - Polarization: VV and VH (dual-pol)\")\nprint(\"\\nWhy these filters?\")\nprint(\"  - IW mode: Standard for land monitoring (250km swath)\")\nprint(\"  - Descending: Consistent geometry\")\nprint(\"  - VV/VH: Sensitive to different surface properties\")\n\n\n\n4.2 Create SAR Composite\nFor SAR, we use mean to reduce speckle noise.\n\n# Select VV and VH bands\ns1_composite = s1.select(['VV', 'VH']).mean().clip(sar_aoi)\n\nprint(\"SAR composite created using mean (reduces speckle)\")\n\n# Calculate VV/VH ratio (useful for land cover)\nvv_vh_ratio = s1_composite.select('VV').divide(s1_composite.select('VH')).rename('VV_VH_ratio')\n\nprint(\"VV/VH ratio calculated (useful for classification)\")\n\n\n\n4.3 Visualize SAR Data\n\n# VV polarization visualization\nvis_params_vv = {\n    'bands': ['VV'],\n    'min': -25,\n    'max': 0\n}\n\nsar_thumb_vv = s1_composite.getThumbURL({\n    'region': sar_aoi,\n    'dimensions': 512,\n    **vis_params_vv\n})\n\nprint(\"Sentinel-1 VV Polarization (dB):\")\nprint(\"Dark areas: Water, calm surfaces (low backscatter)\")\nprint(\"Bright areas: Urban, rough surfaces (high backscatter)\\n\")\ndisplay(Image(url=sar_thumb_vv))\n\n\n# False color SAR (VV, VH, VV/VH ratio)\nsar_false_color = ee.Image.cat([\n    s1_composite.select('VV'),\n    s1_composite.select('VH'),\n    vv_vh_ratio\n])\n\nvis_params_sar_false = {\n    'min': [-25, -25, 0],\n    'max': [0, 0, 2]\n}\n\nsar_thumb_false = sar_false_color.getThumbURL({\n    'region': sar_aoi,\n    'dimensions': 512,\n    **vis_params_sar_false\n})\n\nprint(\"Sentinel-1 False Color (VV-VH-Ratio):\")\ndisplay(Image(url=sar_thumb_false))\n\n\n\nExercise 2: Compare VV and VH Polarizations\nTask: Create side-by-side visualizations of VV and VH polarizations.\nHint: VH is often more sensitive to vegetation volume.\n\n# Your code here\n# Solution:\n\n# VH polarization\nvis_params_vh = {\n    'bands': ['VH'],\n    'min': -25,\n    'max': 0\n}\n\nsar_thumb_vh = s1_composite.getThumbURL({\n    'region': sar_aoi,\n    'dimensions': 512,\n    **vis_params_vh\n})\n\nprint(\"VV Polarization:\")\ndisplay(Image(url=sar_thumb_vv))\n\nprint(\"\\nVH Polarization (more sensitive to vegetation structure):\")\ndisplay(Image(url=sar_thumb_vh))\n\nprint(\"\\nKey Differences:\")\nprint(\"  - VV: Better for water detection, urban areas\")\nprint(\"  - VH: Better for vegetation, forest structure\")\nprint(\"  - Using both improves classification accuracy!\")",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#data-export-for-aiml-workflows",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#data-export-for-aiml-workflows",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "5. Data Export for AI/ML Workflows",
    "text": "5. Data Export for AI/ML Workflows\nTo train custom ML models, we need to export data from Earth Engine.\n\n5.1 Export Image to Google Drive\n\n# Export Sentinel-2 composite\nexport_task_s2 = ee.batch.Export.image.toDrive(\n    image=composite.select(['B2', 'B3', 'B4', 'B8']),  # Select bands to export\n    description='Palawan_S2_Composite_Q1_2024',\n    folder='EarthEngine_Exports',\n    fileNamePrefix='palawan_s2_composite',\n    region=aoi,\n    scale=10,  # 10m resolution\n    crs='EPSG:4326',\n    maxPixels=1e9\n)\n\n# Start the export task\nexport_task_s2.start()\n\nprint(\"Export task started!\")\nprint(f\"Task ID: {export_task_s2.id}\")\nprint(\"\\nExport details:\")\nprint(f\"  - Destination: Google Drive/EarthEngine_Exports/\")\nprint(f\"  - Filename: palawan_s2_composite.tif\")\nprint(f\"  - Bands: B2, B3, B4, B8 (Blue, Green, Red, NIR)\")\nprint(f\"  - Resolution: 10m\")\nprint(f\"  - Format: GeoTIFF\")\nprint(\"\\nMonitor status at: https://code.earthengine.google.com/tasks\")\n\n\n\n5.2 Check Export Status\n\n# Check task status\ntask_status = export_task_s2.status()\nprint(f\"Task Status: {task_status['state']}\")\n\nif task_status['state'] == 'RUNNING':\n    print(\"Task is running... Check back in a few minutes.\")\nelif task_status['state'] == 'COMPLETED':\n    print(\"Task completed! Check your Google Drive.\")\nelif task_status['state'] == 'FAILED':\n    print(f\"Task failed: {task_status.get('error_message', 'Unknown error')}\")\nelse:\n    print(f\"Task state: {task_status['state']}\")\n\n\n\n5.3 Export NDVI\n\n# Export NDVI layer\nexport_task_ndvi = ee.batch.Export.image.toDrive(\n    image=ndvi,\n    description='Palawan_NDVI_Q1_2024',\n    folder='EarthEngine_Exports',\n    fileNamePrefix='palawan_ndvi',\n    region=aoi,\n    scale=10,\n    crs='EPSG:4326',\n    maxPixels=1e9\n)\n\nexport_task_ndvi.start()\n\nprint(f\"NDVI export started!\")\nprint(f\"Task ID: {export_task_ndvi.id}\")\n\n\n\n5.4 Export Training Samples (for ML)\nFor ML model training, we often need to export training samples as vectors.\n\n# Create sample points for different land cover types\n# In practice, you would digitize these in GEE Code Editor or use existing data\n\n# Example: Random sample points\nsample_points = composite.sample(\n    region=aoi,\n    scale=30,  # Sample every 30m\n    numPixels=1000,  # Number of samples\n    seed=42  # For reproducibility\n)\n\nprint(f\"Generated {sample_points.size().getInfo()} sample points\")\n\n# Export samples to Drive as CSV\nexport_task_samples = ee.batch.Export.table.toDrive(\n    collection=sample_points,\n    description='Palawan_Training_Samples',\n    folder='EarthEngine_Exports',\n    fileNamePrefix='palawan_samples',\n    fileFormat='CSV'\n)\n\nexport_task_samples.start()\n\nprint(f\"\\nSample points export started!\")\nprint(f\"Task ID: {export_task_samples.id}\")\nprint(\"\\nThese samples can be used for:\")\nprint(\"  - Training ML classifiers\")\nprint(\"  - Validating model predictions\")\nprint(\"  - Feature engineering\")\n\n\n\nExercise 3: Export Custom AOI Composite\nTask: Export a composite for your chosen location from Exercise 1.\nRequirements: - Use your custom AOI - Export RGB bands (B2, B3, B4) - 10m resolution - Give it a meaningful filename\n\n# Your code here\n# Solution template:\n\nmy_export_task = ee.batch.Export.image.toDrive(\n    image=manila_composite.select(['B2', 'B3', 'B4']),\n    description='My_Custom_Export',\n    folder='EarthEngine_Exports',\n    fileNamePrefix='my_custom_composite',\n    region=manila_aoi,\n    scale=10,\n    crs='EPSG:4326',\n    maxPixels=1e9\n)\n\nmy_export_task.start()\nprint(f\"Custom export started! Task ID: {my_export_task.id}\")",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#integration-with-aiml-workflows",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#integration-with-aiml-workflows",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "6. Integration with AI/ML Workflows",
    "text": "6. Integration with AI/ML Workflows\n\n6.1 Preparing Training Data\nEarth Engine excels at preparing analysis-ready data for ML.\n\n# Create a multi-band image stack for ML\nml_stack = composite.select(['B2', 'B3', 'B4', 'B8', 'B11', 'B12']) \\\n                    .addBands(ndvi) \\\n                    .addBands(s1_composite.select(['VV', 'VH']))\n\nprint(\"ML-ready image stack created!\")\nprint(\"\\nBands included:\")\nband_names = ml_stack.bandNames().getInfo()\nfor i, band in enumerate(band_names, 1):\n    print(f\"  {i}. {band}\")\n\nprint(\"\\nWhy this combination?\")\nprint(\"  - Optical bands (B2-B12): Spectral information\")\nprint(\"  - NDVI: Vegetation index\")\nprint(\"  - SAR (VV, VH): All-weather information\")\nprint(\"  - Multi-sensor fusion improves classification!\")\n\n\n\n6.2 Sampling for Training\nExtract feature vectors for ML model training.\n\n# Define training regions (in practice, digitize or load from shapefile)\n# For demonstration, we'll create simple point collections\n\n# Forest training points\nforest_points = ee.FeatureCollection([\n    ee.Feature(ee.Geometry.Point([118.5, 10.2]), {'landcover': 0, 'class_name': 'Forest'}),\n    ee.Feature(ee.Geometry.Point([118.6, 10.3]), {'landcover': 0, 'class_name': 'Forest'}),\n    ee.Feature(ee.Geometry.Point([118.4, 10.1]), {'landcover': 0, 'class_name': 'Forest'})\n])\n\n# Water training points\nwater_points = ee.FeatureCollection([\n    ee.Feature(ee.Geometry.Point([118.2, 9.5]), {'landcover': 1, 'class_name': 'Water'}),\n    ee.Feature(ee.Geometry.Point([118.3, 9.6]), {'landcover': 1, 'class_name': 'Water'})\n])\n\n# Merge training points\ntraining_points = forest_points.merge(water_points)\n\n# Sample image at training points\ntraining_data = ml_stack.sampleRegions(\n    collection=training_points,\n    properties=['landcover', 'class_name'],\n    scale=10\n)\n\nprint(f\"Training samples created: {training_data.size().getInfo()}\")\nprint(\"\\nSample features:\")\nsample = training_data.first().getInfo()\nprint(f\"Properties: {list(sample['properties'].keys())}\")\n\nprint(\"\\nNext steps (Day 2):\")\nprint(\"  1. Export training data\")\nprint(\"  2. Train Random Forest classifier\")\nprint(\"  3. Apply classifier to image\")\nprint(\"  4. Validate results\")\n\n\n\n6.3 Earth Engine Built-in ML (Preview)\nGEE has built-in classifiers for quick prototyping.\n\n# Train a simple classifier (Random Forest)\n# Note: This is a preview - we'll cover this in detail on Day 2\n\nclassifier = ee.Classifier.smileRandomForest(\n    numberOfTrees=10\n).train(\n    features=training_data,\n    classProperty='landcover',\n    inputProperties=ml_stack.bandNames()\n)\n\n# Classify the image\nclassified = ml_stack.classify(classifier)\n\nprint(\"Simple classification performed!\")\nprint(\"\\nNote: This is a minimal example.\")\nprint(\"On Day 2, we'll learn:\")\nprint(\"  - Proper training data collection\")\nprint(\"  - Feature selection\")\nprint(\"  - Model validation\")\nprint(\"  - Accuracy assessment\")\n\n# Visualize classification\nvis_params_class = {\n    'min': 0,\n    'max': 1,\n    'palette': ['green', 'blue']  # Forest, Water\n}\n\nclass_thumb = classified.getThumbURL({\n    'region': aoi,\n    'dimensions': 512,\n    **vis_params_class\n})\n\nprint(\"\\nSimple Classification Result:\")\ndisplay(Image(url=class_thumb))",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#best-practices-and-tips",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#best-practices-and-tips",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "7. Best Practices and Tips",
    "text": "7. Best Practices and Tips\n\n7.1 Memory Management\n\n# Best Practices:\nprint(\"Earth Engine Best Practices:\")\nprint(\"\\n1. MEMORY MANAGEMENT:\")\nprint(\"   - Avoid .getInfo() on large objects (use for small metadata only)\")\nprint(\"   - Use .limit() to restrict collection size during testing\")\nprint(\"   - Export large results instead of downloading\")\n\n# Example: Limit collection size\nlimited_collection = s2.limit(5)\nprint(f\"\\n   Limited collection size: {limited_collection.size().getInfo()}\")\n\nprint(\"\\n2. COMPUTATIONAL QUOTAS:\")\nprint(\"   - Free tier: 250GB Cloud Storage, 10k+ compute hours/month\")\nprint(\"   - Set maxPixels appropriately (default: 1e8)\")\nprint(\"   - Use appropriate scale (don't oversample)\")\n\nprint(\"\\n3. EFFICIENCY:\")\nprint(\"   - Filter early: bounds → date → metadata\")\nprint(\"   - Select only needed bands\")\nprint(\"   - Clip to AOI before intensive operations\")\n\n\n\n7.2 When to Use GEE vs Local Processing\n\nprint(\"USE GOOGLE EARTH ENGINE FOR:\")\nprint(\"  ✓ Data access and pre-processing\")\nprint(\"  ✓ Large-scale spatial analysis\")\nprint(\"  ✓ Time series analysis\")\nprint(\"  ✓ Cloud masking and compositing\")\nprint(\"  ✓ Simple ML (Random Forest, CART)\")\nprint(\"  ✓ Zonal statistics\")\nprint(\"  ✓ Rapid prototyping\")\n\nprint(\"\\nUSE LOCAL PROCESSING (Python/Colab) FOR:\")\nprint(\"  ✓ Deep learning (CNN, U-Net, LSTM)\")\nprint(\"  ✓ Custom model architectures\")\nprint(\"  ✓ Fine-grained control over training\")\nprint(\"  ✓ Integration with TensorFlow/PyTorch\")\nprint(\"  ✓ Advanced data augmentation\")\nprint(\"  ✓ Transfer learning\")\n\nprint(\"\\nBEST WORKFLOW:\")\nprint(\"  1. Use GEE for data preparation\")\nprint(\"  2. Export training data\")\nprint(\"  3. Train models locally (Colab GPU)\")\nprint(\"  4. Deploy models on new data\")\n\n\n\n7.3 Troubleshooting Common Errors\n\nprint(\"COMMON ERRORS AND SOLUTIONS:\\n\")\n\nprint(\"1. 'User memory limit exceeded'\")\nprint(\"   → Reduce AOI size or increase scale\")\nprint(\"   → Use .limit() on collections\")\nprint(\"   → Export instead of .getInfo()\")\n\nprint(\"\\n2. 'Computation timed out'\")\nprint(\"   → Simplify operations\")\nprint(\"   → Filter collections more aggressively\")\nprint(\"   → Break into smaller exports\")\n\nprint(\"\\n3. 'EEException: Collection.first: No matching elements'\")\nprint(\"   → Check date range (no images available)\")\nprint(\"   → Verify AOI (outside coverage?)\")\nprint(\"   → Relax filters (clouds, etc.)\")\n\nprint(\"\\n4. Export task fails\")\nprint(\"   → Check maxPixels limit\")\nprint(\"   → Verify Google Drive space\")\nprint(\"   → Check region coordinates\")\n\nprint(\"\\n5. 'Image.select: Pattern X did not match any bands'\")\nprint(\"   → Check band names: .bandNames().getInfo()\")\nprint(\"   → Verify dataset (S2 vs S2_SR bands differ)\")",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#key-takeaways",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#key-takeaways",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "8. Key Takeaways",
    "text": "8. Key Takeaways\nWhat You’ve Learned:\n\nGEE Authentication & Setup\n\nOne-time authentication process\nInitialize for each session\nPython API basics\n\nCore GEE Concepts\n\nGeometry: Points, Rectangles, Polygons\nImage & ImageCollection: Raster data\nFeature & FeatureCollection: Vector data\nFilters: Subset data by space, time, metadata\nReducers: Aggregate/summarize data\n\nSentinel-2 Workflows\n\nAccess surface reflectance data\nCloud masking with QA60\nCreate median composites\nCalculate NDVI\nVisualize with thumbnails\n\nSentinel-1 SAR\n\nAll-weather imaging capability\nVV and VH polarizations\nSpeckle reduction with mean\nComplementary to optical data\n\nData Export\n\nExport to Google Drive\nImages (GeoTIFF) and tables (CSV)\nMonitor tasks\nPrepare data for ML\n\nML Integration\n\nPrepare multi-band stacks\nSample training data\nBuilt-in classifiers (preview)\nGEE ↔︎ Local workflow",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#next-steps",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#next-steps",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "9. Next Steps",
    "text": "9. Next Steps\nDay 2: We’ll apply these skills to build Machine Learning classification models:\n\nRandom Forest for land cover classification\nFeature engineering and selection\nTraining data collection strategies\nModel validation and accuracy assessment\nPhilippine case study: Palawan land cover mapping\n\nDay 3-4: Advanced deep learning: - CNNs for image classification - U-Net for semantic segmentation - Object detection - Time series analysis with LSTMs",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#additional-resources",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#additional-resources",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "10. Additional Resources",
    "text": "10. Additional Resources\n\nOfficial Documentation\n\nEarth Engine Guide: https://developers.google.com/earth-engine/\nPython API Intro: https://developers.google.com/earth-engine/tutorials/community/intro-to-python-api\nData Catalog: https://developers.google.com/earth-engine/datasets/\n\n\n\nTutorials\n\nEnd-to-End GEE Course: https://courses.spatialthoughts.com/end-to-end-gee.html\nGEE Community Tutorials: https://github.com/google/earthengine-community\nAwesome Earth Engine: https://github.com/giswqs/Awesome-GEE\n\n\n\nPhilippine Context\n\nPhilSA: https://philsa.gov.ph/\nCoPhil Mirror Site: (Coming 2025)\nDOST-ASTI: https://asti.dost.gov.ph/\n\n\n\nBooks\n\nCloud-Based Remote Sensing with Google Earth Engine (Cardille et al.)\nEarth Observation Using Python (Parente & Pepe)",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#practice-exercises",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#practice-exercises",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "11. Practice Exercises",
    "text": "11. Practice Exercises\nTo reinforce your learning, try these exercises:\n\nExercise A: Multi-temporal Analysis\nCreate composites for different seasons (dry vs wet) and compare NDVI changes.\n\n\nExercise B: Multi-location Comparison\nCompare NDVI between different Philippine regions (urban vs forest vs agriculture).\n\n\nExercise C: Sentinel-1 Flood Detection\nUse SAR data to identify potential flood areas (low VV backscatter).\n\n\nExercise D: Data Fusion\nCombine Sentinel-1 and Sentinel-2 to create a comprehensive dataset for classification.\n\n\nExercise E: Time Series\nPlot NDVI time series for a specific location over an entire year.",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#congratulations",
    "href": "day1/notebooks/Day1_Session4_Google_Earth_Engine.html#congratulations",
    "title": "Day 1, Session 4: Google Earth Engine Python API",
    "section": "Congratulations!",
    "text": "Congratulations!\nYou’ve completed Day 1 of the CoPhil AI/ML Training!\nYou now have: - ✓ Python geospatial skills (GeoPandas, Rasterio) - ✓ Google Earth Engine proficiency - ✓ Access to petabytes of satellite data - ✓ Ability to prepare data for AI/ML\nTomorrow: We build our first machine learning models!\n\nGenerated with Claude Code for CoPhil Digital Space Campus\nEU-Philippines Copernicus Capacity Support Programme\nData-Centric AI for Earth Observation",
    "crumbs": [
      "Notebooks",
      "Day 1, Session 4: Google Earth Engine Python API"
    ]
  },
  {
    "objectID": "day1/presentations/01_session1_copernicus_philippine_eo.html",
    "href": "day1/presentations/01_session1_copernicus_philippine_eo.html",
    "title": "Copernicus Sentinel Data & Philippine EO Ecosystem",
    "section": "",
    "text": "4-Day Advanced Training\n\nAI/ML for Earth Observation\nPhilippine EO Professionals\nFocus: DRR, CCA, NRM\nOnline format\n\n\nToday’s Goals\n\nUnderstand Copernicus data\nExplore Philippine EO ecosystem\nLearn AI/ML fundamentals\nHands-on Python and GEE\n\n\n\n\nWelcome participants to the 4-day advanced training. Emphasize that this is part of the EU-Philippines partnership and will provide practical skills for disaster risk reduction, climate change adaptation, and natural resource management.\n\n\n\n\n\n\n\n\nEU-Philippines space cooperation flagship\nBuilding strong partnerships\nSmart, clean, secure digital links\nStrengthening health, education, research systems globally\n\n\n\n\n\n\n\nThe Global Gateway strategy represents the EU’s commitment to building partnerships that boost smart, clean and secure infrastructure globally. CoPhil is a unique flagship initiative within this framework.\n\n\n\n\n\n\nMission\nSupport Philippine Space Agency (PhilSA) and DOST to improve use of Earth Observation data for:\n\nDisaster Risk Reduction (DRR)\nClimate Change Adaptation (CCA)\nNatural Resource Management (NRM)\n\n\nKey Outputs\n\nCopernicus Mirror Site\nDigital Space Campus\nCapacity building\nPilot services\n\n\n\n\nCoPhil is an EU-funded Technical Assistance programme positioning the Philippines as a pioneer in the EU’s international cooperation on Copernicus.\n\n\n\n\n\n\nPhilippine Space Agency\n\n\nEstablished 2019\nCentral civilian space agency\nSpace+ Data Dashboard\nCo-chair of CoPhil\n\n\nDepartment of Science and Technology\n\n\nASTI AI initiatives\nSkAI-Pinas program\nNational AI investments\nCo-chair of CoPhil\n\n\n\n\nBoth PhilSA and DOST are co-chairs of the CoPhil programme, demonstrating strong national commitment to building EO and AI capacity.\n\n\n\n\n\n\nCopernicus Programme Overview\nSentinel-1 Mission (SAR)\nSentinel-2 Mission (Optical)\nData Access Methods\nPhilippine EO Ecosystem\nCoPhil Infrastructure\n\n\nDuration: 2 hours\n\nThis session provides the foundation for understanding where EO data comes from and how Philippine agencies complement Copernicus data."
  },
  {
    "objectID": "day1/index.html",
    "href": "day1/index.html",
    "title": "Day 1: EO Data & AI/ML Fundamentals",
    "section": "",
    "text": "Home › Day 1: EO Data & AI/ML Fundamentals",
    "crumbs": [
      "Day 1: EO Data & AI/ML Fundamentals"
    ]
  },
  {
    "objectID": "day1/index.html#day-1-overview",
    "href": "day1/index.html#day-1-overview",
    "title": "Day 1: EO Data & AI/ML Fundamentals",
    "section": "Day 1 Overview",
    "text": "Day 1 Overview\nWelcome to Day 1 of the CoPhil EO AI/ML Training Programme! Today you’ll build a solid foundation in Earth Observation data and AI/ML fundamentals. By the end of the day, you’ll understand Copernicus missions, Philippine EO resources, and be ready to start building AI models.\n\n\n\n\n\n\nNoteWhat You’ll Learn Today\n\n\n\nThis day provides the foundation for your AI/ML journey in Earth Observation. You’ll gain both theoretical understanding and hands-on experience with the tools and data that power modern EO applications.",
    "crumbs": [
      "Day 1: EO Data & AI/ML Fundamentals"
    ]
  },
  {
    "objectID": "day1/index.html#learning-objectives",
    "href": "day1/index.html#learning-objectives",
    "title": "Day 1: EO Data & AI/ML Fundamentals",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of Day 1, you will be able to:\n\n\nIdentify the characteristics and applications of Sentinel-1 and Sentinel-2 missions\nNavigate Philippine EO platforms including PhilSA SIYASAT, NAMRIA Geoportal, and DOST-ASTI tools\nExplain the AI/ML workflow for Earth Observation applications\nDistinguish between supervised and unsupervised learning with EO examples\nUnderstand neural network fundamentals and data-centric AI principles\nLoad and visualize vector data using GeoPandas\nRead and process raster imagery using Rasterio\nQuery and filter satellite imagery collections in Google Earth Engine\nApply cloud masking and create temporal composites\nExport processed EO data for AI/ML workflows",
    "crumbs": [
      "Day 1: EO Data & AI/ML Fundamentals"
    ]
  },
  {
    "objectID": "day1/index.html#todays-schedule",
    "href": "day1/index.html#todays-schedule",
    "title": "Day 1: EO Data & AI/ML Fundamentals",
    "section": "Today’s Schedule",
    "text": "Today’s Schedule\n\n\n\nTime\nSession\nTopic\nMaterials\n\n\n\n\n09:00-11:00\n1\nCopernicus Sentinel Data & PH EO Ecosystem\nPresentation, Demos\n\n\n11:00-13:00\n2\nCore Concepts of AI/ML for EO\nPresentation, Case Studies\n\n\n14:00-16:00\n3\nHands-on Python for Geospatial Data\nNotebook 1\n\n\n16:00-18:00\n4\nIntroduction to Google Earth Engine\nNotebook 2",
    "crumbs": [
      "Day 1: EO Data & AI/ML Fundamentals"
    ]
  },
  {
    "objectID": "day1/index.html#training-sessions",
    "href": "day1/index.html#training-sessions",
    "title": "Day 1: EO Data & AI/ML Fundamentals",
    "section": "Training Sessions",
    "text": "Training Sessions\n\n\n\n\n\n\nSession 1\nCopernicus Sentinel Data & Philippine EO Ecosystem\nLearn about Europe’s flagship EO program and the Philippine agencies advancing EO in the country.\nGo to Session 1 \n\n\n\n\n\n\n\nSession 2\nCore Concepts of AI/ML for Earth Observation\nDemystify AI/ML workflows, supervised vs unsupervised learning, neural networks, and data-centric approaches.\nGo to Session 2 \n\n\n\n\n\n\n\nSession 3\nHands-on Python for Geospatial Data\n\nCompleted  2 hours\n\nMaster vector data with GeoPandas and raster data with Rasterio - the foundations of EO data processing.\nGo to Session 3 \n\n\n\n\n\n\n\nSession 4\nIntroduction to Google Earth Engine\n\nCompleted  2 hours\n\nLeverage cloud computing power to access, filter, and preprocess petabytes of Earth observation data.\nGo to Session 4",
    "crumbs": [
      "Day 1: EO Data & AI/ML Fundamentals"
    ]
  },
  {
    "objectID": "day1/index.html#prerequisites",
    "href": "day1/index.html#prerequisites",
    "title": "Day 1: EO Data & AI/ML Fundamentals",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nWhat You Need\nBefore starting Day 1:\n\nComplete setup guide\nGoogle account (for Colab and Earth Engine)\nGoogle Earth Engine account (sign up at earthengine.google.com)\nBasic Python knowledge (variables, loops, functions)\nFamiliarity with remote sensing concepts (helpful but not required)\n\nTechnical Setup:\nAll exercises run in Google Colaboratory - no local installation required! See our Setup Guide for detailed instructions.",
    "crumbs": [
      "Day 1: EO Data & AI/ML Fundamentals"
    ]
  },
  {
    "objectID": "day1/index.html#hands-on-notebooks",
    "href": "day1/index.html#hands-on-notebooks",
    "title": "Day 1: EO Data & AI/ML Fundamentals",
    "section": "Hands-on Notebooks",
    "text": "Hands-on Notebooks\n\n\n\n\n\n\nTipInteractive Learning\n\n\n\nDay 1 includes two comprehensive Jupyter notebooks that you’ll work through during Sessions 3 and 4:\n\nNotebook 1: Python for Geospatial Data - GeoPandas and Rasterio exercises\nNotebook 2: Google Earth Engine - GEE filtering, compositing, and export\n\nBoth notebooks run in Google Colab with all dependencies pre-configured.",
    "crumbs": [
      "Day 1: EO Data & AI/ML Fundamentals"
    ]
  },
  {
    "objectID": "day1/index.html#updates-highlighted",
    "href": "day1/index.html#updates-highlighted",
    "title": "Day 1: EO Data & AI/ML Fundamentals",
    "section": "2025 Updates Highlighted",
    "text": "2025 Updates Highlighted\nThis training incorporates the latest 2025 developments in Earth Observation and AI:\n\nSentinel-2C operational (January 2025) - Three-satellite constellation with 5-day revisit\nSentinel-1C active - Restored dual-satellite SAR coverage\nCopernicus Data Space Ecosystem - New data access platform with SentiBoard dashboard\nPhilSA SIYASAT portal - Secure data archive for NovaSAR-1 and maritime monitoring\nDOST P2.6B AI investment (until 2028) - SkAI-Pinas, DIMER, AIPI platforms\nESA Φsat-2 mission - On-board AI processing demonstration\nNASA-IBM Geospatial Foundation Model - Open-source pre-trained model for EO\nData-centric AI paradigm - Emphasis on data quality over model complexity",
    "crumbs": [
      "Day 1: EO Data & AI/ML Fundamentals"
    ]
  },
  {
    "objectID": "day1/index.html#whats-next",
    "href": "day1/index.html#whats-next",
    "title": "Day 1: EO Data & AI/ML Fundamentals",
    "section": "What’s Next?",
    "text": "What’s Next?\nAfter completing Day 1, you’ll have the foundational knowledge to move to:\nDay 2: Machine Learning for Earth Observation - Where you’ll apply supervised and unsupervised learning algorithms to real satellite imagery for land cover classification and change detection.",
    "crumbs": [
      "Day 1: EO Data & AI/ML Fundamentals"
    ]
  },
  {
    "objectID": "day1/index.html#quick-links",
    "href": "day1/index.html#quick-links",
    "title": "Day 1: EO Data & AI/ML Fundamentals",
    "section": "Quick Links",
    "text": "Quick Links\n\nSession 1: Copernicus & PH EO Session 2: AI/ML Fundamentals Session 3: Python Geospatial Session 4: Google Earth Engine Notebook 1: GeoPandas & Rasterio Notebook 2: Earth Engine Setup Guide Download Materials Philippine EO Resources FAQ",
    "crumbs": [
      "Day 1: EO Data & AI/ML Fundamentals"
    ]
  },
  {
    "objectID": "day1/index.html#need-help",
    "href": "day1/index.html#need-help",
    "title": "Day 1: EO Data & AI/ML Fundamentals",
    "section": "Need Help?",
    "text": "Need Help?\nThroughout Day 1, you can:\n\nAsk questions in the live session\nConsult the FAQ for common issues\nCheck the Glossary for term definitions\nDownload Cheat Sheets for quick reference\nAccess the Philippine EO Resources directory\n\n\n\n\n\n\n\nImportantTechnical Support\n\n\n\nFor technical issues during the training:\n\nGoogle Colab issues: Check Setup Guide\nData access problems: See session-specific troubleshooting sections\nGeneral questions: Contact your instructors or teaching assistants\n\n\n\n\nDay 1 is part of the CoPhil 4-Day Advanced Training on AI/ML for Earth Observation, funded by the European Union under the Global Gateway initiative and delivered in partnership with PhilSA and DOST.",
    "crumbs": [
      "Day 1: EO Data & AI/ML Fundamentals"
    ]
  },
  {
    "objectID": "resources/downloads.html#day-1-eo-data-aiml-fundamentals-geospatial-python",
    "href": "resources/downloads.html#day-1-eo-data-aiml-fundamentals-geospatial-python",
    "title": "Downloads",
    "section": "Day 1: EO Data, AI/ML Fundamentals & Geospatial Python",
    "text": "Day 1: EO Data, AI/ML Fundamentals & Geospatial Python\n\nSession 1: Copernicus Sentinel Data & Philippine EO Ecosystem\nPresentation: View Online Download PDF\nTopics: - Copernicus Programme overview - Sentinel-1 SAR and Sentinel-2 optical missions - Philippine EO agencies (PhilSA, NAMRIA, DOST-ASTI, PAGASA) - CoPhil Mirror Site and infrastructure\n\n\n\nSession 2: AI/ML Fundamentals for Earth Observation\nPresentation: View Online Download PDF\nTopics: - What is AI/ML and the EO workflow - Supervised vs. Unsupervised learning - Introduction to neural networks and CNNs - Data-centric AI paradigm\n\n\n\nSession 3: Python for Geospatial Data\nPresentation: View Online Download PDF\nJupyter Notebook: Download Notebook Open in Colab\nTopics: - Google Colab setup - GeoPandas for vector data - Rasterio for raster data - Coordinate reference systems - Philippine case study: Palawan land cover\n\n\n\nSession 4: Introduction to Google Earth Engine\nPresentation: View Online Download PDF\nJupyter Notebook: Download Notebook Open in Colab\nTopics: - Earth Engine authentication and initialization - ImageCollection filtering - Sentinel-1 SAR and Sentinel-2 optical data access - Cloud masking and temporal compositing - Philippine case study: Metro Manila monitoring",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#day-2-machine-learning-for-land-cover-classification",
    "href": "resources/downloads.html#day-2-machine-learning-for-land-cover-classification",
    "title": "Downloads",
    "section": "Day 2: Machine Learning for Land Cover Classification",
    "text": "Day 2: Machine Learning for Land Cover Classification\n\nSession 1: Random Forest Classification\nPresentation: View Online Download PDF\nJupyter Notebooks: Theory Notebook Hands-on Lab\nTopics: - Decision trees and ensemble methods - Random Forest algorithm - Feature importance and model interpretation - Land cover classification with Sentinel-2\n\n\n\nSession 2: Palawan Land Cover Lab\nPresentation: View Online Download PDF\nJupyter Notebook: Extended Lab\nTopics: - Model evaluation and validation - Hyperparameter tuning - Cross-validation strategies - Handling imbalanced datasets\n\n\n\nSession 3: Deep Learning Fundamentals\nPresentation: View Online Download PDF\nJupyter Notebook: Theory Interactive\nTopics: - Neural networks architecture - Backpropagation and optimization - Introduction to PyTorch/TensorFlow - Building simple neural networks\n\n\n\nSession 4: Convolutional Neural Networks for EO\nPresentation: View Online Download PDF\nJupyter Notebooks: CNN Classification Transfer Learning\nTopics: - CNN architecture and components - Transfer learning for EO - Image classification with CNNs - Philippine land use case studies",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#day-3-semantic-segmentation-object-detection",
    "href": "resources/downloads.html#day-3-semantic-segmentation-object-detection",
    "title": "Downloads",
    "section": "Day 3: Semantic Segmentation & Object Detection",
    "text": "Day 3: Semantic Segmentation & Object Detection\n\nSession 1: U-Net Semantic Segmentation\nPresentation: View Online Download PDF\nTopics: - Pixel-wise classification - U-Net architecture - Encoder-decoder networks - Loss functions for segmentation\n\n\n\nSession 2: Flood Mapping with U-Net\nPresentation: View Online Download PDF\nJupyter Notebook: Flood Mapping Lab\nTopics: - SAR data for flood detection - U-Net implementation - Training and validation - Philippine flood mapping case study\n\n\n\nSession 3: Object Detection Theory\nPresentation: View Online Download PDF\nTopics: - Object detection frameworks - YOLO and Faster R-CNN - Detection vs segmentation - Evaluation metrics\n\n\n\nSession 4: Object Detection Lab\nPresentation: View Online Download PDF\nJupyter Notebook: Object Detection Lab\nTopics: - Building and ship detection - Infrastructure monitoring - Hands-on implementation - Philippine case studies",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "resources/downloads.html#day-4-time-series-analysis-advanced-topics",
    "href": "resources/downloads.html#day-4-time-series-analysis-advanced-topics",
    "title": "Downloads",
    "section": "Day 4: Time Series Analysis & Advanced Topics",
    "text": "Day 4: Time Series Analysis & Advanced Topics\n\nSession 1: LSTM for Time Series\nPresentation: View Online Download PDF\nJupyter Notebook: LSTM Demo\nTopics: - Recurrent neural networks - LSTM architecture - Time series forecasting - Vegetation dynamics modeling\n\n\n\nSession 2: Drought Monitoring with LSTM\nPresentation: View Online Download PDF\nJupyter Notebook: Drought Lab\nTopics: - Multi-variate time series - LSTM implementation for drought - Feature engineering - Philippine drought case study\n\n\n\nSession 3: Emerging AI Technologies\nPresentation: View Online Download PDF\nTopics: - Self-supervised learning - Vision transformers - Foundation models overview - Prithvi and other EO models\n\n\n\nSession 4: Synthesis & Best Practices\nPresentation: View Online Download PDF\nTopics: - End-to-end EO AI/ML workflow - Model deployment - Operational considerations - Future directions in EO AI/ML",
    "crumbs": [
      "Materials",
      "Help & Downloads",
      "Downloads"
    ]
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#session-overview",
    "href": "day2/presentations/session1_random_forest.html#session-overview",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Session Overview",
    "text": "Session Overview\n\n\nPart A: Theory (1.5 hours)\n\nIntroduction to Supervised Classification\nDecision Trees Fundamentals\nRandom Forest Ensemble Method\nFeature Importance\nAccuracy Assessment\nGoogle Earth Engine Platform\n\n\nPart B: Hands-on Lab (1.5 hours)\n\nSentinel-2 Data Acquisition\nFeature Engineering (Spectral Indices)\nTraining Data Preparation\nModel Training & Optimization\nClassification & Validation\nPhilippine NRM Applications\n\n\nLearning Objectives:\n\nUnderstand supervised classification workflow for EO data\nImplement Random Forest using Google Earth Engine\nPerform accuracy assessment and interpret results\nApply classification to Palawan land cover mapping"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#what-is-supervised-classification",
    "href": "day2/presentations/session1_random_forest.html#what-is-supervised-classification",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "What is Supervised Classification?",
    "text": "What is Supervised Classification?\n\nGoal: Assign labels to pixels/objects based on their characteristics\n“Supervised”: We provide labeled training examples to the algorithm\nLearning Process: Algorithm learns patterns from training data\nApplication: Classify entire image based on learned patterns\n\n\n\n\n\n\n\n\nKey Concept\n\n\nSupervised classification requires labeled training data (ground truth) to learn the relationship between spectral signatures and land cover classes."
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#supervised-classification-workflow",
    "href": "day2/presentations/session1_random_forest.html#supervised-classification-workflow",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Supervised Classification Workflow",
    "text": "Supervised Classification Workflow\n\n\n\n\n\nflowchart LR\n    A[Satellite Imagery] --&gt; B[Preprocessing]\n    B --&gt; C[Feature Extraction]\n    C --&gt; D[Training Data Collection]\n    D --&gt; E[Model Training]\n    E --&gt; F[Classification]\n    F --&gt; G[Accuracy Assessment]\n    G --&gt; H{Acceptable?}\n    H --&gt;|No| D\n    H --&gt;|Yes| I[Final Map]\n    style E fill:#4A90E2\n    style F fill:#4A90E2\n\n\n\n\n\n\nKey Steps:\n\nPreprocessing: Cloud masking, atmospheric correction\nFeature Extraction: Spectral bands, indices (NDVI, NDWI)\nTraining Data: Collect representative samples for each class\nModel Training: Train classifier on training data\nClassification: Apply model to entire scene\nValidation: Assess accuracy with independent test data"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#common-land-cover-classes-in-philippines",
    "href": "day2/presentations/session1_random_forest.html#common-land-cover-classes-in-philippines",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Common Land Cover Classes in Philippines",
    "text": "Common Land Cover Classes in Philippines\n\n\nNatural Ecosystems:\n\nPrimary Forest (dipterocarp)\nSecondary Forest\nMangroves\nGrasslands\nWater Bodies (rivers, lakes, coastal)\n\n\nHuman-Modified:\n\nAgricultural Land (rice paddies, coconut)\nUrban/Built-up Areas\nBare Soil\nMining Areas\nRoads and Infrastructure\n\n\n\n\n\n\n\n\n\nPhilippine Context\n\n\nAccurate land cover classification supports monitoring of Protected Areas, REDD+ programs, agricultural expansion, and disaster response (typhoons, floods)."
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#what-is-a-decision-tree",
    "href": "day2/presentations/session1_random_forest.html#what-is-a-decision-tree",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "What is a Decision Tree?",
    "text": "What is a Decision Tree?\nA tree-like structure that makes decisions by asking a series of questions about features.\n\n\n\n\n\n\n\nflowchart TD\n    A[NDVI &gt; 0.4?] --&gt;|Yes| B[NIR &gt; 3000?]\n    A --&gt;|No| C[NDWI &gt; 0?]\n    B --&gt;|Yes| D[Forest]\n    B --&gt;|No| E[Agriculture]\n    C --&gt;|Yes| F[Water]\n    C --&gt;|No| G[Urban/Bare]\n    style D fill:#2E7D32\n    style E fill:#FBC02D\n    style F fill:#1976D2\n    style G fill:#757575\n\n\n\n\n\n\n\nHow it Works:\n\nStart at root node\nTest condition (e.g., NDVI &gt; 0.4?)\nBranch based on answer\nRepeat until reaching leaf node\nLeaf node = predicted class"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#decision-tree-splitting",
    "href": "day2/presentations/session1_random_forest.html#decision-tree-splitting",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Decision Tree Splitting",
    "text": "Decision Tree Splitting\nHow does a tree decide where to split?\n\nGoal: Create pure nodes (all samples belong to one class)\nMetric: Information Gain or Gini Impurity\nProcess: Test all possible splits, choose the best one\nRecursion: Repeat for each branch until stopping criteria\n\n\nGini Impurity Formula:\n\\[\nGini = 1 - \\sum_{i=1}^{n} (p_i)^2\n\\]\nWhere \\(p_i\\) is the probability of class \\(i\\) in the node.\n\nGini = 0: Pure node (all samples same class) ✓\nGini = 0.5: Maximum impurity (50/50 split) ✗"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#decision-tree-example-spectral-splitting",
    "href": "day2/presentations/session1_random_forest.html#decision-tree-example-spectral-splitting",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Decision Tree Example: Spectral Splitting",
    "text": "Decision Tree Example: Spectral Splitting\n\nSplit 1Split 2Split 3\n\n\nRoot Node: All 1000 samples\nTest: NDVI &gt; 0.4?\n\nLeft branch (NDVI ≤ 0.4): 400 samples → Mostly Water, Urban, Bare\nRight branch (NDVI &gt; 0.4): 600 samples → Mostly Forest, Agriculture\n\nInformation Gain: High ✓ (classes becoming more separated)\n\n\nRight Branch: 600 samples with NDVI &gt; 0.4\nTest: NIR Reflectance &gt; 3000?\n\nLeft branch (NIR ≤ 3000): 250 samples → Agriculture (less canopy density)\nRight branch (NIR &gt; 3000): 350 samples → Forest (dense canopy)\n\nInformation Gain: High ✓\n\n\nLeft Branch: 400 samples with NDVI ≤ 0.4\nTest: NDWI &gt; 0?\n\nLeft branch (NDWI ≤ 0): 200 samples → Urban/Bare\nRight branch (NDWI &gt; 0): 200 samples → Water\n\nInformation Gain: High ✓"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#decision-tree-advantages-limitations",
    "href": "day2/presentations/session1_random_forest.html#decision-tree-advantages-limitations",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Decision Tree Advantages & Limitations",
    "text": "Decision Tree Advantages & Limitations\n\n\nAdvantages:\n\n✓ Easy to understand and visualize\n✓ No data normalization needed\n✓ Handles non-linear relationships\n✓ Feature importance easily extracted\n✓ Fast prediction\n\n\nLimitations:\n\n✗ Overfitting: Can memorize training data\n✗ High variance: Small data changes → big tree changes\n✗ Instability: Greedy algorithm (local optima)\n✗ Bias: Favor features with many levels\n\n\n\n\n\n\n\n\n\nThe Solution\n\n\nRandom Forest addresses these limitations by combining many decision trees!"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#what-is-random-forest",
    "href": "day2/presentations/session1_random_forest.html#what-is-random-forest",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "What is Random Forest?",
    "text": "What is Random Forest?\nAn ensemble learning method that combines many decision trees to improve accuracy and reduce overfitting.\n\n\n\n\n\n\n\nflowchart TD\n    A[Training Data&lt;br/&gt;1000 samples] --&gt; B1[Bootstrap&lt;br/&gt;Sample 1]\n    A --&gt; B2[Bootstrap&lt;br/&gt;Sample 2]\n    A --&gt; B3[Bootstrap&lt;br/&gt;Sample 3]\n    A --&gt; B4[...]\n    A --&gt; B5[Bootstrap&lt;br/&gt;Sample N]\n\n    B1 --&gt; T1[Tree 1]\n    B2 --&gt; T2[Tree 2]\n    B3 --&gt; T3[Tree 3]\n    B4 --&gt; T4[...]\n    B5 --&gt; T5[Tree N]\n\n    T1 --&gt; V[Majority Vote]\n    T2 --&gt; V\n    T3 --&gt; V\n    T4 --&gt; V\n    T5 --&gt; V\n\n    V --&gt; F[Final Prediction]\n\n    style F fill:#4CAF50\n\n\n\n\n\n\n\nKey Ideas:\n\nBootstrap Aggregating (Bagging)\n\nRandom sampling with replacement\nEach tree sees different data\n\nRandom Feature Selection\n\nEach split uses random subset of features\nReduces correlation between trees\n\nMajority Voting\n\nClassification: Most common class\nRegression: Average prediction"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#random-forest-the-forest-analogy",
    "href": "day2/presentations/session1_random_forest.html#random-forest-the-forest-analogy",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Random Forest: The “Forest” Analogy",
    "text": "Random Forest: The “Forest” Analogy\n\nOne tree (Decision Tree): One expert’s opinion\n\nCan be very confident but sometimes wrong\nMight overfit to specific training examples\n\nForest (Random Forest): Committee of experts\n\nEach expert sees slightly different data\nEach expert considers different features\nFinal decision: Majority vote\nWisdom of crowds: Group decision more reliable than individual\n\n\n\n\n\n\n\n\n\nIntuition\n\n\nIf you ask 100 independent experts and 75 say “Forest”, you can be more confident than if only 1 expert says “Forest”."
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#bootstrap-aggregating-bagging",
    "href": "day2/presentations/session1_random_forest.html#bootstrap-aggregating-bagging",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Bootstrap Aggregating (Bagging)",
    "text": "Bootstrap Aggregating (Bagging)\nBootstrap Sampling:\n\nOriginal training set: 1000 samples\nEach tree gets: 1000 samples (with replacement)\nSome samples repeated, some never selected (~37% out-of-bag)\n\n\n\nOriginal Data:\nSample IDs: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\nTree 1 Bootstrap:\nSample IDs: 1, 3, 3, 5, 7, 7, 7, 9, 9, 10\nTree 2 Bootstrap:\nSample IDs: 2, 2, 4, 5, 5, 6, 8, 8, 9, 10\n\nWhy Bootstrap?\n\nIntroduces diversity between trees\nEach tree specializes on different samples\nReduces overfitting\nEnables Out-of-Bag (OOB) validation\n\nOut-of-Bag Samples: - Samples not used by a tree (~37%) - Used for internal validation - No separate validation set needed"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#random-feature-selection",
    "href": "day2/presentations/session1_random_forest.html#random-feature-selection",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Random Feature Selection",
    "text": "Random Feature Selection\nAt each split, only consider a random subset of features.\n\n\nAll Features (13 for Sentinel-2 + indices):\n\nB2 (Blue)\nB3 (Green)\nB4 (Red)\nB8 (NIR)\nB11 (SWIR1)\nB12 (SWIR2)\nNDVI\nNDWI\nNDBI\nEVI\nSAVI\nTexture features\nElevation\n\n\nRandom Subset at Each Split:\nTypical: \\(\\sqrt{n}\\) features\nFor 13 features: \\(\\sqrt{13} \\approx 4\\) features\nTree 1, Split 1: {NDVI, B4, B11, Elevation}\nTree 1, Split 2: {B8, NDWI, B3, SAVI}\nTree 2, Split 1: {NDBI, B12, NDVI, B2}\n. . .\nResult: - Trees decorrelated - Prevents strong features from dominating - Better generalization"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#random-forest-prediction-majority-voting",
    "href": "day2/presentations/session1_random_forest.html#random-forest-prediction-majority-voting",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Random Forest Prediction: Majority Voting",
    "text": "Random Forest Prediction: Majority Voting\nExample Classification:\nClassify a pixel with spectral signature: NDVI=0.65, NIR=4500, SWIR=2000\n\n\n100 Trees Vote:\n\nTree 1 → Forest 🌲\nTree 2 → Forest 🌲\nTree 3 → Agriculture 🌾\nTree 4 → Forest 🌲\nTree 5 → Forest 🌲\n…\nTree 100 → Forest 🌲\n\n\nVote Count:\n\nForest: 78 votes\nAgriculture: 22 votes\n\nFinal Prediction: Forest (78%)\nConfidence: 78% confidence in prediction\n\n\n\n\n\n\n\n\nPrediction Confidence\n\n\nThe proportion of votes can be interpreted as confidence. Higher consensus → more confident prediction."
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#random-forest-hyperparameters",
    "href": "day2/presentations/session1_random_forest.html#random-forest-hyperparameters",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Random Forest Hyperparameters",
    "text": "Random Forest Hyperparameters\nKey parameters to tune:\n\n\n\n\n\n\n\n\n\nParameter\nDescription\nTypical Values\nImpact\n\n\n\n\nn_trees\nNumber of trees in forest\n50-500\nMore trees → better performance (diminishing returns)\n\n\nmax_depth\nMaximum depth of each tree\n10-50 or None\nDeeper → more complex, risk overfitting\n\n\nmin_samples_split\nMin samples to split node\n2-10\nHigher → simpler trees, less overfitting\n\n\nmax_features\nFeatures per split\n\\(\\sqrt{n}\\) or \\(\\log_2(n)\\)\nBalance between accuracy and diversity\n\n\nbootstrap\nUse bootstrap sampling\nTrue\nAlmost always True for RF\n\n\n\n\n\n\n\n\n\n\nGoogle Earth Engine Default\n\n\nGEE’s ee.Classifier.smileRandomForest() defaults: - numberOfTrees: 100 - variablesPerSplit: \\(\\sqrt{n}\\) (automatic) - minLeafPopulation: 1"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#random-forest-advantages",
    "href": "day2/presentations/session1_random_forest.html#random-forest-advantages",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Random Forest Advantages",
    "text": "Random Forest Advantages\n\nHigh Accuracy\n\nOften achieves excellent performance out-of-the-box\nHandles complex non-linear relationships\n\nRobust to Overfitting\n\nEnsemble averaging reduces variance\nHarder to overfit than single decision tree\n\nFeature Importance\n\nQuantifies which features matter most\nHelps understand classification drivers\n\nHandles Missing Data\n\nCan work with incomplete feature sets\nRobust to noisy data\n\nNo Normalization Needed\n\nWorks with features on different scales\nSimplifies preprocessing\n\nEfficient\n\nFast training (parallelizable)\nFast prediction"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#understanding-feature-importance",
    "href": "day2/presentations/session1_random_forest.html#understanding-feature-importance",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Understanding Feature Importance",
    "text": "Understanding Feature Importance\nQuestion: Which spectral bands/indices contribute most to classification accuracy?\nFeature Importance measures the contribution of each feature to the model’s predictions.\n\n\nCalculation Methods:\n\nMean Decrease in Impurity (MDI)\n\nHow much each feature reduces impurity (Gini)\nAveraged across all trees\nDefault in most implementations\n\nPermutation Importance\n\nMeasure accuracy drop when feature is randomly shuffled\nMore reliable but slower\n\n\n\nInterpretation:\n\nHigh importance: Feature strongly discriminates classes\nLow importance: Feature adds little information\nZero importance: Feature not used by any tree"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#example-feature-importance-for-palawan",
    "href": "day2/presentations/session1_random_forest.html#example-feature-importance-for-palawan",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Example: Feature Importance for Palawan",
    "text": "Example: Feature Importance for Palawan\nLand Cover Classification (7 classes)\n\n\nTop Features:\n\n\n\nRank\nFeature\nImportance\nUse Case\n\n\n\n\n1\nNDVI\n0.285\nForest vs. non-forest\n\n\n2\nNIR (B8)\n0.192\nVegetation density\n\n\n3\nSWIR1 (B11)\n0.156\nMoisture content\n\n\n4\nNDWI\n0.128\nWater detection\n\n\n5\nRed (B4)\n0.089\nVegetation health\n\n\n6\nNDBI\n0.067\nUrban areas\n\n\n7\nElevation\n0.045\nTopographic context\n\n\n\n\nInsights:\n\nNDVI dominant: Vegetation indices most important\nNIR crucial: Distinguishes vegetation types\nSWIR useful: Separates forest from agriculture\nNDWI essential: Water body identification\nElevation helps: Mountains → forest, lowlands → agriculture\n\nActionable: - Focus on acquiring high-quality NIR and SWIR data - Ensure accurate NDVI calculation - Include DEM for improved accuracy"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#feature-importance-visualization",
    "href": "day2/presentations/session1_random_forest.html#feature-importance-visualization",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Feature Importance Visualization",
    "text": "Feature Importance Visualization\nTypical Output:\n# Example feature importance from trained RF\nFeature Importances:\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nNDVI        ████████████████████████████  0.285\nNIR         ███████████████████           0.192\nSWIR1       █████████████████             0.156\nNDWI        ██████████████                0.128\nRed         █████████                     0.089\nNDBI        ███████                       0.067\nElevation   █████                         0.045\nBlue        ███                           0.020\nGreen       ███                           0.018\nApplications:\n\nFeature selection: Remove low-importance features\nData collection priorities: Focus on important bands\nModel interpretation: Understand classification logic\nDomain validation: Does importance match EO theory?"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#trainingtest-data-splitting",
    "href": "day2/presentations/session1_random_forest.html#trainingtest-data-splitting",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Training/Test Data Splitting",
    "text": "Training/Test Data Splitting\nCritical Decision: How to split data for training vs. validation?\n\n\nCommon Split Ratios:\n\n\n\nSplit\nTraining\nTesting\nUse Case\n\n\n\n\n80/20\n80%\n20%\nStandard (sufficient data)\n\n\n70/30\n70%\n30%\nMore robust validation\n\n\n60/40\n60%\n40%\nLimited training data\n\n\n50/50\n50%\n50%\nVery small datasets\n\n\n\nGoogle Earth Engine: Use .randomColumn() to assign splits\n\nSplitting Strategies:\n1. Random Split (most common)\n# Add random column\ndata = data.randomColumn('random')\n\n# Split 80/20\ntraining = data.filter(ee.Filter.lt('random', 0.8))\ntesting = data.filter(ee.Filter.gte('random', 0.8))\n2. Stratified Split (recommended) - Maintain class proportions in both sets - Important for imbalanced datasets - Ensures all classes in test set\n3. Spatial Split - Training from one region, testing from another - Tests geographic transferability - More realistic for operational use\n\n\nRandom 80/20 is standard, but stratified ensures each class has representation in test set. Spatial split is most rigorous - tests if model works in new areas."
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#why-accuracy-assessment",
    "href": "day2/presentations/session1_random_forest.html#why-accuracy-assessment",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Why Accuracy Assessment?",
    "text": "Why Accuracy Assessment?\n\nQuantify performance: How good is the classification?\nCompare models: Which classifier performs better?\nIdentify weaknesses: Which classes are confused?\nBuild confidence: Can we trust the map for decisions?\nReport to stakeholders: Scientific credibility\n\n\n\n\n\n\n\n\nGolden Rule\n\n\nALWAYS use independent test data that was NOT used for training. Otherwise, you’re measuring memorization, not generalization."
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#confusion-matrix",
    "href": "day2/presentations/session1_random_forest.html#confusion-matrix",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nA table showing predicted classes vs. actual classes for test data.\nExample: 5-class classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted →\nForest\nAgriculture\nWater\nUrban\nBare\nTotal\nUser’s Acc\n\n\n\n\nActual ↓\n\n\n\n\n\n\n\n\n\n\nForest\n\n85\n8\n0\n2\n5\n100\n85%\n\n\nAgriculture\n\n12\n73\n0\n5\n10\n100\n73%\n\n\nWater\n\n0\n1\n95\n2\n2\n100\n95%\n\n\nUrban\n\n3\n7\n3\n82\n5\n100\n82%\n\n\nBare\n\n5\n11\n2\n9\n78\n105\n74%\n\n\nTotal\n\n105\n100\n100\n100\n100\n505\n\n\n\nProducer’s Acc\n\n81%\n73%\n95%\n82%\n78%\n\nOA: 82.6%"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#accuracy-metrics-explained",
    "href": "day2/presentations/session1_random_forest.html#accuracy-metrics-explained",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Accuracy Metrics Explained",
    "text": "Accuracy Metrics Explained\n\nOverall AccuracyProducer’s AccuracyUser’s AccuracyKappa Coefficient\n\n\nDefinition: Percentage of correctly classified samples\n\\[\n\\text{Overall Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Samples}} = \\frac{85+73+95+82+78}{505} = \\frac{413}{500} = 82.6\\%\n\\]\nInterpretation: - Simple, intuitive metric - Limitation: Can be misleading with imbalanced classes\nExample: 95% accuracy sounds great, but if 95% of pixels are forest, a “classify everything as forest” model achieves 95%!\n\n\nDefinition: Percentage of actual class correctly identified (per class)\nAlso called: Recall, Sensitivity\n\\[\n\\text{Producer's Accuracy}_{\\text{Forest}} = \\frac{85}{105} = 81\\%\n\\]\nInterpretation: - “Of all actual forest pixels, how many did we correctly identify?” - Producer’s perspective: How complete is the map? - Low producer’s accuracy → omission error (missing true positives)\nExample: 81% for Forest means we missed 19% of actual forest pixels.\n\n\nDefinition: Percentage of predicted class that is correct (per class)\nAlso called: Precision, Positive Predictive Value\n\\[\n\\text{User's Accuracy}_{\\text{Forest}} = \\frac{85}{100} = 85\\%\n\\]\nInterpretation: - “Of all pixels we labeled as forest, how many are truly forest?” - User’s perspective: How reliable is the map? - Low user’s accuracy → commission error (false positives)\nExample: 85% for Forest means 15% of “forest” pixels are actually other classes.\n\n\nDefinition: Measure of agreement beyond chance\n\\[\n\\kappa = \\frac{p_o - p_e}{1 - p_e}\n\\]\nWhere: - \\(p_o\\) = observed accuracy (overall accuracy) - \\(p_e\\) = expected accuracy by chance\nInterpretation: - κ &lt; 0.4: Poor agreement - κ = 0.4-0.6: Moderate agreement - κ = 0.6-0.8: Good agreement - κ &gt; 0.8: Excellent agreement\nWhy use Kappa? Accounts for agreement by random chance, especially important for imbalanced datasets."
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#common-confusion-patterns",
    "href": "day2/presentations/session1_random_forest.html#common-confusion-patterns",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Common Confusion Patterns",
    "text": "Common Confusion Patterns\nExample: Forest vs. Agriculture confusion\n\n\n\n\nPredicted Forest\nPredicted Agriculture\n\n\n\n\nActual Forest\n85\n8 ← Confusion\n\n\nActual Agriculture\n12 ← Confusion\n73\n\n\n\nWhy confusion occurs:\n\nSpectral Similarity\n\nTree crops (coconut, fruit trees) look like forest\nYoung forest regeneration looks like agriculture\n\nMixed Pixels\n\nAgroforestry systems\nForest edges with agriculture\n\nTemporal Variability\n\nAgriculture changes rapidly (planting, harvesting)\nSingle-date imagery may miss phenology\n\nClass Definition Ambiguity\n\nWhere does “forest” end and “tree plantation” begin?"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#best-practices-training-data-collection",
    "href": "day2/presentations/session1_random_forest.html#best-practices-training-data-collection",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Best Practices: Training Data Collection",
    "text": "Best Practices: Training Data Collection\nPractical Tips for High-Quality Training Samples:\n\n\nSample Size Guidelines:\n\n\n\nClass\nMin Samples\nRecommended\nNotes\n\n\n\n\nCommon (forest)\n50\n100-200\nMore coverage\n\n\nModerate (agriculture)\n50\n100-150\nCapture variability\n\n\nRare (bare soil)\n30\n50-100\nGet what you can\n\n\n\nSampling Strategies:\n1. Stratified Random: - Distribute samples across study area - Avoid clustering in one region - Ensure all sub-types represented\n2. Purposive Sampling: - Target known pure pixels - Use high-resolution imagery (Google Earth) - Field visits when possible\n\nQuality Criteria:\n✓ Pure Pixels - Homogeneous within polygon - Avoid edges and mixed areas - Use ≥3x3 pixel minimum areas\n✓ Clear Definition - Unambiguous class membership - Document class definitions - Use consistent interpretation rules\n✓ Temporal Match - Training data date matches imagery - Account for phenology (crops) - Update for multi-temporal analysis\nPhilippine-Specific Tips: - Use PhilSA Space+ Dashboard for recent imagery - Leverage NAMRIA land cover for reference - Consult LGU land use plans for urban areas - Use Google Street View for ground truth\n\n\nQuality training data is more important than quantity. 100 high-quality samples beats 500 noisy samples. For Philippine applications, leverage existing government datasets as starting points."
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#improving-classification-accuracy",
    "href": "day2/presentations/session1_random_forest.html#improving-classification-accuracy",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Improving Classification Accuracy",
    "text": "Improving Classification Accuracy\n\nBetter Training DataMore FeaturesBetter ModelPost-Processing\n\n\n\nMore samples: 50-100 per class minimum\nBetter quality: Pure pixels, clear boundaries\nBalanced: Equal samples per class\nRepresentative: Cover all variations within class\nDistributed: Spatial coverage across study area\n\n\n\n\nMulti-temporal data: Capture seasonal differences\nTexture features: Spatial patterns (GLCM)\nTopographic features: Elevation, slope, aspect\nRadar data: Sentinel-1 SAR (cloud-free)\nAncillary data: Roads, protected areas\n\n\n\n\nHyperparameter tuning: Optimize RF parameters\nMore trees: 200-500 trees (diminishing returns after ~300)\nCross-validation: Ensure generalization\nEnsemble methods: Combine RF with other classifiers\n\n\n\n\nMajority filter: Smooth noisy pixels\nMinimum mapping unit: Remove small isolated pixels\nExpert rules: Apply domain knowledge\nVisual inspection: Manual correction of obvious errors"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#why-google-earth-engine",
    "href": "day2/presentations/session1_random_forest.html#why-google-earth-engine",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Why Google Earth Engine?",
    "text": "Why Google Earth Engine?\n\n\nChallenges with Desktop GIS:\n\n✗ Downloading large satellite data\n✗ Storage requirements (TBs)\n✗ Computational limitations\n✗ Manual preprocessing\n✗ Time-consuming workflows\n\n\nGoogle Earth Engine Solutions:\n\n✓ Petabyte-scale catalog (Landsat, Sentinel, MODIS…)\n✓ Cloud computing (no downloads)\n✓ Pre-processed data (atmospherically corrected)\n✓ Scalable processing (parallel)\n✓ Free for research & education\n\n\n\n\n\n\n\n\n\nPerfect for This Course\n\n\nGEE enables us to process years of Sentinel-2 data for entire provinces in minutes!"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#google-earth-engine-architecture",
    "href": "day2/presentations/session1_random_forest.html#google-earth-engine-architecture",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Google Earth Engine Architecture",
    "text": "Google Earth Engine Architecture\n\n\n\n\n\nflowchart TD\n    A[User Code&lt;br/&gt;Python/JavaScript] --&gt; B[GEE API]\n    B --&gt; C[GEE Cloud&lt;br/&gt;Processing]\n\n    D[Satellite Data&lt;br/&gt;Catalog] --&gt; C\n    E[Landsat&lt;br/&gt;1972-present] --&gt; D\n    F[Sentinel-1/2&lt;br/&gt;2014-present] --&gt; D\n    G[MODIS&lt;br/&gt;2000-present] --&gt; D\n    H[Climate Data&lt;br/&gt;ERA5, etc.] --&gt; D\n\n    C --&gt; I[Results]\n    I --&gt; J[Interactive Map]\n    I --&gt; K[Export to Drive]\n    I --&gt; L[Charts/Stats]\n\n    style C fill:#4285F4\n    style D fill:#34A853\n\n\n\n\n\n\nKey Concepts:\n\nServer-side processing: Code runs on Google servers, not your laptop\nLazy evaluation: Operations queued, executed only when needed (e.g., map display, export)\nParallel processing: Automatically distributed across many machines"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#gee-data-catalog-highlights",
    "href": "day2/presentations/session1_random_forest.html#gee-data-catalog-highlights",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "GEE Data Catalog Highlights",
    "text": "GEE Data Catalog Highlights\nRelevant for Philippine EO:\n\n\nOptical Imagery:\n\nSentinel-2 MSI: 10m, 13 bands, 5-day revisit\nLandsat 8/9 OLI: 30m, 11 bands, 16-day revisit\nMODIS: 250-500m, daily, long time series\n\nRadar:\n\nSentinel-1 SAR: 10m, cloud-free, day/night\n\nTerrain:\n\nSRTM DEM: 30m elevation\nALOS World 3D: 30m (better for SE Asia)\n\n\nClimate:\n\nERA5: Hourly reanalysis (temp, precip)\nCHIRPS: Daily rainfall\nMODIS LST: Land surface temperature\n\nPre-processed Products:\n\nHansen Global Forest Change: Annual tree cover loss\nESA WorldCover: Global 10m land cover\nGlobal Surface Water: Water occurrence"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#gee-code-editor-vs.-python-api",
    "href": "day2/presentations/session1_random_forest.html#gee-code-editor-vs.-python-api",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "GEE Code Editor vs. Python API",
    "text": "GEE Code Editor vs. Python API\n\n\nJavaScript (Code Editor)\n\nPros:\n\nBrowser-based (no installation)\nInteractive map interface\nBuilt-in visualization\nGreat for exploration\n\nCons:\n\nLimited to GEE environment\nHarder to integrate with other tools\nLess powerful for data science\n\n\nUse Case: Quick exploration, visualization\n\nPython API\n\nPros:\n\nIntegrate with NumPy, Pandas, scikit-learn\nJupyter notebooks\nReproducible workflows\nVersion control (Git)\nAdvanced analysis\n\nCons:\n\nRequires installation/setup\nSlightly steeper learning curve\n\n\nUse Case: Reproducible research, production workflows\n\n\n\n\n\n\n\n\nOur Approach\n\n\nWe’ll use Python API with geemap library for best of both worlds: Python ecosystem + interactive maps!"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#gee-random-forest-workflow",
    "href": "day2/presentations/session1_random_forest.html#gee-random-forest-workflow",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "GEE Random Forest Workflow",
    "text": "GEE Random Forest Workflow\nHigh-level workflow for today’s lab:\nimport ee\nimport geemap\n\n# 1. Initialize GEE\nee.Initialize()\n\n# 2. Load Sentinel-2 imagery\ns2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n    .filterBounds(palawan_boundary) \\\n    .filterDate('2024-01-01', '2024-12-31') \\\n    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n\n# 3. Compute composite and indices\ncomposite = s2.median()\nndvi = composite.normalizedDifference(['B8', 'B4']).rename('NDVI')\nndwi = composite.normalizedDifference(['B3', 'B8']).rename('NDWI')\n\n# 4. Stack features\nfeatures = composite.select(['B2','B3','B4','B8','B11','B12']) \\\n    .addBands([ndvi, ndwi])\n\n# 5. Sample training data\ntraining = features.sampleRegions(collection=training_polygons,\n                                   properties=['class'],\n                                   scale=10)\n\n# 6. Train Random Forest\nclassifier = ee.Classifier.smileRandomForest(numberOfTrees=100) \\\n    .train(features=training, classProperty='class', inputProperties=features.bandNames())\n\n# 7. Classify image\nclassified = features.classify(classifier)\n\n# 8. Visualize\nMap = geemap.Map()\nMap.addLayer(classified, vis_params, 'Land Cover')\nMap"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#forest-monitoring-denr-redd",
    "href": "day2/presentations/session1_random_forest.html#forest-monitoring-denr-redd",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Forest Monitoring (DENR, REDD+)",
    "text": "Forest Monitoring (DENR, REDD+)\n\n\nChallenges:\n\n7,641 islands, 30 million hectares\nCloud cover year-round\nRapid deforestation in some areas\nLimited ground-based monitoring\n\nRF Classification Helps:\n\nAnnual forest cover maps\nDeforestation hotspot detection\nREDD+ MRV (Monitoring, Reporting, Verification)\nProtected area encroachment\n\n\nExample: Palawan Biosphere Reserve\n\nArea: 1.1 million hectares\nProtection: UNESCO MAB, NIPAS\nThreats: Illegal logging, mining, agriculture\n\nWorkflow:\n\nAnnual Sentinel-2 composites (2016-2024)\nRF classification (primary forest, secondary, non-forest)\nChange detection (forest loss/gain)\nAlert system for encroachment\nReports for PCSDS, DENR"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#agricultural-monitoring-da-philrice",
    "href": "day2/presentations/session1_random_forest.html#agricultural-monitoring-da-philrice",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Agricultural Monitoring (DA, PhilRice)",
    "text": "Agricultural Monitoring (DA, PhilRice)\n\n\nRice Production Monitoring:\n\nGoal: Estimate planted area and yield\nImportance: Food security planning\nTraditional method: Field surveys (slow, expensive)\n\nRF Approach:\n\nMulti-temporal Sentinel-2 (capture crop phenology)\nTraining data from field surveys\nClassify: Rice, Other crops, Non-ag\nArea calculation per province/municipality\nEarly warning for production shortfalls\n\n\nExample: Central Luzon Rice Bowl\nClasses: - Rice (wet season) - Rice (dry season) - Vegetables - Fallow/bare - Non-agricultural\nFeatures: - NDVI time series (captures growth cycle) - LSWI (Land Surface Water Index) - EVI (Enhanced Vegetation Index)\nValidation: - PhilRice field surveys - DA crop cut experiments"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#urban-expansion-monitoring-neda-hlurb",
    "href": "day2/presentations/session1_random_forest.html#urban-expansion-monitoring-neda-hlurb",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Urban Expansion Monitoring (NEDA, HLURB)",
    "text": "Urban Expansion Monitoring (NEDA, HLURB)\n\n\nMetro Manila & Major Cities:\n\nRapid urbanization: 3-5% annual growth\nPlanning needs: Infrastructure, transport, housing\nEnvironmental concerns: Loss of green space, flooding\n\nRF Classification:\n\nUrban/built-up\nRoads and infrastructure\nVegetation (parks, trees)\nBare soil (construction sites)\nWater bodies\n\n\nApplications:\n\nUrban growth tracking\n\nCompare 2015 vs. 2024\nIdentify sprawl patterns\nPredict future expansion\n\nGreen space monitoring\n\nUrban vegetation loss\nPark accessibility analysis\n\nFlood risk\n\nImpervious surface mapping\nDrainage planning\n\nCompliance\n\nIllegal construction detection\nZoning violations"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#water-resources-nwrb-lgus",
    "href": "day2/presentations/session1_random_forest.html#water-resources-nwrb-lgus",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Water Resources (NWRB, LGUs)",
    "text": "Water Resources (NWRB, LGUs)\nApplications:\n\n\nSurface Water Mapping:\n\nRivers, lakes, reservoirs\nSeasonal variations\nDrought monitoring\nFlood extent mapping\n\nRF Advantages: - NDWI as strong predictor - Multi-temporal captures seasonal changes - Can detect small water bodies\n\nWatershed Management:\n\nLand cover within watersheds\nForest cover (water regulation)\nAgriculture (erosion risk)\nUrban (runoff)\n\nExample: Angat Dam Watershed - Critical for Metro Manila water supply - Monitor forest cover changes - Detect encroachment - Sediment risk assessment"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#disaster-response-ndrrmc-pagasa",
    "href": "day2/presentations/session1_random_forest.html#disaster-response-ndrrmc-pagasa",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Disaster Response (NDRRMC, PAGASA)",
    "text": "Disaster Response (NDRRMC, PAGASA)\nPost-Typhoon Damage Assessment:\n\n\nChallenge: - Philippines: ~20 typhoons/year - Rapid assessment needed for relief - Cloud-free imagery rare after storms\nRF Classification Approach:\n\nPre-event baseline: Land cover map\nPost-event imagery: First clear Sentinel-2\nDamage classes:\n\nIntact forest/vegetation\nDamaged vegetation\nExposed soil/landslides\nFlooded areas\nBuilding damage (requires very high res)\n\n\n\nExample: Typhoon Odette (2021)\n\nAffected: Visayas, Mindanao\nAssessment needs:\n\nAgricultural damage (coconut, rice)\nForest destruction\nCoastal erosion\nFlooded areas\n\n\nRF Workflow: - Pre-typhoon: December 2021 composite - Post-typhoon: January 2022 composite - Classify: Intact, Damaged, Destroyed - Area statistics per municipality - Priority areas for relief"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#recap-session-1-theory",
    "href": "day2/presentations/session1_random_forest.html#recap-session-1-theory",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Recap: Session 1 Theory",
    "text": "Recap: Session 1 Theory\n\n\nWhat We Learned:\n✓ Supervised classification workflow\n✓ Decision trees: Intuitive but limited\n✓ Random Forest: Ensemble of trees - Bootstrap sampling - Random feature selection - Majority voting\n✓ Feature importance: Which bands matter?\n✓ Accuracy assessment: - Confusion matrix - Overall, Producer’s, User’s accuracy - Kappa coefficient\n✓ Google Earth Engine: Cloud-based EO\n\nKey Takeaways:\n\nRandom Forest is powerful for EO classification\n\nHigh accuracy\nHandles non-linear relationships\nRobust to overfitting\n\nTraining data quality is critical\n\nRepresentative samples\nBalanced classes\nSufficient quantity\n\nFeature engineering improves results\n\nSpectral indices (NDVI, NDWI)\nMulti-temporal data\nAuxiliary data (DEM)\n\nAccuracy assessment builds confidence\n\nAlways use independent test data\nUnderstand confusion patterns"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#break",
    "href": "day2/presentations/session1_random_forest.html#break",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Break",
    "text": "Break\n15-minute break before hands-on lab\n\n🔸 Stretch\n🔸 Coffee/water\n🔸 Check your setup: - Google Earth Engine account - Python environment activated - Jupyter notebook ready\n\nComing up: Hands-on lab with Palawan land cover classification!"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#lab-overview",
    "href": "day2/presentations/session1_random_forest.html#lab-overview",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Lab Overview",
    "text": "Lab Overview\nWhat We’ll Build: Palawan Land Cover Classification using Random Forest\n\n\nSteps:\n\nSetup and authentication\nLoad Sentinel-2 imagery\nCreate cloud-free composite\nCalculate spectral indices\nPrepare training data\nTrain Random Forest model\nGenerate classification map\nValidate accuracy\nAnalyze results\n\nDuration: ~1.5 hours\n\nStudy Area: Palawan Province\n\nLocation: Western Philippines\nArea: ~14,649 km²\nSignificance: UNESCO Biosphere Reserve\nDiversity: Forest, mangroves, agriculture, urban\n\nClasses: 1. Forest 2. Agriculture 3. Water 4. Urban 5. Bare Soil"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#spectral-indices-for-classification",
    "href": "day2/presentations/session1_random_forest.html#spectral-indices-for-classification",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Spectral Indices for Classification",
    "text": "Spectral Indices for Classification\nKey Features Beyond Raw Bands:\n\n\nVegetation Indices:\n\n\n\n\n\n\n\n\nIndex\nFormula\nPurpose\n\n\n\n\nNDVI\n(NIR - Red) / (NIR + Red)\nVegetation vigor\n\n\nEVI\n2.5 × (NIR - Red) / (NIR + 6×Red - 7.5×Blue + 1)\nEnhanced sensitivity in high biomass\n\n\n\n# Calculate NDVI\nndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n\n# Calculate EVI\nevi = image.expression(\n    '2.5 * ((NIR - RED) / (NIR + 6*RED - 7.5*BLUE + 1))',\n    {'NIR': image.select('B8'),\n     'RED': image.select('B4'),\n     'BLUE': image.select('B2')\n    }).rename('EVI')\n\nWater & Built-up Indices:\n\n\n\n\n\n\n\n\nIndex\nFormula\nPurpose\n\n\n\n\nNDWI\n(Green - NIR) / (Green + NIR)\nWater bodies\n\n\nMNDWI\n(Green - SWIR) / (Green + SWIR)\nWater/wetlands (better separation)\n\n\nNDBI\n(SWIR - NIR) / (SWIR + NIR)\nBuilt-up areas\n\n\n\n# Calculate water indices\nndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')\nmndwi = image.normalizedDifference(['B3', 'B11']).rename('MNDWI')\n\n# Calculate built-up index\nndbi = image.normalizedDifference(['B11', 'B8']).rename('NDBI')\n\n\nWhy MNDWI? Better separates water from built-up areas than NDWI (uses SWIR instead of NIR)\n\n\nMNDWI is particularly useful in coastal areas like Palawan where you need to distinguish water bodies from urban areas. SWIR absorption by water creates stronger contrast."
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#lab-instructions",
    "href": "day2/presentations/session1_random_forest.html#lab-instructions",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Lab Instructions",
    "text": "Lab Instructions\nFollow along in Jupyter notebook:\n../notebooks/session1_hands_on_lab_student.ipynb\nStudent version: With TODO markers for exercises\nInstructor version: Complete solutions\n\n\n\n\n\n\n\nTips for Success\n\n\n\nRead markdown cells carefully before running code\nExperiment with parameters\nVisualize intermediate results\nAsk questions when stuck!"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#expected-outputs",
    "href": "day2/presentations/session1_random_forest.html#expected-outputs",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Expected Outputs",
    "text": "Expected Outputs\nBy the end of the lab, you will have:\n\n✓ Interactive map of Palawan with Sentinel-2 composite\n✓ Calculated spectral indices (NDVI, NDWI, NDBI)\n✓ Trained Random Forest classifier (100 trees)\n✓ Land cover classification map\n✓ Confusion matrix and accuracy metrics\n✓ Feature importance ranking\n✓ Area statistics per land cover class\n✓ Exported classification to Google Drive\n\nAccuracy Target: &gt;80% overall accuracy"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#session-1-summary",
    "href": "day2/presentations/session1_random_forest.html#session-1-summary",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Session 1 Summary",
    "text": "Session 1 Summary\n\n\nTheory Concepts:\n\nSupervised classification workflow\nDecision trees → Random Forest\nBootstrap aggregating\nRandom feature selection\nFeature importance\nAccuracy assessment metrics\nConfusion matrix interpretation\n\nTools:\n\nGoogle Earth Engine\nPython API (geemap)\nSentinel-2 imagery\n\n\nPractical Skills:\n\nGEE authentication\nImageCollection filtering\nComposite generation\nSpectral index calculation\nTraining data preparation\nRF model training\nClassification execution\nAccuracy validation\nMap visualization\n\nPhilippine Context:\n\nPalawan land cover mapping\nDENR forest monitoring\nDA agricultural mapping\nNDRRMC disaster response"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#next-session-preview",
    "href": "day2/presentations/session1_random_forest.html#next-session-preview",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Next Session Preview",
    "text": "Next Session Preview\nSession 2: Advanced Palawan Land Cover Lab\n\nMulti-temporal composites (dry/wet season)\nAdvanced feature engineering (GLCM texture)\nTopographic features (DEM)\n8-class detailed classification\nHyperparameter tuning\nChange detection (2020 vs. 2024)\nDeforestation analysis\nStakeholder reporting\n\n\nPreparation:\n\nComplete Session 1 exercises\nReview confusion matrix analysis\nThink about classification improvements"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#resources",
    "href": "day2/presentations/session1_random_forest.html#resources",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Resources",
    "text": "Resources\nDocumentation:\n\nGoogle Earth Engine: https://developers.google.com/earth-engine\ngeemap: https://geemap.org\nSentinel-2: https://sentinel.esa.int/web/sentinel/missions/sentinel-2\nRandom Forest paper: Breiman (2001) - Machine Learning 45:5-32\n\nPhilippine EO:\n\nPhilSA: https://philsa.gov.ph\nNAMRIA: https://namria.gov.ph\nDOST-ASTI PANDA: https://panda.stamina4space.upd.edu.ph\n\nCourse Materials:\n\nGitHub: [repository link]\nDatasets: [Google Drive link]"
  },
  {
    "objectID": "day2/presentations/session1_random_forest.html#thank-you",
    "href": "day2/presentations/session1_random_forest.html#thank-you",
    "title": "Session 1: Supervised Classification with Random Forest",
    "section": "Thank You!",
    "text": "Thank You!\n\nQuestions?\n\nContact:\n\nEmail: skotsopoulos@neuralio.ai\nOffice Hours: [schedule]\n\n\nLet’s move to the hands-on lab!\n🚀 Open: session1_hands_on_lab.ipynb"
  },
  {
    "objectID": "day2/notebooks/session1_theory_notebook_STUDENT.html",
    "href": "day2/notebooks/session1_theory_notebook_STUDENT.html",
    "title": "Session 1 Theory: Understanding Random Forest for Earth Observation",
    "section": "",
    "text": "CoPhil 4-Day Advanced Online Training\nDAY 2 - Session 1: Supervised Machine Learning - Part 1",
    "crumbs": [
      "Notebooks",
      "Session 1 Theory: Understanding Random Forest for Earth Observation"
    ]
  },
  {
    "objectID": "day2/notebooks/session1_theory_notebook_STUDENT.html#learning-objectives",
    "href": "day2/notebooks/session1_theory_notebook_STUDENT.html#learning-objectives",
    "title": "Session 1 Theory: Understanding Random Forest for Earth Observation",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this notebook, you will be able to:\n\nUnderstand Decision Trees: Explain how a single decision tree makes predictions through recursive splitting\nGrasp Ensemble Learning: Describe how Random Forest combines multiple trees through bootstrap sampling and random feature selection\nInterpret Feature Importance: Analyze which spectral bands or derived indices contribute most to classification\nEvaluate Model Performance: Read and interpret confusion matrices to assess classification accuracy\nApply to EO Context: Connect these concepts to satellite image classification tasks",
    "crumbs": [
      "Notebooks",
      "Session 1 Theory: Understanding Random Forest for Earth Observation"
    ]
  },
  {
    "objectID": "day2/notebooks/session1_theory_notebook_STUDENT.html#why-random-forest-for-earth-observation",
    "href": "day2/notebooks/session1_theory_notebook_STUDENT.html#why-random-forest-for-earth-observation",
    "title": "Session 1 Theory: Understanding Random Forest for Earth Observation",
    "section": "Why Random Forest for Earth Observation?",
    "text": "Why Random Forest for Earth Observation?\nRandom Forest is one of the most popular algorithms for land cover classification because:\n\nHandles high-dimensional data: Works well with many spectral bands (Sentinel-2 has 13 bands)\nRobust to overfitting: Ensemble approach reduces variance\nFeature importance: Reveals which bands are most informative\nNo feature scaling required: Unlike neural networks\nFast training: Efficient even with large datasets\nInterpretable: Can visualize decision rules\n\n\nEstimated Time: 70 minutes",
    "crumbs": [
      "Notebooks",
      "Session 1 Theory: Understanding Random Forest for Earth Observation"
    ]
  },
  {
    "objectID": "day2/notebooks/session1_theory_notebook_STUDENT.html#a.-introduction-and-setup-5-minutes",
    "href": "day2/notebooks/session1_theory_notebook_STUDENT.html#a.-introduction-and-setup-5-minutes",
    "title": "Session 1 Theory: Understanding Random Forest for Earth Observation",
    "section": "A. Introduction and Setup (5 minutes)",
    "text": "A. Introduction and Setup (5 minutes)\nLet’s start by importing the necessary libraries and setting up our environment for reproducible results.\n\n# Core scientific computing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scikit-learn for machine learning\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n# Suppress warnings for cleaner output\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\n# Configure plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"colorblind\")  # Color-blind friendly palette\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.rcParams['font.size'] = 11\n\nprint(\"✓ Libraries imported successfully!\")\nprint(f\"✓ Random state set to: {RANDOM_STATE}\")\nprint(f\"✓ NumPy version: {np.__version__}\")\nprint(f\"✓ Pandas version: {pd.__version__}\")",
    "crumbs": [
      "Notebooks",
      "Session 1 Theory: Understanding Random Forest for Earth Observation"
    ]
  },
  {
    "objectID": "day2/notebooks/session1_theory_notebook_STUDENT.html#b.-decision-trees-interactive-demo-15-minutes",
    "href": "day2/notebooks/session1_theory_notebook_STUDENT.html#b.-decision-trees-interactive-demo-15-minutes",
    "title": "Session 1 Theory: Understanding Random Forest for Earth Observation",
    "section": "B. Decision Trees Interactive Demo (15 minutes)",
    "text": "B. Decision Trees Interactive Demo (15 minutes)\n\nWhat is a Decision Tree?\nA Decision Tree is a supervised learning algorithm that makes predictions by learning a series of if-then-else decision rules from data. Think of it like a flowchart:\nIs NDVI &gt; 0.3?\n├─ Yes: Is NIR &gt; 0.5?\n│  ├─ Yes: Forest\n│  └─ No: Grassland\n└─ No: Is SWIR &lt; 0.2?\n   ├─ Yes: Water\n   └─ No: Urban\n\n\nKey Concepts:\n\nRoot Node: The first decision point (top of the tree)\nInternal Nodes: Intermediate decision points\nLeaf Nodes: Final predictions (bottom of the tree)\nSplitting: How the algorithm decides which feature and threshold to use\nDepth: Number of levels in the tree (deeper = more complex)\n\n\n\nLet’s Build a Simple Example\n\n# Create a simple 2D classification dataset\n# This simulates two spectral bands (e.g., NIR and Red)\nX, y = make_moons(n_samples=200, noise=0.25, random_state=RANDOM_STATE)\n\n# Add feature names for EO context\nfeature_names = ['NIR Reflectance', 'Red Reflectance']\nclass_names = ['Water/Urban', 'Vegetation']\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Number of samples: {X.shape[0]}\")\nprint(f\"Number of features: {X.shape[1]}\")\nprint(f\"Classes: {np.unique(y)}\")\nprint(f\"Class distribution: {np.bincount(y)}\")\n\n\n# Visualize the dataset\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n                     s=50, alpha=0.7, edgecolors='k', linewidth=0.5)\nplt.xlabel(feature_names[0], fontsize=12)\nplt.ylabel(feature_names[1], fontsize=12)\nplt.title('Training Data: Two Spectral Bands', fontsize=14, fontweight='bold')\nplt.colorbar(scatter, label='Class', ticks=[0, 1])\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n💡 TIP: In real EO applications, each point would represent a pixel with its spectral reflectance values.\")\n\n\n\nTrain a Single Decision Tree\nLet’s train a decision tree and visualize how it splits the feature space.\n\n# Train a decision tree with limited depth\ntree = DecisionTreeClassifier(max_depth=3, random_state=RANDOM_STATE)\ntree.fit(X, y)\n\n# Calculate training accuracy\ntrain_accuracy = tree.score(X, y)\nprint(f\"Training Accuracy: {train_accuracy:.3f}\")\nprint(f\"Tree Depth: {tree.get_depth()}\")\nprint(f\"Number of Leaves: {tree.get_n_leaves()}\")\n\n\n# Visualize decision boundaries\ndef plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n    \"\"\"\n    Plot decision boundary for a 2D classification problem.\n    \n    Parameters:\n    -----------\n    model : trained classifier\n    X : array-like, shape (n_samples, 2)\n    y : array-like, shape (n_samples,)\n    title : str\n    \"\"\"\n    # Create mesh grid\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                         np.linspace(y_min, y_max, 200))\n    \n    # Predict on mesh grid\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    # Plot\n    plt.figure(figsize=(10, 6))\n    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis', levels=1)\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n                         s=50, alpha=0.8, edgecolors='k', linewidth=0.5)\n    plt.xlabel(feature_names[0], fontsize=12)\n    plt.ylabel(feature_names[1], fontsize=12)\n    plt.title(title, fontsize=14, fontweight='bold')\n    plt.colorbar(scatter, label='Class', ticks=[0, 1])\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\nplot_decision_boundary(tree, X, y, \n                      title=\"Decision Tree: How It Splits the Feature Space\")\n\nprint(\"\\n💡 TIP: Notice the rectangular decision boundaries. Trees can only make\")\nprint(\"   axis-aligned splits (e.g., 'NIR &gt; 0.5'), not diagonal lines.\")\n\n\n\nVisualize the Tree Structure\nLet’s look inside the tree to see the actual decision rules it learned.\n\n# Plot the tree structure\nplt.figure(figsize=(20, 10))\nplot_tree(tree, \n         feature_names=feature_names,\n         class_names=class_names,\n         filled=True,\n         rounded=True,\n         fontsize=10)\nplt.title('Decision Tree Structure', fontsize=16, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nHow to Read This Tree:\")\nprint(\"━\" * 60)\nprint(\"• Each box is a node with a decision rule (e.g., 'NIR &lt;= 0.5')\")\nprint(\"• 'gini' measures impurity (0 = pure, 0.5 = mixed)\")\nprint(\"• 'samples' shows how many training points reach this node\")\nprint(\"• 'value' shows class distribution [class 0, class 1]\")\nprint(\"• Color intensity indicates class majority (darker = more confident)\")\nprint(\"• Leaf nodes (bottom) make the final prediction\")\n\n\n\n🎯 Interactive Exercise: Effect of Tree Depth\nTask: Experiment with different max_depth values and observe how the decision boundary changes.\nQuestions to consider: 1. What happens with max_depth=1 (a “decision stump”)? 2. What happens with max_depth=10 (very deep tree)? 3. Which depth seems to balance simplicity and accuracy? 4. Can you identify overfitting?\n\n# TODO: Experiment with different max_depth values\n# Try: max_depth = 1, 2, 5, 10, None (unlimited)\n\nmax_depth_to_test = 1  # TODO: Change this value\n\ntree_experiment = DecisionTreeClassifier(max_depth=max_depth_to_test, \n                                        random_state=RANDOM_STATE)\ntree_experiment.fit(X, y)\n\naccuracy = tree_experiment.score(X, y)\nprint(f\"Max Depth: {max_depth_to_test}\")\nprint(f\"Training Accuracy: {accuracy:.3f}\")\nprint(f\"Actual Tree Depth: {tree_experiment.get_depth()}\")\nprint(f\"Number of Leaves: {tree_experiment.get_n_leaves()}\")\n\nplot_decision_boundary(tree_experiment, X, y, \n                      title=f\"Decision Tree with max_depth={max_depth_to_test}\")\n\nprint(\"\\n⚠️ COMMON MISTAKE: Setting max_depth=None can lead to overfitting!\")\nprint(\"   The tree will memorize training data instead of learning patterns.\")",
    "crumbs": [
      "Notebooks",
      "Session 1 Theory: Understanding Random Forest for Earth Observation"
    ]
  },
  {
    "objectID": "day2/notebooks/session1_theory_notebook_STUDENT.html#c.-random-forest-voting-mechanism-15-minutes",
    "href": "day2/notebooks/session1_theory_notebook_STUDENT.html#c.-random-forest-voting-mechanism-15-minutes",
    "title": "Session 1 Theory: Understanding Random Forest for Earth Observation",
    "section": "C. Random Forest Voting Mechanism (15 minutes)",
    "text": "C. Random Forest Voting Mechanism (15 minutes)\n\nThe Power of Ensemble Learning\nA single decision tree can be unstable: - Small changes in data can lead to completely different trees - Prone to overfitting (memorizing training data) - High variance in predictions\nRandom Forest solves this by combining many trees:\n\nBootstrap Sampling: Each tree trains on a random subset of data (sampling with replacement)\nRandom Feature Selection: Each split only considers a random subset of features\nMajority Voting: Final prediction is the class chosen by most trees\n\nAnalogy: Instead of asking one expert (one tree), you ask a committee of experts (forest) and take a vote. This “wisdom of the crowd” is more robust!\n\n# Train a Random Forest with just 5 trees (for visualization)\nn_trees = 5\nrf_small = RandomForestClassifier(n_estimators=n_trees, \n                                 max_depth=3,\n                                 random_state=RANDOM_STATE)\nrf_small.fit(X, y)\n\nrf_accuracy = rf_small.score(X, y)\nprint(f\"Random Forest Accuracy (5 trees): {rf_accuracy:.3f}\")\nprint(f\"Single Tree Accuracy (from before): {train_accuracy:.3f}\")\nprint(f\"\\nImprovement: {rf_accuracy - train_accuracy:.3f}\")\n\n\n\nVisualize Individual Trees in the Forest\nLet’s see how each tree makes different decisions.\n\n# Plot decision boundaries for each individual tree\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.ravel()\n\n# Plot each individual tree\nfor idx, tree in enumerate(rf_small.estimators_):\n    ax = axes[idx]\n    \n    # Create mesh grid\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                         np.linspace(y_min, y_max, 200))\n    \n    # Predict\n    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    # Plot\n    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis', levels=1)\n    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n              s=30, alpha=0.6, edgecolors='k', linewidth=0.3)\n    ax.set_xlabel(feature_names[0])\n    ax.set_ylabel(feature_names[1])\n    ax.set_title(f'Tree {idx + 1}', fontweight='bold')\n    ax.grid(True, alpha=0.3)\n\n# Plot the ensemble (Random Forest)\nax = axes[5]\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                     np.linspace(y_min, y_max, 200))\nZ = rf_small.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis', levels=1)\nax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n          s=30, alpha=0.6, edgecolors='k', linewidth=0.3)\nax.set_xlabel(feature_names[0])\nax.set_ylabel(feature_names[1])\nax.set_title('Random Forest (Ensemble)', fontweight='bold', color='red')\nax.grid(True, alpha=0.3)\n\nplt.suptitle('Individual Trees vs. Ensemble Decision', \n            fontsize=16, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n💡 TIP: Notice how each tree is slightly different due to bootstrap\")\nprint(\"   sampling and random feature selection. The ensemble smooths out\")\nprint(\"   individual errors and creates more stable boundaries.\")\n\n\n\nVisualize Voting Confidence\nRandom Forest can provide prediction probabilities based on the proportion of trees voting for each class.\n\n# Get prediction probabilities\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                     np.linspace(y_min, y_max, 200))\n\n# Predict probabilities for class 1 (Vegetation)\nZ_proba = rf_small.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\nZ_proba = Z_proba.reshape(xx.shape)\n\n# Plot confidence\nplt.figure(figsize=(12, 7))\ncontour = plt.contourf(xx, yy, Z_proba, levels=20, cmap='RdYlGn', alpha=0.8)\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', \n           s=50, alpha=0.7, edgecolors='k', linewidth=0.5)\nplt.colorbar(contour, label='Confidence for Vegetation Class')\nplt.xlabel(feature_names[0], fontsize=12)\nplt.ylabel(feature_names[1], fontsize=12)\nplt.title('Random Forest Prediction Confidence\\n(Based on Voting Proportions)', \n         fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nInterpreting Confidence:\")\nprint(\"━\" * 60)\nprint(\"• Green (high values): Most trees vote for 'Vegetation'\")\nprint(\"• Red (low values): Most trees vote for 'Water/Urban'\")\nprint(\"• Yellow (middle values): Trees are uncertain (mixed votes)\")\nprint(\"\\n💡 TIP: Low confidence regions often indicate:\")\nprint(\"   - Class boundaries\")\nprint(\"   - Mixed pixels (in EO context)\")\nprint(\"   - Need for more training data\")\n\n\n\n🎯 Interactive Exercise: Effect of Number of Trees\nTask: Test how the number of trees affects model stability and accuracy.\nHypothesis: More trees → more stable predictions, but diminishing returns after a certain point.\n\n# TODO: Test different numbers of trees\ntree_counts = [1, 5, 10, 50, 100, 200]\naccuracies = []\n\nfor n in tree_counts:\n    # TODO: Create and train a Random Forest with n trees\n    rf = RandomForestClassifier(n_estimators=n, \n                               max_depth=3,\n                               random_state=RANDOM_STATE)\n    rf.fit(X, y)\n    acc = rf.score(X, y)\n    accuracies.append(acc)\n    print(f\"n_estimators={n:3d} → Accuracy: {acc:.4f}\")\n\n# Plot accuracy vs. number of trees\nplt.figure(figsize=(10, 6))\nplt.plot(tree_counts, accuracies, marker='o', linewidth=2, markersize=8)\nplt.xlabel('Number of Trees', fontsize=12)\nplt.ylabel('Training Accuracy', fontsize=12)\nplt.title('Effect of Ensemble Size on Accuracy', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n📊 Observation: Accuracy stabilizes after ~50-100 trees.\")\nprint(\"   In practice, 100-500 trees is common for EO applications.\")",
    "crumbs": [
      "Notebooks",
      "Session 1 Theory: Understanding Random Forest for Earth Observation"
    ]
  },
  {
    "objectID": "day2/notebooks/session1_theory_notebook_STUDENT.html#d.-feature-importance-analysis-10-minutes",
    "href": "day2/notebooks/session1_theory_notebook_STUDENT.html#d.-feature-importance-analysis-10-minutes",
    "title": "Session 1 Theory: Understanding Random Forest for Earth Observation",
    "section": "D. Feature Importance Analysis (10 minutes)",
    "text": "D. Feature Importance Analysis (10 minutes)\n\nWhy Feature Importance Matters in EO\nFeature importance tells us: - Which spectral bands contribute most to classification - Whether derived indices (NDVI, NDWI) are valuable - If certain features are redundant - How to optimize future data collection\nHow Random Forest Calculates Importance: - Measures how much each feature decreases impurity (Gini or entropy) - Averaged across all trees in the forest - Higher values = more important for classification\n\n# Create a dataset mimicking Sentinel-2 spectral bands\nnp.random.seed(RANDOM_STATE)\n\n# Simulate 1000 pixels with 8 \"spectral bands\"\nn_samples = 1000\nn_features = 8\n\n# Feature names mimicking Sentinel-2 bands and indices\neo_feature_names = [\n    'Blue (B2)',\n    'Green (B3)',\n    'Red (B4)',\n    'NIR (B8)',\n    'SWIR1 (B11)',\n    'SWIR2 (B12)',\n    'NDVI',\n    'NDWI'\n]\n\n# Create synthetic data with realistic patterns\n# Class 0: Water (low NIR, high Blue, high NDWI)\n# Class 1: Vegetation (high NIR, low Red, high NDVI)\n# Class 2: Urban (moderate all, low NDVI, low NDWI)\n\nX_eo = np.random.rand(n_samples, n_features)\ny_eo = np.random.choice([0, 1, 2], size=n_samples)\n\n# Add class-specific patterns\nfor i in range(n_samples):\n    if y_eo[i] == 0:  # Water\n        X_eo[i, 0] += 0.3  # Higher Blue\n        X_eo[i, 3] -= 0.3  # Lower NIR\n        X_eo[i, 7] += 0.4  # Higher NDWI\n    elif y_eo[i] == 1:  # Vegetation\n        X_eo[i, 3] += 0.5  # Higher NIR\n        X_eo[i, 2] -= 0.2  # Lower Red\n        X_eo[i, 6] += 0.5  # Higher NDVI\n    else:  # Urban\n        X_eo[i, 4] += 0.2  # Higher SWIR1\n        X_eo[i, 5] += 0.2  # Higher SWIR2\n\n# Clip to [0, 1] range\nX_eo = np.clip(X_eo, 0, 1)\n\nprint(f\"EO Dataset shape: {X_eo.shape}\")\nprint(f\"Features: {eo_feature_names}\")\nprint(f\"Classes: 0=Water, 1=Vegetation, 2=Urban\")\nprint(f\"Class distribution: {np.bincount(y_eo)}\")\n\n\n# Train Random Forest on EO-like data\nrf_eo = RandomForestClassifier(n_estimators=100, \n                              max_depth=10,\n                              random_state=RANDOM_STATE)\nrf_eo.fit(X_eo, y_eo)\n\n# Extract feature importances\nimportances = rf_eo.feature_importances_\nindices = np.argsort(importances)[::-1]  # Sort descending\n\nprint(\"Feature Importance Ranking:\")\nprint(\"━\" * 60)\nfor i, idx in enumerate(indices):\n    print(f\"{i+1}. {eo_feature_names[idx]:15s}: {importances[idx]:.4f}\")\n\n\n# Visualize feature importances\nplt.figure(figsize=(12, 7))\nbars = plt.barh(range(len(importances)), importances[indices], align='center')\n\n# Color bars by importance\ncolors = plt.cm.viridis(importances[indices] / importances.max())\nfor bar, color in zip(bars, colors):\n    bar.set_color(color)\n\nplt.yticks(range(len(importances)), [eo_feature_names[i] for i in indices])\nplt.xlabel('Importance (Mean Decrease in Impurity)', fontsize=12)\nplt.ylabel('Feature', fontsize=12)\nplt.title('Feature Importance for Land Cover Classification', \n         fontsize=14, fontweight='bold')\nplt.grid(True, axis='x', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n💡 TIP: High importance doesn't always mean causation!\")\nprint(\"   - NDVI is derived from NIR and Red, so they're correlated\")\nprint(\"   - Consider domain knowledge alongside feature importance\")\nprint(\"   - Importance can be unstable with correlated features\")\n\n\n\n🎯 Exercise: Interpret Feature Importance\nQuestions: 1. Which feature is most important? Why might this be? 2. Are the derived indices (NDVI, NDWI) more or less important than raw bands? 3. Which features could potentially be removed to simplify the model? 4. How does this align with your knowledge of land cover spectral signatures?\nYour answers here (double-click to edit):\n\nMost important feature:\n\nTODO: Write your observation\n\nDerived indices vs. raw bands:\n\nTODO: Write your analysis\n\nFeatures that could be removed:\n\nTODO: Write your suggestions\n\nAlignment with spectral signatures:\n\nTODO: Write your interpretation",
    "crumbs": [
      "Notebooks",
      "Session 1 Theory: Understanding Random Forest for Earth Observation"
    ]
  },
  {
    "objectID": "day2/notebooks/session1_theory_notebook_STUDENT.html#e.-confusion-matrix-interpretation-15-minutes",
    "href": "day2/notebooks/session1_theory_notebook_STUDENT.html#e.-confusion-matrix-interpretation-15-minutes",
    "title": "Session 1 Theory: Understanding Random Forest for Earth Observation",
    "section": "E. Confusion Matrix Interpretation (15 minutes)",
    "text": "E. Confusion Matrix Interpretation (15 minutes)\n\nWhy Confusion Matrix?\nOverall accuracy can be misleading! Consider: - Dataset: 95% Forest, 5% Mangrove - Model: Predicts everything as Forest - Accuracy: 95% (sounds great!) - Problem: Completely missed mangroves!\nConfusion Matrix reveals: - Which classes are well-predicted - Which classes are confused with each other - Class-specific performance (precision, recall)\n\n\nKey Metrics:\n\nPrecision (User’s Accuracy): Of all pixels predicted as class X, how many are actually class X?\n\nFormula: TP / (TP + FP)\nImportant when false positives are costly\n\nRecall (Producer’s Accuracy): Of all actual class X pixels, how many did we correctly identify?\n\nFormula: TP / (TP + FN)\nImportant when false negatives are costly\n\nF1-Score: Harmonic mean of precision and recall\n\nFormula: 2 × (Precision × Recall) / (Precision + Recall)\nBalances both metrics\n\n\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_eo, y_eo, test_size=0.3, random_state=RANDOM_STATE, stratify=y_eo\n)\n\nprint(f\"Training set size: {X_train.shape[0]}\")\nprint(f\"Test set size: {X_test.shape[0]}\")\nprint(f\"Training class distribution: {np.bincount(y_train)}\")\nprint(f\"Test class distribution: {np.bincount(y_test)}\")\n\nprint(\"\\n💡 TIP: We use stratified split to maintain class proportions.\")\n\n\n# Train Random Forest\nrf_final = RandomForestClassifier(n_estimators=100, \n                                 max_depth=10,\n                                 random_state=RANDOM_STATE)\nrf_final.fit(X_train, y_train)\n\n# Make predictions\ny_pred = rf_final.predict(X_test)\n\n# Calculate overall accuracy\noverall_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Overall Test Accuracy: {overall_accuracy:.3f}\")\n\n\n# Generate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nclass_labels = ['Water', 'Vegetation', 'Urban']\n\nprint(\"Confusion Matrix (raw counts):\")\nprint(\"━\" * 60)\nprint(cm)\nprint(\"\\nRows = Actual class, Columns = Predicted class\")\n\n\n# Visualize confusion matrix as heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n           xticklabels=class_labels, \n           yticklabels=class_labels,\n           cbar_kws={'label': 'Number of Samples'},\n           linewidths=1, linecolor='gray')\nplt.xlabel('Predicted Class', fontsize=12, fontweight='bold')\nplt.ylabel('Actual Class', fontsize=12, fontweight='bold')\nplt.title('Confusion Matrix: Land Cover Classification', \n         fontsize=14, fontweight='bold', pad=15)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nHow to Read This Matrix:\")\nprint(\"━\" * 60)\nprint(\"• Diagonal (top-left to bottom-right): Correct predictions\")\nprint(\"• Off-diagonal: Confusion between classes\")\nprint(\"• Dark blue cells indicate high counts\")\nprint(\"\\n💡 TIP: Look for patterns in confusion:\")\nprint(\"   - Are certain class pairs often confused?\")\nprint(\"   - Do confusions make spectral sense?\")\n\n\n# Calculate normalized confusion matrix (percentages)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn', \n           xticklabels=class_labels, \n           yticklabels=class_labels,\n           vmin=0, vmax=1,\n           cbar_kws={'label': 'Percentage'},\n           linewidths=1, linecolor='gray')\nplt.xlabel('Predicted Class', fontsize=12, fontweight='bold')\nplt.ylabel('Actual Class', fontsize=12, fontweight='bold')\nplt.title('Normalized Confusion Matrix (Row Percentages)', \n         fontsize=14, fontweight='bold', pad=15)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n💡 TIP: Normalized matrix shows recall (producer's accuracy) for each class.\")\nprint(\"   Diagonal values are the percentage correctly classified for each class.\")\n\n\n\nCalculate Detailed Metrics\n\n# Generate classification report\nprint(\"Classification Report:\")\nprint(\"━\" * 80)\nreport = classification_report(y_test, y_pred, \n                              target_names=class_labels,\n                              digits=3)\nprint(report)\n\nprint(\"\\nMetric Definitions:\")\nprint(\"━\" * 80)\nprint(\"• Precision (User's Accuracy): TP / (TP + FP)\")\nprint(\"  → Of predictions for this class, how many were correct?\")\nprint(\"  → Important when false alarms are costly\")\nprint(\"\")\nprint(\"• Recall (Producer's Accuracy): TP / (TP + FN)\")\nprint(\"  → Of actual samples of this class, how many were found?\")\nprint(\"  → Important when missing instances is costly\")\nprint(\"\")\nprint(\"• F1-Score: 2 × (Precision × Recall) / (Precision + Recall)\")\nprint(\"  → Harmonic mean balancing precision and recall\")\nprint(\"\")\nprint(\"• Support: Number of actual samples in test set\")\n\n\n# Visualize per-class metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nprecision = precision_score(y_test, y_pred, average=None)\nrecall = recall_score(y_test, y_pred, average=None)\nf1 = f1_score(y_test, y_pred, average=None)\n\n# Create DataFrame for easier plotting\nmetrics_df = pd.DataFrame({\n    'Precision': precision,\n    'Recall': recall,\n    'F1-Score': f1\n}, index=class_labels)\n\n# Plot\nax = metrics_df.plot(kind='bar', figsize=(12, 7), width=0.8)\nplt.xlabel('Land Cover Class', fontsize=12)\nplt.ylabel('Score', fontsize=12)\nplt.title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\nplt.xticks(rotation=0)\nplt.ylim([0, 1.05])\nplt.legend(loc='lower right', fontsize=11)\nplt.grid(True, axis='y', alpha=0.3)\nplt.axhline(y=0.8, color='r', linestyle='--', alpha=0.5, label='80% threshold')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n💡 TIP: In EO applications, different thresholds matter:\")\nprint(\"   - Disaster mapping: High recall for affected areas (don't miss damage)\")\nprint(\"   - Urban planning: High precision for built-up (avoid false alarms)\")\nprint(\"   - Balanced: Use F1-score for overall assessment\")\n\n\n\n🎯 Exercise: Confusion Analysis\nTask: Analyze the confusion matrix and answer these questions:\n\nWhich class has the highest recall (producer’s accuracy)?\nWhich class has the lowest precision (user’s accuracy)?\nWhich two classes are most often confused with each other?\nWhy might this confusion occur from a spectral perspective?\nWhat could you do to improve classification of the weakest class?\n\nYour answers here (double-click to edit):\n\nHighest recall class:\n\nTODO: Identify and explain\n\nLowest precision class:\n\nTODO: Identify and explain\n\nMost confused class pair:\n\nTODO: Identify the pair\n\nSpectral reason for confusion:\n\nTODO: Explain using spectral signature knowledge\n\nImprovement strategies:\n\nTODO: List 2-3 practical approaches",
    "crumbs": [
      "Notebooks",
      "Session 1 Theory: Understanding Random Forest for Earth Observation"
    ]
  },
  {
    "objectID": "day2/notebooks/session1_theory_notebook_STUDENT.html#f.-concept-check-quiz-10-minutes",
    "href": "day2/notebooks/session1_theory_notebook_STUDENT.html#f.-concept-check-quiz-10-minutes",
    "title": "Session 1 Theory: Understanding Random Forest for Earth Observation",
    "section": "F. Concept Check Quiz (10 minutes)",
    "text": "F. Concept Check Quiz (10 minutes)\nTest your understanding of Random Forest concepts!\n\nQuestion 1: Decision Tree Splitting\nQ: How does a decision tree decide where to split at each node?\n\nRandomly selects a feature and threshold\n\nUses the feature and threshold that maximizes information gain (or minimizes impurity)\n\nAlways splits at the median value of each feature\n\nSplits based on alphabetical order of feature names\n\nYour answer: TODO (A, B, C, or D)\n\n\nClick to reveal answer\n\nCorrect Answer: B\nDecision trees evaluate all possible splits and choose the one that best separates classes (maximizes information gain or minimizes Gini impurity). This greedy approach finds locally optimal splits at each node.\n\n\n\nQuestion 2: Bootstrap Sampling\nQ: In Random Forest, what is bootstrap sampling?\n\nSampling pixels only from the edges of images\n\nSampling with replacement to create training subsets for each tree\n\nSampling only the most important features\n\nSampling validation data separately from training data\n\nYour answer: TODO (A, B, C, or D)\n\n\nClick to reveal answer\n\nCorrect Answer: B\nBootstrap sampling means randomly selecting samples WITH replacement. Each tree gets a different random subset of the training data (approximately 63.2% unique samples), which introduces diversity and reduces correlation between trees.\n\n\n\nQuestion 3: Random Feature Selection\nQ: At each split in a Random Forest tree, what does “random feature selection” mean?\n\nAll features are considered for splitting\n\nFeatures are selected in alphabetical order\n\nOnly a random subset of features is considered (typically √n or log₂n)\n\nThe most important feature is always selected\n\nYour answer: TODO (A, B, C, or D)\n\n\nClick to reveal answer\n\nCorrect Answer: C\nAt each split, Random Forest only considers a random subset of features (controlled by max_features parameter). Default is √n for classification. This decorrelates trees and prevents dominant features from being used in every tree.\n\n\n\nQuestion 4: Feature Importance Interpretation\nQ: You’re classifying land cover and find that NDVI has the highest feature importance. What should you conclude?\n\nNDVI is the only feature needed; remove all others\n\nNDVI contributes most to reducing impurity, but other features may still be valuable\n\nNDVI causes the land cover types (causal relationship)\n\nAll other features are completely irrelevant\n\nYour answer: TODO (A, B, C, or D)\n\n\nClick to reveal answer\n\nCorrect Answer: B\nHigh importance means NDVI is most useful for discrimination, but: - Other features may capture complementary information - Importance doesn’t imply causation - Correlated features share importance - Context and domain knowledge matter!\n\n\n\nQuestion 5: Confusion Matrix - Precision vs. Recall\nScenario: You’re mapping forest fire damage. The confusion matrix shows: - Actual Burned: 100 pixels - Predicted as Burned: 150 pixels - Correctly identified Burned: 90 pixels\nQ: Calculate precision and recall for the “Burned” class. Which is more important for this application?\nYour calculations: - Precision = TODO (show calculation) - Recall = TODO (show calculation) - More important: TODO (Precision or Recall, and why?)\n\n\nClick to reveal answer\n\nAnswers: - Precision = 90 / 150 = 0.60 (60%) - Of pixels predicted as burned, 60% actually were - Recall = 90 / 100 = 0.90 (90%) - Of actual burned pixels, we found 90%\nMore Important: Recall\nFor fire damage assessment: - High recall is critical: We don’t want to miss burned areas (false negatives could delay aid) - Lower precision is acceptable: False alarms can be verified with field checks - It’s better to overestimate damage than underestimate\n\n\n\nQuestion 6: Overfitting in Random Forest\nQ: Which scenario is MOST likely to cause overfitting in Random Forest?\n\nUsing 100 trees instead of 10\n\nSetting max_depth=None (unlimited depth)\n\nUsing bootstrap sampling\n\nUsing random feature selection\n\nYour answer: TODO (A, B, C, or D)\n\n\nClick to reveal answer\n\nCorrect Answer: B\nUnlimited depth allows trees to grow until leaves are pure (or nearly pure), memorizing training data. Signs of overfitting: - Very high training accuracy (&gt;99%) - Much lower test accuracy - Overly complex decision boundaries\nPrevention: - Set max_depth (e.g., 10-20) - Set min_samples_split (e.g., 5-10) - Set min_samples_leaf (e.g., 2-5)\nNote: More trees (A) actually reduces overfitting! Bootstrap (C) and feature selection (D) also help prevent it.",
    "crumbs": [
      "Notebooks",
      "Session 1 Theory: Understanding Random Forest for Earth Observation"
    ]
  },
  {
    "objectID": "day2/notebooks/session1_theory_notebook_STUDENT.html#summary-and-key-takeaways",
    "href": "day2/notebooks/session1_theory_notebook_STUDENT.html#summary-and-key-takeaways",
    "title": "Session 1 Theory: Understanding Random Forest for Earth Observation",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nDecision Trees\n\nLearn hierarchical decision rules through recursive splitting\nCreate axis-aligned decision boundaries\nProne to overfitting if too deep\nEasy to interpret and visualize\n\n\n\nRandom Forest Ensemble\n\nCombines many trees to reduce variance and improve stability\nUses bootstrap sampling (bagging) for training diversity\nUses random feature selection to decorrelate trees\nFinal prediction by majority voting (classification) or averaging (regression)\nMore robust than single trees, less prone to overfitting\n\n\n\nFeature Importance\n\nMeasures contribution of each feature to reducing impurity\nHelps identify most informative spectral bands/indices\nUseful for feature selection and model interpretation\nShould be interpreted with domain knowledge\nCan be unstable with correlated features\n\n\n\nConfusion Matrix & Metrics\n\nOverall accuracy can hide class-specific problems\nPrecision (user’s accuracy): Reliability of positive predictions\nRecall (producer’s accuracy): Completeness of detection\nF1-score: Harmonic mean balancing precision and recall\nChoice of metric depends on application cost (false positives vs. false negatives)\n\n\n\nFor Earth Observation\n\nRandom Forest works well with multi-spectral data\nNo feature scaling needed (unlike neural networks)\nFeature importance reveals spectral signature insights\nConfusion patterns often reflect spectral similarity\nFast training enables rapid iteration",
    "crumbs": [
      "Notebooks",
      "Session 1 Theory: Understanding Random Forest for Earth Observation"
    ]
  },
  {
    "objectID": "day2/notebooks/session1_theory_notebook_STUDENT.html#next-steps",
    "href": "day2/notebooks/session1_theory_notebook_STUDENT.html#next-steps",
    "title": "Session 1 Theory: Understanding Random Forest for Earth Observation",
    "section": "Next Steps",
    "text": "Next Steps\nIn the Hands-On Session, you will: 1. Load real Sentinel-2 data for Palawan, Philippines 2. Extract training samples from land cover polygons 3. Train Random Forest for multi-class land cover classification 4. Optimize hyperparameters (n_estimators, max_depth, etc.) 5. Generate wall-to-wall land cover maps 6. Validate results and interpret errors\nPrepare by reviewing: - Sentinel-2 band characteristics (B2, B3, B4, B8, B11, B12) - Philippine land cover types (forest, mangrove, agriculture, urban, water) - Google Earth Engine Python API basics",
    "crumbs": [
      "Notebooks",
      "Session 1 Theory: Understanding Random Forest for Earth Observation"
    ]
  },
  {
    "objectID": "day2/notebooks/session1_theory_notebook_STUDENT.html#references",
    "href": "day2/notebooks/session1_theory_notebook_STUDENT.html#references",
    "title": "Session 1 Theory: Understanding Random Forest for Earth Observation",
    "section": "References",
    "text": "References\n\nBreiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32.\nBelgiu, M., & Drăguţ, L. (2016). Random forest in remote sensing: A review of applications and future directions. ISPRS Journal of Photogrammetry and Remote Sensing, 114, 24-31.\nScikit-learn Documentation: Random Forest Classifier\nESA Sentinel-2 User Handbook: https://sentinels.copernicus.eu/documents/247904/685211/Sentinel-2_User_Handbook\n\n\nEnd of Theory Notebook\nDeveloped for CoPhil 4-Day Advanced Online Training on AI/ML for Earth Observation",
    "crumbs": [
      "Notebooks",
      "Session 1 Theory: Understanding Random Forest for Earth Observation"
    ]
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#session-overview",
    "href": "day3/presentations/session1_unet_segmentation.html#session-overview",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "Session Overview",
    "text": "Session Overview\n\n\nDuration: 1.5 hours\nType: Theory + Discussion\nGoal: Understand semantic segmentation and U-Net for EO\nYou will learn: - What semantic segmentation is and why it matters for EO - U-Net architecture: encoder, decoder, skip connections - Losses for segmentation: CE, Dice, IoU - EO applications: floods, land cover, roads, buildings\n\nPrerequisites: - Day 2 Session 3 (CNNs) - Basic Python/Colab\nResources: - Hands-on lab in Session 2 (flood mapping)"
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#pixel-wise-classification-vs-other-cv-tasks",
    "href": "day3/presentations/session1_unet_segmentation.html#pixel-wise-classification-vs-other-cv-tasks",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "Pixel-wise classification vs other CV tasks",
    "text": "Pixel-wise classification vs other CV tasks\n\nClassification → single label per image\nObject detection → boxes + labels per instance\nSemantic segmentation → label for every pixel\n\n\n\n\n\n\nflowchart TB\n    A[Computer Vision Tasks] --&gt; B[Classification]\n    A --&gt; C[Object Detection]\n    A --&gt; D[Semantic Segmentation]\n    B --&gt; B1[Image-level]\n    C --&gt; C1[Boxes + classes]\n    D --&gt; D1[Pixel masks]\n\n\n\n\n\n\n\n\n\n\n\n\nWhy segmentation for EO?\n\n\n\nPrecise boundaries (flood edges, building footprints)\nAccurate area calculations (km² flooded)\nEnables fine-grained change detection"
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#high-level-overview",
    "href": "day3/presentations/session1_unet_segmentation.html#high-level-overview",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "High-level overview",
    "text": "High-level overview\n\nEncoder (contracting path): features ↑, resolution ↓\nBottleneck: global context\nDecoder (expansive path): upsample + refine\nSkip connections: fuse detail from encoder into decoder\n\n\n\n\n\n\nflowchart TD\n    A[Input] --&gt; B[Encoder]\n    B --&gt; C[Bottleneck]\n    C --&gt; D[Decoder]\n    D --&gt; E[Output Mask]\n    B -.-&gt; D"
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#skip-connections-the-key",
    "href": "day3/presentations/session1_unet_segmentation.html#skip-connections-the-key",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "Skip connections: the key",
    "text": "Skip connections: the key\n\nPreserve high-resolution details lost during pooling\nConcatenate encoder features with upsampled decoder features\nSharper boundaries, better small structures (roads, levees)"
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#encoder-details",
    "href": "day3/presentations/session1_unet_segmentation.html#encoder-details",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "Encoder details",
    "text": "Encoder details\n\nBlocks: 2× Conv(3×3, ReLU) → MaxPool(2×2)\nSpatial dims halve, channels double\nMulti-scale feature hierarchy (edges → textures → semantics)"
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#decoder-details",
    "href": "day3/presentations/session1_unet_segmentation.html#decoder-details",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "Decoder details",
    "text": "Decoder details\n\nUpsample via TransposedConv or Bilinear + Conv\nConcatenate with corresponding encoder features (same size)\n1×1 Conv at end to produce class logits (per pixel)"
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#imbalance-is-common-in-eo",
    "href": "day3/presentations/session1_unet_segmentation.html#imbalance-is-common-in-eo",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "Imbalance is common in EO",
    "text": "Imbalance is common in EO\n\nFlood pixels &lt;&lt; background pixels\nBuildings/roads are sparse\n\nOptions\n\nPixel-wise Cross-Entropy (with class weights)\nDice loss (overlap-focused)\nIoU/Jaccard loss\nCombined loss (BCE + Dice) → robust baseline\n\n\n\n\n\n\n\nPractical recommendation\n\n\nUse Combined (BCE + Dice) for flood/building tasks with imbalance."
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#flood-mapping-drr",
    "href": "day3/presentations/session1_unet_segmentation.html#flood-mapping-drr",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "Flood Mapping (DRR)",
    "text": "Flood Mapping (DRR)\n\nSentinel-1 SAR (VV/VH) → binary flood masks\nU-Net excels at delineating flood boundaries in dark backscatter regions"
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#land-cover-mapping-nrm",
    "href": "day3/presentations/session1_unet_segmentation.html#land-cover-mapping-nrm",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "Land Cover Mapping (NRM)",
    "text": "Land Cover Mapping (NRM)\n\nMulti-class masks from Sentinel-2 bands/indices\nPrecise boundaries between forest / agriculture / urban"
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#roads-buildings-urban",
    "href": "day3/presentations/session1_unet_segmentation.html#roads-buildings-urban",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "Roads & Buildings (Urban)",
    "text": "Roads & Buildings (Urban)\n\nThin linear features and compact footprints benefit from skips"
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#implementation-notes",
    "href": "day3/presentations/session1_unet_segmentation.html#implementation-notes",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "Implementation Notes",
    "text": "Implementation Notes\n\nInput sizes: 256×256 or 512×512 chips\nNormalization: SAR (dB scaling), Optical (0–1)\nAugmentation: flips/rotations; ensure image & mask transformed identically\nMetrics: Dice, IoU, Precision/Recall (minority class focus)"
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#key-takeaways",
    "href": "day3/presentations/session1_unet_segmentation.html#key-takeaways",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "Key takeaways",
    "text": "Key takeaways\n\nU-Net = encoder–decoder + skip connections → precise masks\nOverlap-focused losses improve minority-class performance\nPerfect fit for EO segmentation tasks (floods, land cover, roads)"
  },
  {
    "objectID": "day3/presentations/session1_unet_segmentation.html#next-session-2",
    "href": "day3/presentations/session1_unet_segmentation.html#next-session-2",
    "title": "Session 1: Semantic Segmentation with U-Net",
    "section": "Next (Session 2)",
    "text": "Next (Session 2)\n\nHands-on: Train U-Net flood mapper on Sentinel-1 SAR\nEvaluate with IoU/Dice; export masks for GIS"
  }
]