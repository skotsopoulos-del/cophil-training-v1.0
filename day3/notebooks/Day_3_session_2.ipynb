{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xnDYuuXwTfa"
      },
      "source": [
        "# Flood Segmentation with PyTorch U-Net (Sentinel-1 SAR Flood Dataset)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Flood mapping from satellite imagery is a crucial task for disaster response. In this notebook, we replicate a PyTorch U-Net model for segmenting floodwater in Sentinel-1 SAR images, based on the Cloud to Street – Microsoft Flood Dataset. The U-Net architecture is a convolutional neural network designed for pixel-wise segmentation tasks. Our goal is to download and prepare the dataset, define the U-Net model, train it on the SAR images with corresponding water masks, evaluate the segmentation performance (e.g. using Intersection over Union), and visualize some prediction results.\n",
        "\n",
        "Example Sentinel-1 SAR image (left) of a flood event and its corresponding ground-truth water mask (right). The SAR image shows flood-affected areas in radar intensity, and the mask highlights water pixels in white."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdFfirDzwTfe"
      },
      "source": [
        "## Installing Dependencies\n",
        "\n",
        "First, we install and import the necessary packages. We use PyTorch for building and training the U-Net, rasterio for reading geospatial image files (GeoTIFF format), and other common libraries for data handling and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XARQAooxwTff",
        "outputId": "d5aa6892-9903-4cd8-9a2f-5b70a09b4fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.10.5)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.0)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision rasterio numpy pandas matplotlib tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ioK8jrpwTfg",
        "outputId": "69146105-031a-4137-8450-7cb200b74a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import rasterio\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Check PyTorch version and availability of GPU\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esSszQMvwTfg"
      },
      "source": [
        "## Dataset Download and Preparation\n",
        "\n",
        "### Dataset Overview\n",
        "\n",
        "The Cloud to Street – Microsoft Flood dataset contains Sentinel-1 SAR image chips (with VV and VH polarizations) and corresponding binary flood masks. Each image chip covers an area affected by a flood event, and the mask indicates flooded vs. non-flooded pixels (permanent water and floodwater).\n",
        "\n",
        "### Download Instructions\n",
        "\n",
        "The dataset can be downloaded from Radiant Earth MLHub. You'll need to download two files:\n",
        "- `c2smsfloods_v1_source_s1.tar.gz` (Sentinel-1 VV/VH images)\n",
        "- `c2smsfloods_v1_labels_s1_water.tar.gz` (corresponding water masks)\n",
        "\n",
        "You can download them using AWS CLI:\n",
        "```bash\n",
        "aws s3 cp --no-sign-request s3://radiant-mlhub/c2smsfloods/c2smsfloods_v1_source_s1.tar.gz .\n",
        "aws s3 cp --no-sign-request s3://radiant-mlhub/c2smsfloods/c2smsfloods_v1_labels_s1_water.tar.gz .\n",
        "```\n",
        "\n",
        "Or use the Radiant MLHub Python API if you have an API key.\n",
        "\n",
        "### Extraction and Tiling\n",
        "\n",
        "We'll extract the downloaded files and tile each 512×512 chip into four 256×256 images to increase training samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jElcMp3nwTfg"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "# Paths to the downloaded tar files (update these if needed)\n",
        "s1_tar_path = \"c2smsfloods_v1_source_s1.tar.gz\"\n",
        "label_tar_path = \"c2smsfloods_v1_labels_s1_water.tar.gz\"\n",
        "\n",
        "# Extract tar files to temporary directories\n",
        "extract_dir_s1 = Path(\"source_data\")\n",
        "extract_dir_labels = Path(\"label_data\")\n",
        "extract_dir_s1.mkdir(parents=True, exist_ok=True)\n",
        "extract_dir_labels.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Extracting Sentinel-1 source data...\")\n",
        "with tarfile.open(s1_tar_path, 'r:gz') as tar:\n",
        "    tar.extractall(path=extract_dir_s1)\n",
        "print(\"Extracting water label data...\")\n",
        "with tarfile.open(label_tar_path, 'r:gz') as tar:\n",
        "    tar.extractall(path=extract_dir_labels)\n",
        "\n",
        "# After extraction, we expect 'train_features' and 'train_labels' directories\n",
        "feature_dir = extract_dir_s1 / \"train_features\"\n",
        "label_dir   = extract_dir_labels / \"train_labels\"\n",
        "assert feature_dir.exists() and label_dir.exists(), \"Extracted directories not found. Check extraction paths.\"\n",
        "\n",
        "# Create output directories for 256x256 tiles\n",
        "tile_dir_vv = Path(\"data/chips/VV\"); tile_dir_vv.mkdir(parents=True, exist_ok=True)\n",
        "tile_dir_vh = Path(\"data/chips/VH\"); tile_dir_vh.mkdir(parents=True, exist_ok=True)\n",
        "tile_dir_label = Path(\"data/labels\"); tile_dir_label.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Tiling each chip\n",
        "tile_count = 0\n",
        "print(\"Tiling chips into 256x256 patches...\")\n",
        "\n",
        "# Get all label files\n",
        "label_files = sorted(label_dir.glob(\"*.tif\"))\n",
        "\n",
        "for label_path in tqdm(label_files):\n",
        "    chip_id = label_path.stem  # e.g. \"awc00\" from \"awc00.tif\"\n",
        "    vv_path = feature_dir / f\"{chip_id}_vv.tif\"\n",
        "    vh_path = feature_dir / f\"{chip_id}_vh.tif\"\n",
        "    if not vv_path.exists() or not vh_path.exists():\n",
        "        continue  # skip if corresponding images not found\n",
        "\n",
        "    # Read label and polarization images\n",
        "    with rasterio.open(label_path) as lab_src, \\\n",
        "         rasterio.open(vv_path) as vv_src, \\\n",
        "         rasterio.open(vh_path) as vh_src:\n",
        "        label_img = lab_src.read(1)           # 2D array\n",
        "        vv_img = vv_src.read(1)              # 2D array\n",
        "        vh_img = vh_src.read(1)              # 2D array\n",
        "\n",
        "        # Replace no-data values (-1) in label with 0 (non-water)\n",
        "        if lab_src.nodata is not None:\n",
        "            nodata_val = lab_src.nodata\n",
        "            label_img[label_img == nodata_val] = 0\n",
        "\n",
        "        H, W = label_img.shape\n",
        "        tile_size = 256\n",
        "        # Slide a 256x256 window over the image (non-overlapping)\n",
        "        for y in range(0, H, tile_size):\n",
        "            for x in range(0, W, tile_size):\n",
        "                # Ensure we don't go out of bounds\n",
        "                if y+tile_size > H or x+tile_size > W:\n",
        "                    continue\n",
        "                sub_lab = label_img[y:y+tile_size, x:x+tile_size]\n",
        "                sub_vv  = vv_img[y:y+tile_size, x:x+tile_size]\n",
        "                sub_vh  = vh_img[y:y+tile_size, x:x+tile_size]\n",
        "\n",
        "                # Define tile file name based on chip and tile indices\n",
        "                tile_id = f\"{chip_id}_{int(y/tile_size)}_{int(x/tile_size)}\"\n",
        "\n",
        "                # Save the tile images using rasterio\n",
        "                # Save VV\n",
        "                profile = vv_src.profile.copy()\n",
        "                profile.update(height=tile_size, width=tile_size, count=1, transform=None)\n",
        "                with rasterio.open(tile_dir_vv / f\"{tile_id}.tif\", 'w', **profile) as dst:\n",
        "                    dst.write(sub_vv, 1)\n",
        "                # Save VH\n",
        "                profile = vh_src.profile.copy()\n",
        "                profile.update(height=tile_size, width=tile_size, count=1, transform=None)\n",
        "                with rasterio.open(tile_dir_vh / f\"{tile_id}.tif\", 'w', **profile) as dst:\n",
        "                    dst.write(sub_vh, 1)\n",
        "                # Save label (as uint8)\n",
        "                profile = lab_src.profile.copy()\n",
        "                profile.update(height=tile_size, width=tile_size, count=1, dtype=rasterio.uint8, transform=None, nodata=None)\n",
        "                with rasterio.open(tile_dir_label / f\"{tile_id}.tif\", 'w', **profile) as dst:\n",
        "                    dst.write(sub_lab.astype(np.uint8), 1)\n",
        "\n",
        "                tile_count += 1\n",
        "\n",
        "print(f\"Tiling completed: {tile_count} tiles created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K1yKpAQwTfh"
      },
      "source": [
        "## Dataset Preprocessing and Loading\n",
        "\n",
        "Now we'll create a custom PyTorch Dataset to load the tiled data, normalize the SAR images, and split the data into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTmzuil2wTfh"
      },
      "outputs": [],
      "source": [
        "# Define custom Dataset\n",
        "class FloodDataset(Dataset):\n",
        "    def __init__(self, ids, data_dir_vv, data_dir_vh, data_dir_label, transform=None):\n",
        "        \"\"\"\n",
        "        ids: list of tile IDs (filenames without extension)\n",
        "        data_dir_vv, data_dir_vh, data_dir_label: directories containing VV, VH, and label tiles\n",
        "        transform: optional augmentation or preprocessing function\n",
        "        \"\"\"\n",
        "        self.ids = ids\n",
        "        self.data_dir_vv = data_dir_vv\n",
        "        self.data_dir_vh = data_dir_vh\n",
        "        self.data_dir_label = data_dir_label\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tile_id = self.ids[idx]\n",
        "        vv_path = self.data_dir_vv / f\"{tile_id}.tif\"\n",
        "        vh_path = self.data_dir_vh / f\"{tile_id}.tif\"\n",
        "        label_path = self.data_dir_label / f\"{tile_id}.tif\"\n",
        "\n",
        "        # Read images using rasterio\n",
        "        with rasterio.open(vv_path) as vv_src:\n",
        "            vv = vv_src.read(1).astype(np.float32)\n",
        "        with rasterio.open(vh_path) as vh_src:\n",
        "            vh = vh_src.read(1).astype(np.float32)\n",
        "        with rasterio.open(label_path) as lab_src:\n",
        "            mask = lab_src.read(1).astype(np.uint8)\n",
        "\n",
        "        # Normalize VV and VH to [0,1] range (min-max normalization per tile)\n",
        "        if vv.max() > vv.min():  # avoid division by zero\n",
        "            vv = (vv - vv.min()) / (vv.max() - vv.min())\n",
        "        else:\n",
        "            vv = vv * 0.0\n",
        "        if vh.max() > vh.min():\n",
        "            vh = (vh - vh.min()) / (vh.max() - vh.min())\n",
        "        else:\n",
        "            vh = vh * 0.0\n",
        "\n",
        "        # Expand dims to [C, H, W] and convert to tensor\n",
        "        vv = torch.from_numpy(vv).unsqueeze(0)  # shape [1,256,256]\n",
        "        vh = torch.from_numpy(vh).unsqueeze(0)  # shape [1,256,256]\n",
        "        image = torch.cat([vv, vh], dim=0)      # shape [2,256,256]\n",
        "        mask = torch.from_numpy(mask).unsqueeze(0).float()  # shape [1,256,256], as float (0/1)\n",
        "\n",
        "        if self.transform:\n",
        "            # Apply transforms (e.g., augmentation) - not implemented in this example\n",
        "            image, mask = self.transform(image, mask)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "# Gather all tile IDs from the data directories\n",
        "tile_ids = [f.stem for f in tile_dir_label.glob(\"*.tif\")]\n",
        "tile_ids = sorted(tile_ids)\n",
        "print(f\"Total tiles available: {len(tile_ids)}\")\n",
        "\n",
        "# Shuffle and split into train/test\n",
        "random.seed(42)\n",
        "random.shuffle(tile_ids)\n",
        "split_idx = int(0.8 * len(tile_ids))  # 80% for training\n",
        "train_ids = tile_ids[:split_idx]\n",
        "test_ids  = tile_ids[split_idx:]\n",
        "print(f\"Training tiles: {len(train_ids)}, Test tiles: {len(test_ids)}\")\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "train_dataset = FloodDataset(train_ids, tile_dir_vv, tile_dir_vh, tile_dir_label)\n",
        "test_dataset  = FloodDataset(test_ids,  tile_dir_vv, tile_dir_vh, tile_dir_label)\n",
        "\n",
        "# For training, we'll use a batch size (e.g. 8 or 16 depending on memory)\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Inspect a sample batch shape\n",
        "images, masks = next(iter(train_loader))\n",
        "print(\"Batch image tensor shape:\", images.shape)  # (B, 2, 256, 256)\n",
        "print(\"Batch mask tensor shape:\", masks.shape)    # (B, 1, 256, 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpL96FvSwTfi"
      },
      "source": [
        "## U-Net Model Architecture\n",
        "\n",
        "The U-Net is a convolutional network that consists of an encoder (downsampling path) and a decoder (upsampling path) with skip connections between corresponding layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHBInQcbwTfi"
      },
      "outputs": [],
      "source": [
        "# Define the U-Net model components\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"Two consecutive conv (3x3) layers with ReLU (encoder or decoder block).\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downsampling: MaxPool then DoubleConv.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Down, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "    def forward(self, x):\n",
        "        x = self.pool(x)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upsampling: ConvTranspose (2x2) then DoubleConv. Skip connection from encoder is concatenated.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Up, self).__init__()\n",
        "        # ConvTranspose2d to upsample by 2\n",
        "        self.up = nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "    def forward(self, x1, x2):\n",
        "        # x1 is decoder feature, x2 is encoder feature to concatenate\n",
        "        x1 = self.up(x1)\n",
        "        # Pad x1 if needed when dimensions are odd\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        if diffY != 0 or diffX != 0:\n",
        "            # pad the upsampled feature map so that it matches size of encoder feature map\n",
        "            x1 = F.pad(x1, [0, diffX, 0, diffY])\n",
        "        # Concatenate along channels\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    \"\"\"1x1 convolution for output.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# Full U-Net Model\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels=2, n_classes=1):\n",
        "        super(UNet, self).__init__()\n",
        "        self.in_conv = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 512)  # bottom layer\n",
        "        self.up1   = Up(512, 256)\n",
        "        self.up2   = Up(256, 128)\n",
        "        self.up3   = Up(128, 64)\n",
        "        self.up4   = Up(64, 64)\n",
        "        self.out_conv = OutConv(64, n_classes)\n",
        "    def forward(self, x):\n",
        "        # Encoder (downsampling)\n",
        "        x1 = self.in_conv(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        # Decoder (upsampling)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        # Output layer\n",
        "        logits = self.out_conv(x)\n",
        "        return logits\n",
        "\n",
        "model = UNet(n_channels=2, n_classes=1).to(device)\n",
        "print(\"U-Net model initialized. Number of parameters:\", sum(p.numel() for p in model.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJyGsn-_wTfi"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "We'll use Binary Cross-Entropy loss with logits and the Adam optimizer. During training, we'll track the training loss and evaluate on the test set to monitor the Intersection over Union (IoU) score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_wIKASFwTfi"
      },
      "outputs": [],
      "source": [
        "# Define loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 5  # for demonstration; you can increase this for better performance\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_ious = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Training loop with progress bar\n",
        "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{num_epochs} [Train]')\n",
        "    for images, masks in train_pbar:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        logits = model(images)              # shape [B,1,256,256]\n",
        "        loss = criterion(logits, masks)     # compute loss against ground truth mask\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    train_losses.append(epoch_loss)\n",
        "\n",
        "    # Evaluate on test set for this epoch\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    # IoU components\n",
        "    inter_sum = 0\n",
        "    union_sum = 0\n",
        "\n",
        "    test_pbar = tqdm(test_loader, desc=f'Epoch {epoch}/{num_epochs} [Test]')\n",
        "    with torch.no_grad():\n",
        "        for images, masks in test_pbar:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "            logits = model(images)\n",
        "            # Compute loss\n",
        "            loss = criterion(logits, masks)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            # Compute IoU\n",
        "            probs = torch.sigmoid(logits)  # convert logits to probabilities\n",
        "            preds = (probs >= 0.5).float()  # threshold to binary predictions\n",
        "            # Intersection and Union for IoU\n",
        "            inter = (preds * masks).sum(dim=[1,2,3])  # sum of element-wise multiplication (true positives per image)\n",
        "            union = ((preds + masks) >= 1).float().sum(dim=[1,2,3])  # sum of element-wise logical OR\n",
        "            inter_sum += inter.sum().item()\n",
        "            union_sum += union.sum().item()\n",
        "\n",
        "            current_iou = inter_sum / union_sum if union_sum > 0 else 0.0\n",
        "            test_pbar.set_postfix({'iou': current_iou})\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_dataset)\n",
        "    test_losses.append(avg_test_loss)\n",
        "    # Calculate IoU score\n",
        "    iou_score = inter_sum / union_sum if union_sum > 0 else 0.0\n",
        "    test_ious.append(iou_score)\n",
        "    print(f\"Epoch {epoch}/{num_epochs}: Train Loss = {epoch_loss:.4f}, Test Loss = {avg_test_loss:.4f}, Test IoU = {iou_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U6yx5tIwTfj"
      },
      "source": [
        "## Training Progress Visualization\n",
        "\n",
        "Let's plot the training curve and IoU to visualize the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8yxzApEwTfj"
      },
      "outputs": [],
      "source": [
        "# Plot training and test loss over epochs\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label=\"Train Loss\")\n",
        "plt.plot(range(1, num_epochs+1), test_losses, label=\"Test Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"BCE Loss\")\n",
        "plt.title(\"Loss Curve\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot IoU over epochs\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs+1), test_ious, marker='o', color='green')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"IoU\")\n",
        "plt.title(\"Test IoU over Epochs\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z4Sbek1wTfj"
      },
      "source": [
        "## Evaluation on Test Set\n",
        "\n",
        "After training, we evaluate the model on the test set to obtain final metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wW9IeChwTfj"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "inter_sum = 0\n",
        "union_sum = 0\n",
        "all_preds = []\n",
        "all_masks = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, masks in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "        logits = model(images)\n",
        "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
        "        inter = (preds * masks.to(device)).sum(dim=[1,2,3])\n",
        "        union = ((preds + masks.to(device)) >= 1).float().sum(dim=[1,2,3])\n",
        "        inter_sum += inter.sum().item()\n",
        "        union_sum += union.sum().item()\n",
        "\n",
        "        # Store for additional metrics if needed\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_masks.append(masks.cpu())\n",
        "\n",
        "iou_final = inter_sum / union_sum if union_sum > 0 else 0.0\n",
        "print(f\"Final Test IoU: {iou_final:.4f}\")\n",
        "\n",
        "# Calculate additional metrics\n",
        "all_preds = torch.cat(all_preds)\n",
        "all_masks = torch.cat(all_masks)\n",
        "\n",
        "# Pixel accuracy\n",
        "accuracy = (all_preds == all_masks).float().mean()\n",
        "print(f\"Pixel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision and Recall\n",
        "true_positives = (all_preds * all_masks).sum()\n",
        "false_positives = (all_preds * (1 - all_masks)).sum()\n",
        "false_negatives = ((1 - all_preds) * all_masks).sum()\n",
        "\n",
        "precision = true_positives / (true_positives + false_positives + 1e-8)\n",
        "recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLDhaZOXwTfj"
      },
      "source": [
        "## Visualization of Segmentation Results\n",
        "\n",
        "Finally, let's visualize some predictions versus ground truth on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHZOUll_wTfj"
      },
      "outputs": [],
      "source": [
        "# Visualize a few test samples with their predicted masks\n",
        "model.eval()\n",
        "num_samples_to_show = 3\n",
        "samples = random.sample(test_ids, num_samples_to_show)\n",
        "\n",
        "plt.figure(figsize=(12, num_samples_to_show*4))\n",
        "for i, tile_id in enumerate(samples, start=1):\n",
        "    # Load the VV image, VH image, and true mask for the tile\n",
        "    vv_path = tile_dir_vv / f\"{tile_id}.tif\"\n",
        "    vh_path = tile_dir_vh / f\"{tile_id}.tif\"\n",
        "    lab_path = tile_dir_label / f\"{tile_id}.tif\"\n",
        "    with rasterio.open(vv_path) as vv_src:\n",
        "        vv = vv_src.read(1).astype(np.float32)\n",
        "    with rasterio.open(vh_path) as vh_src:\n",
        "        vh = vh_src.read(1).astype(np.float32)\n",
        "    with rasterio.open(lab_path) as lab_src:\n",
        "        true_mask = lab_src.read(1).astype(np.uint8)\n",
        "        true_mask[true_mask != 1] = 0  # ensure binary\n",
        "\n",
        "    # Normalize VV for display\n",
        "    vv_disp = (vv - vv.min()) / (vv.max() - vv.min() + 1e-9)\n",
        "    # Get model prediction\n",
        "    img_tensor = torch.from_numpy(np.stack([vv, vh], axis=0)).unsqueeze(0).to(device)  # shape [1,2,H,W]\n",
        "    with torch.no_grad():\n",
        "        logit = model(img_tensor)\n",
        "        pred_prob = torch.sigmoid(logit).cpu().numpy()[0,0]  # probability map\n",
        "        pred_mask = (pred_prob >= 0.5).astype(np.uint8)  # binary mask array\n",
        "\n",
        "    # Plot SAR image (VV polarization)\n",
        "    plt.subplot(num_samples_to_show, 4, 4*(i-1) + 1)\n",
        "    plt.imshow(vv_disp, cmap='gray')\n",
        "    plt.title(f\"SAR Image (VV)\\n{tile_id}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot SAR image (VH polarization)\n",
        "    plt.subplot(num_samples_to_show, 4, 4*(i-1) + 2)\n",
        "    vh_disp = (vh - vh.min()) / (vh.max() - vh.min() + 1e-9)\n",
        "    plt.imshow(vh_disp, cmap='gray')\n",
        "    plt.title(\"SAR Image (VH)\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot true mask\n",
        "    plt.subplot(num_samples_to_show, 4, 4*(i-1) + 3)\n",
        "    plt.imshow(true_mask, cmap='Blues')  # Blues colormap to highlight water\n",
        "    plt.title(\"Ground Truth Mask\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Plot predicted mask\n",
        "    plt.subplot(num_samples_to_show, 4, 4*(i-1) + 4)\n",
        "    plt.imshow(pred_mask, cmap='Blues')\n",
        "    plt.title(\"Predicted Mask\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBjeoQGtwTfj"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated a complete pipeline for flood segmentation using PyTorch U-Net on Sentinel-1 SAR data. Key steps included:\n",
        "\n",
        "1. **Data Preparation**: Downloading, extracting, and tiling the Cloud to Street – Microsoft Flood dataset\n",
        "2. **Model Architecture**: Implementing a U-Net with encoder-decoder structure and skip connections\n",
        "3. **Training**: Using BCE loss and Adam optimizer to train the segmentation model\n",
        "4. **Evaluation**: Measuring performance with IoU, precision, recall, and F1-score\n",
        "5. **Visualization**: Comparing predictions with ground truth masks\n",
        "\n",
        "The model successfully learned to identify flood areas in SAR imagery. For improved performance, consider:\n",
        "- Training for more epochs\n",
        "- Using data augmentation\n",
        "- Incorporating additional input channels (DEM, permanent water maps)\n",
        "- Implementing the dual-model approach from the original paper\n",
        "- Using more sophisticated loss functions (Dice loss, Focal loss)\n",
        "\n",
        "This approach can be valuable for rapid flood mapping and disaster response applications."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}