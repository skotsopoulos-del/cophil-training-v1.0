{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Session 2: Hands-on Flood Mapping with U-Net and Sentinel-1 SAR\"\n",
        "subtitle: \"Practical Implementation for Disaster Risk Reduction\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: show\n",
        "    code-tools: true\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    number-sections: false\n",
        "    css: ../../styles/custom.css\n",
        "date: last-modified\n",
        "author: \"CoPhil Advanced Training Program\"\n",
        "execute:\n",
        "  enabled: false\n",
        "  eval: false\n",
        "  echo: true\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ“ Educational Note: Synthetic Data\n",
        "\n",
        "This notebook uses **synthetic SAR data** for immediate execution and learning. The U-Net architecture, training workflow, and evaluation metrics are identical to real-world applications.\n",
        "\n",
        "**Benefits:**\n",
        "- âœ… No data download required\n",
        "- âœ… Runs in 5-10 minutes (vs. hours for real data preprocessing)\n",
        "- âœ… Perfect for understanding the workflow\n",
        "- âœ… Easy to experiment and modify\n",
        "\n",
        "**For production work:** Replace synthetic data with real Sentinel-1 SAR from Google Earth Engine or the CoPhil Mirror Site. See the [Data Acquisition Guide](../DATA_GUIDE.md) for details.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from glob import glob\n",
        "import random\n",
        "\n",
        "# Deep learning framework (TensorFlow/Keras)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Metrics and evaluation\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(f\\\"TensorFlow version: {tf.__version__}\\\")\n",
        "print(f\\\"GPU Available: {tf.config.list_physical_devices('GPU')}\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download Dataset\n",
        "\n",
        "::: callout-note\n",
        "## Dataset Information\n",
        "\n",
        "**Size:** ~450MB compressed  \n",
        "**Contents:** \n",
        "- ~800 training image patches (256Ã—256, VV+VH)\n",
        "- ~200 validation patches\n",
        "- ~200 test patches\n",
        "- Binary flood masks for all patches\n",
        "\n",
        "**Pre-processing Applied:**\n",
        "- Speckle filtering (Lee filter, 7Ã—7 window)\n",
        "- Radiometric calibration to Ïƒ0 (dB)\n",
        "- Geometric terrain correction\n",
        "- Resampling to 10m resolution\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Synthetic SAR Data\n",
        "\n",
        "::: {.callout-note}\n",
        "## Synthetic Data Approach\n",
        "\n",
        "For this lab, we'll generate **synthetic SAR data** that mimics real Sentinel-1 characteristics. This allows you to:\n",
        "- âœ… Run the notebook immediately without downloads\n",
        "- âœ… Understand data structure and formats\n",
        "- âœ… Practice the complete U-Net workflow\n",
        "- âœ… Learn model training and evaluation\n",
        "\n",
        "**The workflow is identical to using real data** - only the data source differs. See the [Data Acquisition Guide](../DATA_GUIDE.md) for instructions on obtaining real Central Luzon SAR flood data.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_synthetic_sar_flood_data(n_train=800, n_val=200, n_test=200, \n",
        "                                       img_size=256, seed=42):\n",
        "    \"\"\"\n",
        "    Generate synthetic SAR flood mapping dataset\n",
        "    \n",
        "    Simulates Sentinel-1 dual-polarization (VV, VH) imagery with flood masks\n",
        "    \n",
        "    Args:\n",
        "        n_train: Number of training samples\n",
        "        n_val: Number of validation samples\n",
        "        n_test: Number of test samples\n",
        "        img_size: Image dimension (default 256x256)\n",
        "        seed: Random seed for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with paths to generated data\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    print(\"Generating synthetic SAR flood data...\")\n",
        "    print(f\"Train: {n_train}, Val: {n_val}, Test: {n_test} samples\")\n",
        "    \n",
        "    # Create directory structure\n",
        "    data_dir = '/content/data/flood_mapping_dataset'\n",
        "    for subset in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(data_dir, subset, 'images'), exist_ok=True)\n",
        "        os.makedirs(os.path.join(data_dir, subset, 'masks'), exist_ok=True)\n",
        "    \n",
        "    def generate_sample(idx, subset):\n",
        "        \"\"\"Generate one SAR image + flood mask pair\"\"\"\n",
        "        \n",
        "        # Simulate SAR backscatter (in dB)\n",
        "        # VV: -25 to 5 dB (typical range)\n",
        "        # VH: -30 to 0 dB (typical range)\n",
        "        vv = np.random.normal(-10, 5, (img_size, img_size))\n",
        "        vh = np.random.normal(-15, 5, (img_size, img_size))\n",
        "        \n",
        "        # Create flood mask with realistic patterns\n",
        "        # Floods appear as connected regions (not random noise)\n",
        "        \n",
        "        # Start with base mask\n",
        "        mask = np.zeros((img_size, img_size), dtype=np.float32)\n",
        "        \n",
        "        # Add 1-3 flood regions per image\n",
        "        n_floods = np.random.randint(1, 4)\n",
        "        \n",
        "        for _ in range(n_floods):\n",
        "            # Random flood center\n",
        "            center_x = np.random.randint(50, img_size-50)\n",
        "            center_y = np.random.randint(50, img_size-50)\n",
        "            \n",
        "            # Random flood size (elliptical shape)\n",
        "            radius_x = np.random.randint(20, 80)\n",
        "            radius_y = np.random.randint(20, 80)\n",
        "            \n",
        "            # Create elliptical flood region\n",
        "            y, x = np.ogrid[:img_size, :img_size]\n",
        "            ellipse = ((x - center_x)**2 / radius_x**2 + \n",
        "                      (y - center_y)**2 / radius_y**2 <= 1)\n",
        "            mask[ellipse] = 1.0\n",
        "        \n",
        "        # Apply Gaussian smoothing to make edges more realistic\n",
        "        from scipy.ndimage import gaussian_filter\n",
        "        mask = gaussian_filter(mask, sigma=2.0)\n",
        "        mask = (mask > 0.3).astype(np.float32)  # Threshold\n",
        "        \n",
        "        # Modify SAR values in flooded regions\n",
        "        # Flooded areas have LOW backscatter (dark in SAR)\n",
        "        flood_mask_bool = mask > 0.5\n",
        "        vv[flood_mask_bool] = np.random.normal(-20, 3, flood_mask_bool.sum())\n",
        "        vh[flood_mask_bool] = np.random.normal(-25, 3, flood_mask_bool.sum())\n",
        "        \n",
        "        # Non-flooded areas have HIGHER backscatter\n",
        "        non_flood = ~flood_mask_bool\n",
        "        vv[non_flood] = np.random.normal(-5, 4, non_flood.sum())\n",
        "        vh[non_flood] = np.random.normal(-10, 4, non_flood.sum())\n",
        "        \n",
        "        # Clip to realistic SAR ranges\n",
        "        vv = np.clip(vv, -30, 10)\n",
        "        vh = np.clip(vh, -35, 5)\n",
        "        \n",
        "        # Stack VV and VH\n",
        "        sar_image = np.stack([vv, vh], axis=-1).astype(np.float32)\n",
        "        \n",
        "        # Expand mask dimension\n",
        "        mask = np.expand_dims(mask, axis=-1).astype(np.float32)\n",
        "        \n",
        "        # Save\n",
        "        img_path = os.path.join(data_dir, subset, 'images', f'sar_{idx:04d}.npy')\n",
        "        mask_path = os.path.join(data_dir, subset, 'masks', f'mask_{idx:04d}.npy')\n",
        "        \n",
        "        np.save(img_path, sar_image)\n",
        "        np.save(mask_path, mask)\n",
        "    \n",
        "    # Generate all samples\n",
        "    print(\"Generating training samples...\")\n",
        "    for i in range(n_train):\n",
        "        generate_sample(i, 'train')\n",
        "        if (i+1) % 200 == 0:\n",
        "            print(f\"  Generated {i+1}/{n_train} training samples\")\n",
        "    \n",
        "    print(\"Generating validation samples...\")\n",
        "    for i in range(n_val):\n",
        "        generate_sample(i, 'val')\n",
        "    \n",
        "    print(\"Generating test samples...\")\n",
        "    for i in range(n_test):\n",
        "        generate_sample(i, 'test')\n",
        "    \n",
        "    print(f\"\\nâœ… Synthetic dataset generated successfully!\")\n",
        "    print(f\"Location: {data_dir}\")\n",
        "    print(f\"Train: {n_train} samples\")\n",
        "    print(f\"Val: {n_val} samples\")\n",
        "    print(f\"Test: {n_test} samples\")\n",
        "    \n",
        "    return {\n",
        "        'data_dir': data_dir,\n",
        "        'n_train': n_train,\n",
        "        'n_val': n_val,\n",
        "        'n_test': n_test\n",
        "    }\n",
        "\n",
        "# Generate synthetic data (takes ~2-3 minutes)\n",
        "dataset_info = generate_synthetic_sar_flood_data(\n",
        "    n_train=800,  # 800 training samples\n",
        "    n_val=200,    # 200 validation samples\n",
        "    n_test=200,   # 200 test samples\n",
        "    img_size=256,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "DATA_DIR = dataset_info['data_dir']\n",
        "print(f\"\\nDataset ready at: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2: Data Exploration\n",
        "\n",
        "### Load Sample Data\n",
        "\n",
        "Understanding your data is crucial before training. Let's explore the SAR imagery and flood masks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_sample_data(data_dir, subset='train', n_samples=5):\n",
        "    \"\"\"Load sample SAR images and masks\"\"\"\n",
        "    img_dir = os.path.join(data_dir, subset, 'images')\n",
        "    mask_dir = os.path.join(data_dir, subset, 'masks')\n",
        "    \n",
        "    img_files = sorted(glob(os.path.join(img_dir, '*.npy')))[:n_samples]\n",
        "    mask_files = sorted(glob(os.path.join(mask_dir, '*.npy')))[:n_samples]\n",
        "    \n",
        "    images = [np.load(f) for f in img_files]\n",
        "    masks = [np.load(f) for f in mask_files]\n",
        "    \n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Load samples\n",
        "sample_images, sample_masks = load_sample_data(DATA_DIR, 'train', n_samples=5)\n",
        "print(f\"Sample images shape: {sample_images.shape}\")  # (5, 256, 256, 2)\n",
        "print(f\"Sample masks shape: {sample_masks.shape}\")    # (5, 256, 256, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize SAR Data\n",
        "\n",
        "::: {.callout-tip}\n",
        "## Understanding SAR Backscatter\n",
        "\n",
        "**VV Polarization:** Vertical transmit, vertical receive\n",
        "- Better for detecting open water (low backscatter)\n",
        "- Values typically -30 to 10 dB\n",
        "\n",
        "**VH Polarization:** Vertical transmit, horizontal receive  \n",
        "- Sensitive to volume scattering (vegetation, urban areas)\n",
        "- Helps distinguish water from wet soil\n",
        "\n",
        "**Flood Detection:** Flooded areas appear **dark** (low backscatter) in both polarizations\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_sar_samples(images, masks, n_samples=3):\n",
        "    \"\"\"Visualize SAR images (VV, VH) and flood masks\"\"\"\n",
        "    fig, axes = plt.subplots(n_samples, 4, figsize=(16, n_samples*4))\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        # VV polarization\n",
        "        axes[i, 0].imshow(images[i, :, :, 0], cmap='gray', vmin=-25, vmax=5)\n",
        "        axes[i, 0].set_title(f'Sample {i+1}: VV (dB)')\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        # VH polarization\n",
        "        axes[i, 1].imshow(images[i, :, :, 1], cmap='gray', vmin=-30, vmax=0)\n",
        "        axes[i, 1].set_title(f'Sample {i+1}: VH (dB)')\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        # Flood mask (ground truth)\n",
        "        axes[i, 2].imshow(masks[i, :, :, 0], cmap='Blues', vmin=0, vmax=1)\n",
        "        axes[i, 2].set_title(f'Ground Truth Mask')\n",
        "        axes[i, 2].axis('off')\n",
        "        \n",
        "        # Overlay on VV\n",
        "        overlay = images[i, :, :, 0].copy()\n",
        "        overlay_rgb = plt.cm.gray((overlay + 25) / 30)[:, :, :3]\n",
        "        mask_overlay = masks[i, :, :, 0]\n",
        "        overlay_rgb[mask_overlay > 0.5] = [0, 0.5, 1]  # Blue for flood\n",
        "        axes[i, 3].imshow(overlay_rgb)\n",
        "        axes[i, 3].set_title(f'Overlay: Flood in Blue')\n",
        "        axes[i, 3].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_sar_samples(sample_images, sample_masks, n_samples=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"SAR Data Statistics:\")\n",
        "print(f\"VV min: {sample_images[:,:,:,0].min():.2f} dB\")\n",
        "print(f\"VV max: {sample_images[:,:,:,0].max():.2f} dB\")\n",
        "print(f\"VV mean: {sample_images[:,:,:,0].mean():.2f} dB\")\n",
        "print(f\"VH min: {sample_images[:,:,:,1].min():.2f} dB\")\n",
        "print(f\"VH max: {sample_images[:,:,:,1].max():.2f} dB\")\n",
        "print(f\"VH mean: {sample_images[:,:,:,1].mean():.2f} dB\")\n",
        "\n",
        "print(\"\\nFlood Mask Statistics:\")\n",
        "flood_ratio = sample_masks.mean() * 100\n",
        "print(f\"Flood pixels: {flood_ratio:.2f}%\")\n",
        "print(f\"Non-flood pixels: {100-flood_ratio:.2f}%\")\n",
        "print(f\"Class imbalance ratio: 1:{(100-flood_ratio)/flood_ratio:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3: Data Preprocessing\n",
        "\n",
        "### Normalization Strategy\n",
        "\n",
        "SAR data requires proper normalization for neural network training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_sar(image, method='minmax'):\n",
        "    \"\"\"\n",
        "    Normalize SAR backscatter values\n",
        "    \n",
        "    Methods:\n",
        "    - 'minmax': Scale to [0, 1] based on typical SAR range\n",
        "    - 'zscore': Standardize to mean=0, std=1\n",
        "    \"\"\"\n",
        "    if method == 'minmax':\n",
        "        # Typical SAR range: -30 to 10 dB\n",
        "        vv_normalized = (image[:, :, 0] + 30) / 40  # Scale VV\n",
        "        vh_normalized = (image[:, :, 1] + 35) / 35  # Scale VH\n",
        "        return np.stack([vv_normalized, vh_normalized], axis=-1)\n",
        "    \n",
        "    elif method == 'zscore':\n",
        "        # Standardize each channel\n",
        "        mean = image.mean(axis=(0, 1), keepdims=True)\n",
        "        std = image.std(axis=(0, 1), keepdims=True)\n",
        "        return (image - mean) / (std + 1e-8)\n",
        "\n",
        "# Test normalization\n",
        "normalized_sample = normalize_sar(sample_images[0], method='minmax')\n",
        "print(f\"Normalized range: [{normalized_sample.min():.3f}, {normalized_sample.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Augmentation\n",
        "\n",
        "::: {.callout-important}\n",
        "## Critical: Augment Image AND Mask Together\n",
        "\n",
        "For segmentation, **both the image and mask must receive identical transformations**. Augmenting only the image will cause misalignment.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def augment_data(image, mask, augment=True):\n",
        "    \"\"\"Apply data augmentation to image and mask\"\"\"\n",
        "    if not augment:\n",
        "        return image, mask\n",
        "    \n",
        "    # Random horizontal flip\n",
        "    if np.random.random() > 0.5:\n",
        "        image = np.fliplr(image)\n",
        "        mask = np.fliplr(mask)\n",
        "    \n",
        "    # Random vertical flip\n",
        "    if np.random.random() > 0.5:\n",
        "        image = np.flipud(image)\n",
        "        mask = np.flipud(mask)\n",
        "    \n",
        "    # Random 90-degree rotations (valid for nadir satellite views)\n",
        "    k = np.random.randint(0, 4)  # 0, 90, 180, 270 degrees\n",
        "    image = np.rot90(image, k)\n",
        "    mask = np.rot90(mask, k)\n",
        "    \n",
        "    return image, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create TensorFlow Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_tf_dataset(data_dir, subset='train', batch_size=16, augment=False):\n",
        "    \"\"\"Create TensorFlow dataset with preprocessing\"\"\"\n",
        "    img_dir = os.path.join(data_dir, subset, 'images')\n",
        "    mask_dir = os.path.join(data_dir, subset, 'masks')\n",
        "    \n",
        "    img_files = sorted(glob(os.path.join(img_dir, '*.npy')))\n",
        "    mask_files = sorted(glob(os.path.join(mask_dir, '*.npy')))\n",
        "    \n",
        "    def load_and_preprocess(img_path, mask_path):\n",
        "        # Load\n",
        "        img = np.load(img_path.numpy().decode('utf-8'))\n",
        "        mask = np.load(mask_path.numpy().decode('utf-8'))\n",
        "        \n",
        "        # Normalize\n",
        "        img = normalize_sar(img, method='minmax')\n",
        "        \n",
        "        # Augment\n",
        "        if augment:\n",
        "            img, mask = augment_data(img, mask, augment=True)\n",
        "        \n",
        "        return img.astype(np.float32), mask.astype(np.float32)\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((img_files, mask_files))\n",
        "    dataset = dataset.map(\n",
        "        lambda x, y: tf.py_function(\n",
        "            load_and_preprocess, [x, y], [tf.float32, tf.float32]\n",
        "        ),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# Create datasets\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = create_tf_dataset(DATA_DIR, 'train', BATCH_SIZE, augment=True)\n",
        "val_dataset = create_tf_dataset(DATA_DIR, 'val', BATCH_SIZE, augment=False)\n",
        "test_dataset = create_tf_dataset(DATA_DIR, 'test', BATCH_SIZE, augment=False)\n",
        "\n",
        "print(f\"Train batches: {len(list(train_dataset))}\")\n",
        "print(f\"Val batches: {len(list(val_dataset))}\")\n",
        "print(f\"Test batches: {len(list(test_dataset))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 4: U-Net Model Implementation\n",
        "\n",
        "Now we'll implement the U-Net architecture from Session 1. This is where theory meets practice.\n",
        "\n",
        "### Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def unet_model(input_shape=(256, 256, 2), num_classes=1):\n",
        "    \"\"\"\n",
        "    U-Net architecture for binary flood segmentation\n",
        "    \n",
        "    Args:\n",
        "        input_shape: (height, width, channels) - (256, 256, 2) for VV+VH\n",
        "        num_classes: 1 for binary segmentation (sigmoid output)\n",
        "    \n",
        "    Returns:\n",
        "        Keras Model\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    # Encoder (Contracting Path)\n",
        "    # Block 1\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "    \n",
        "    # Block 2\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "    \n",
        "    # Block 3\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "    \n",
        "    # Block 4\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
        "    \n",
        "    # Bottleneck\n",
        "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
        "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
        "    \n",
        "    # Decoder (Expansive Path)\n",
        "    # Block 6\n",
        "    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = layers.concatenate([u6, c4])  # Skip connection\n",
        "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
        "    \n",
        "    # Block 7\n",
        "    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = layers.concatenate([u7, c3])  # Skip connection\n",
        "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
        "    \n",
        "    # Block 8\n",
        "    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = layers.concatenate([u8, c2])  # Skip connection\n",
        "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
        "    \n",
        "    # Block 9\n",
        "    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = layers.concatenate([u9, c1])  # Skip connection\n",
        "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
        "    \n",
        "    # Output layer\n",
        "    outputs = layers.Conv2D(num_classes, (1, 1), activation='sigmoid')(c9)\n",
        "    \n",
        "    model = keras.Model(inputs=[inputs], outputs=[outputs], name='U-Net')\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = unet_model(input_shape=(256, 256, 2), num_classes=1)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Functions\n",
        "\n",
        "Implementing the loss functions from Session 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"Dice coefficient for evaluation\"\"\"\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    \"\"\"Dice loss for training\"\"\"\n",
        "    return 1 - dice_coefficient(y_true, y_pred)\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    \"\"\"Combined Binary Cross-Entropy + Dice Loss\"\"\"\n",
        "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "    dice = dice_loss(y_true, y_pred)\n",
        "    return 0.5 * bce + 0.5 * dice\n",
        "\n",
        "def iou_score(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"IoU metric (Intersection over Union)\"\"\"\n",
        "    y_true_f = tf.keras.backend.flatten(y_true)\n",
        "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
        "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
        "    union = tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) - intersection\n",
        "    return (intersection + smooth) / (union + smooth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 5: Model Training\n",
        "\n",
        "### Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile with combined loss\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=combined_loss,\n",
        "    metrics=['accuracy', dice_coefficient, iou_score]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directories\n",
        "os.makedirs('/content/models', exist_ok=True)\n",
        "os.makedirs('/content/logs', exist_ok=True)\n",
        "\n",
        "# Callbacks for training\n",
        "checkpoint_cb = callbacks.ModelCheckpoint(\n",
        "    '/content/models/unet_flood_best.h5',\n",
        "    monitor='val_iou_score',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stop_cb = callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr_cb = callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "tensorboard_cb = callbacks.TensorBoard(\n",
        "    log_dir='/content/logs',\n",
        "    histogram_freq=1\n",
        ")\n",
        "\n",
        "callback_list = [checkpoint_cb, early_stop_cb, reduce_lr_cb, tensorboard_cb]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the Model\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Training Time Estimate\n",
        "\n",
        "- **With GPU (T4):** 15-25 minutes for 50 epochs\n",
        "- **With CPU:** 4-6 hours (not recommended)\n",
        "\n",
        "The model will likely converge in 20-30 epochs with early stopping.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model\n",
        "EPOCHS = 50\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callback_list,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation metrics\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss\n",
        "    axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
        "    axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n",
        "    axes[0, 0].set_title('Model Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "    \n",
        "    # Dice Coefficient\n",
        "    axes[0, 1].plot(history.history['dice_coefficient'], label='Train Dice')\n",
        "    axes[0, 1].plot(history.history['val_dice_coefficient'], label='Val Dice')\n",
        "    axes[0, 1].set_title('Dice Coefficient')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Dice')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "    \n",
        "    # IoU Score\n",
        "    axes[1, 0].plot(history.history['iou_score'], label='Train IoU')\n",
        "    axes[1, 0].plot(history.history['val_iou_score'], label='Val IoU')\n",
        "    axes[0, 1].set_title('IoU Score')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('IoU')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "    \n",
        "    # Accuracy\n",
        "    axes[1, 1].plot(history.history['accuracy'], label='Train Acc')\n",
        "    axes[1, 1].plot(history.history['val_accuracy'], label='Val Acc')\n",
        "    axes[1, 1].set_title('Pixel Accuracy')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Accuracy')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 6: Model Evaluation\n",
        "\n",
        "### Load Best Model\n",
        "\n",
        "After training completes, load the best model weights (saved by ModelCheckpoint):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "best_model = keras.models.load_model(\n",
        "    '/content/models/unet_flood_best.h5',\n",
        "    custom_objects={\n",
        "        'combined_loss': combined_loss,\n",
        "        'dice_coefficient': dice_coefficient,\n",
        "        'iou_score': iou_score\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"âœ“ Best model loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test dataset\n",
        "test_results = best_model.evaluate(test_dataset, verbose=1)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Loss: {test_results[0]:.4f}\")\n",
        "print(f\"Pixel Accuracy: {test_results[1]:.4f}\")\n",
        "print(f\"Dice Coefficient: {test_results[2]:.4f}\")\n",
        "print(f\"IoU Score: {test_results[3]:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Metrics Calculation\n",
        "\n",
        "Calculate per-class precision, recall, and F1-score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_detailed_metrics(model, dataset):\n",
        "    \"\"\"Calculate comprehensive segmentation metrics\"\"\"\n",
        "    y_true_all = []\n",
        "    y_pred_all = []\n",
        "    \n",
        "    for images, masks in dataset:\n",
        "        predictions = model.predict(images, verbose=0)\n",
        "        y_true_all.append(masks.numpy().flatten())\n",
        "        y_pred_all.append((predictions > 0.5).astype(np.float32).flatten())\n",
        "    \n",
        "    y_true = np.concatenate(y_true_all)\n",
        "    y_pred = np.concatenate(y_pred_all)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "    \n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    \n",
        "    # Confusion matrix components\n",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "    \n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'true_positives': tp,\n",
        "        'true_negatives': tn,\n",
        "        'false_positives': fp,\n",
        "        'false_negatives': fn\n",
        "    }\n",
        "\n",
        "# Calculate metrics\n",
        "metrics = calculate_detailed_metrics(best_model, test_dataset)\n",
        "\n",
        "print(\"\\nDETAILED METRICS (Flood Class)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {metrics['recall']:.4f}\")\n",
        "print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
        "print(f\"\\nTrue Positives: {metrics['true_positives']:,}\")\n",
        "print(f\"True Negatives: {metrics['true_negatives']:,}\")\n",
        "print(f\"False Positives: {metrics['false_positives']:,}\")\n",
        "print(f\"False Negatives: {metrics['false_negatives']:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(metrics):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = np.array([\n",
        "        [metrics['true_negatives'], metrics['false_positives']],\n",
        "        [metrics['false_negatives'], metrics['true_positives']]\n",
        "    ])\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=',d', cmap='Blues', \n",
        "                xticklabels=['Non-Flood', 'Flood'],\n",
        "                yticklabels=['Non-Flood', 'Flood'])\n",
        "    plt.title('Confusion Matrix - Flood Detection')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "## Interpreting Results\n",
        "\n",
        "**Good Performance Indicators:**\n",
        "- **IoU > 0.70:** Strong overlap between prediction and ground truth\n",
        "- **High Precision:** Few false alarms (predicted flood where there's none)\n",
        "- **High Recall:** Catches most actual floods (few missed floods)\n",
        "- **F1 > 0.75:** Balanced performance\n",
        "\n",
        "**For Disaster Response:**\n",
        "- **Precision matters:** Avoid sending resources to non-flooded areas\n",
        "- **Recall matters more:** Don't miss flooded communities needing help\n",
        "- Trade-off depends on operational priorities\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7: Visualization and Interpretation\n",
        "\n",
        "### Predict on Test Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(model, dataset, n_samples=5):\n",
        "    \"\"\"Visualize model predictions vs ground truth\"\"\"\n",
        "    # Get samples\n",
        "    images, masks = next(iter(dataset))\n",
        "    predictions = model.predict(images[:n_samples], verbose=0)\n",
        "    \n",
        "    fig, axes = plt.subplots(n_samples, 4, figsize=(20, n_samples*5))\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        # Original SAR VV\n",
        "        axes[i, 0].imshow(images[i, :, :, 0], cmap='gray', vmin=0, vmax=1)\n",
        "        axes[i, 0].set_title(f'SAR VV (Normalized)')\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        # Ground Truth\n",
        "        axes[i, 1].imshow(masks[i, :, :, 0], cmap='Blues', vmin=0, vmax=1)\n",
        "        axes[i, 1].set_title('Ground Truth Mask')\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        # Prediction\n",
        "        axes[i, 2].imshow(predictions[i, :, :, 0], cmap='Blues', vmin=0, vmax=1)\n",
        "        axes[i, 2].set_title(f'Prediction (IoU: {iou_score(masks[i:i+1], predictions[i:i+1]).numpy():.3f})')\n",
        "        axes[i, 2].axis('off')\n",
        "        \n",
        "        # Overlay: Green=Correct, Red=FP, Yellow=FN\n",
        "        overlay = np.zeros((256, 256, 3))\n",
        "        gt = masks[i, :, :, 0] > 0.5\n",
        "        pred = predictions[i, :, :, 0] > 0.5\n",
        "        \n",
        "        # True Positives (Green)\n",
        "        overlay[gt & pred] = [0, 1, 0]\n",
        "        # False Positives (Red)\n",
        "        overlay[~gt & pred] = [1, 0, 0]\n",
        "        # False Negatives (Yellow)\n",
        "        overlay[gt & ~pred] = [1, 1, 0]\n",
        "        \n",
        "        axes[i, 3].imshow(overlay)\n",
        "        axes[i, 3].set_title('Overlay: Green=TP, Red=FP, Yellow=FN')\n",
        "        axes[i, 3].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions(best_model, test_dataset, n_samples=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_errors(model, dataset):\n",
        "    \"\"\"Analyze common error patterns\"\"\"\n",
        "    total_samples = 0\n",
        "    high_iou = 0  # IoU > 0.8\n",
        "    medium_iou = 0  # 0.5 < IoU <= 0.8\n",
        "    low_iou = 0  # IoU <= 0.5\n",
        "    \n",
        "    for images, masks in dataset:\n",
        "        predictions = model.predict(images, verbose=0)\n",
        "        \n",
        "        for i in range(len(images)):\n",
        "            iou = iou_score(masks[i:i+1], predictions[i:i+1]).numpy()\n",
        "            total_samples += 1\n",
        "            \n",
        "            if iou > 0.8:\n",
        "                high_iou += 1\n",
        "            elif iou > 0.5:\n",
        "                medium_iou += 1\n",
        "            else:\n",
        "                low_iou += 1\n",
        "    \n",
        "    print(f\"\\nERROR ANALYSIS (n={total_samples} patches)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"High Quality (IoU > 0.8): {high_iou} ({high_iou/total_samples*100:.1f}%)\")\n",
        "    print(f\"Medium Quality (0.5 < IoU â‰¤ 0.8): {medium_iou} ({medium_iou/total_samples*100:.1f}%)\")\n",
        "    print(f\"Poor Quality (IoU â‰¤ 0.5): {low_iou} ({low_iou/total_samples*100:.1f}%)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "analyze_errors(best_model, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-tip}\n",
        "## Common Error Patterns\n",
        "\n",
        "**False Positives (Red areas):**\n",
        "- Wet soil after rain (similar backscatter to water)\n",
        "- Shadows in mountainous terrain\n",
        "- Very calm water bodies (pre-flood)\n",
        "\n",
        "**False Negatives (Yellow areas):**\n",
        "- Flooded vegetation (volume scattering increases backscatter)\n",
        "- Mixed pixels at flood boundaries\n",
        "- Speckle noise in SAR data\n",
        "\n",
        "**Improvement Strategies:**\n",
        "- Use multi-temporal data (before/after comparison)\n",
        "- Incorporate DEM (elevation-based flood likelihood)\n",
        "- Ensemble multiple models\n",
        "- Post-processing with GIS constraints\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Step 8: Export and GIS Integration\n",
        "\n",
        "### Save Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model in different formats\n",
        "best_model.save('/content/models/unet_flood_final.h5')  # Full model\n",
        "best_model.save('/content/models/unet_flood_final.keras')  # New Keras format\n",
        "\n",
        "# Save to Google Drive for persistence\n",
        "!cp /content/models/unet_flood_final.h5 /content/drive/MyDrive/flood_mapping/\n",
        "\n",
        "print(\"âœ“ Model saved successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_predictions(model, dataset, output_dir='/content/outputs'):\n",
        "    \"\"\"Export predictions as NumPy arrays\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    batch_idx = 0\n",
        "    for images, masks in dataset:\n",
        "        predictions = model.predict(images, verbose=0)\n",
        "        \n",
        "        for i in range(len(images)):\n",
        "            # Save prediction\n",
        "            pred_file = os.path.join(output_dir, f'prediction_{batch_idx:04d}.npy')\n",
        "            np.save(pred_file, predictions[i])\n",
        "            \n",
        "            # Save binary mask (threshold at 0.5)\n",
        "            binary_file = os.path.join(output_dir, f'binary_mask_{batch_idx:04d}.npy')\n",
        "            binary_mask = (predictions[i] > 0.5).astype(np.uint8)\n",
        "            np.save(binary_file, binary_mask)\n",
        "            \n",
        "            batch_idx += 1\n",
        "    \n",
        "    print(f\"âœ“ Exported {batch_idx} predictions to {output_dir}\")\n",
        "\n",
        "export_predictions(best_model, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Flood Polygons (Conceptual)\n",
        "\n",
        "::: {.callout-note}\n",
        "## GIS Integration Workflow\n",
        "\n",
        "**For operational use, follow these steps:**\n",
        "\n",
        "1. **Georeferencing:**\n",
        "   - Match predictions back to original SAR geocoordinates\n",
        "   - Use metadata from Sentinel-1 GRD products\n",
        "\n",
        "2. **Vectorization:**\n",
        "\n",
        "```python\n",
        "# Pseudocode - requires rasterio and geopandas\n",
        "import rasterio\n",
        "from rasterio.features import shapes\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import shape\n",
        "\n",
        "# Convert binary mask to polygons\n",
        "mask = (prediction > 0.5).astype(np.uint8)\n",
        "shapes_gen = shapes(mask, transform=affine_transform)\n",
        "polygons = [shape(s) for s, v in shapes_gen if v == 1]\n",
        "\n",
        "# Create GeoDataFrame\n",
        "gdf = gpd.GeoDataFrame({'geometry': polygons}, crs='EPSG:4326')\n",
        "gdf.to_file('flood_extent.geojson')\n",
        "```\n",
        "\n",
        "3. **Export Formats:**\n",
        "   - **GeoTIFF:** Raster format for GIS software\n",
        "   - **Shapefile/GeoJSON:** Vector format for flood polygons\n",
        "   - **KML:** For Google Earth visualization\n",
        "\n",
        "4. **Integration with QGIS/ArcGIS:**\n",
        "   - Load flood extent layer\n",
        "   - Overlay with administrative boundaries\n",
        "   - Calculate affected area and population\n",
        "   - Generate maps for disaster response teams\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Download Results\n",
        "\n",
        "```python\n",
        "# Zip outputs for download\n",
        "!zip -r /content/flood_mapping_results.zip /content/outputs /content/models\n",
        "\n",
        "# Copy to Google Drive\n",
        "!cp /content/flood_mapping_results.zip /content/drive/MyDrive/\n",
        "\n",
        "print(\"âœ“ Results ready for download from Google Drive\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "python\n",
        "# Zip outputs for download\n",
        "!zip -r /content/flood_mapping_results.zip /content/outputs /content/models\n",
        "\n",
        "# Copy to Google Drive\n",
        "!cp /content/flood_mapping_results.zip /content/drive/MyDrive/\n",
        "\n",
        "print(\"âœ“ Results ready for download from Google Drive\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Troubleshooting Guide\n",
        "\n",
        "### Common Issues and Solutions\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Issue 1: Out of Memory (OOM) Errors\n",
        "\n",
        "**Symptoms:**\n",
        "- \"ResourceExhaustedError: OOM when allocating tensor\"\n",
        "- Training crashes during forward/backward pass\n",
        "\n",
        "**Solutions:**\n",
        "1. **Reduce batch size:**\n",
        "   ```python\n",
        "   BATCH_SIZE = 8  # Instead of 16\n",
        "   ```\n",
        "\n",
        "2. **Use mixed precision training:**\n",
        "   ```python\n",
        "   from tensorflow.keras import mixed_precision\n",
        "   policy = mixed_precision.Policy('mixed_float16')\n",
        "   mixed_precision.set_global_policy(policy)\n",
        "   ```\n",
        "\n",
        "3. **Clear memory between runs:**\n",
        "   ```python\n",
        "   from tensorflow.keras import backend as K\n",
        "   K.clear_session()\n",
        "   ```\n",
        "\n",
        "4. **Use smaller model:**\n",
        "   - Reduce filters in U-Net layers (64â†’32, 128â†’64, etc.)\n",
        ":::\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Issue 2: Model Not Learning (Loss Plateau)\n",
        "\n",
        "**Symptoms:**\n",
        "- Loss stuck at high value (>0.5)\n",
        "- Validation metrics don't improve\n",
        "- Model predicts all zeros or all ones\n",
        "\n",
        "**Solutions:**\n",
        "1. **Check data normalization:**\n",
        "   ```python\n",
        "   # Verify normalized range\n",
        "   print(f\"Min: {images.min()}, Max: {images.max()}\")\n",
        "   # Should be in [0, 1] range\n",
        "   ```\n",
        "\n",
        "2. **Verify labels are correct:**\n",
        "   ```python\n",
        "   # Check mask values\n",
        "   print(f\"Unique mask values: {np.unique(masks)}\")\n",
        "   # Should be [0, 1] for binary\n",
        "   ```\n",
        "\n",
        "3. **Adjust learning rate:**\n",
        "   ```python\n",
        "   # Try higher initial LR\n",
        "   optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
        "   ```\n",
        "\n",
        "4. **Use stronger loss function:**\n",
        "   ```python\n",
        "   # Switch to pure Dice loss if class imbalance is severe\n",
        "   model.compile(optimizer=optimizer, loss=dice_loss, ...)\n",
        "   ```\n",
        ":::\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Issue 3: Overfitting (High Train, Low Val Accuracy)\n",
        "\n",
        "**Symptoms:**\n",
        "- Training accuracy > 95%, validation < 80%\n",
        "- Large gap between train and val metrics\n",
        "- Model memorizes training data\n",
        "\n",
        "**Solutions:**\n",
        "1. **Increase data augmentation:**\n",
        "   ```python\n",
        "   # Add more aggressive augmentation\n",
        "   if augment:\n",
        "       # Add brightness adjustment\n",
        "       image = image * np.random.uniform(0.8, 1.2)\n",
        "       # Add Gaussian noise\n",
        "       image += np.random.normal(0, 0.05, image.shape)\n",
        "   ```\n",
        "\n",
        "2. **Add dropout layers:**\n",
        "   ```python\n",
        "   c1 = layers.Dropout(0.2)(c1)  # After conv blocks\n",
        "   ```\n",
        "\n",
        "3. **Reduce model complexity:**\n",
        "   ```python\n",
        "   # Use fewer filters or fewer blocks\n",
        "   ```\n",
        "\n",
        "4. **Get more training data:**\n",
        "   - Extract more patches from available imagery\n",
        "   - Use data from different typhoon events\n",
        ":::\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Issue 4: Predictions All Black or All White\n",
        "\n",
        "**Symptoms:**\n",
        "- Model outputs all 0s or all 1s\n",
        "- No meaningful segmentation\n",
        "\n",
        "**Solutions:**\n",
        "1. **Check output activation:**\n",
        "   ```python\n",
        "   # Ensure sigmoid for binary\n",
        "   outputs = layers.Conv2D(1, 1, activation='sigmoid')(c9)\n",
        "   ```\n",
        "\n",
        "2. **Verify loss handles imbalance:**\n",
        "   ```python\n",
        "   # Use Dice or combined loss, not pure BCE\n",
        "   loss = combined_loss\n",
        "   ```\n",
        "\n",
        "3. **Check threshold:**\n",
        "   ```python\n",
        "   # Try different thresholds\n",
        "   binary_pred = (prediction > 0.3).astype(np.uint8)\n",
        "   ```\n",
        "\n",
        "4. **Inspect raw predictions:**\n",
        "   ```python\n",
        "   print(f\"Prediction range: {predictions.min():.3f} to {predictions.max():.3f}\")\n",
        "   # Should vary, not all same value\n",
        "   ```\n",
        ":::\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Issue 5: Colab Disconnections\n",
        "\n",
        "**Symptoms:**\n",
        "- Session times out during training\n",
        "- \"Runtime disconnected\" message\n",
        "- Lost training progress\n",
        "\n",
        "**Solutions:**\n",
        "1. **Keep browser active:**\n",
        "   - Don't minimize tab\n",
        "   - Use Colab Pro for longer runtimes\n",
        "\n",
        "2. **Save checkpoints frequently:**\n",
        "   ```python\n",
        "   # Already configured in ModelCheckpoint callback\n",
        "   checkpoint_cb = callbacks.ModelCheckpoint(\n",
        "       filepath='model.h5',\n",
        "       save_freq='epoch'  # Save every epoch\n",
        "   )\n",
        "   ```\n",
        "\n",
        "3. **Save to Google Drive:**\n",
        "   ```python\n",
        "   # Mount Drive and save there\n",
        "   drive.mount('/content/drive')\n",
        "   model.save('/content/drive/MyDrive/checkpoints/model_epoch_{epoch}.h5')\n",
        "   ```\n",
        "\n",
        "4. **Use console keepalive (JavaScript):**\n",
        "   ```javascript\n",
        "   // Run in browser console\n",
        "   function ClickConnect(){\n",
        "     console.log(\"Keeping alive\");\n",
        "     document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "   }\n",
        "   setInterval(ClickConnect, 60000)\n",
        "   ```\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "::: {.callout-important}\n",
        "## What You've Accomplished\n",
        "\n",
        "**Technical Skills:**\n",
        "âœ… Loaded and preprocessed Sentinel-1 SAR data for deep learning  \n",
        "âœ… Implemented complete U-Net architecture from scratch  \n",
        "âœ… Trained a segmentation model with appropriate loss functions  \n",
        "âœ… Evaluated performance using multiple metrics (IoU, Dice, F1)  \n",
        "âœ… Visualized and interpreted model predictions  \n",
        "âœ… Exported results for GIS integration\n",
        "\n",
        "**Conceptual Understanding:**\n",
        "âœ… How SAR backscatter relates to flood detection  \n",
        "âœ… Why skip connections are critical for precise segmentation  \n",
        "âœ… How to handle class imbalance in segmentation tasks  \n",
        "âœ… Trade-offs between precision and recall for disaster response  \n",
        "âœ… Common error patterns and improvement strategies\n",
        "\n",
        "**Philippine DRR Context:**\n",
        "âœ… Applied deep learning to real Typhoon Ulysses flood data  \n",
        "âœ… Understood operational requirements for disaster response  \n",
        "âœ… Prepared outputs for integration with PAGASA/DOST systems\n",
        ":::\n",
        "\n",
        "### Critical Lessons\n",
        "\n",
        "1. **Data Quality >> Model Complexity**\n",
        "   - Well-prepared SAR data is more important than model tweaks\n",
        "   - Ground truth quality directly impacts performance\n",
        "\n",
        "2. **Loss Function Selection Matters**\n",
        "   - Combined loss (BCE + Dice) works best for imbalanced flood data\n",
        "   - Pure cross-entropy fails when flood pixels are <10%\n",
        "\n",
        "3. **Evaluation Beyond Accuracy**\n",
        "   - Pixel accuracy misleading for imbalanced classes\n",
        "   - IoU and Dice give true performance picture\n",
        "   - Confusion matrix reveals error types\n",
        "\n",
        "4. **Operational Considerations**\n",
        "   - For disaster response, recall > precision (catch all floods)\n",
        "   - Speed matters: Train once, inference in minutes\n",
        "   - GIS integration essential for actionable outputs\n",
        "\n",
        "---\n",
        "\n",
        "## Resources and Further Learning\n",
        "\n",
        "### Datasets\n",
        "\n",
        "**Flood Mapping:**\n",
        "- [Sen1Floods11](https://github.com/cloudtostreet/Sen1Floods11) - Global flood dataset with Sentinel-1\n",
        "- [FloodNet](https://github.com/BinaLab/FloodNet-Supervised_v1.0) - High-resolution flood imagery\n",
        "- [UNOSAT Flood Portal](https://flood.unosat.org/) - Validated flood extent maps\n",
        "\n",
        "**SAR Data:**\n",
        "- [Copernicus Open Access Hub](https://scihub.copernicus.eu/) - Download Sentinel-1 GRD\n",
        "- [Alaska Satellite Facility (ASF)](https://search.asf.alaska.edu/) - SAR data archive\n",
        "- [Google Earth Engine](https://earthengine.google.com/) - Cloud-based SAR processing\n",
        "\n",
        "### Papers and Tutorials\n",
        "\n",
        "**U-Net and Segmentation:**\n",
        "- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) - Original paper (Ronneberger et al., 2015)\n",
        "- [TensorFlow Image Segmentation Tutorial](https://www.tensorflow.org/tutorials/images/segmentation)\n",
        "- [PyTorch Semantic Segmentation](https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n",
        "\n",
        "**SAR Flood Mapping:**\n",
        "- [Flood Detection with SAR: A Review](https://doi.org/10.3390/rs12020304) - Comprehensive review\n",
        "- [Deep Learning for SAR Image Analysis](https://arxiv.org/abs/2006.10027)\n",
        "- [Automated Flood Mapping Using Sentinel-1](https://doi.org/10.1016/j.isprsjprs.2020.08.012)\n",
        "\n",
        "**Loss Functions:**\n",
        "- [Dice Loss for Imbalanced Segmentation](https://arxiv.org/abs/1707.03237)\n",
        "- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)\n",
        "- [Combo Loss: Handling Input and Output Imbalance](https://arxiv.org/abs/1805.02798)\n",
        "\n",
        "### Code Repositories\n",
        "\n",
        "- [Segmentation Models](https://github.com/qubvel/segmentation_models) - Pre-built architectures\n",
        "- [TorchGeo](https://github.com/microsoft/torchgeo) - PyTorch for geospatial data\n",
        "- [RasterVision](https://github.com/azavea/raster-vision) - End-to-end pipeline for EO\n",
        "\n",
        "### Philippine EO Context\n",
        "\n",
        "- **PhilSA Space+ Data Dashboard:** [https://data.philsa.gov.ph](https://data.philsa.gov.ph)\n",
        "- **DOST-ASTI DATOS:** Rapid mapping for disasters\n",
        "- **NAMRIA GeoPortal:** Hazard maps and basemaps\n",
        "- **PAGASA:** Weather and climate data\n",
        "\n",
        "---\n",
        "\n",
        "## Discussion Questions\n",
        "\n",
        "Before moving to Session 3, reflect on these questions:\n",
        "\n",
        "1. **Real-World Application:**\n",
        "   - How would you deploy this flood mapping system for real-time disaster response in your agency?\n",
        "   - What infrastructure and data pipelines would you need?\n",
        "\n",
        "2. **Model Limitations:**\n",
        "   - What types of floods might this model miss (based on SAR characteristics)?\n",
        "   - How would you validate predictions in areas with no ground truth?\n",
        "\n",
        "3. **Improvements:**\n",
        "   - If you had multi-temporal data (before and after), how would you modify the approach?\n",
        "   - How could you incorporate elevation data (DEM) to improve predictions?\n",
        "\n",
        "4. **Operational Challenges:**\n",
        "   - What's the acceptable latency for flood mapping in disaster response?\n",
        "   - How would you handle uncertainty quantification for decision-makers?\n",
        "\n",
        "5. **Ethical Considerations:**\n",
        "   - What happens if the model misses a flooded community (false negative)?\n",
        "   - How do you balance automation with human expertise in critical decisions?\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Results Summary\n",
        "\n",
        "After completing this lab, you should achieve:\n",
        "\n",
        "| Metric | Expected Range | Interpretation |\n",
        "|--------|----------------|----------------|\n",
        "| **IoU (Test)** | 0.65 - 0.80 | Good to excellent overlap |\n",
        "| **Dice Coefficient** | 0.70 - 0.85 | Strong agreement with ground truth |\n",
        "| **Precision** | 0.70 - 0.90 | Few false flood alarms |\n",
        "| **Recall** | 0.75 - 0.95 | Catches most actual floods |\n",
        "| **F1-Score** | 0.72 - 0.88 | Balanced performance |\n",
        "| **Training Time** | 15-30 min | With GPU (T4) |\n",
        "\n",
        "::: {.callout-tip}\n",
        "## If Your Results Are Lower\n",
        "\n",
        "**IoU < 0.60:**\n",
        "- Check data quality and normalization\n",
        "- Increase training epochs or adjust learning rate\n",
        "- Try different loss function combinations\n",
        "- Ensure adequate training data diversity\n",
        "\n",
        "**High Precision, Low Recall:**\n",
        "- Model is too conservative (missing floods)\n",
        "- Increase weight on positive class\n",
        "- Use Dice loss instead of BCE\n",
        "\n",
        "**High Recall, Low Precision:**\n",
        "- Model predicting too much flood\n",
        "- Add more negative examples to training\n",
        "- Use stricter threshold (>0.6 instead of >0.5)\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "::: {.callout-important}\n",
        "## Preparation for Session 3: Object Detection\n",
        "\n",
        "**Session 3** will introduce object detection techniques for identifying and localizing specific features in EO imagery.\n",
        "\n",
        "**Topics:**\n",
        "- R-CNN, YOLO, and SSD architectures\n",
        "- Bounding box regression\n",
        "- Anchor boxes and non-maximum suppression\n",
        "- Applications: Ship detection, building detection, vehicle counting\n",
        "\n",
        "**Preparation:**\n",
        "- Review CNN concepts from Day 2\n",
        "- Understand difference between segmentation (pixel-wise) and detection (bounding boxes)\n",
        "- Consider: What EO applications need object detection vs segmentation?\n",
        "\n",
        "[Preview Session 3 â†’](../sessions/session3.qmd){.btn .btn-outline-primary}\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "## Lab Completion Checklist\n",
        "\n",
        "Before finishing, ensure you've completed:\n",
        "\n",
        "- [ ] Successfully trained U-Net model\n",
        "- [ ] Achieved IoU > 0.60 on test set\n",
        "- [ ] Visualized predictions vs ground truth\n",
        "- [ ] Analyzed error patterns\n",
        "- [ ] Saved trained model to Google Drive\n",
        "- [ ] Exported predictions\n",
        "- [ ] Understood key troubleshooting strategies\n",
        "- [ ] Thought about operational deployment\n",
        "\n",
        "::: {.callout-success}\n",
        "## Congratulations! ðŸŽ‰\n",
        "\n",
        "You've completed a full deep learning pipeline for flood mapping using Sentinel-1 SAR and U-Net. This is a **production-ready workflow** used by disaster response agencies worldwide.\n",
        "\n",
        "**What You Built:**\n",
        "- A trained semantic segmentation model\n",
        "- Automated flood detection system\n",
        "- Export pipeline for GIS integration\n",
        "- Performance evaluation framework\n",
        "\n",
        "**Impact:**\n",
        "Your skills can now contribute to saving lives through rapid, accurate flood extent mapping for Philippine disaster response operations.\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "::: {.session-nav}\n",
        "[â† Back to Session 1](../sessions/session1.qmd){.btn .btn-outline-secondary}\n",
        "[Next: Session 3 - Object Detection â†’](../sessions/session3.qmd){.btn .btn-primary}\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "*This hands-on lab is part of the CoPhil 4-Day Advanced Training on AI/ML for Earth Observation, funded by the European Union under the Global Gateway initiative. Materials developed in collaboration with PhilSA, DOST-ASTI, and the European Space Agency.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
